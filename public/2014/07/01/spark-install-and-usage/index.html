<!DOCTYPE html>













<html class="theme-next mist" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






  
  
    
  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/Han/3.3.0/han.min.css">
















  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  

  
    
      
    

    
  

  
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Roboto Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext">
  






  

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.2/css/font-awesome.min.css">

<link rel="stylesheet" href="/css/main.css?v=7.1.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico?v=7.1.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico?v=7.1.2">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.2" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.1.2',
    sidebar: {"position":"left","display":"remove","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="本文主要记录 CDH5 集群中 Spark 集群模式的安装过程配置过程并测试 Spark 的一些基本使用方法。">
<meta name="keywords" content="yarn,spark,mesos">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark集群安装和使用">
<meta property="og:url" content="http://javachen.github.io/2014/07/01/spark-install-and-usage/index.html">
<meta property="og:site_name" content="JavaChen Blog">
<meta property="og:description" content="本文主要记录 CDH5 集群中 Spark 集群模式的安装过程配置过程并测试 Spark 的一些基本使用方法。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://javachen.github.io/images/spark-master-web-ui.jpg">
<meta property="og:image" content="http://javachen.github.io/images/spark-hs-web-ui.jpg">
<meta property="og:updated_time" content="2019-07-03T16:40:25.619Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark集群安装和使用">
<meta name="twitter:description" content="本文主要记录 CDH5 集群中 Spark 集群模式的安装过程配置过程并测试 Spark 的一些基本使用方法。">
<meta name="twitter:image" content="http://javachen.github.io/images/spark-master-web-ui.jpg">



  <link rel="alternate" href="/atom.xml" title="JavaChen Blog" type="application/atom+xml">



  
  
  <link rel="canonical" href="http://javachen.github.io/2014/07/01/spark-install-and-usage/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Spark集群安装和使用 | JavaChen Blog</title>
  






  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?50bc6f5d9b045b5895ff44f8bbdbc611";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>







  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">JavaChen Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <h1 class="site-subtitle" itemprop="description">Ramblings of a coder</h1>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://javachen.github.io/2014/07/01/spark-install-and-usage/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JavaChen">
      <meta itemprop="description" content="Rumblings by a coder on Java、Hadoop and so on">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JavaChen Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Spark集群安装和使用<a href="https://github.com/javachen/javachen-blog-theme/tree/master/source/_posts/2014/2014-07-01-spark-install-and-usage.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pencil"></i></a>

              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2014-07-01 00:00:00" itemprop="dateCreated datePublished" datetime="2014-07-01T00:00:00+08:00">2014-07-01</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-07-04 00:40:25" itemprop="dateModified" datetime="2019-07-04T00:40:25+08:00">2019-07-04</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/spark/" itemprop="url" rel="index"><span itemprop="name">spark</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <!--/删除
          
              <div class="post-description">
                  本文主要记录 CDH5 集群中 Spark 集群模式的安装过程配置过程并测试 Spark 的一些基本使用方法。
              </div>
          
          -->

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <p>本文主要记录 CDH5 集群中 Spark 集群模式的安装过程配置过程并测试 Spark 的一些基本使用方法。</p>
<p>安装环境如下：</p>
<ul>
<li>操作系统：CentOs 6.5</li>
<li>Hadoop 版本：<code>cdh-5.4.0</code></li>
<li>Spark 版本：<code>cdh5-1.3.0_5.4.0</code></li>
</ul>
<p>关于 yum 源的配置以及 Hadoop 集群的安装，请参考 <a href="/2013/04/06/install-cloudera-cdh-by-yum">使用yum安装CDH Hadoop集群</a>。</p>
<h1 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h1><p>首先查看 Spark 相关的包有哪些：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ yum list |grep spark</span><br><span class="line">spark-core.noarch                  1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6</span><br><span class="line">spark-history-server.noarch        1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6</span><br><span class="line">spark-master.noarch                1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6</span><br><span class="line">spark-python.noarch                1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6</span><br><span class="line">spark-worker.noarch                1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6</span><br><span class="line">hue-spark.x86_64                   3.7.0+cdh5.4.0+1145-1.cdh5.4.0.p0.58.el6</span><br></pre></td></tr></table></figure>
<p>以上包作用如下：</p>
<ul>
<li><code>spark-core</code>: spark 核心功能</li>
<li><code>spark-worker</code>: spark-worker 初始化脚本</li>
<li><code>spark-master</code>: spark-master 初始化脚本</li>
<li><code>spark-python</code>: spark 的 Python 客户端</li>
<li><code>hue-spark</code>: spark 和 hue 集成包</li>
<li><code>spark-history-server</code></li>
</ul>
<p>在已经存在的 Hadoop 集群中，选择一个节点来安装 Spark Master，其余节点安装 Spark worker ，例如：在 cdh1 上安装 master，在 cdh1、cdh2、cdh3 上安装 worker：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 cdh1 节点上运行</span></span><br><span class="line">$ sudo yum install spark-core spark-master spark-worker spark-python spark-history-server -y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 cdh1、cdh2、cdh3 上运行</span></span><br><span class="line">$ sudo yum install spark-core spark-worker spark-python -y</span><br></pre></td></tr></table></figure>
<p>安装成功后，我的集群各节点部署如下：</p>
<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">cdh1节点</span>:  spark-master、spark-worker、spark-history-server</span><br><span class="line"><span class="attribute">cdh2节点</span>:  spark-worker </span><br><span class="line"><span class="attribute">cdh3节点</span>:  spark-worker</span><br></pre></td></tr></table></figure>
<h1 id="2-配置"><a href="#2-配置" class="headerlink" title="2. 配置"></a>2. 配置</h1><h2 id="2-1-修改配置文件"><a href="#2-1-修改配置文件" class="headerlink" title="2.1 修改配置文件"></a>2.1 修改配置文件</h2><p>设置环境变量，在 <code>.bashrc</code> 或者 <code>/etc/profile</code> 中加入下面一行，并使其生效：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HOME=/usr/lib</span><br></pre></td></tr></table></figure>
<p>可以修改配置文件 <code>/etc/conf-env.sh</code>，其内容如下，你可以根据需要做一些修改，例如，修改 master 的主机名称为cdh1。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置 master 主机名称</span></span><br><span class="line"><span class="built_in">export</span> STANDALONE_SPARK_MASTER_HOST=cdh1</span><br></pre></td></tr></table></figure>
<p>设置 shuffle 和 RDD 数据存储路径，该值默认为<code>/tmp</code>。使用默认值，可能会出现<code>No space left on device</code>的异常，建议修改为空间较大的分区中的一个目录。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_LOCAL_DIRS=/data</span><br></pre></td></tr></table></figure>
<p>如果你和我一样使用的是虚拟机运行 spark，则你可能需要修改 spark 进程使用的 jvm 大小（关于 jvm 大小设置的相关逻辑见 <code>/usr/lib/bin-class</code>）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_DAEMON_MEMORY=256m</span><br></pre></td></tr></table></figure>
<p>更多spark相关的配置参数，请参考 <a href="https:/.apache.org/docs/latest/configuration.html" target="_blank" rel="noopener">Spark Configuration</a>。</p>
<h2 id="2-2-配置-Spark-History-Server"><a href="#2-2-配置-Spark-History-Server" class="headerlink" title="2.2 配置 Spark History Server"></a>2.2 配置 Spark History Server</h2><p> 在运行Spark应用程序的时候，driver会提供一个webUI给出应用程序的运行信息，但是该webUI随着应用程序的完成而关闭端口，也就是说，Spark应用程序运行完后，将无法查看应用程序的历史记录。Spark history server就是为了应对这种情况而产生的，通过配置，Spark应用程序在运行完应用程序之后，将应用程序的运行信息写入指定目录，而Spark history server可以将这些运行信息装载并以web的方式供用户浏览。</p>
<p>创建 <code>/etc/conf-defaults.conf</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/conf-defaults.conf.template /etc/conf-defaults.conf</span><br></pre></td></tr></table></figure>
<p>添加下面配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark.master=spark://cdh1:7077</span><br><span class="line">spark.eventLog.dir=/user/applicationHistory</span><br><span class="line">spark.eventLog.enabled=true</span><br><span class="line">spark.yarn.historyServer.address=cdh1:18082</span><br></pre></td></tr></table></figure>
<p>如果你是在hdfs上运行Spark，则执行下面命令创建<code>/user/applicationHistory</code>目录：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo -u hdfs hadoop fs -mkdir /user</span><br><span class="line">$ sudo -u hdfs hadoop fs -mkdir /user/applicationHistory</span><br><span class="line">$ sudo -u hdfs hadoop fs -chown -R spark:spark /user</span><br><span class="line">$ sudo -u hdfs hadoop fs -chmod 1777 /user/applicationHistory</span><br></pre></td></tr></table></figure>
<p>设置 <code>spark.history.fs.logDirectory</code> 参数：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_HISTORY_OPTS=<span class="string">"<span class="variable">$SPARK_HISTORY_OPTS</span> -Dspark.history.fs.logDirectory=/tmp -Dspark.history.ui.port=18082"</span></span><br></pre></td></tr></table></figure>
<p>创建 /tmp 目录：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir -p /tmp</span><br><span class="line">$ chown spark:spark /tmp</span><br></pre></td></tr></table></figure>
<p>如果集群配置了 kerberos ，则添加下面配置：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HOSTNAME=`hostname -f`</span><br><span class="line"><span class="built_in">export</span> SPARK_HISTORY_OPTS=<span class="string">"<span class="variable">$SPARK_HISTORY_OPTS</span> -Dspark.history.kerberos.enabled=true -Dspark.history.kerberos.principal=spark/<span class="variable">$&#123;HOSTNAME&#125;</span>@LASHOU.COM -Dspark.history.kerberos.keytab=/etc/conf.keytab -Dspark.history.ui.acls.enable=true"</span></span><br></pre></td></tr></table></figure>
<h2 id="2-3-和Hive集成"><a href="#2-3-和Hive集成" class="headerlink" title="2.3 和Hive集成"></a>2.3 和Hive集成</h2><p>Spark和hive集成，最好是将hive的配置文件链接到Spark的配置文件目录：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ln -s /etc/hive/conf/hive-site.xml /etc/conf/hive-site.xml</span><br></pre></td></tr></table></figure>
<h2 id="2-4-同步配置文件"><a href="#2-4-同步配置文件" class="headerlink" title="2.4 同步配置文件"></a>2.4 同步配置文件</h2><p>修改完 cdh1 节点上的配置文件之后，需要同步到其他节点：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /etc/conf  cdh2:/etc</span><br><span class="line">scp -r /etc/conf  cdh3:/etc</span><br></pre></td></tr></table></figure>
<h1 id="3-启动和停止"><a href="#3-启动和停止" class="headerlink" title="3. 启动和停止"></a>3. 启动和停止</h1><h2 id="3-1-使用系统服务管理集群"><a href="#3-1-使用系统服务管理集群" class="headerlink" title="3.1 使用系统服务管理集群"></a>3.1 使用系统服务管理集群</h2><p>启动脚本：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 cdh1 节点上运行</span></span><br><span class="line">$ sudo service spark-master start</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 cdh1 节点上运行，如果 hadoop 集群配置了 kerberos，则运行之前需要先获取 spark 用户的凭证</span></span><br><span class="line"><span class="comment"># kinit -k -t /etc/conf.keytab spark/cdh1@JAVACHEN.COM</span></span><br><span class="line">$ sudo service spark-history-server start</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在cdh2、cdh3 节点上运行</span></span><br><span class="line">$ sudo service spark-worker start</span><br></pre></td></tr></table></figure>
<p>停止脚本：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo service spark-master stop</span><br><span class="line">$ sudo service spark-worker stop</span><br><span class="line">$ sudo service spark-history-server stop</span><br></pre></td></tr></table></figure>
<p>当然，你还可以设置开机启动：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo chkconfig spark-master on</span><br><span class="line">$ sudo chkconfig spark-worker on</span><br><span class="line">$ sudo chkconfig spark-history-server on</span><br></pre></td></tr></table></figure>
<h2 id="3-2-使用-Spark-自带脚本管理集群"><a href="#3-2-使用-Spark-自带脚本管理集群" class="headerlink" title="3.2 使用 Spark 自带脚本管理集群"></a>3.2 使用 Spark 自带脚本管理集群</h2><p>另外，你也可以使用 Spark 自带的脚本来启动和停止，这些脚本在 <code>/usr/lib/sbin</code> 目录下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ls /usr/lib/sbin</span><br><span class="line">slaves.sh        spark-daemons.sh  start-master.sh  stop-all.sh</span><br><span class="line">spark-config.sh  spark-executor    start-slave.sh   stop-master.sh</span><br><span class="line">spark-daemon.sh  start-all.sh      start-slaves.sh  stop-slaves.sh</span><br></pre></td></tr></table></figure>
<p>在master节点修改 <code>/etc/conf/slaves</code> 文件添加worker节点的主机名称，并且还需要在master和worker节点之间配置无密码登陆。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># A Spark Worker will be started on each of the machines listed below.</span></span><br><span class="line">cdh2</span><br><span class="line">cdh3</span><br></pre></td></tr></table></figure>
<p>然后，你也可以通过下面脚本启动 master 和 worker：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /usr/lib/sbin</span><br><span class="line">$ ./start-master.sh</span><br><span class="line">$ ./start-slaves.sh</span><br></pre></td></tr></table></figure>
<p>当然，你也可以通过<code>spark-class</code>脚本来启动，例如，下面脚本以standalone模式启动worker：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin-class org.apache.spark.deploy.worker.Worker spark://cdh1:18080</span><br></pre></td></tr></table></figure>
<h2 id="3-3-访问web界面"><a href="#3-3-访问web界面" class="headerlink" title="3.3 访问web界面"></a>3.3 访问web界面</h2><p>你可以通过 <a href="http://cdh1:18080/" target="_blank" rel="noopener">http://cdh1:18080/</a> 访问 spark master 的 web 界面。</p>
<p><img src="/images/spark-master-web-ui.jpg" alt="spark-master-web-ui"></p>
<p>访问Spark History Server页面：<a href="http://cdh1:18082/。" target="_blank" rel="noopener">http://cdh1:18082/。</a></p>
<p><img src="/images/spark-hs-web-ui.jpg" alt="spark-hs-web-ui"></p>
<p>注意：我这里使用的是CDH版本的 Spark，Spark master UI的端口为<code>18080</code>，不是 Apache Spark 的 <code>8080</code> 端口。CDH发行版中Spark使用的端口列表如下：</p>
<ul>
<li><code>7077</code> – Default Master RPC port</li>
<li><code>7078</code> – Default Worker RPC port</li>
<li><code>18080</code> – Default Master web UI port</li>
<li><code>18081</code> – Default Worker web UI port</li>
<li><code>18080</code> – Default HistoryServer web UI port</li>
</ul>
<h1 id="4-测试"><a href="#4-测试" class="headerlink" title="4. 测试"></a>4. 测试</h1><p>Spark可以以<a href="/2015/03/30-test-in-local-mode.html">本地模式运行</a>，也支持三种集群管理模式：</p>
<ul>
<li><a href="https:/.apache.org/docs/latest-standalone.html" target="_blank" rel="noopener">Standalone</a>  – Spark原生的资源管理，由Master负责资源的分配。</li>
<li><a href="https:/.apache.org/docs/latest/running-on-mesos.html" target="_blank" rel="noopener">Apache Mesos</a>  – 运行在Mesos之上，由Mesos进行资源调度</li>
<li><a href="https:/.apache.org/docs/latest/running-on-yarn.html" target="_blank" rel="noopener">Hadoop YARN</a> –  运行在Yarn之上，由Yarn进行资源调度。</li>
</ul>
<p>另外 Spark 的 <a href="https:/.apache.org/docs/latest/ec2-scripts.html" target="_blank" rel="noopener">EC2 launch scripts</a> 可以帮助你容易地在Amazon EC2上启动standalone cluster.</p>
<blockquote>
<ul>
<li>在集群不是特别大，并且没有 mapReduce 和 Spark 同时运行的需求的情况下，用 Standalone 模式效率最高。</li>
<li>Spark可以在应用间（通过集群管理器）和应用中（如果一个 SparkContext 中有多项计算任务）进行资源调度。</li>
</ul>
</blockquote>
<h2 id="4-1-Standalone-模式"><a href="#4-1-Standalone-模式" class="headerlink" title="4.1 Standalone 模式"></a>4.1 Standalone 模式</h2><p>该模式中，资源调度是Spark框架自己实现的，其节点类型分为Master和Worker节点，其中Driver节点运行在Master节点中，并且有常驻内存的Master进程守护，Worker节点上常驻Worker守护进程，负责与Master通信。</p>
<p>Standalone 模式是Master-Slaves架构的集群模式，Master存在着单点故障问题，目前，Spark提供了两种解决办法：基于文件系统的故障恢复模式，基于Zookeeper的HA方式。</p>
<p>Standalone 模式需要在每一个节点部署Spark应用，并按照实际情况配置故障恢复模式。</p>
<p>你可以使用交互式命令spark-shell、pyspark或者<a href="https:/.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">spark-submit script</a>连接到集群，下面以wordcount程序为例：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ spark-shell --master spark://cdh1:7077</span><br><span class="line">scala&gt; val file = sc.textFile(<span class="string">"hdfs://cdh1:8020/tmp/test.txt"</span>)</span><br><span class="line">scala&gt; val counts = file.flatMap(line =&gt; line.split(<span class="string">" "</span>)).map(word =&gt; (word, 1)).reduceByKey(_ + _)</span><br><span class="line">scala&gt; counts.count()</span><br><span class="line">scala&gt; counts.saveAsTextFile(<span class="string">"hdfs://cdh1:8020/tmp/output"</span>)</span><br></pre></td></tr></table></figure>
<p>如果运行成功，可以打开浏览器访问 <a href="http://cdh1:4040" target="_blank" rel="noopener">http://cdh1:4040</a> 查看应用运行情况。</p>
<p>运行过程中，可能会出现下面的异常：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">14</span>/<span class="number">10</span>/<span class="number">24</span> <span class="number">14</span>:<span class="number">51</span>:<span class="number">40</span> WARN hdfs<span class="selector-class">.BlockReaderLocal</span>: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.</span><br><span class="line"><span class="number">14</span>/<span class="number">10</span>/<span class="number">24</span> <span class="number">14</span>:<span class="number">51</span>:<span class="number">40</span> ERROR lzo<span class="selector-class">.GPLNativeCodeLoader</span>: Could not load native gpl library</span><br><span class="line">java<span class="selector-class">.lang</span><span class="selector-class">.UnsatisfiedLinkError</span>: no gplcompression <span class="keyword">in</span> java<span class="selector-class">.library</span><span class="selector-class">.path</span></span><br><span class="line">	at java<span class="selector-class">.lang</span><span class="selector-class">.ClassLoader</span><span class="selector-class">.loadLibrary</span>(ClassLoader<span class="selector-class">.java</span>:<span class="number">1738</span>)</span><br><span class="line">	at java<span class="selector-class">.lang</span><span class="selector-class">.Runtime</span><span class="selector-class">.loadLibrary0</span>(Runtime<span class="selector-class">.java</span>:<span class="number">823</span>)</span><br><span class="line">	at java<span class="selector-class">.lang</span><span class="selector-class">.System</span><span class="selector-class">.loadLibrary</span>(System<span class="selector-class">.java</span>:<span class="number">1028</span>)</span><br><span class="line">	at com<span class="selector-class">.hadoop</span><span class="selector-class">.compression</span><span class="selector-class">.lzo</span><span class="selector-class">.GPLNativeCodeLoader</span>.&lt;clinit&gt;(GPLNativeCodeLoader<span class="selector-class">.java</span>:<span class="number">32</span>)</span><br><span class="line">	at com<span class="selector-class">.hadoop</span><span class="selector-class">.compression</span><span class="selector-class">.lzo</span><span class="selector-class">.LzoCodec</span>.&lt;clinit&gt;(LzoCodec<span class="selector-class">.java</span>:<span class="number">71</span>)</span><br><span class="line">	at java<span class="selector-class">.lang</span><span class="selector-class">.Class</span><span class="selector-class">.forName0</span>(Native Method)</span><br><span class="line">	at java<span class="selector-class">.lang</span><span class="selector-class">.Class</span><span class="selector-class">.forName</span>(Class<span class="selector-class">.java</span>:<span class="number">249</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.conf</span><span class="selector-class">.Configuration</span><span class="selector-class">.getClassByNameOrNull</span>(Configuration<span class="selector-class">.java</span>:<span class="number">1836</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.conf</span><span class="selector-class">.Configuration</span><span class="selector-class">.getClassByName</span>(Configuration<span class="selector-class">.java</span>:<span class="number">1801</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.io</span><span class="selector-class">.compress</span><span class="selector-class">.CompressionCodecFactory</span><span class="selector-class">.getCodecClasses</span>(CompressionCodecFactory<span class="selector-class">.java</span>:<span class="number">128</span>)</span><br></pre></td></tr></table></figure>
<p>解决方法可以参考 <a href="http://blog.csdn.net/pelick/article/details/11599391" target="_blank" rel="noopener">Spark连接Hadoop读取HDFS问题小结</a> 这篇文章，执行以下命令，然后重启服务即可：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp /usr/lib/hadoop/lib/native/libgplcompression.so <span class="variable">$JAVA_HOME</span>/jre/lib/amd64/</span><br><span class="line">cp /usr/lib/hadoop/lib/native/libhadoop.so <span class="variable">$JAVA_HOME</span>/jre/lib/amd64/</span><br><span class="line">cp /usr/lib/hadoop/lib/native/libsnappy.so <span class="variable">$JAVA_HOME</span>/jre/lib/amd64/</span><br></pre></td></tr></table></figure>
<p>使用 spark-submit 以 Standalone 模式运行 SparkPi 程序的命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ spark-submit --class org.apache.spark.examples.SparkPi  --master spark://cdh1:7077 /usr/lib/lib-examples-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar 10</span><br></pre></td></tr></table></figure>
<p><strong>需要说明的是</strong>：<code>Standalone mode does not support talking to a kerberized HDFS</code>，如果你以 <code>spark-shell --master spark://cdh1:7077</code> 方式访问安装有 kerberos 的 HDFS 集群上访问数据时，会出现下面异常:</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">15/04/02 11:58:32 <span class="builtin-name">INFO</span> TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, <span class="keyword">from</span> pool</span><br><span class="line">org.apache.spark.SparkException: Job aborted due <span class="keyword">to</span> stage failure: Task 0 <span class="keyword">in</span> stage 0.0 failed 4 times, most recent failure: Lost task 0.3 <span class="keyword">in</span> stage 0.0 (TID 6, bj03-bi-pro-hdpnamenn): java.io.IOException: Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException:<span class="built_in"> Client </span>cannot authenticate via:[TOKEN, KERBEROS]; Host Details : local host is: <span class="string">"cdh1/192.168.56.121"</span>; destination host is: <span class="string">"192.168.56.121"</span>:8020;</span><br><span class="line">        org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)</span><br><span class="line">        org.apache.hadoop.ipc.Client.call(Client.java:1415)</span><br><span class="line">        org.apache.hadoop.ipc.Client.call(Client.java:1364)</span><br><span class="line">        org.apache.hadoop.ipc.ProtobufRpcEngine<span class="variable">$Invoker</span>.invoke(ProtobufRpcEngine.java:206)</span><br><span class="line">        com.sun.proxy.<span class="variable">$Proxy17</span>.getBlockLocations(Unknown Source)</span><br></pre></td></tr></table></figure>
<h2 id="4-2-Spark-On-Mesos-模式"><a href="#4-2-Spark-On-Mesos-模式" class="headerlink" title="4.2 Spark On Mesos 模式"></a>4.2 Spark On Mesos 模式</h2><p>参考 <a href="http://dongxicheng.org/framework-on-yarn/apache-spark-comparing-three-deploying-ways/" target="_blank" rel="noopener">http://dongxicheng.org/framework-on-yarn/apache-spark-comparing-three-deploying-ways/</a>。</p>
<h2 id="4-3-Spark-on-Yarn-模式"><a href="#4-3-Spark-on-Yarn-模式" class="headerlink" title="4.3 Spark on Yarn 模式"></a>4.3 Spark on Yarn 模式</h2><p>Spark on Yarn 模式同样也支持两种在 Yarn 上启动 Spark 的方式，一种是 cluster 模式，Spark driver 在 Yarn 的 application master 进程中运行，客户端在应用初始化完成之后就会退出；一种是 client 模式，Spark driver 运行在客户端进程中。Spark on Yarn 模式是可以访问配置有 kerberos 的 HDFS 文件的。</p>
<p>CDH Spark中，以 cluster 模式启动，命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ spark-submit --class path.to.your.Class --deploy-mode cluster --master yarn [options] &lt;app jar&gt; [app options]</span><br></pre></td></tr></table></figure>
<p>CDH Spark中，以 client 模式启动，命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ spark-submit --class path.to.your.Class --deploy-mode client --master yarn [options] &lt;app jar&gt; [app options]</span><br></pre></td></tr></table></figure>
<p>以SparkPi程序为例：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --deploy-mode cluster  \</span><br><span class="line">    --master yarn  \</span><br><span class="line">    --num-executors 3 \</span><br><span class="line">    --driver-memory 4g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --queue thequeue \</span><br><span class="line">    /usr/lib/lib-examples-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar \</span><br><span class="line">    10</span><br></pre></td></tr></table></figure>
<p>另外，运行在 YARN 集群之上的时候，可以手动把 spark-assembly 相关的 jar 包拷贝到 hdfs 上去，然后设置 <code>SPARK_JAR</code> 环境变量：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hdfs dfs -mkdir -p /user/share/lib</span><br><span class="line">$ hdfs dfs -put <span class="variable">$SPARK_HOME</span>/lib-assembly.jar  /user/share/lib-assembly.jar</span><br><span class="line"></span><br><span class="line">$ SPARK_JAR=hdfs://&lt;nn&gt;:&lt;port&gt;/user/share/lib-assembly.jar</span><br></pre></td></tr></table></figure>
<h1 id="5-Spark-SQL"><a href="#5-Spark-SQL" class="headerlink" title="5. Spark-SQL"></a>5. Spark-SQL</h1><p>Spark 安装包中包括了 Spark-SQL ，运行 spark-sql 命令，在 cdh5.2 中会出现下面异常：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /usr/lib/bin</span><br><span class="line">$ .-sql</span><br><span class="line">java.lang.ClassNotFoundException: org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver</span><br><span class="line">	at java.net.URLClassLoader<span class="variable">$1</span>.run(URLClassLoader.java:202)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)</span><br><span class="line">	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)</span><br><span class="line">	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)</span><br><span class="line">	at java.lang.Class.forName0(Native Method)</span><br><span class="line">	at java.lang.Class.forName(Class.java:247)</span><br><span class="line">	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:319)</span><br><span class="line">	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)</span><br><span class="line">	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)</span><br><span class="line"></span><br><span class="line">Failed to load Spark SQL CLI main class org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.</span><br><span class="line">You need to build Spark with -Phive.</span><br></pre></td></tr></table></figure>
<p>在 cdh5.4 中会出现下面异常：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java<span class="selector-class">.lang</span><span class="selector-class">.ClassNotFoundException</span>: org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.hive</span><span class="selector-class">.cli</span><span class="selector-class">.CliDriver</span></span><br><span class="line">  at java<span class="selector-class">.net</span><span class="selector-class">.URLClassLoader</span>$<span class="number">1</span>.run(URLClassLoader<span class="selector-class">.java</span>:<span class="number">366</span>)</span><br><span class="line">  at java<span class="selector-class">.net</span><span class="selector-class">.URLClassLoader</span>$<span class="number">1</span>.run(URLClassLoader<span class="selector-class">.java</span>:<span class="number">355</span>)</span><br><span class="line">  at java<span class="selector-class">.security</span><span class="selector-class">.AccessController</span><span class="selector-class">.doPrivileged</span>(Native Method)</span><br><span class="line">  at java<span class="selector-class">.net</span><span class="selector-class">.URLClassLoader</span><span class="selector-class">.findClass</span>(URLClassLoader<span class="selector-class">.java</span>:<span class="number">354</span>)</span><br><span class="line">  at java<span class="selector-class">.lang</span><span class="selector-class">.ClassLoader</span><span class="selector-class">.loadClass</span>(ClassLoader<span class="selector-class">.java</span>:<span class="number">425</span>)</span><br><span class="line">  at sun<span class="selector-class">.misc</span><span class="selector-class">.Launcher</span><span class="variable">$AppClassLoader</span>.loadClass(Launcher<span class="selector-class">.java</span>:<span class="number">308</span>)</span><br><span class="line">  at java<span class="selector-class">.lang</span><span class="selector-class">.ClassLoader</span><span class="selector-class">.loadClass</span>(ClassLoader<span class="selector-class">.java</span>:<span class="number">358</span>)</span><br><span class="line">  ... <span class="number">18</span> more</span><br></pre></td></tr></table></figure>
<p>从上可以知道  Spark-SQL 编译时没有集成 Hive，故需要重新编译 spark 源代码。</p>
<h2 id="编译-Spark-SQL"><a href="#编译-Spark-SQL" class="headerlink" title="编译 Spark-SQL"></a>编译 Spark-SQL</h2><p>以下内容参考 <a href="/2015/04/28/compile-cdh-spark-source-code">编译Spark源代码</a>。</p>
<p>下载cdh5-1.3.0_5.4.0分支的代码：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> git@github.com:cloudera.git</span><br><span class="line">$ <span class="built_in">cd</span> spark</span><br><span class="line">$ git checkout -b origin/cdh5-1.3.0_5.4.0</span><br></pre></td></tr></table></figure>
<p>使用maven 编译，先修改根目录下的 pom.xml，添加一行 <code>&lt;module&gt;sql/hive-thriftserver&lt;/module&gt;</code>：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">modules</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>core<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>bagel<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>graphx<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>mllib<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>tools<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>streaming<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>sql/catalyst<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>sql/core<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>sql/hive<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>sql/hive-thriftserver<span class="tag">&lt;/<span class="name">module</span>&gt;</span> <span class="comment">&lt;!--添加的一行--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>repl<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>assembly<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>external/twitter<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>external/kafka<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>external/flume<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>external/flume-sink<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>external/zeromq<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>external/mqtt<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">module</span>&gt;</span>examples<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">modules</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>然后运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> MAVEN_OPTS=<span class="string">"-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"</span></span><br><span class="line">$ mvn -Pyarn -Dhadoop.version=2.6.0-cdh5.4.0 -Phive -Phive-thriftserver -DskipTests clean package</span><br></pre></td></tr></table></figure>
<p>如果编译成功之后， 会在 assembly/target/scala-2.10 目录下生成：spark-assembly-1.3.0-cdh5.4.0.jar，在 examples/target/scala-2.10 目录下生成：spark-examples-1.3.0-cdh5.4.0.jar，然后将 spark-assembly-1.3.0-cdh5.4.0.jar 拷贝到 /usr/lib/lib 目录，然后再来运行 spark-sql。</p>
<p>但是，经测试 cdh5.4.0 版本中的 spark 的 sql/hive-thriftserver 模块存在编译错误，最后无法编译成功，故需要等到 cloudera 官方更新源代码或者等待下一个 cdh 版本集成 spark-sql。</p>
<p>虽然 spark-sql 命令用不了，但是我们可以在 spark-shell 中使用 SQLContext 来运行 sql 语句，限于篇幅，这里不做介绍，你可以参考 <a href="http://www.infoobjects.com-sql-schemardd-programmatically-specifying-schema/" target="_blank" rel="noopener">http://www.infoobjects.com-sql-schemardd-programmatically-specifying-schema/</a>。</p>
<h1 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h1><p>本文主要介绍了 CDH5 集群中 Spark 的安装过程以及三种集群运行模式：</p>
<ul>
<li>Standalone – <code>spark-shell --master spark://host:port</code> </li>
<li>Apache Mesos – <code>spark-shell --master mesos://host:port</code></li>
<li>Hadoop YARN – <code>spark-shell --master yarn</code></li>
</ul>
<p>如果以本地模式运行，则为 <code>spark-shell --master local</code>。</p>
<p>关于 Spark 的更多介绍可以参考官网或者一些<a href="http://colobu.com/tags/Spark/" target="_blank" rel="noopener">中文翻译的文章</a>。</p>
<h1 id="7-参考文章"><a href="#7-参考文章" class="headerlink" title="7. 参考文章"></a>7. 参考文章</h1><ul>
<li><a href="https:/.apache.org/docs/latest-standalone.html" target="_blank" rel="noopener">Spark Standalone Mode</a></li>
<li><a href="http://blog.csdn.net/pelick/article/details/11599391" target="_blank" rel="noopener">Spark连接Hadoop读取HDFS问题小结</a> </li>
<li><a href="http://dongxicheng.org/framework-on-yarn/apache-spark-comparing-three-deploying-ways/" target="_blank" rel="noopener">Apache Spark探秘：三种分布式部署方式比较</a></li>
</ul>

      
    </div>

    

    
    
    


    
      <div>
        




  



<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>JavaChen</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    
    <a href="http://javachen.github.io/2014/07/01/spark-install-and-usage/" title="Spark集群安装和使用">Spark集群安装和使用</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-Hans" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li>
</ul>

      </div>
    

    

    
      
    
    
      <div>
        <div id="reward-container">
  <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">

    
      
      
        
      
      <div style="display: inline-block">
        <img src="/images/wechatpay.jpg" alt="JavaChen 微信支付">
        <p>微信支付</p>
      </div>
    
      
      
        
      
      <div style="display: inline-block">
        <img src="/images/alipay.jpg" alt="JavaChen 支付宝">
        <p>支付宝</p>
      </div>
    

  </div>
</div>

      </div>
    



    <footer class="post-footer">
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/yarn/" rel="tag"><i class="fa fa-tag"></i> yarn</a>
          
            <a href="/tags/spark/" rel="tag"><i class="fa fa-tag"></i> spark</a>
          
            <a href="/tags/mesos/" rel="tag"><i class="fa fa-tag"></i> mesos</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        

        

        
          
          <div class="social_share">
            
            
            
              <div>
                
  <div class="bdsharebuttonbox">
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
    <a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a>
    <a href="#" class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
    <a href="#" class="bds_tieba" data-cmd="tieba" title="分享到百度贴吧"></a>
    <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
    <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a class="bds_count" data-cmd="count"></a>
  </div>
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "2",
        "bdMiniList": false,
        "bdPic": ""
      },
      "share": {
        "bdSize": "16",
        "bdStyle": "0"
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

              </div>
            
          </div>
        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2014/06/26/some-tips-about-hbase/" rel="next" title="HBase中的一些注意事项">
                <i class="fa fa-chevron-left"></i> HBase中的一些注意事项
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2014/07/18/install-hdfs-ha-in-cdh/" rel="prev" title="CDH中配置HDFS HA">
                CDH中配置HDFS HA <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2009 – <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JavaChen</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>

  
  <script src="//cdnjs.cloudflare.com/ajax/libs/velocity/1.2.1/velocity.min.js"></script>

  
  <script src="//cdnjs.cloudflare.com/ajax/libs/velocity/1.2.1/velocity.ui.min.js"></script>


  


  <script src="/js/utils.js?v=7.1.2"></script>

  <script src="/js/motion.js?v=7.1.2"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.1.2"></script>




  
  <script src="/js/scrollspy.js?v=7.1.2"></script>
<script src="/js/post-details.js?v=7.1.2"></script>



  


  <script src="/js/next-boot.js?v=7.1.2"></script>


  

  

  

  


  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

  

  

  

  

  

  

  
<script>
  $('.highlight').not('.gist .highlight').each(function(i, e) {
    var $wrap = $('<div>').addClass('highlight-wrap');
    $(e).after($wrap);
    $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function(e) {
      var code = $(this).parent().find('.code').find('.line').map(function(i, e) {
        return $(e).text();
      }).toArray().join('\n');
      var ta = document.createElement('textarea');
      var yPosition = window.pageYOffset || document.documentElement.scrollTop;
      ta.style.top = yPosition + 'px'; // Prevent page scroll
      ta.style.position = 'absolute';
      ta.style.opacity = '0';
      ta.readOnly = true;
      ta.value = code;
      document.body.appendChild(ta);
      const selection = document.getSelection();
      const selected = selection.rangeCount > 0 ? selection.getRangeAt(0) : false;
      ta.select();
      ta.setSelectionRange(0, code.length);
      ta.readOnly = false;
      var result = document.execCommand('copy');
      
      ta.blur(); // For iOS
      $(this).blur();
      if (selected) {
        selection.removeAllRanges();
        selection.addRange(selected);
      }
    })).on('mouseleave', function(e) {
      var $b = $(this).find('.copy-btn');
      setTimeout(function() {
        $b.text('复制');
      }, 300);
    }).append(e);
  })
</script>


  

  

</body>
</html>
