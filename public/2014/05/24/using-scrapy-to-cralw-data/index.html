<!DOCTYPE html>













<html class="theme-next mist" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






  
  
    
  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/Han/3.3.0/han.min.css">
















  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  

  
    
      
    

    
  

  
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Roboto Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext">
  






  

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.2/css/font-awesome.min.css">

<link rel="stylesheet" href="/css/main.css?v=7.1.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico?v=7.1.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico?v=7.1.2">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.2" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.1.2',
    sidebar: {"position":"left","display":"remove","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Scrapy是Python开发的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。">
<meta name="keywords" content="scrapy,python">
<meta property="og:type" content="article">
<meta property="og:title" content="使用Scrapy抓取数据">
<meta property="og:url" content="http://javachen.github.io/2014/05/24/using-scrapy-to-cralw-data/index.html">
<meta property="og:site_name" content="JavaChen Blog">
<meta property="og:description" content="Scrapy是Python开发的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://blog.pluskid.org/wp-content/uploads/2009/08/scrapy_architecture.png">
<meta property="og:updated_time" content="2019-06-29T12:24:11.034Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="使用Scrapy抓取数据">
<meta name="twitter:description" content="Scrapy是Python开发的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。">
<meta name="twitter:image" content="http://blog.pluskid.org/wp-content/uploads/2009/08/scrapy_architecture.png">



  <link rel="alternate" href="/atom.xml" title="JavaChen Blog" type="application/atom+xml">



  
  
  <link rel="canonical" href="http://javachen.github.io/2014/05/24/using-scrapy-to-cralw-data/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>使用Scrapy抓取数据 | JavaChen Blog</title>
  






  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?50bc6f5d9b045b5895ff44f8bbdbc611";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>







  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">JavaChen Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <h1 class="site-subtitle" itemprop="description">Ramblings of a coder</h1>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://javachen.github.io/2014/05/24/using-scrapy-to-cralw-data/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JavaChen">
      <meta itemprop="description" content="Rumblings by a coder on Java、Hadoop and so on">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JavaChen Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">使用Scrapy抓取数据<a href="https://github.com/javachen/javachen-blog-theme/tree/master/source/_posts/2014/2014-05-24-using-scrapy-to-cralw-data.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pencil"></i></a>

              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2014-05-24 00:00:00" itemprop="dateCreated datePublished" datetime="2014-05-24T00:00:00+08:00">2014-05-24</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-29 20:24:11" itemprop="dateModified" datetime="2019-06-29T20:24:11+08:00">2019-06-29</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <!--/删除
          
              <div class="post-description">
                  Scrapy是Python开发的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。
              </div>
          
          -->

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <p>Scrapy是Python开发的一个快速,高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。</p>
<ul>
<li>官方主页： <a href="http://www.scrapy.org/" target="_blank" rel="noopener">http://www.scrapy.org/</a></li>
<li>中文文档：<a href="http://scrapy-chs.readthedocs.org/zh_CN/latest/index.html" target="_blank" rel="noopener">Scrapy 0.22 文档</a></li>
<li>GitHub项目主页：<a href="https://github.com/scrapy/scrapy" target="_blank" rel="noopener">https://github.com/scrapy/scrapy</a></li>
</ul>
<p>Scrapy 使用了 Twisted 异步网络库来处理网络通讯。整体架构大致如下（注：图片来自互联网）：</p>
<p><img src="http://blog.pluskid.org/wp-content/uploads/2009/08/scrapy_architecture.png" alt="scrapy"></p>
<p>Scrapy主要包括了以下组件：</p>
<ul>
<li>引擎，用来处理整个系统的数据流处理，触发事务。</li>
<li>调度器，用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回。</li>
<li>下载器，用于下载网页内容，并将网页内容返回给蜘蛛。</li>
<li>蜘蛛，蜘蛛是主要干活的，用它来制订特定域名或网页的解析规则。</li>
<li>项目管道，负责处理有蜘蛛从网页中抽取的项目，他的主要任务是清晰、验证和存储数据。当页面被蜘蛛解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</li>
<li>下载器中间件，位于Scrapy引擎和下载器之间的钩子框架，主要是处理Scrapy引擎与下载器之间的请求及响应。</li>
<li>蜘蛛中间件，介于Scrapy引擎和蜘蛛之间的钩子框架，主要工作是处理蜘蛛的响应输入和请求输出。</li>
<li>调度中间件，介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。</li>
</ul>
<p>使用Scrapy可以很方便的完成网上数据的采集工作，它为我们完成了大量的工作，而不需要自己费大力气去开发。</p>
<h1 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h1><h2 id="安装-python"><a href="#安装-python" class="headerlink" title="安装 python"></a>安装 python</h2><p>Scrapy 目前最新版本为0.22.2，该版本需要 python 2.7，故需要先安装 python 2.7。这里我使用 centos 服务器来做测试，因为系统自带了 python ，需要先检查 python 版本。</p>
<p>查看python版本：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ python -V</span><br><span class="line">Python 2.6.6</span><br></pre></td></tr></table></figure>

<p>升级版本到2.7：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ wget http://python.org/ftp/python/2.7.6/Python-2.7.6.tar.xz</span><br><span class="line">$ tar xf Python-2.7.6.tar.xz</span><br><span class="line">$ <span class="built_in">cd</span> Python-2.7.6</span><br><span class="line">$ ./configure --prefix=/usr/<span class="built_in">local</span> --<span class="built_in">enable</span>-unicode=ucs4 --<span class="built_in">enable</span>-shared LDFLAGS=<span class="string">"-Wl,-rpath /usr/local/lib"</span></span><br><span class="line">$ make &amp;&amp; make altinstall</span><br></pre></td></tr></table></figure>

<p>建立软连接，使系统默认的 python指向 python2.7</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ mv /usr/bin/python /usr/bin/python2.6.6 </span><br><span class="line">$ ln -s /usr/<span class="built_in">local</span>/bin/python2.7 /usr/bin/python</span><br></pre></td></tr></table></figure>

<p>再次查看python版本：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ python -V</span><br><span class="line">Python 2.7.6</span><br></pre></td></tr></table></figure>

<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>这里使用 wget 的方式来安装 <a href="http://pypi.python.org/pypi/setuptools" target="_blank" rel="noopener">setuptools</a> :</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://bootstrap.pypa.io/ez_setup.py -O - | python</span><br></pre></td></tr></table></figure>

<h2 id="安装-zope-interface"><a href="#安装-zope-interface" class="headerlink" title="安装 zope.interface"></a>安装 zope.interface</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ easy_install zope.interface</span><br></pre></td></tr></table></figure>

<h2 id="安装-twisted"><a href="#安装-twisted" class="headerlink" title="安装 twisted"></a>安装 twisted</h2><p>Scrapy 使用了 Twisted 异步网络库来处理网络通讯，故需要安装 twisted。</p>
<p>安装 twisted 前，需要先安装 gcc：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install gcc -y</span><br></pre></td></tr></table></figure>

<p>然后，再通过 easy_install 安装 twisted：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ easy_install twisted</span><br></pre></td></tr></table></figure>

<p>如果出现下面错误：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ easy_install twisted</span><br><span class="line">Searching <span class="keyword">for</span> twisted</span><br><span class="line">Reading https://pypi.python.org/simple/twisted/</span><br><span class="line">Best match: Twisted 14.0.0</span><br><span class="line">Downloading https://pypi.python.org/packages/<span class="built_in">source</span>/T/Twisted/Twisted-14.0.0.tar.bz2<span class="comment">#md5=9625c094e0a18da77faa4627b98c9815</span></span><br><span class="line">Processing Twisted-14.0.0.tar.bz2</span><br><span class="line">Writing /tmp/easy_install-kYHKjn/Twisted-14.0.0/setup.cfg</span><br><span class="line">Running Twisted-14.0.0/setup.py -q bdist_egg --dist-dir /tmp/easy_install-kYHKjn/Twisted-14.0.0/egg-dist-tmp-vu1n6Y</span><br><span class="line">twisted/runner/portmap.c:10:20: error: Python.h: No such file or directory</span><br><span class="line">twisted/runner/portmap.c:14: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘*’ token</span><br><span class="line">twisted/runner/portmap.c:31: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘*’ token</span><br><span class="line">twisted/runner/portmap.c:45: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘PortmapMethods’</span><br><span class="line">twisted/runner/portmap.c: In <span class="keyword">function</span> ‘initportmap’:</span><br><span class="line">twisted/runner/portmap.c:55: warning: implicit declaration of <span class="keyword">function</span> ‘Py_InitModule’</span><br><span class="line">twisted/runner/portmap.c:55: error: ‘PortmapMethods’ undeclared (first use <span class="keyword">in</span> this <span class="keyword">function</span>)</span><br><span class="line">twisted/runner/portmap.c:55: error: (Each undeclared identifier is reported only once</span><br><span class="line">twisted/runner/portmap.c:55: error: <span class="keyword">for</span> each <span class="keyword">function</span> it appears <span class="keyword">in</span>.)</span><br></pre></td></tr></table></figure>

<p>请安装 python-devel 然后再次运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ yum install python-devel -y</span><br><span class="line">$ easy_install twisted</span><br></pre></td></tr></table></figure>

<p>如果出现下面异常：</p>
<figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">error: </span>Not a recognized archive type: /tmp/easy_install-tVwC5O/Twisted<span class="string">-14</span>.0.0.tar.bz2</span><br></pre></td></tr></table></figure>

<p>请手动下载然后安装，下载地址在<a href="http://pypi.python.org/pypi/Twisted" target="_blank" rel="noopener">这里</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://pypi.python.org/packages/<span class="built_in">source</span>/T/Twisted/Twisted-14.0.0.tar.bz2<span class="comment">#md5=9625c094e0a18da77faa4627b98c9815</span></span><br><span class="line">$ tar -vxjf Twisted-14.0.0.tar.bz2</span><br><span class="line">$ <span class="built_in">cd</span> Twisted-14.0.0</span><br><span class="line">$ python setup.py install</span><br></pre></td></tr></table></figure>

<h2 id="安装-pyOpenSSL"><a href="#安装-pyOpenSSL" class="headerlink" title="安装 pyOpenSSL"></a>安装 pyOpenSSL</h2><p>先安装一些依赖：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install libffi libffi-devel openssl-devel -y</span><br></pre></td></tr></table></figure>

<p>然后，再通过 easy_install 安装 pyOpenSSL：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ easy_install pyOpenSSL</span><br></pre></td></tr></table></figure>

<h2 id="安装-Scrapy"><a href="#安装-Scrapy" class="headerlink" title="安装 Scrapy"></a>安装 Scrapy</h2><p>先安装一些依赖：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install libxml2 libxslt libxslt-devel -y</span><br></pre></td></tr></table></figure>

<p>最后再来安装 Scrapy ：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ easy_install scrapy</span><br></pre></td></tr></table></figure>

<h1 id="2-使用-Scrapy"><a href="#2-使用-Scrapy" class="headerlink" title="2. 使用 Scrapy"></a>2. 使用 Scrapy</h1><p>在安装成功之后，你可以了解一些 Scrapy 的基本概念和使用方法，并学习 Scrapy 项目的例子 dirbot 。</p>
<p>Dirbot 项目位于 <a href="https://github.com/scrapy/dirbot" target="_blank" rel="noopener">https://github.com/scrapy/dirbot</a>，该项目包含一个 README 文件，它详细描述了项目的内容。如果你熟悉 git，你可以 checkout 它的源代码。或者你可以通过点击 Downloads 下载 tarball 或 zip 格式的文件。</p>
<p>下面以该例子来描述如何使用 Scrapy 创建一个爬虫项目。</p>
<h2 id="新建工程"><a href="#新建工程" class="headerlink" title="新建工程"></a>新建工程</h2><p>在抓取之前，你需要新建一个 Scrapy 工程。进入一个你想用来保存代码的目录，然后执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy startproject tutorial</span><br></pre></td></tr></table></figure>

<p>这个命令会在当前目录下创建一个新目录 tutorial，它的结构如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── scrapy.cfg</span><br><span class="line">└── tutorial</span><br><span class="line">    ├── __init__.py</span><br><span class="line">    ├── items.py</span><br><span class="line">    ├── pipelines.py</span><br><span class="line">    ├── settings.py</span><br><span class="line">    └── spiders</span><br><span class="line">        └── __init__.py</span><br></pre></td></tr></table></figure>

<p>这些文件主要是：</p>
<ul>
<li>scrapy.cfg: 项目配置文件</li>
<li>tutorial/: 项目python模块, 呆会代码将从这里导入</li>
<li>tutorial/items.py: 项目items文件</li>
<li>tutorial/pipelines.py: 项目管道文件</li>
<li>tutorial/settings.py: 项目配置文件</li>
<li>tutorial/spiders: 放置spider的目录</li>
</ul>
<h2 id="定义Item"><a href="#定义Item" class="headerlink" title="定义Item"></a>定义Item</h2><p>Items是将要装载抓取的数据的容器，它工作方式像 python 里面的字典，但它提供更多的保护，比如对未定义的字段填充以防止拼写错误。</p>
<p>它通过创建一个 <code>scrapy.item.Item</code> 类来声明，定义它的属性为 <code>scrpy.item.Field</code> 对象，就像是一个对象关系映射(ORM).<br>我们通过将需要的item模型化，来控制从 dmoz.org 获得的站点数据，比如我们要获得站点的名字，url 和网站描述，我们定义这三种属性的域。要做到这点，我们编辑在 tutorial 目录下的 items.py 文件，我们的 Item 类将会是这样</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.item <span class="keyword">import</span> Item, Field </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozItem</span><span class="params">(Item)</span>:</span></span><br><span class="line">    title = Field()</span><br><span class="line">    link = Field()</span><br><span class="line">    desc = Field()</span><br></pre></td></tr></table></figure>

<p>刚开始看起来可能会有些困惑，但是定义这些 item 能让你用其他 Scrapy 组件的时候知道你的 items 到底是什么。</p>
<h2 id="编写爬虫-Spider"><a href="#编写爬虫-Spider" class="headerlink" title="编写爬虫(Spider)"></a>编写爬虫(Spider)</h2><p>Spider 是用户编写的类，用于从一个域（或域组）中抓取信息。们定义了用于下载的URL的初步列表，如何跟踪链接，以及如何来解析这些网页的内容用于提取items。</p>
<p>要建立一个 Spider，你可以为 <code>scrapy.spider.BaseSpider</code> 创建一个子类，并确定三个主要的、强制的属性：</p>
<ul>
<li><code>name</code>：爬虫的识别名，它必须是唯一的，在不同的爬虫中你必须定义不同的名字.</li>
<li><code>start_urls</code>：爬虫开始爬的一个 URL 列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些 URLS 开始。其他子 URL 将会从这些起始 URL 中继承性生成。</li>
<li><code>parse()</code>：爬虫的方法，调用时候传入从每一个 URL 传回的 Response 对象作为参数，response 将会是 parse 方法的唯一的一个参数,</li>
</ul>
<p>这个方法负责解析返回的数据、匹配抓取的数据(解析为 item )并跟踪更多的 URL。</p>
<p>在 tutorial/spiders 目录下创建 DmozSpider.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.spider <span class="keyword">import</span> BaseSpider</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozSpider</span><span class="params">(BaseSpider)</span>:</span></span><br><span class="line">    name = <span class="string">"dmoz"</span></span><br><span class="line">    allowed_domains = [<span class="string">"dmoz.org"</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"</span>,</span><br><span class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        filename = response.url.split(<span class="string">"/"</span>)[<span class="number">-2</span>]</span><br><span class="line">        open(filename, <span class="string">'wb'</span>).write(response.body)</span><br></pre></td></tr></table></figure>

<h2 id="运行项目"><a href="#运行项目" class="headerlink" title="运行项目"></a>运行项目</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy crawl dmoz</span><br></pre></td></tr></table></figure>

<p>该命令从 dmoz.org 域启动爬虫，第三个参数为 DmozSpider.py 中的 name 属性值。</p>
<h2 id="xpath选择器"><a href="#xpath选择器" class="headerlink" title="xpath选择器"></a>xpath选择器</h2><p>Scrapy 使用一种叫做 XPath selectors 的机制，它基于 XPath 表达式。如果你想了解更多selectors和其他机制你可以查阅<a href="http://doc.scrapy.org/topics/selectors.html#topics-selectors" target="_blank" rel="noopener">资料</a>。</p>
<p>这是一些XPath表达式的例子和他们的含义：</p>
<ul>
<li><code>/html/head/title</code>: 选择HTML文档 <code>&lt;head&gt;</code> 元素下面的 <code>&lt;title&gt;</code> 标签。</li>
<li><code>/html/head/title/text()</code>: 选择前面提到的<code>&lt;title&gt;</code> 元素下面的文本内容</li>
<li><code>//td</code>: 选择所有 <code>&lt;td&gt;</code> 元素</li>
<li><code>//div[@class=&quot;mine&quot;]</code>: 选择所有包含 <code>class=&quot;mine&quot;</code> 属性的div 标签元素</li>
</ul>
<p>这只是几个使用 XPath 的简单例子，但是实际上 XPath 非常强大。如果你想了解更多 XPATH 的内容，我们向你推荐这个 <a href="http://www.w3schools.com/XPath/default.asp" target="_blank" rel="noopener">XPath 教程</a></p>
<p>为了方便使用 XPaths，Scrapy 提供 Selector 类， 有三种方法</p>
<ul>
<li><code>xpath()</code>：返回selectors列表, 每一个select表示一个xpath参数表达式选择的节点.</li>
<li><code>extract()</code>：返回一个unicode字符串，该字符串为XPath选择器返回的数据</li>
<li><code>re()</code>： 返回unicode字符串列表，字符串作为参数由正则表达式提取出来</li>
<li><code>css()</code></li>
</ul>
<h2 id="提取数据"><a href="#提取数据" class="headerlink" title="提取数据"></a>提取数据</h2><p>我们可以通过如下命令选择每个在网站中的 <code>&lt;li&gt;</code> 元素:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sel.xpath(<span class="string">'//ul/li'</span>)</span><br></pre></td></tr></table></figure>

<p>然后是网站描述:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sel.xpath(<span class="string">'//ul/li/text()'</span>).extract()</span><br></pre></td></tr></table></figure>

<p>网站标题:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sel.xpath(<span class="string">'//ul/li/a/text()'</span>).extract()</span><br></pre></td></tr></table></figure>

<p>网站链接:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sel.xpath(<span class="string">'//ul/li/a/@href'</span>).extract()</span><br></pre></td></tr></table></figure>

<p>如前所述，每个 <code>xpath()</code> 调用返回一个 selectors 列表，所以我们可以结合 <code>xpath()</code> 去挖掘更深的节点。我们将会用到这些特性，所以:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sites = sel.xpath(<span class="string">'//ul/li'</span>)</span><br><span class="line"><span class="keyword">for</span> site <span class="keyword">in</span> sites:</span><br><span class="line">    title = site.xpath(<span class="string">'a/text()'</span>).extract()</span><br><span class="line">    link = site.xpath(<span class="string">'a/@href'</span>).extract()</span><br><span class="line">    desc = site.xpath(<span class="string">'text()'</span>).extract()</span><br><span class="line">    <span class="keyword">print</span> title, link, desc</span><br></pre></td></tr></table></figure>

<h2 id="使用Item"><a href="#使用Item" class="headerlink" title="使用Item"></a>使用Item</h2><p><code>scrapy.item.Item</code> 的调用接口类似于 python 的 dict ，Item 包含多个 <code>scrapy.item.Field</code>。这跟 django 的 Model 与  </p>
<p>Item 通常是在 Spider 的 parse 方法里使用，它用来保存解析到的数据。</p>
<p>最后修改爬虫类，使用 Item 来保存数据，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.spider <span class="keyword">import</span> Spider</span><br><span class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dirbot.items <span class="keyword">import</span> Website</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozSpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"dmoz"</span></span><br><span class="line">    allowed_domains = [<span class="string">"dmoz.org"</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"</span>,</span><br><span class="line">        <span class="string">"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        The lines below is a spider contract. For more info see:</span></span><br><span class="line"><span class="string">        http://doc.scrapy.org/en/latest/topics/contracts.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @url http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/</span></span><br><span class="line"><span class="string">        @scrapes name</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        sel = Selector(response)</span><br><span class="line">        sites = sel.xpath(<span class="string">'//ul[@class="directory-url"]/li'</span>)</span><br><span class="line">        items = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> site <span class="keyword">in</span> sites:</span><br><span class="line">            item = Website()</span><br><span class="line">            item[<span class="string">'name'</span>] = site.xpath(<span class="string">'a/text()'</span>).extract()</span><br><span class="line">            item[<span class="string">'url'</span>] = site.xpath(<span class="string">'a/@href'</span>).extract()</span><br><span class="line">            item[<span class="string">'description'</span>] = site.xpath(<span class="string">'text()'</span>).re(<span class="string">'-\s([^\n]*?)\\n'</span>)</span><br><span class="line">            items.append(item)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> items</span><br></pre></td></tr></table></figure>

<p>现在，可以再次运行该项目查看运行结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy crawl dmoz</span><br></pre></td></tr></table></figure>

<h2 id="使用Item-Pipeline"><a href="#使用Item-Pipeline" class="headerlink" title="使用Item Pipeline"></a>使用Item Pipeline</h2><p>在 settings.py 中设置 <code>ITEM_PIPELINES</code>，其默认为<code>[]</code>，与 django 的 <code>MIDDLEWARE_CLASSES</code> 等相似。<br>从 Spider 的 parse 返回的 Item 数据将依次被 <code>ITEM_PIPELINES</code> 列表中的 Pipeline 类处理。</p>
<p>一个 Item Pipeline 类必须实现以下方法：</p>
<ul>
<li><code>process_item(item, spider)</code> 为每个 item pipeline 组件调用，并且需要返回一个 <code>scrapy.item.Item</code> 实例对象或者抛出一个 <code>scrapy.exceptions.DropItem</code> 异常。当抛出异常后该 item 将不会被之后的 pipeline 处理。参数:<ul>
<li><code>item (Item object)</code> – 由 parse 方法返回的 Item 对象</li>
<li><code>spider (BaseSpider object)</code> – 抓取到这个 Item 对象对应的爬虫对象</li>
</ul>
</li>
</ul>
<p>也可额外的实现以下两个方法：</p>
<ul>
<li><code>open_spider(spider)</code> 当爬虫打开之后被调用。参数: <code>spider (BaseSpider object)</code> – 已经运行的爬虫</li>
<li><code>close_spider(spider)</code> 当爬虫关闭之后被调用。参数: <code>spider (BaseSpider object)</code> – 已经关闭的爬虫</li>
</ul>
<h2 id="保存抓取的数据"><a href="#保存抓取的数据" class="headerlink" title="保存抓取的数据"></a>保存抓取的数据</h2><p>保存信息的最简单的方法是通过 <a href="http://doc.scrapy.org/en/0.22/topics/feed-exports.html#topics-feed-exports" target="_blank" rel="noopener">Feed exports</a>，命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy crawl dmoz -o items.json -t json</span><br></pre></td></tr></table></figure>

<p>除了 json 格式之外，还支持 JSON lines、CSV、XML格式，你也可以通过接口扩展一些格式。</p>
<p>对于小项目用这种方法也足够了。如果是比较复杂的数据的话可能就需要编写一个 Item Pipeline 进行处理了。</p>
<p>所有抓取的 items 将以 JSON 格式被保存在新生成的 items.json 文件中</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>上面描述了如何创建一个爬虫项目的过程，你可以参照上面过程联系一遍。作为学习的例子，你还可以参考这篇文章：<a href="http://wsky.org/archives/191.html" target="_blank" rel="noopener">scrapy 中文教程（爬cnbeta实例）</a> 。</p>
<p>这篇文章中的爬虫类代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.contrib.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.linkextractors.sgml <span class="keyword">import</span> SgmlLinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> cnbeta.items <span class="keyword">import</span> CnbetaItem</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBSpider</span><span class="params">(CrawlSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'cnbeta'</span></span><br><span class="line">    allowed_domains = [<span class="string">'cnbeta.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.cnbeta.com'</span>]</span><br><span class="line"> </span><br><span class="line">    rules = (</span><br><span class="line">        Rule(SgmlLinkExtractor(allow=(<span class="string">'/articles/.*\.htm'</span>, )),</span><br><span class="line">             callback=<span class="string">'parse_page'</span>, follow=<span class="literal">True</span>),</span><br><span class="line">    )</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_page</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        item = CnbetaItem()</span><br><span class="line">        sel = Selector(response)</span><br><span class="line">        item[<span class="string">'title'</span>] = sel.xpath(<span class="string">'//title/text()'</span>).extract()</span><br><span class="line">        item[<span class="string">'url'</span>] = response.url</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<p>需要说明的是：</p>
<ul>
<li>该爬虫类继承的是 <code>CrawlSpider</code> 类，并且定义规则，rules指定了含有 <code>/articles/.*\.htm</code> 的链接都会被匹配。</li>
<li>该类并没有实现parse方法，并且规则中定义了回调函数 <code>parse_page</code>，你可以参考更多资料了解 CrawlSpider 的用法</li>
</ul>
<h1 id="3-学习资料"><a href="#3-学习资料" class="headerlink" title="3. 学习资料"></a>3. 学习资料</h1><p>接触 Scrapy，是因为想爬取一些知乎的数据，最开始的时候搜索了一些相关的资料和别人的实现方式。</p>
<p>Github 上已经有人或多或少的实现了对知乎数据的爬取，我搜索到的有以下几个仓库：</p>
<ul>
<li><a href="https://github.com/KeithYue/Zhihu_Spider" target="_blank" rel="noopener">https://github.com/KeithYue/Zhihu_Spider</a> 实现先通过用户名和密码登陆再爬取数据，代码见 <a href="https://github.com/KeithYue/Zhihu_Spider/blob/master/zhihu/zhihu/spiders/zhihu_spider.py" target="_blank" rel="noopener">zhihu_spider.py</a>。</li>
<li><a href="https://github.com/immzz/zhihu-scrapy" target="_blank" rel="noopener">https://github.com/immzz/zhihu-scrapy</a> 使用 selenium 下载和执行 javascript 代码。</li>
<li><a href="https://github.com/tangerinewhite32/zhihu-stat-py" target="_blank" rel="noopener">https://github.com/tangerinewhite32/zhihu-stat-py</a></li>
<li><a href="https://github.com/Zcc/zhihu" target="_blank" rel="noopener">https://github.com/Zcc/zhihu</a> 主要是爬指定话题的topanswers，还有用户个人资料，添加了登录代码。</li>
<li><a href="https://github.com/pelick/VerticleSearchEngine" target="_blank" rel="noopener">https://github.com/pelick/VerticleSearchEngine</a> 基于爬取的学术资源，提供搜索、推荐、可视化、分享四块。使用了 Scrapy、MongoDB、Apache Lucene/Solr、Apache Tika等技术。</li>
<li><a href="https://github.com/geekan/scrapy-examples" target="_blank" rel="noopener">https://github.com/geekan/scrapy-examples</a> scrapy的一些例子，包括获取豆瓣数据、linkedin、腾讯招聘数据等例子。</li>
<li><a href="https://github.com/owengbs/deeplearning" target="_blank" rel="noopener">https://github.com/owengbs/deeplearning</a> 实现分页获取话题。</li>
<li><a href="https://github.com/gnemoug/distribute_crawler" target="_blank" rel="noopener">https://github.com/gnemoug/distribute_crawler</a> 使用scrapy、redis、mongodb、graphite实现的一个分布式网络爬虫,底层存储mongodb集群,分布式使用redis实现,爬虫状态显示使用graphite实现</li>
<li><a href="https://github.com/weizetao/spider-roach" target="_blank" rel="noopener">https://github.com/weizetao/spider-roach</a> 一个分布式定向抓取集群的简单实现。</li>
<li><a href="https://github.com/scrapinghub/portia" target="_blank" rel="noopener">https://github.com/scrapinghub/portia</a> 这是一个可视化爬虫，基于Scrapy。它提供了可视化操作的Web页面，你只需点击页面上你要抽取的数据就行</li>
<li><a href="https://github.com/binux/pyspider" target="_blank" rel="noopener">https://github.com/binux/pyspider</a> 你如果不喜欢 Scrapy，可以试试 pyspider ，他让你在 WEB 界面编写调试脚本，监控执行状态，查看历史和结果 ，你可以在线试下 demo：<a href="http://demo.pyspider.org/" target="_blank" rel="noopener">Dashboard - pyspider</a></li>
</ul>
<p>其他资料：</p>
<ul>
<li><a href="http://www.52ml.net/tags/Scrapy" target="_blank" rel="noopener">http://www.52ml.net/tags/Scrapy</a> 收集了很多关于 Scrapy 的文章，<strong>推荐阅读</strong></li>
<li><a href="http://zihaolucky.github.io/using-python-to-build-zhihu-cralwer/" target="_blank" rel="noopener">用Python Requests抓取知乎用户信息</a></li>
<li><a href="http://www.it165.net/pro/html/images05/13112.html" target="_blank" rel="noopener">使用scrapy框架爬取自己的博文</a></li>
<li><a href="http://github.windwild.net/images/03/scrapy002/" target="_blank" rel="noopener">Scrapy 深入一点点</a></li>
<li><a href="http://www.kankanews.com/ICkengine/archives/94817.shtml" target="_blank" rel="noopener">使用python，scrapy写（定制）爬虫的经验，资料，杂。</a></li>
<li><a href="http://blog.pluskid.org/?p=366&cpage=1" target="_blank" rel="noopener">Scrapy 轻松定制网络爬虫</a></li>
<li><a href="http://my.oschina.net/chengye/blog/124162" target="_blank" rel="noopener">在scrapy中怎么让Spider自动去抓取豆瓣小组页面</a></li>
</ul>
<p>scrapy 和 javascript 交互例子：</p>
<ul>
<li><a href="http://www.xuebuyuan.com/2017949.html" target="_blank" rel="noopener">用scrapy框架爬取js交互式表格数据</a></li>
<li><a href="http://wsky.org/archives/211.html" target="_blank" rel="noopener">scrapy + selenium 解析javascript 实例</a></li>
</ul>
<p>还有一些待整理的知识点：</p>
<ul>
<li><em>如何先登陆再爬数据</em></li>
<li><em>如何使用规则做过滤</em></li>
<li><em>如何递归爬取数据</em></li>
<li><em>scrapy的参数设置和优化</em></li>
<li><em>如何实现分布式爬取</em></li>
</ul>
<h1 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h1><p>以上就是最近几天学习 Scrapy 的一个笔记和知识整理，参考了一些网上的文章才写成此文，对此表示感谢，也希望这篇文章能够对你有所帮助。如果你有什么想法，欢迎留言；如果喜欢此文，请帮忙分享，谢谢!</p>

      
    </div>

    

    
    
    


    
      <div>
        




  



<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>JavaChen</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    
    <a href="http://javachen.github.io/2014/05/24/using-scrapy-to-cralw-data/" title="使用Scrapy抓取数据">使用Scrapy抓取数据</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-Hans" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li>
</ul>

      </div>
    

    

    
      
    
    
      <div>
        <div id="reward-container">
  <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">

    
      
      
        
      
      <div style="display: inline-block">
        <img src="/images/wechatpay.jpg" alt="JavaChen 微信支付">
        <p>微信支付</p>
      </div>
    
      
      
        
      
      <div style="display: inline-block">
        <img src="/images/alipay.jpg" alt="JavaChen 支付宝">
        <p>支付宝</p>
      </div>
    

  </div>
</div>

      </div>
    



    <footer class="post-footer">
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/scrapy/" rel="tag"><i class="fa fa-tag"></i> scrapy</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        

        

        
          
          <div class="social_share">
            
            
            
              <div>
                
  <div class="bdsharebuttonbox">
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
    <a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a>
    <a href="#" class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
    <a href="#" class="bds_tieba" data-cmd="tieba" title="分享到百度贴吧"></a>
    <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
    <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a class="bds_count" data-cmd="count"></a>
  </div>
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "2",
        "bdMiniList": false,
        "bdPic": ""
      },
      "share": {
        "bdSize": "16",
        "bdStyle": "0"
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

              </div>
            
          </div>
        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2014/05/20/nutch-intro/" rel="next" title="Nutch介绍及使用">
                <i class="fa fa-chevron-left"></i> Nutch介绍及使用
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2014/05/30/note-about-brewers-cap-theorem/" rel="prev" title="关于CAP理论的一些笔记">
                关于CAP理论的一些笔记 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2009 – <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JavaChen</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>

  
  <script src="//cdnjs.cloudflare.com/ajax/libs/velocity/1.2.1/velocity.min.js"></script>

  
  <script src="//cdnjs.cloudflare.com/ajax/libs/velocity/1.2.1/velocity.ui.min.js"></script>


  


  <script src="/js/utils.js?v=7.1.2"></script>

  <script src="/js/motion.js?v=7.1.2"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.1.2"></script>




  
  <script src="/js/scrollspy.js?v=7.1.2"></script>
<script src="/js/post-details.js?v=7.1.2"></script>



  


  <script src="/js/next-boot.js?v=7.1.2"></script>


  

  

  

  


  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

  

  

  

  

  

  

  
<script>
  $('.highlight').not('.gist .highlight').each(function(i, e) {
    var $wrap = $('<div>').addClass('highlight-wrap');
    $(e).after($wrap);
    $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function(e) {
      var code = $(this).parent().find('.code').find('.line').map(function(i, e) {
        return $(e).text();
      }).toArray().join('\n');
      var ta = document.createElement('textarea');
      var yPosition = window.pageYOffset || document.documentElement.scrollTop;
      ta.style.top = yPosition + 'px'; // Prevent page scroll
      ta.style.position = 'absolute';
      ta.style.opacity = '0';
      ta.readOnly = true;
      ta.value = code;
      document.body.appendChild(ta);
      const selection = document.getSelection();
      const selected = selection.rangeCount > 0 ? selection.getRangeAt(0) : false;
      ta.select();
      ta.setSelectionRange(0, code.length);
      ta.readOnly = false;
      var result = document.execCommand('copy');
      
      ta.blur(); // For iOS
      $(this).blur();
      if (selected) {
        selection.removeAllRanges();
        selection.addRange(selected);
      }
    })).on('mouseleave', function(e) {
      var $b = $(this).find('.copy-btn');
      setTimeout(function() {
        $b.text('复制');
      }, 300);
    }).append(e);
  })
</script>


  

  

</body>
</html>
