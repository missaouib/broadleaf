<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spring Cloud之Eureka服务发现和注册示例]]></title>
    <url>%2F2019%2F06%2F05%2Fspring-cloud-eureka-service-discovery-and-register-example%2F</url>
    <content type="text"><![CDATA[创建生产者项目在 spring-cloud-examples工程中创建 provider 子模块。首先编辑pom文件： 12345678910111213141516&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 编写一个controller如下： 1234567@RestControllerpublic class GreetingController &#123; @RequestMapping("/greeting/&#123;username&#125;") public String greeting(@PathVariable("username") String username) &#123; return String.format("Hello %s!\n", username); &#125;&#125; 修改application.yml配置文件： 12345678910111213spring: application: name: providerserver: port: 8081eureka: client: serviceUrl: defaultZone: http://eureka-8761.com:8761/eureka/ instance: preferIpAddress: true 这里把生产者作为客户端注册到eureka了，可以打开eureka的主界面查看注册的服务信息。 启动应用，然后访问 http://localhost:8081//greeting/aaa，可以看到返回的结果： 1Hello aaa! 创建消费者项目创建consumer子模块，编辑pom： 123456 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 编写一个控制类访问生产者接口： 12345678910@RestControllerpublic class GreetingController &#123; @Autowired private GreetingService greetingService; @RequestMapping("/greeting/&#123;username&#125;") public String greeting(@PathVariable("username") String username) &#123; return greetingService.greeting(username); &#125;&#125; 创建Service类： 123456789101112@Servicepublic class GreetingService &#123; private String REST_URL_PREFIX = "http://localhost:8081"; @Autowired private RestTemplate restTemplate; public String greeting(String username) &#123; return restTemplate.getForObject(REST_URL_PREFIX + "/greeting/&#123;username&#125;", String.class, username); &#125;&#125; 这里是使用RestTemplate类访问rest接口，需要注入RestTemplate的bean： 1234567@Configurationpublic class ConfigBean &#123; @Bean public RestTemplate getRestTemplate()&#123; return new RestTemplate(); &#125;&#125; 修改application.yml配置文件： 123456server: port: 9091spring: application: name: consumer 最后创建启动类： 123456@SpringBootApplicationpublic class ConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ConsumerEurekaApplication.class, args); &#125;&#125; 启动应用，然后访问 http://localhost:9091//greeting/aaa，可以看到返回的结果： 1Hello aaa! 集成Eureka访问服务上面的消费者代码是通过rest的http接口访问生产者的接口，这里修改为通过eureka的注册中心去访问服务。 创建consumer-eureka 子模块，编辑pom： 1234567891011&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 修改application.yml配置文件，注册到eureka： 12345678910111213141516server: port: 9092spring: application: name: consumer-eurekaeureka: client: registerWithEureka: true fetchRegistry: true serviceUrl: defaultZone: http://eureka-8761.com:8761/eureka/ instance: preferIpAddress: true 创建controller类： 12345678910@RestControllerpublic class GreetingController &#123; @Autowired private GreetingService greetingService; @RequestMapping(&quot;/greeting/&#123;username&#125;&quot;) public String greeting(@PathVariable(&quot;username&quot;) String username) &#123; return greetingService.greeting(username); &#125;&#125; 修改service类： 123456789101112@Servicepublic class GreetingService &#123; private String REST_URL_PREFIX = "PROVIDER"; @Autowired private RestTemplate restTemplate; public String greeting(String username) &#123; return restTemplate.getForObject("http://" + REST_URL_PREFIX + "/greeting/&#123;username&#125;", String.class, username); &#125;&#125; 启动类： 1234567@SpringBootApplication@EnableDiscoveryClientpublic class ConsumerEurekaApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ConsumerEurekaApplication.class, args); &#125;&#125; 启动应用，然后访问 http://localhost:9092//greeting/aaa，可以看到返回的结果： 1Hello aaa! 使用Feign调用生产者接口创建 consumer-feign 子模块，编辑pom文件： 123456789101112131415161718192021&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; controller类： 12345678910@RestControllerpublic class GreetingController &#123; @Autowired private GreetingService greetingService; @RequestMapping("/greeting/&#123;username&#125;") public String getGreeting(@PathVariable("username") String username) &#123; return greetingService.greeting(username); &#125;&#125; 修改GreetingService接口： 12345@FeignClient(name = "provider")public interface GreetingService &#123; @GetMapping("/greeting/&#123;username&#125;") public String greeting(@PathVariable("username") String username);&#125; 修改启动类，添加@EnableFeignClients注解： 1234567@SpringBootApplication@EnableFeignClientspublic class ConsumerFeignApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ConsumerFeignApplication.class, args); &#125;&#125; application.yml配置文件: 123456789101112131415server: port: 9093spring: application: name: consumer-feigneureka: client: registerWithEureka: true fetchRegistry: true serviceUrl: defaultZone: http://eureka-8761.com:8761/eureka/ instance: preferIpAddress: true 启动应用，然后访问 http://localhost:9093/greeting/aaa，可以看到返回的结果： 1Hello aaa! 源代码源代码在：spring-cloud-examples。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring cloud</tag>
        <tag>eureka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud之Eureka配置示例]]></title>
    <url>%2F2019%2F04%2F15%2Fspring-cloud-eureka-config-example%2F</url>
    <content type="text"><![CDATA[Eureka是Netflix开源的服务发现组件，本身是一个基于REST的服务，包含Server和Client两部分，Spring Cloud将它集成在子项目Spring Cloud Netflix中，主要负责完成微服务架构中的服务治理功能。 版本说明： Spring Cloud：Greenwich.RELEASE Spring Boot：2.1.3.RELEASE 1、创建服务注册中心创建 spring-cloud-eureka-server 模块，并添加依赖12345678910111213141516171819202122232425&lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;spring-cloud-dependencies.version&gt;Greenwich.RELEASE&lt;/spring-cloud-dependencies.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-parent&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud-dependencies.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 启用EurekaServer1234567@SpringBootApplication@EnableEurekaServerpublic class EurekaServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaServerApplication.class, args); &#125;&#125; 修改配置文件123456789101112131415server: port: 8761spring: application: name: spring-cloud-eureka-servereureka: instance: hostname: eureka-8761.com client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://eureka-8761.com:8761/eureka/ 访问页面http://eureka-8761.com:8761/ 配置权限认证修改配置文件 开启基于HTTP basic的认证，并设置用户名和密码： 12345678910111213141516171819202122server: port: 8761spring: application: name: spring-cloud-eureka-server-security security: basic: enabled: true # 开启基于HTTP basic的认证 user: name: admin password: 654321 roles: SYSTEMeureka: instance: hostname: eureka-8761.com client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ 2、配置客户端代码创建 spring-cloud-eureka-client 子模块，并添加依赖1234567891011&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 启用EurekaClient1234567@SpringBootApplication@EnableDiscoveryClientpublic class EurekaClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaClientApplication.class, args); &#125;&#125; 修改 application.yml12345678910111213141516spring: application: name: spring-cloud-eureka-clientserver: port: 8701eureka: client: registerWithEureka: true fetchRegistry: true serviceUrl: defaultZone: http://eureka-8761.com:8761/eureka/ instance: hostname: localhost preferIpAddress: true 访问页面http://localhost:8701/greeting 3、配置集群高可用修改配置文件application.yml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980server: port: 8761spring: application: name: spring-cloud-eureka-server profiles: active: eureka-servereureka: instance: hostname: localhost client: registerWithEureka: false fetchRegistry: false---server: port: 8761spring: application: name: spring-cloud-eureka-servereureka: instance: hostname: eureka-8761.com client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://eureka-8761.com:8761/eureka/---spring: profiles: eureka-server1server: port: 8761eureka: instance: hostname: eureka-8761.com client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://eureka-8762.com:8762/eureka/,http://eureka-8763.com:8763/eureka/---spring: profiles: eureka-server2server: port: 8762eureka: instance: hostname: eureka-8762.com client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://eureka-8761.com:8761/eureka/,http://eureka-8763.com:8763/eureka/---spring: profiles: eureka-server3server: port: 8763eureka: instance: hostname: eureka-8763.com client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://eureka-8761.com:8761/eureka/,http://eureka-8762.com:8762/eureka/ 这里创建了三个profile。 根据不同profile启动应用添加参数 --spring.profiles.active=eureka-server1 启动实例1。 添加参数 --spring.profiles.active=eureka-server2 启动实例2。 添加参数 --spring.profiles.active=eureka-server3 启动实例3。 4、源代码源代码在：https://github.com/javachen/java-tutorials/tree/master/spring-cloud/spring-cloud-eureka。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring cloud</tag>
        <tag>eureka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot整合视图层]]></title>
    <url>%2F2019%2F04%2F07%2Fspring-boot-with-web-view%2F</url>
    <content type="text"><![CDATA[SpringBoot访问静态资源1、从classpath/static目录访问静态资源，目录名称必须是static 2、从ServletContext根目录下src/main/webapp访问 3、如果上面都有，则以src/main/webapp为准 SpringBoot整合JSP1、添加jstl依赖：12345678910&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-jasper&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 2、配置视图创建springBoot的全局配置文件application.properties： 12spring.mvc.view.prefix=/WEB-INF/jsp/spring.mvc.view.suffix=.jsp 3、编写Controller返回视图名称123456789101112131415@RestControllerpublic class HelloController &#123; @RequestMapping("/hello") public Map&lt;String, Object&gt; showHelloWork() &#123; Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); map.put("msg", "Hello World ！"); return map; &#125; @RequestMapping("/jsp") public String test() &#123; return "test1"; &#125;&#125; 上面test方法返回的是视图名称，他会在指定的目录（配置文件中指定的/WEB-INF/jsp/目录）返回test1.jsp文件给前端页面。 4、访问浏览器访问 http://localhost:8080/jsp ，将看到： 1test jsp SpringBoot整合Freemarker1、添加Freemarker依赖：1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-freemarker&lt;/artifactId&gt;&lt;/dependency&gt; 2、编写视图springBoot要求模板形式的视图层技术的文件必须要放到src/main/resources目录下必须要一个名称为templates。 创建test2.ftl文件： 1test freemarker 3、编写控制类方法1234@RequestMapping("/ftl")public String test() &#123; return "test2";&#125; 上面的test方法返回test视图，会到templates找相应的模板文件test2.ftl。 4、访问浏览器访问 http://localhost:8080/ftl ，将看到： 1test freemarker SpringBoot整合Thymeleaf1、添加依赖：1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; 2、创建存放视图的目录创建存放视图的目录src/main/resources/templates 3、编写模板文件Thymelaef是通过他特定语法对html的标记做渲染，所以模板文件都是html文件。 创建test3.html 1&lt;span th:text="Hello"&gt;&lt;/span&gt; &lt;span th:text="$&#123;msg&#125;"&gt;&lt;/span&gt; 4、编写控制类12345@RequestMapping("/thy")public String testThymeleaf(Model model) &#123; model.addAttribute("msg", "Thymeleaf第一个案例"); return "test3";&#125; 5、访问浏览器访问 http://localhost:8080/thy ，将看到： 1Hello Thymeleaf第一个案例 注意： 使用Thymeleaf之后，视图文件会在src/main/resources/templates查找，这时候访问 http://localhost:8080/jsp 会报错，提示找不到文件。 1org.thymeleaf.exceptions.TemplateInputException: Error resolving template "test1", template might not exist or might not be accessible by any of the configured Template Resolvers]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot整合Servlet、Filter、Listener]]></title>
    <url>%2F2019%2F04%2F07%2Fspring-boot-with-servlet-filter-listener%2F</url>
    <content type="text"><![CDATA[Spring Boot整合Servlet、Filter、Listener有两种方式：一是通过注解扫描完成；二是通过方法完成。 通过注解扫描完成主要是用到了三个注解:@WebServlet、@WebFilter、@WebListener，例如，整合Servlet： 1234567891011/** * 在springBoot启动时会扫描@WebServlet，并将该类实例化 */@WebServlet(name = "FirstServlet",urlPatterns="/firstServlet")public class FirstServlet extends HttpServlet&#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println("FirstServlet doGet......"); &#125;&#125; 整合Filter如下： 12345678910111213141516@WebFilter(filterName = "FirstFilter", urlPatterns = "/fristFilter")public class FirstFilter implements Filter &#123; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; &#125; public void doFilter(ServletRequest arg0, ServletResponse arg1, FilterChain arg2) throws IOException, ServletException &#123; System.out.println("FirstFilter doFilter......."); &#125; @Override public void destroy() &#123; &#125;&#125; 整合Listener如下： 12345678910111213@WebListenerpublic class FirstListener implements ServletContextListener &#123; @Override public void contextDestroyed(ServletContextEvent arg0) &#123; &#125; @Override public void contextInitialized(ServletContextEvent arg0) &#123; System.out.println("FirstListener init......"); &#125;&#125; 修改启动类，添加@ServletComponentScan注解实现对Servlet组件的扫描： 1234567@SpringBootApplication@ServletComponentScanpublic class App &#123; public static void main( String[] args ) &#123; SpringApplication.run(App.class, args); &#125;&#125; 通过方法完成使用这种方式不需要在Servlet、Filter、Listener上添加注解，其实就是一个普通类。需要做的是通过方法注册bean实体，与之对应的有三个对象：ServletRegistrationBean、FilterRegistrationBean、ServletListenerRegistrationBean，在启动类中创建这三个实体方法如下： 123456789101112131415161718192021222324252627282930/** * spring boot启动类 */@SpringBootApplication@ServletComponentScanpublic class App &#123; public static void main( String[] args )&#123; SpringApplication.run(App.class, args); &#125; @Bean public ServletRegistrationBean getServletRegistrationBean() &#123; ServletRegistrationBean bean = new ServletRegistrationBean(new SecondServlet()); bean.addUrlMappings("/secondServlet"); return bean; &#125; @Bean public FilterRegistrationBean getFilterRegistrationBean() &#123; FilterRegistrationBean bean = new FilterRegistrationBean(new SecondFilter()); bean.addUrlPatterns("/secondFilter"); return bean; &#125; @Bean public ServletListenerRegistrationBean&lt;SecondListener&gt; getServletListenerRegistrationBean() &#123; ServletListenerRegistrationBean&lt;SecondListener&gt; bean = new ServletListenerRegistrationBean(new SecondListener()); return bean; &#125;&#125;]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix安装过程]]></title>
    <url>%2F2019%2F03%2F29%2Finstall-zabbix%2F</url>
    <content type="text"><![CDATA[环境介绍： OS：rhel6.2 软件版本：zabbix-2.0.6 serverIP：192.168.56.10 clientIP：92.168.56.11-13 1、安装相关依赖包 yum install mysql-libs mysql-server mysql-devel php* net-snmp* openssl httpd libcurl libcurl-devel gcc g++ make2、创建zabbix用户 useradd zabbixvi sudo添加如下内容，给予Zabbix用户sudo权限 zabbix ALL=(ALL) NOPASSWD:ALL3、下载源码包并解压 # tar -zxf zabbix-2.0.6.tar.gz3.1、初始化数据库 #mysql -u root -proot -e &apos;create database zabbix;&apos; #mysql -u root -proot -e &quot;grant all privileges on zabbix.* to zabbix@localhost identified by &apos;zabbix&apos;;&quot; # mysql -uzabbix -pzabbix -D zabbix&lt; zabbix-2.0.6/database/mysql/schema.sql # mysql -uzabbix -pzabbix -D zabbix&lt; zabbix-2.0.6/database/mysql/images.sql # mysql -uzabbix -pzabbix -D zabbix&lt; zabbix-2.0.6/database/mysql/data.sql3.2、编译安装 #cd zabbix-2.0.6 # ./configure --prefix=/usr/local/zabbix --with-mysql --with-net-snmp --with-libcurl --enable-server --enable-agent #make #make install3.3、修改数据库配置文件 #cd /usr/local/zabbix/etc # vim zabbix_server.conf DBUser=zabbix DBPassword=zabbix3.4、创建服务管理脚本 #cd zabbix-2.0.6/misc/init.d/tru64/ #cp zabbix_* /etc/init.d/ #ln -s /usr/local/zabbix/sbin/zabbix_server /usr/local/sbin/zabbix_server #ln -s /usr/local/zabbix/sbin/zabbix_agentd /usr/local/sbin/zabbix_agentd3.5、启动服务 # /etc/init.d/zabbix_server start #/etc/init.d/zabbix_agentd start #ps aux| grep zabbix3.6、复制网站代码文件 #mkdir -p /var/www/html/zabbix/public_html #cp -R zabbix-2.0.6/frontends/php/* /var/www/html/zabbix/public_html/3.7、配置虚拟主机（不用也可以） $ sudo vim /etc/apache2/sites-enabled/000-default Alias /zabbix /home/zabbix/public_html/ &lt;Directory /home/zabbix/public_html&gt; AllowOverride FileInfo AuthConfig Limit Indexes Options MultiViews Indexes SymLinksIfOwnerMatch IncludesNoExec &lt;Limit GET POST OPTIONS PROPFIND&gt; Order allow,deny Allow from all &lt;/Limit&gt; &lt;LimitExcept GET POST OPTIONS PROPFIND&gt; Order deny,allow Deny from all &lt;/LimitExcept&gt; &lt;/Directory&gt;3.8、配置php #vim /etc/php.ini max_execution_time = 300 max_input_time= 600 post_max_size= 16M date.timezone = Asia/Shanghai3.9、重启httpd #/etc/init.d/httpd restart3.10、添加alert.d目录 #vim /usr/local/zabbix/etc/zabbix_server.conf AlertScriptsPath=/usr/local/zabbix/etc/alert.d3.11、安装mailutils #yum install sendmail mailutils（下载源码包自行安装）3.12、打开网页安装向导 打开 http://192.168.56.10/zabbix/public_html/setup.php 3.13、下载zabbix.conf.php文件 并将其上传到/var/www/html/zabbix/public_html/conf/下，然后点击retry。至此，Zabbix的Server端已经部署完成，接下来我们在client上部署agent。 4、安装部署agent 4.1、安装相关依赖包 yum install mysql-libs mysql-server mysql-devel php* net-snmp* openssl httpd libcurl libcurl-devel gcc g++ make4.2、新建用户 useradd zabbix4.3、下载源码包 4.4、编译安装 #tar -zxf zabbix-2.0.6.tar.gz #cd zabbix-2.0.6 # ./configure --prefix=/usr/local/zabbix --with-net-snmp --with-libcurl --enable-agent #make #make install4.4、创建服务管理脚本 #cp zabbix-2.0.6/misc/init.d/tru64/zabbix_agentd /etc/init.d/ #ln -s /usr/local/zabbix/sbin/zabbix_agentd /usr/local/sbin/zabbix_agentd #chmod 755 /etc/init.d/zabbix_agentd4.5、启动服务 #/etc/init.d/zabbix_agent start #ps aux| grep zabbix至此，agent安装完。 5、Zabbix分布式监控系统自定义配置 5.1、在agent节点 #vim /usr/local/zabbix/etc/zabbix_agentd.conf Server=192.168.56.10 ServerActive=192.168.56.10 Hostname=node2(这是客户端主机名)5.2、返回web界面创建Host。]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cloudera Manager安装Haddop集群]]></title>
    <url>%2F2019%2F03%2F28%2Finstall-hadoop-cluster-with-cm6%2F</url>
    <content type="text"><![CDATA[在开始之前，请参考我博客中的关于如何安装cdh集群的文章，这里只做简单说明。因为只是为了测试，所以是在vagrant虚拟机中创建三个虚拟机搭建一个集群来安装cdh6。 环境准备参考使用yum源安装CDH Hadoop集群文章中的第一部分。 安装CMsudo yum install cloudera-manager-server安装数据库这里使用的是postgresql yum install postgresql-server初始化数据： echo &apos;LC_ALL=&quot;zh_CN.UTF-8&quot;&apos; &gt;&gt; /etc/locale.conf sudo su -l postgres -c &quot;postgresql-setup initdb&quot;修改 pg_hba.conf 文件，在/var/lib/pgsql/data或者/etc/postgresql/&lt;version&gt;/main目录： host all all 127.0.0.1/32 md5修改postgresql.conf优化参数，参考https://www.cloudera.com/documentation/enterprise/6/6.0/topics/cm_ig_extrnl_pstgrs.html#cmig_topic_5_6。 listen_addresses = &apos;*&apos;启动posgresql： sudo systemctl restart postgresql创建数据库： sudo -u postgres psql CREATE ROLE scm LOGIN PASSWORD &apos;scm&apos;; CREATE DATABASE scm OWNER scm ENCODING &apos;UTF8&apos;; ALTER DATABASE scm SET standard_conforming_strings=off; #for the Hive Metastore and Oozie databases:使用cm自带脚本创建数据库： sudo /opt/cloudera/cm/schema/scm_prepare_database.sh postgresql scm scm安装cdh和其他模块启动cm： sudo systemctl start cloudera-scm-server查看日志： sudo tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log待启动成功之后，访问http://&lt;server_host&gt;:7180。 如果你配置了auto-TLS，可以通过https登录https://&lt;server_host&gt;:7183 用户名admin，密码admin。 参考文章 使用Vagrant创建虚拟机安装Hadoop 手动安装Cloudera Hadoop CDH Cloudera Manager6安装文档]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>cdh</tag>
        <tag>vagrant</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis源码分析：如何解析配置文件]]></title>
    <url>%2F2016%2F04%2F21%2Fhow-to-parse-mybatis-configuration%2F</url>
    <content type="text"><![CDATA[MyBatis可以使用xml或者注解的方式进行配置，不管是哪种方式，最终会将获取到的配置参数设置到Configuration类中，例如，SqlSessionFactoryBuilder类中就是通过解析XML来创建Configuration。 123456789101112131415public SqlSessionFactory build(InputStream inputStream, String environment, Properties properties) &#123; try &#123; XMLConfigBuilder parser = new XMLConfigBuilder(inputStream, environment, properties); return build(parser.parse()); &#125; catch (Exception e) &#123; throw ExceptionFactory.wrapException("Error building SqlSession.", e); &#125; finally &#123; ErrorContext.instance().reset(); try &#123; inputStream.close(); &#125; catch (IOException e) &#123; // Intentionally ignore. Prefer previous error. &#125; &#125;&#125; MyBatis通过SqlSessionFactoryBuilder作为入口，通过传入配置文件，使用了BaseBuilder实现类进行配置文件解析，具体实现类是XMLConfigBuilder，在这里MyBatis对配置的项进行了全面解析，只不过不是所有的解析都放在了XMLConfigBuilder，XMLConfigBuilder解析了二级节点，并作为一个总入口，还有另外几个类继承了BaseBuilder，用于解析不同的配置。而解析到的配置项信息，基本都保存在了Configuration这个类，可以看到多处地方依赖到它。 XMLConfigBuilder类位于org.apache.ibatis.builder中，我们来看看该包下有哪些类及每个类实现过程。 XMLConfigBuilder是继承BaseBuilder的，BaseBuilder是一个抽象类，提供了一些protected的方法和三个属性： Configuration TypeAliasRegistry：从Configuration中获取 TypeHandlerRegistry：从Configuration中获取 BaseBuilder的子类有： XMLConfigBuilder：解析MyBatis config文件 XMLMapperBuilder：解析SqlMap映射文件 XMLStatementBuilder：解析sql语句 SqlSourceBuilder： MapperAnnotationBuilder：解析注解 MapperBuilderAssistant：负责解析配置和mapping包下的接口交换 这里为什么不使用接口呢？ XMLConfigBuilder是用于解析XML配置文件，解析XML用的是sax方式，相关代码在org.apache.ibatis.parsing包下面，该包下面代码比较独立，可以复用到自己的项目中去，你懂的。 XMLConfigBuilder提供了几个构造方法，主要是为了初始化XPathParser。真正开始解析文件从下面代码开始： 12345678public Configuration parse() &#123; if (parsed) &#123; throw new BuilderException("Each XMLConfigBuilder can only be used once."); &#125; parsed = true; parseConfiguration(parser.evalNode("/configuration")); return configuration;&#125; 该方法是调用parseConfiguration方法开始解析/configuration节点。 123456789101112131415161718192021private void parseConfiguration(XNode root) &#123; try &#123; Properties settings = settingsAsPropertiess(root.evalNode("settings")); //issue #117 read properties first propertiesElement(root.evalNode("properties")); loadCustomVfs(settings); typeAliasesElement(root.evalNode("typeAliases")); pluginElement(root.evalNode("plugins")); objectFactoryElement(root.evalNode("objectFactory")); objectWrapperFactoryElement(root.evalNode("objectWrapperFactory")); reflectionFactoryElement(root.evalNode("reflectionFactory")); settingsElement(settings); // read it after objectFactory and objectWrapperFactory issue #631 environmentsElement(root.evalNode("environments")); databaseIdProviderElement(root.evalNode("databaseIdProvider")); typeHandlerElement(root.evalNode("typeHandlers")); mapperElement(root.evalNode("mappers")); &#125; catch (Exception e) &#123; throw new BuilderException("Error parsing SQL Mapper Configuration. Cause: " + e, e); &#125;&#125; 该方法包括以下几个步骤： 解析settings节点 解析properties 解析typeAliases 解析plugins 解析objectFactory 解析objectWrapperFactory 解析environments 解析databaseIdProvider 解析typeHandlers 解析mappers，调用XMLMapperBuilder进行解析 其中，最复杂的是解析mappers： 1234567891011121314151617181920212223242526272829303132private void mapperElement(XNode parent) throws Exception &#123; if (parent != null) &#123; for (XNode child : parent.getChildren()) &#123; if ("package".equals(child.getName())) &#123; String mapperPackage = child.getStringAttribute("name"); //添加包下面的mapper configuration.addMappers(mapperPackage); &#125; else &#123; String resource = child.getStringAttribute("resource"); String url = child.getStringAttribute("url"); String mapperClass = child.getStringAttribute("class"); //三个参数只能设置一个 if (resource != null &amp;&amp; url == null &amp;&amp; mapperClass == null) &#123; ErrorContext.instance().resource(resource); InputStream inputStream = Resources.getResourceAsStream(resource); XMLMapperBuilder mapperParser = new XMLMapperBuilder(inputStream, configuration, resource, configuration.getSqlFragments()); mapperParser.parse(); &#125; else if (resource == null &amp;&amp; url != null &amp;&amp; mapperClass == null) &#123; ErrorContext.instance().resource(url); InputStream inputStream = Resources.getUrlAsStream(url); XMLMapperBuilder mapperParser = new XMLMapperBuilder(inputStream, configuration, url, configuration.getSqlFragments()); mapperParser.parse(); &#125; else if (resource == null &amp;&amp; url == null &amp;&amp; mapperClass != null) &#123; Class&lt;?&gt; mapperInterface = Resources.classForName(mapperClass); configuration.addMapper(mapperInterface); &#125; else &#123; throw new BuilderException("A mapper element may only specify a url, resource or class, but not more than one."); &#125; &#125; &#125; &#125;&#125; 分析XMLMapperBuilder： 1234private XPathParser parser;private MapperBuilderAssistant builderAssistant;private Map&lt;String, XNode&gt; sqlFragments;private String resource; XMLMapperBuilder有四个属性，一个XMLMapperBuilder对应一个资源文件，需要一个XPath解析器和一个Mapper构建的帮助类，并保存sql片段。 提供了几个构造方法类初始化属性： 1234567891011121314151617181920212223242526272829@Deprecatedpublic XMLMapperBuilder(Reader reader, Configuration configuration, String resource, Map&lt;String, XNode&gt; sqlFragments, String namespace) &#123; this(reader, configuration, resource, sqlFragments); this.builderAssistant.setCurrentNamespace(namespace);&#125;@Deprecatedpublic XMLMapperBuilder(Reader reader, Configuration configuration, String resource, Map&lt;String, XNode&gt; sqlFragments) &#123; this(new XPathParser(reader, true, configuration.getVariables(), new XMLMapperEntityResolver()), configuration, resource, sqlFragments);&#125;public XMLMapperBuilder(InputStream inputStream, Configuration configuration, String resource, Map&lt;String, XNode&gt; sqlFragments, String namespace) &#123; this(inputStream, configuration, resource, sqlFragments); this.builderAssistant.setCurrentNamespace(namespace);&#125;public XMLMapperBuilder(InputStream inputStream, Configuration configuration, String resource, Map&lt;String, XNode&gt; sqlFragments) &#123; this(new XPathParser(inputStream, true, configuration.getVariables(), new XMLMapperEntityResolver()), configuration, resource, sqlFragments);&#125;private XMLMapperBuilder(XPathParser parser, Configuration configuration, String resource, Map&lt;String, XNode&gt; sqlFragments) &#123; super(configuration); this.builderAssistant = new MapperBuilderAssistant(configuration, resource); this.parser = parser; this.sqlFragments = sqlFragments; this.resource = resource;&#125; XMLMapperBuilder真正开始解析是在parse方法： 1234567891011121314public void parse() &#123; //先判断文件是否加载 if (!configuration.isResourceLoaded(resource)) &#123; //解析/mapper节点 configurationElement(parser.evalNode("/mapper")); configuration.addLoadedResource(resource); //绑定mapper到命名空间 bindMapperForNamespace(); &#125; parsePendingResultMaps(); parsePendingChacheRefs(); parsePendingStatements();&#125; configurationElement方法如下： 123456789101112131415161718192021222324private void configurationElement(XNode context) &#123; try &#123; //设置命名空间 String namespace = context.getStringAttribute("namespace"); if (namespace == null || namespace.equals("")) &#123; throw new BuilderException("Mapper's namespace cannot be empty"); &#125; builderAssistant.setCurrentNamespace(namespace); //cache-ref节点 cacheRefElement(context.evalNode("cache-ref")); //cache节点 cacheElement(context.evalNode("cache")); //parameterMap节点 parameterMapElement(context.evalNodes("/mapper/parameterMap")); //resultMap节点 resultMapElements(context.evalNodes("/mapper/resultMap")); //sql节点 sqlElement(context.evalNodes("/mapper/sql")); //select|insert|update|delete节点 buildStatementFromContext(context.evalNodes("select|insert|update|delete")); &#125; catch (Exception e) &#123; throw new BuilderException("Error parsing Mapper XML. Cause: " + e, e); &#125; 从上面例子可以看出mapper下面一共有以下几种节点： cache-ref cache parameterMap resultMap sql select insert update delete 每个节点的解析过程请看具体的某一个方法，这里不做说明。]]></content>
      <categories>
        <category>mybatis</category>
      </categories>
      <tags>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis源码分析：SqlSession]]></title>
    <url>%2F2016%2F04%2F21%2Fmybatis-sqlSession-source-code%2F</url>
    <content type="text"><![CDATA[从这段代码中可以看到MyBatis的初始化过程： 12345678DataSource dataSource = ...;TransactionFactory transactionFactory = new JdbcTransactionFactory();Environment environment = new Environment("Production", transactionFactory, dataSource);Configuration configuration = new Configuration(environment);configuration.setLazyLoadingEnabled(true);configuration.getTypeAliasRegistry().registerAlias(Blog.class);configuration.addMapper(BlogMapper.class);SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(configuration); 在创建了Configuration之后，通过SqlSessionFactoryBuilder来构建一个SqlSessionFactory。SqlSessionFactoryBuilder是构建SqlSessionFactory的一个工具类，但其不是被声明为静态或者单例的，而是需要通过new实例化。 SqlSessionFactoryBuilder类重载了以下几个build方法，每个build方法其实最终会调用build(Configuration config)方法： 123456789public SqlSessionFactory build(Reader reader)public SqlSessionFactory build(Reader reader, String environment)public SqlSessionFactory build(Reader reader, Properties properties)public SqlSessionFactory build(Reader reader, String environment, Properties properties)public SqlSessionFactory build(InputStream inputStream)public SqlSessionFactory build(InputStream inputStream, String environment)public SqlSessionFactory build(InputStream inputStream, Properties properties)public SqlSessionFactory build(InputStream inputStream, String environment, Properties properties)public SqlSessionFactory build(Configuration config) MyBatis通过SqlSessionFactoryBuilder作为入口，通过传入配置文件，使用了BaseBuilder实现类进行配置文件解析，具体实现类是XMLConfigBuilder，在这里mybatis对配置的项进行了全面解析，只不过不是所有的解析都放在了XMLConfigBuilder，XMLConfigBuilder解析了二级节点，并作为一个总入口，还有另外几个类继承了BaseBuilder，用于解析不同的配置。而解析到的配置项信息，基本都保存在了Configuration这个类，可以看到多处地方依赖到它。 从源码可以看出，SqlSessionFactoryBuilder的作用是完成从输入流、environment、Properties实例化一个Configuration，最终调用new DefaultSqlSessionFactory(config)来构建一个SqlSessionFactory的实例。 123public SqlSessionFactory build(Configuration config) &#123; return new DefaultSqlSessionFactory(config);&#125; DefaultSqlSessionFactory只有一个构造方法： 12345678public class DefaultSqlSessionFactory implements SqlSessionFactory &#123; private final Configuration configuration; public DefaultSqlSessionFactory(Configuration configuration) &#123; this.configuration = configuration; &#125;&#125; 如果是我实现DefaultSqlSessionFactory该类的逻辑，可能就会重载多个构造方法，事实上以前也是这样想的，例如： 123456789public DefaultSqlSessionFactory(Reader reader)public DefaultSqlSessionFactory(Reader reader, String environment)public DefaultSqlSessionFactory(Reader reader, Properties properties)public DefaultSqlSessionFactory(Reader reader, String environment, Properties properties)public DefaultSqlSessionFactory(InputStream inputStream)public DefaultSqlSessionFactory(InputStream inputStream, String environment)public DefaultSqlSessionFactory(InputStream inputStream, Properties properties)public DefaultSqlSessionFactory(InputStream inputStream, String environment, Properties properties)public DefaultSqlSessionFactory(Configuration config) 这样的话，DefaultSqlSessionFactory的构造方法就会比较臃肿，DefaultSqlSessionFactory实际上只需要和Configuration打交道即可，故可以将从Reader、InputStream、environment、Properties生成Configuration的逻辑移到另外一个类中，从而减少DefaultSqlSessionFactory类干的事情，让其只和Configuration产生依赖。这样每个类的职责更加分明，SqlSessionFactoryBuilder负责构建SqlSessionFactory，DefaultSqlSessionFactory只负责生产SqlSession。 SqlSessionFactory是从一个连接或者数据源创建一个SqlSession，一个SqlSession代表一次会话，会话结束之后，连接和数据源并不会关闭。 12345678910111213141516public interface SqlSessionFactory &#123; SqlSession openSession(); SqlSession openSession(boolean autoCommit); SqlSession openSession(Connection connection); SqlSession openSession(TransactionIsolationLevel level); SqlSession openSession(ExecutorType execType); SqlSession openSession(ExecutorType execType, boolean autoCommit); SqlSession openSession(ExecutorType execType, TransactionIsolationLevel level); SqlSession openSession(ExecutorType execType, Connection connection); Configuration getConfiguration();&#125; DefaultSqlSessionFactory是MyBatis提供的SqlSessionFactory接口的默认实现类，其主要逻辑如下： 123456789101112131415private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) &#123; Transaction tx = null; try &#123; final Environment environment = configuration.getEnvironment(); final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); final Executor executor = configuration.newExecutor(tx, execType); return new DefaultSqlSession(configuration, executor, autoCommit); &#125; catch (Exception e) &#123; closeTransaction(tx); // may have fetched a connection so lets call close() throw ExceptionFactory.wrapException("Error opening session. Cause: " + e, e); &#125; finally &#123; ErrorContext.instance().reset(); &#125;&#125; 其主要逻辑如下： 从configuration获取environment 从environment获取事务工厂类TransactionFactory，并由工厂类创建事务。 通过事务和执行类型来创建执行类Executor 通过执行类Executor来创建默认的DefaultSqlSession，可以想象的到DefaultSqlSession的主要逻辑都是委托给执行类Executor来执行。 DefaultSqlSession主要实现了SqlSession接口定义的方法： 123456789101112131415161718192021222324252627282930&lt;T&gt; T selectOne(String statement);&lt;T&gt; T selectOne(String statement, Object parameter);&lt;E&gt; List&lt;E&gt; selectList(String statement);&lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter);&lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter, RowBounds rowBounds);&lt;K, V&gt; Map&lt;K, V&gt; selectMap(String statement, String mapKey);&lt;K, V&gt; Map&lt;K, V&gt; selectMap(String statement, Object parameter, String mapKey);&lt;K, V&gt; Map&lt;K, V&gt; selectMap(String statement, Object parameter, String mapKey, RowBounds rowBounds);&lt;T&gt; Cursor&lt;T&gt; selectCursor(String statement);&lt;T&gt; Cursor&lt;T&gt; selectCursor(String statement, Object parameter);&lt;T&gt; Cursor&lt;T&gt; selectCursor(String statement, Object parameter, RowBounds rowBounds);void select(String statement, Object parameter, ResultHandler handler);void select(String statement, ResultHandler handler);void select(String statement, Object parameter, RowBounds rowBounds, ResultHandler handler);int insert(String statement);int insert(String statement, Object parameter);int update(String statement);int update(String statement, Object parameter);int delete(String statement);int delete(String statement, Object parameter);void commit();void commit(boolean force);void rollback();void rollback(boolean force);List&lt;BatchResult&gt; flushStatements();void close();void clearCache();Configuration getConfiguration();&lt;T&gt; T getMapper(Class&lt;T&gt; type);Connection getConnection(); SqlSession提供了对数据库连接、事务、缓存以及对数据的增删改查的操作。默认的实现类DefaultSqlSession，其实是将这些操作委托给了Executor来执行。例如，selectList方法： 1234567891011@Overridepublic &lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter, RowBounds rowBounds) &#123; try &#123; MappedStatement ms = configuration.getMappedStatement(statement); return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER); &#125; catch (Exception e) &#123; throw ExceptionFactory.wrapException("Error querying database. Cause: " + e, e); &#125; finally &#123; ErrorContext.instance().reset(); &#125;&#125; 主要逻辑是： 在configuration类中获取sql语句对应的MappedStatement executor.query需要四个参数：mappedStatement、parameter、rowBounds、resultHandler。 对于传入的参数parameter进行处理： 123456789101112131415private Object wrapCollection(final Object object) &#123; if (object instanceof Collection) &#123; StrictMap&lt;Object&gt; map = new StrictMap&lt;Object&gt;(); map.put("collection", object); if (object instanceof List) &#123; map.put("list", object); &#125; return map; &#125; else if (object != null &amp;&amp; object.getClass().isArray()) &#123; StrictMap&lt;Object&gt; map = new StrictMap&lt;Object&gt;(); map.put("array", object); return map; &#125; return object;&#125; 最后清除ErrorContext SqlSessionManager用于管理SqlSession，其实现了SqlSessionFactory, SqlSession接口，内部分别维护了SqlSessionFactory, SqlSession实例。 123456789101112131415public class SqlSessionManager implements SqlSessionFactory, SqlSession &#123; private final SqlSessionFactory sqlSessionFactory; private final SqlSession sqlSessionProxy; private ThreadLocal&lt;SqlSession&gt; localSqlSession = new ThreadLocal&lt;SqlSession&gt;(); private SqlSessionManager(SqlSessionFactory sqlSessionFactory) &#123; this.sqlSessionFactory = sqlSessionFactory; this.sqlSessionProxy = (SqlSession) Proxy.newProxyInstance( SqlSessionFactory.class.getClassLoader(), new Class[]&#123;SqlSession.class&#125;, new SqlSessionInterceptor()); &#125;&#125; 说明： sqlSessionFactory主要是用于创建SqlSession localSqlSession保存的是受管理的SqlSession，可以通过startManagedSession()方法创建受管理的SqlSession并将其放置到ThreadLocal里面； 1234567891011121314151617181920212223242526272829303132333435public void startManagedSession() &#123; this.localSqlSession.set(openSession());&#125;public void startManagedSession(boolean autoCommit) &#123; this.localSqlSession.set(openSession(autoCommit));&#125;public void startManagedSession(Connection connection) &#123; this.localSqlSession.set(openSession(connection));&#125;public void startManagedSession(TransactionIsolationLevel level) &#123; this.localSqlSession.set(openSession(level));&#125;public void startManagedSession(ExecutorType execType) &#123; this.localSqlSession.set(openSession(execType));&#125;public void startManagedSession(ExecutorType execType, boolean autoCommit) &#123; this.localSqlSession.set(openSession(execType, autoCommit));&#125;public void startManagedSession(ExecutorType execType, TransactionIsolationLevel level) &#123; this.localSqlSession.set(openSession(execType, level));&#125;public void startManagedSession(ExecutorType execType, Connection connection) &#123; this.localSqlSession.set(openSession(execType, connection));&#125;public boolean isManagedSessionStarted() &#123; return this.localSqlSession.get() != null;&#125; 创建SqlSession使用了动态代理，调用增删改查接口时，实际上调用的sqlSessionProxy类，代理逻辑是：先判断是否存在受管理的SqlSession，如果有，则从ThreadLocal取出SqlSession并调用被代理的方法，返回结果；否则，使用sqlSessionFactory创建一个默认的SqlSession，调用被代理的方法，并提交事务，返回结果。 SqlSessionManager提供了静态方法newInstance来实例化一个SqlSessionManager，但SqlSessionManager类并不是单例的。既然不是单例的，其和直接使用构造方法进行重载应该没什么区别吧？ 123456789101112131415161718192021222324252627public static SqlSessionManager newInstance(Reader reader) &#123; return new SqlSessionManager(new SqlSessionFactoryBuilder().build(reader, null, null));&#125;public static SqlSessionManager newInstance(Reader reader, String environment) &#123; return new SqlSessionManager(new SqlSessionFactoryBuilder().build(reader, environment, null));&#125;public static SqlSessionManager newInstance(Reader reader, Properties properties) &#123; return new SqlSessionManager(new SqlSessionFactoryBuilder().build(reader, null, properties));&#125;public static SqlSessionManager newInstance(InputStream inputStream) &#123; return new SqlSessionManager(new SqlSessionFactoryBuilder().build(inputStream, null, null));&#125;public static SqlSessionManager newInstance(InputStream inputStream, String environment) &#123; return new SqlSessionManager(new SqlSessionFactoryBuilder().build(inputStream, environment, null));&#125;public static SqlSessionManager newInstance(InputStream inputStream, Properties properties) &#123; return new SqlSessionManager(new SqlSessionFactoryBuilder().build(inputStream, null, properties));&#125;public static SqlSessionManager newInstance(SqlSessionFactory sqlSessionFactory) &#123; return new SqlSessionManager(sqlSessionFactory);&#125; 总结起来就是，SqlSessionManager提供了两种方式管理SqlSession，你可以启动一个你自己创建的SqlSession或者使用动态代理创建一个SqlSession，相应的方法并提交事务。这里使用代理的原因是，在执行完方法之后，需要提交事务，如果执行失败，还需要回滚事务，最后还需要关闭会话。 如果你直接调用SqlSessionManager的getConnection、clearCache、commit、rollback、flushStatements、close，最终会调用ThreadLocal里的，即受管理的SqlSession，如果其为空，则会抛出异常，例如： 12345678@Overridepublic void commit(boolean force) &#123; final SqlSession sqlSession = localSqlSession.get(); if (sqlSession == null) &#123; throw new SqlSessionException("Error: Cannot commit. No managed session is started."); &#125; sqlSession.commit(force);&#125;]]></content>
      <categories>
        <category>mybatis</category>
      </categories>
      <tags>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UML类之间关系]]></title>
    <url>%2F2016%2F04%2F01%2Fuml-class-realation%2F</url>
    <content type="text"><![CDATA[前面两篇文章讲到了使用PlantUML来画类图，要想准确地画出类与类之间的关系，必须理清类和类之间的关系。类的关系有泛化(Generalization)、实现（Realization）、依赖(Dependency)和关联(Association)，其中关联又分为一般关联关系和聚合关系(Aggregation)、组成关系(Composition)。 基本概念类图类图（Class Diagram）: 类图是面向对象系统建模中最常用和最重要的图，是定义其它图的基础。类图主要是用来显示系统中的类、接口以及它们之间的静态结构和关系的一种静态模型。 类图的3个基本组件：类名、属性、方法。 泛化泛化(generalization)：表示is-a的关系，表示一个对象是另外一个对象的意思，即继承的关系。泛化是对象之间耦合度最大的一种关系，子类继承父类的所有细节。例如，自行车是车，猫是动物。 在类图中使用带三角箭头的实线表示，箭头从子类指向父类。例如，下图表示猫继承动物。 注意：最终代码中，泛化关系表现为一个类继承一个非抽象类。 在PlantUML中，泛化使用&lt;|--来表示，例如，上面的类图表示为： 1Animal &lt;|-- Cat 实现实现（Realization）:在类图中就是接口和实现类的关系。接口定义标准，实现类来实现该标准。例如，跑步是一个接口，人是跑步的实现类，因为人能够跑步。 在类图中使用带三角箭头的虚线表示，箭头从实现类指向接口。 在PlantUML中，实现使用&lt;|..来表示，例如，上面的类图表示为： 123456789101112131415161718192021222324252627282930interface RunRun &lt;|.. Human~~~ ## 依赖依赖(Dependency)：对象之间最弱的一种关联方式，是临时性的`关联`。代码中一般指由`局部变量、函数参数、返回值`建立的对于其他对象的调用关系。一个类调用被依赖类中的某些方法而得以完成这个类的一些职责。`在类图使用带箭头的虚线表示，箭头从使用类指向被依赖的类。`![](http://plantuml.com:80/plantuml/png/oymhIIrAIqnELGWjJYqAJYqgoqnEvKhEIImkHXRnp2t8gUPIK02BkIJcAvH2Q91GMNvcYa9nObcg1aWIBAF9LSk5P0WN5v9H2ZOrUdhePdEXyHNqzEp0QW00)说明：- UserServiceImpl`实现`了UserService接口- UserServiceImpl类的save方法`依赖`User对象，因为方法参数类型是User 在PlantUML中，`依赖`使用`&lt;..`来表示，例如，上面的类图表示为：~~~javainterface UserServiceclass UserServiceImpl&#123; UserDao userDao void save(User user)&#125;class UserUserService &lt;|.. UserServiceImplUserServiceImpl ..&gt; User 关联关联(Association) : 它描述不同类的对象之间的结构关系；它是一种静态关系， 通常与运行状态无关，一般由常识等因素决定的；它一般用来定义对象之间静态的、天然的结构； 所以，关联关系是一种强关联的关系。这种关系通常使用类的属性表达。关联又分为一般关联、聚合关联与组合关联。后两种在后面分析。 比如，乘车人和车票之间就是一种关联关系；学生和学校就是一种关联关系。 在类图使用带箭头的实线表示，箭头从使用类指向被关联的类。 在PlantUML中，关联使用&lt;--来表示，例如，上面的类图表示为： 123456789101112interface UserServiceinterface UserDaoclass UserServiceImpl&#123; UserDao userDao void save(User user)&#125;class UserUserService &lt;|.. UserServiceImplUserServiceImpl ..&gt; UserUserServiceImpl --&gt; UserDao 关联关系默认不强调方向，表示对象间相互知道；如果特别强调方向，如上图，表示UserServiceImpl知道User，但User不知道UserServiceImpl； 注意：在最终代码中，关联对象通常是以成员变量的形式实现的； 聚合聚合(Aggregation) : 表示has-a的关系，用于表示实体对象之间的关系，表示整体由部分构成的语义；例如一个公司由多个员工组成；与组合关系不同的是，整体和部分不是强依赖的，即使整体不存在了，部分仍然存在；例如， 公司倒闭了，员工依然可以换公司，他们依然存在。 在类图使用空心的菱形表示，菱形从局部指向整体。 在PlantUML中，聚合使用o--来表示，例如，上面的类图表示为： 1234567class Company&#123; List&lt;Employee&gt; employees&#125;class EmployeeCompany o-- Employee 组合组合(Composition) : 表示contains-a的关系，是一种强烈的包含关系。与聚合关系一样，组合关系同样表示整体由部分构成的语义；比如公司由多个部门组成；但组合关系是一种强依赖的特殊聚合关系，如果整体不存在了，则部分也不存在了；例如公司和部门的关系，没有了公司，部门也不能存在了；调查问卷中问题和选项的关系；订单和订单选项的关系。 在类图使用实心的菱形表示，菱形从局部指向整体。 在PlantUML中，组合使用*--来表示，例如，上面的类图表示为： 123456789101112131415161718class Company&#123; List&lt;Department&gt; departments&#125;class Departmentclass Question&#123; List&lt;Option&gt; options&#125;class Optionclass Order&#123; List&lt;Item&gt; items&#125;class ItemCompany *-- DepartmentQuestion *-- OptionOrder *-- Item 多重性多重性(Multiplicity) : 通常在关联、聚合、组合中使用。就是代表有多少个关联对象存在。使用数字..星号（数字）表示。例如，上面例子中的一个公司有0到多个员工。 在PlantUML中，多重性使用0..*这样的符号来表示，例如，上面的类图表示为： 1234567class Company&#123; List&lt;Employee&gt; employees&#125;class EmployeeCompany o--"0..*" Employee 聚合和组合的区别聚合和组合的区别在于：聚合关系是has-a关系，组合关系是contains-a关系；聚合关系表示整体与部分的关系比较弱，而组合比较强；聚合关系中代表部分事物的对象与代表聚合事物的对象的生存期无关，一旦删除了聚合对象不一定就删除了代表部分事物的对象。组合关系中一旦删除了组合对象，同时也就删除了代表部分事物的对象。 总结对于继承、实现这两种关系没多少疑问，他们体现的是一种类与类、或者类与接口间的纵向关系；其他的四者关系则体现的是类与类、或者类与接口间的引用、横向关系，是比较难区分的，有很多事物间的关系要想准备定位是很难的，前面也提到，这几种关系都是语义级别的，所以从代码层面并不能完全区分各种关系； 但总的来说，后几种关系所表现的强弱程度依次为：组合 &gt; 聚合 &gt; 关联 &gt; 依赖。 说明：本文中的类图都是通过http://plantuml.com/plantuml/生成。 参考文章 UML类图与类的关系详解 看懂UML类图和时序图]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>uml</tag>
        <tag>plantuml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PlantUML安装和使用]]></title>
    <url>%2F2016%2F02%2F29%2Fplantuml-install-and-usage%2F</url>
    <content type="text"><![CDATA[什么是PlantUMLPlantUML是一个快速创建UML图形的组件，PlantUML支持的图形有： sequence diagram, use case diagram, class diagram, activity diagram, component diagram, state diagram, object diagram, wireframe graphical interface PlantUML通过简单和直观的语言来定义图形，语法参见PlantUML Language Reference Guide，它支持很多工具，可以生成PNG、SVG、LaTeX和二进制图片。例如，下面的例子是通过在线示例工具生成的。 ASCII Art格式： 12345678┌───┐ ┌─────┐│Bob│ │Alice│└─┬─┘ └──┬──┘ │ hello │ │──────────────&gt;│ ┌─┴─┐ ┌──┴──┐│Bob│ │Alice│└───┘ └─────┘ http://www.planttext.com/planttext也是一个类似的导出工具，甚至你可以自建一个服务器生成图片。使用在线生成工具的好处是不用保存图片，可以直接应用生成的图片地址。 主页官网地址：http://plantuml.com/。 安装PlantUML下载地址：http://plantuml.com/download.html。你可以下载jar包和java的开发工具集成使用，更多的安装或者集成方式见http://plantuml.com/running.html。 对于我来说，有用的是在Chrome上集成PlantUML插件和在Sublime Text中集成。 Sublime Text集成PlantUMLPlantUML依赖Graphviz，故先安装： 1brew install graphviz Sublime Text 的集成使用的是sublime_diagram_plugin因为默认的包管理中没有，所以需要自己添加源。 使用 Command-Shift-P 打开 Command Palette 输入 add repository 找到 Package Control:Add Repository在下方出现的输入框中输入 https://github.com/jvantuyl/sublime_diagram_plugin.git， 然后回车 等待添加完成后再次使用Command-Shift-P打开Command Palette 输入install package找到Package Control:Install Package 等待列表加载完毕，输入diagram找到sublime_diagram_plugin 安装 重启Sublime Text 重启后可以在Preferences -&gt; Packages Setting看到Diagram，默认绑定的渲染快捷键是super + m也就是Command + m如果不冲突直接使用即可。 为了简化使用，可以在 Sublime 里配置个快捷键。打开 Preferences -&gt; Key Binding - User，添加一个快捷键： 1&#123; &quot;keys&quot;: [&quot;alt+d&quot;], &quot;command&quot;: &quot;display_diagrams&quot;&#125; 上面的代码配置成按住 Alt + d 来生成 PlantUML 图片，你可以修改成你自己喜欢的按键。 简单使用使用的话比较简单，绘图的内容需要包含在@startuml和@enduml中，不然会报错。 在文本中输入以下内容： 1234@startumlBob -&gt; Alice : Hello, how are youAlice -&gt; Bob : Fine, thank you, and you?@enduml 按Command + m会在当前工作目录下生成这个图片文件，同时自动弹出窗口显示如下图片。 将其保存为basic.txt之后，可以在命令行运行： 1java -jar /path/to/jar/plantuml.jar -tsvg basic.txt 这样会在当前路径生成了名为 basic.svg 的图片。 命令行使用参见：http://plantuml.sourceforge.net/command_line.html 。 参考文章 在 Mac 上使用 PlantUML 高效画图]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>uml</tag>
        <tag>plantuml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PlantUML类图]]></title>
    <url>%2F2016%2F02%2F29%2Fplantuml-class-diagram%2F</url>
    <content type="text"><![CDATA[类之间的关系PlantUML用下面的符号来表示类之间的关系： 泛化，Generalization：&lt;|-- 关联，Association：&lt;-- 组合，Composition：*-- 聚合，Aggregation：o-- 实现，Realization：&lt;|.. 依赖，Dependency：&lt;.. 以上是常见的六种关系，--可以替换成..就可以得到虚线。另外，其中的符号是可以改变方向的，例如：&lt;|--表示右边的类泛化左边的类；--|&gt;表示左边的类泛化右边的类。 例如，下面的是--： 123456789@startumlClass01 &lt;|-- Class02:泛化Class03 &lt;-- Class04:关联Class05 *-- Class06:组合Class07 o-- Class08:聚合Class09 -- Class10@enduml 生成的类图如下： --可以替换成..，对应的虚线： 123456789@startumlClass11 &lt;|.. Class12:实现Class13 &lt;.. Class14:依赖Class15 *.. Class16Class17 o.. Class18Class19 .. Class20@enduml 生成的类图如下： 关系上的标签可以在关系上添加标签，只需要在文本后面添加冒号和标签名称即可。可以在关联的两边使用双引号。例如： 1234567@startumlClass01 "1" *-- "many" Class02 : containsClass03 o-- Class04 : aggregationClass05 --&gt; "1" Class06@enduml 生成的类图如下： 你可以在关系上使用&lt;或者&gt;表名两个类之间的关系，例如： 123456789@startumlclass CarDriver - Car : drives &gt;Car *- Wheel : have 4 &gt;Car -- Person : &lt; owns@enduml 生成的类图如下： 上面的类图意思是： Driver 驾驶 Car Car 有4个 Wheel Person 拥有 Car 添加方法在类名后面添加冒号可以添加方法和方法的参数，例如： 123456789@startumlObject &lt;|-- ArrayListObject : equals()ArrayList : Object[] elementDataArrayList : size()@enduml 生成的类图如下： 也可以使用{}来定义所有的字段及字段和方法，例如： 1234567891011@startumlclass Dummy &#123; String data void methods()&#125;class Flight &#123; flightNumber : Integer departureTime : Date&#125;@enduml 生成的类图如下： 定义可见性以下符号定义字段或者方法的可见性： -：private #：protected ~：package private +：public 例如： 12345678910@startumlclass Dummy &#123; -field1 #field2 ~method1() +method2()&#125;@enduml 你可以使用skinparam classAttributeIconSize 0关掉icon的显示： 12345678910@startumlskinparam classAttributeIconSize 0class Dummy &#123; -field1 #field2 ~method1() +method2()&#125;@enduml 抽象和静态你可以使用{static}或者{abstract}来修饰字段或者方法，修饰符需要在行的开头或者末尾使用。你也可以使用{classifier}代替{static}。 1234567@startumlclass Dummy &#123; &#123;static&#125; String id &#123;classifier&#125; String name &#123;abstract&#125; void methods()&#125;@enduml 类主体默认的，字段和方法是由PlantUML自动分组的，你也可以使用: -- .. == __这些分隔符手动进行分组。 1234567891011121314151617181920212223242526272829@startumlclass Foo1 &#123; You can use several lines .. as you want and group == things together. __ You can have as many groups as you want -- End of classes&#125;class User &#123; .. Simple Getter .. + getName() + getAddress() .. Some setter .. + setName() __ private data __ int age -- encrypted -- String password&#125;@enduml 注释和原型原型使用class、&lt;&lt;和&gt;&gt;进行定义。 注释使用note left of、note right of、note top of、note bottom of关键字进行定义。 你也可以在最后一个定义的类上使用note left、note right、note top、note bottom关键字。 注释可以使用..与其他对象进行连接。 123456789101112131415@startumlclass Object &lt;&lt; general &gt;&gt;Object &lt;|--- ArrayListnote top of Object : In java, every class\nextends this one.note &quot;This is a floating note&quot; as N1note &quot;This note is connected\nto several objects.&quot; as N2Object .. N2N2 .. ArrayListclass Foonote left: On last defined class@enduml 注释的其他特性注释可以使用一些html标签进行修饰： &lt;b&gt; &lt;u&gt; &lt;i&gt; &lt;s&gt;, &lt;del&gt;, &lt;strike&gt; &lt;font color=&quot;#AAAAAA&quot;&gt; 或者 &lt;font color=&quot;colorName&quot;&gt; &lt;color:#AAAAAA&gt; 或者 &lt;color:colorName&gt; &lt;size:nn&gt; 该表font大小 &lt;img src=&quot;file&quot;&gt; 或者 &lt;img:file&gt;，文件必须是可以访问的。 12345678910111213141516171819@startumlclass Foonote left: On last defined classnote top of Object In java, &lt;size:18&gt;every&lt;/size&gt; &lt;u&gt;class&lt;/u&gt; &lt;b&gt;extends&lt;/b&gt; &lt;i&gt;this&lt;/i&gt; one.end notenote as N1 This note is &lt;u&gt;also&lt;/u&gt; &lt;b&gt;&lt;color:royalBlue&gt;on several&lt;/color&gt; &lt;s&gt;words&lt;/s&gt; lines And this is hosted by &lt;img:sourceforge.jpg&gt;end note@enduml 连接上的注释可以在连接上定义注释，只需要使用note on link，你可以使用note left on link、note right on link、note top on link、note bottom on link来改变注释的位置。 12345678910111213@startumlclass DummyDummy --&gt; Foo : A linknote on link #red: note that is redDummy --&gt; Foo2 : Another linknote right on link #blue this is my note on right link and in blueend note@enduml 抽象类和接口可以使用abstract或者interface来定义抽象类或者接口，也可以使用annotation、enum关键字来定义注解或者枚举。 12345678910111213141516171819202122232425262728@startumlabstract class AbstractListabstract AbstractCollectioninterface Listinterface CollectionList &lt;|-- AbstractListCollection &lt;|-- AbstractCollectionCollection &lt;|- ListAbstractCollection &lt;|- AbstractListAbstractList &lt;|-- ArrayListclass ArrayList &#123; Object[] elementData size()&#125;enum TimeUnit &#123; DAYS HOURS MINUTES&#125;annotation SuppressWarnings@enduml 使用非字母类名可以使用非字母的方式显示： 123456@startumlclass &quot;This is my class&quot; as class1class class2 as &quot;It works this way too&quot;class2 *-- &quot;foo/dummy&quot; : use@enduml 隐藏字段和方法1234567891011121314151617181920@startumlclass Dummy1 &#123; +myMethods()&#125;class Dummy2 &#123; +hiddenMethod()&#125;class Dummy3 &lt;&lt;Serializable&gt;&gt; &#123; String name&#125;hide membershide &lt;&lt;Serializable&gt;&gt; circleshow Dummy1 methodsshow &lt;&lt;Serializable&gt;&gt; fields@enduml 隐藏类12345678910@startumlclass Foo1class Foo2Foo2 *-- Foo1hide Foo2@enduml 使用泛型12345678@startumlclass Foo&lt;? extends Element&gt; &#123; int size()&#125;Foo *- Element@enduml 包123456789101112@startumlpackage "Classic Collections" #yellow&#123; Object &lt;|-- ArrayList&#125;package net.sourceforge.plantuml &#123; Object &lt;|-- Demo1 Demo1 *- Demo2&#125;@enduml 包可以设置样式，也可以使用skinparam packageStyle设置为默认样式。 123456789101112131415161718192021222324252627@startumlscale 750 widthpackage foo1 &lt;&lt;Node&gt;&gt; &#123; class Class1&#125;package foo2 &lt;&lt;Rect&gt;&gt; &#123; class Class2&#125;package foo3 &lt;&lt;Folder&gt;&gt; &#123; class Class3&#125;package foo4 &lt;&lt;Frame&gt;&gt; &#123; class Class4&#125;package foo5 &lt;&lt;Cloud&gt;&gt; &#123; class Class5&#125;package foo6 &lt;&lt;Database&gt;&gt; &#123; class Class6&#125;@enduml 也可以在包之间设置联系： 1234567891011121314@startumlskinparam packageStyle rectpackage foo1.foo2 &#123;&#125;package foo1.foo2.foo3 &#123; class Object&#125;foo1.foo2 +-- foo1.foo2.foo3@enduml 命名空间命名空间内使用默认的类，需要在类名前加上点。 123456789101112131415161718192021@startumlclass BaseClassnamespace net.dummy #DDDDDD &#123; .BaseClass &lt;|-- Person Meeting o-- Person .BaseClass &lt;|- Meeting&#125;namespace net.foo &#123; net.dummy.Person &lt;|- Person .BaseClass &lt;|-- Person net.dummy.Meeting o-- Person&#125;BaseClass &lt;|-- net.unused.Person@enduml 命名空间可以自动设置，通过set namespaceSeparator ???设置命名空间分隔符，使用set namespaceSeparator none可以关闭自动设置命名空间。 12345678@startumlset namespaceSeparator ::class X1::X2::foo &#123; some info&#125;@enduml 改变箭头方向1234@startumlRoom o- StudentRoom *-- Chair@enduml 换个方向： 1234@startumlStudent -o RoomChair --* Room@enduml 也可以在箭头上使用left, right, up or down关键字： 123456@startumlfoo -left-&gt; dummyLeft foo -right-&gt; dummyRight foo -up-&gt; dummyUp foo -down-&gt; dummyDown@enduml 这些关键字可以使用开头的几个字符简写，例如，使用-d-代替-down-。 标题使用title或者title end title。 123456@startumltitle Simple &lt;b&gt;example&lt;/b&gt;\nof title Object &lt;|-- ArrayList@enduml 设置Legend12345678910@startumlObject &lt;|- ArrayListlegend right &lt;b&gt;Object&lt;/b&gt; and &lt;b&gt;ArrayList&lt;/b&gt; are simple classendlegend@enduml 关联类一个类和两个类有关联时设置关系： 123456789101112@startumlclass Student &#123; Name&#125;Student "0..*" - "1..*" Course(Student, Course) .. Enrollmentclass Enrollment &#123; drop() cancel()&#125;@enduml 换一个方向： 123456789101112@startumlclass Student &#123; Name&#125;Student "0..*" -- "1..*" Course(Student, Course) . Enrollmentclass Enrollment &#123; drop() cancel()&#125;@enduml 其他还有一些特性，如设置皮肤参数、颜色、拆分大文件等等，请参考官方文档。 例子一个完整的例子： 123456789101112131415161718192021222324252627282930313233343536@startuml title class-diagram.pngscale 1.5/&apos;组合关系(composition)&apos;/class Human &#123; - Head mHead; - Heart mHeart; .. - CreditCard mCard; -- + void travel(Vehicle vehicle);&#125;Human *-up- Head : contains &gt;Human *-up- Heart : contains &gt;/&apos;聚合关系(aggregation)&apos;/Human o-left- CreditCard : owns &gt;/&apos;依赖关系(dependency)&apos;/Human .down.&gt; Vehicle : dependent/&apos;关联关系(association&apos;/Human -down-&gt; Company : associate/&apos;继承关系(extention)&apos;/interface IProgram &#123; + void program();&#125;class Programmer &#123; + void program();&#125;Programmer -left-|&gt; Human : extendProgrammer .up.|&gt; IProgram : implement@enduml 下面例子，参考自Class Diagram。 Java集合类图123456789101112131415161718192021@startumlabstract class AbstractListabstract AbstractCollectioninterface Listinterface Collection List &lt;|-- AbstractListCollection &lt;|-- AbstractCollection Collection &lt;|- ListAbstractCollection &lt;|- AbstractListAbstractList &lt;|-- ArrayList ArrayList : Object[] elementDataArrayList : size() enum TimeUnitTimeUnit : DAYSTimeUnit : HOURSTimeUnit : MINUTES@enduml 类和接口123456789101112131415@startumlObject -- AbstractList class ArrayList extends Object &#123; int size&#125; interface List extends Collection &#123; add()&#125; interface Set extends Collection class TreeSet implements SortedSet@enduml Repository接口1234567891011121314151617181920212223242526272829303132333435363738@startumlpackage framework &lt;&lt;Folder&gt;&gt;&#123; interface BasicRepository &#123; E find(Object pk); List&lt;E&gt; findAll(); void save(E entity); void update(E entity); void remove(E entity); &#125; class AbstractHibernateRepository &lt;&lt; @Repository &gt;&gt; &#123; -EntityManager entityManager; &#125; &#125; interface PartnerRepository &#123; List&lt;PartnerEntity&gt; findByFoo(...); List&lt;PartnerEntity&gt; search(String pattern, int maxResult); &#125; class HibernatePartnerRepository &lt;&lt; @Repository &gt;&gt; &#123; &#125; class InMemoryPartnerRepository &#123; &#125; BasicRepository &lt;|.. PartnerRepositoryBasicRepository &lt;|.. AbstractHibernateRepositoryAbstractHibernateRepository &lt;|-- HibernatePartnerRepositoryPartnerRepository &lt;|.. HibernatePartnerRepositoryPartnerRepository &lt;|.. InMemoryPartnerRepository@enduml Java异常层次123456789101112131415161718192021222324252627@startumlnamespace java.lang #DDDDDD &#123; class Error &lt;&lt; unchecked &gt;&gt; Throwable &lt;|-- Error Throwable &lt;|-- Exception Exception &lt;|-- CloneNotSupportedException Exception &lt;|-- RuntimeException RuntimeException &lt;|-- ArithmeticException RuntimeException &lt;|-- ClassCastException RuntimeException &lt;|-- IllegalArgumentException RuntimeException &lt;|-- IllegalStateException Exception &lt;|-- ReflectiveOperationException ReflectiveOperationException &lt;|-- ClassNotFoundException&#125; namespace java.io #DDDDDD &#123; java.lang.Exception &lt;|-- IOException IOException &lt;|-- EOFException IOException &lt;|-- FileNotFoundException&#125; namespace java.net #DDDDDD &#123; java.io.IOException &lt;|-- MalformedURLException java.io.IOException &lt;|-- UnknownHostException &#125;@enduml 参考文章 PlantUML Classes Class Diagram]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>uml</tag>
        <tag>plantuml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis源码分析：Configuration]]></title>
    <url>%2F2016%2F02%2F26%2Fsource-code-analytic-of-mybatis-configuration%2F</url>
    <content type="text"><![CDATA[MyBatis依赖的jar不多，而且代码行数也没多少，其中使用了大量的设计模式，值得好好学习。下图是MyBatis的一张架构图，来自Java框架篇—Mybatis 入门。 Mybatis的功能架构分为三层： API接口层：提供给外部使用的接口API，开发人员通过这些本地API来操纵数据库。接口层一接收到调用请求就会调用数据处理层来完成具体的数据处理。 数据处理层：负责具体的SQL查找、SQL解析、SQL执行和执行结果映射处理等。它主要的目的是根据调用的请求完成一次数据库操作。 基础支撑层：负责最基础的功能支撑，包括连接管理、事务管理、配置加载和缓存处理，这些都是共用的东西，将他们抽取出来作为最基础的组件。为上层的数据处理层提供最基础的支撑。 MyBatis整个项目的包结构如下： 1234567891011121314151617181920212223.└── org └── apache └── ibatis ├── annotations ├── binding ├── builder ├── cache ├── cursor ├── datasource ├── exceptions ├── executor ├── io ├── jdbc ├── logging ├── mapping ├── parsing ├── plugin ├── reflection ├── scripting ├── session ├── transaction └── type MyBatis的最基础和常用的是Configuration类，它负责保存MyBatis的一些设置参数，它依赖或者相关的包有： mapping：通过Environment设置应用环境，Environment依赖datasource和transaction包 transaction：事务 logging：设置日志 type：管理type注册器，默认添加了很多常用类的注册器 reflection：反射相关基础类 cache：管理缓存 scripting：运行脚本 plugin：管理插件 execution：获取execution Configuration类的所有属性都是protected，其中一些属性是final的，主要提供了两个构造方法： 1234567891011121314151617181920212223242526272829303132333435363738public Configuration(Environment environment) &#123; this(); this.environment = environment;&#125;public Configuration() &#123; typeAliasRegistry.registerAlias("JDBC", JdbcTransactionFactory.class); typeAliasRegistry.registerAlias("MANAGED", ManagedTransactionFactory.class); typeAliasRegistry.registerAlias("JNDI", JndiDataSourceFactory.class); typeAliasRegistry.registerAlias("POOLED", PooledDataSourceFactory.class); typeAliasRegistry.registerAlias("UNPOOLED", UnpooledDataSourceFactory.class); typeAliasRegistry.registerAlias("PERPETUAL", PerpetualCache.class); typeAliasRegistry.registerAlias("FIFO", FifoCache.class); typeAliasRegistry.registerAlias("LRU", LruCache.class); typeAliasRegistry.registerAlias("SOFT", SoftCache.class); typeAliasRegistry.registerAlias("WEAK", WeakCache.class); typeAliasRegistry.registerAlias("DB_VENDOR", VendorDatabaseIdProvider.class); typeAliasRegistry.registerAlias("XML", XMLLanguageDriver.class); typeAliasRegistry.registerAlias("RAW", RawLanguageDriver.class); typeAliasRegistry.registerAlias("SLF4J", Slf4jImpl.class); typeAliasRegistry.registerAlias("COMMONS_LOGGING", JakartaCommonsLoggingImpl.class); typeAliasRegistry.registerAlias("LOG4J", Log4jImpl.class); typeAliasRegistry.registerAlias("LOG4J2", Log4j2Impl.class); typeAliasRegistry.registerAlias("JDK_LOGGING", Jdk14LoggingImpl.class); typeAliasRegistry.registerAlias("STDOUT_LOGGING", StdOutImpl.class); typeAliasRegistry.registerAlias("NO_LOGGING", NoLoggingImpl.class); typeAliasRegistry.registerAlias("CGLIB", CglibProxyFactory.class); typeAliasRegistry.registerAlias("JAVASSIST", JavassistProxyFactory.class); languageRegistry.setDefaultDriverClass(XMLLanguageDriver.class); languageRegistry.register(RawLanguageDriver.class);&#125; 说明： 第一个构造方法是通过Environment进行初始化，并调用第二个构造方法，Environment封装了数据源和事务，不同Environment下使用的数据源和事务不一样，例如，实际使用中会有生产和测试环境。 第二个构造方法，主要是注册一些别名并设置默认的语言驱动。 Configuration内部维护了以下几种注册器： MapperRegistry：管理Mapper接口 TypeHandlerRegistry：管理类型处理器 TypeAliasRegistry：注册别名，别名都是用的大写 LanguageDriverRegistry：管理语言驱动 这些注册器内部都会维护一个或多个map，然后提供注册register的方法，并且他们都是需要实例化才能使用的，而不是单例的。在一个MyBatis应用的整个生命周期中，只会存在一个Configuration实例。 Configuration内部有一些Map类型的属性： 1234567protected final Map&lt;String, MappedStatement&gt; mappedStatements = new StrictMap&lt;MappedStatement&gt;("Mapped Statements collection");protected final Map&lt;String, Cache&gt; caches = new StrictMap&lt;Cache&gt;("Caches collection");protected final Map&lt;String, ResultMap&gt; resultMaps = new StrictMap&lt;ResultMap&gt;("Result Maps collection");protected final Map&lt;String, ParameterMap&gt; parameterMaps = new StrictMap&lt;ParameterMap&gt;("Parameter Maps collection");protected final Map&lt;String, KeyGenerator&gt; keyGenerators = new StrictMap&lt;KeyGenerator&gt;("Key Generators collection");protected final Map&lt;String, XNode&gt; sqlFragments = new StrictMap&lt;XNode&gt;("XML fragments parsed from previous mappers"); 可以看到，实际上是使用的StrictMap。每一个StrictMap都有一个名称，put和get操作时需要判断key是否为空，如果为空，则抛出异常。 Configuration内部有一些Collection类型的属性： 1234protected final Collection&lt;XMLStatementBuilder&gt; incompleteStatements = new LinkedList&lt;XMLStatementBuilder&gt;();protected final Collection&lt;CacheRefResolver&gt; incompleteCacheRefs = new LinkedList&lt;CacheRefResolver&gt;();protected final Collection&lt;ResultMapResolver&gt; incompleteResultMaps = new LinkedList&lt;ResultMapResolver&gt;();protected final Collection&lt;MethodResolver&gt; incompleteMethods = new LinkedList&lt;MethodResolver&gt;(); 为什么接口使用的是Collection而不是List？为什么实现类使用的是LinkedList而不是ArrayList？ 更多的配置参数说明，见XML 映射配置文件。 Configuration使用的一个例子： 12345678DataSource dataSource = ...;TransactionFactory transactionFactory = new JdbcTransactionFactory();Environment environment = new Environment("Production", transactionFactory, dataSource);Configuration configuration = new Configuration(environment);configuration.setLazyLoadingEnabled(true);configuration.getTypeAliasRegistry().registerAlias(Blog.class);configuration.addMapper(BlogMapper.class);SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(configuration); 总结： Configuration类保存着MyBatis的所有配置参数，并以此为入口为其他服务，如缓存、执行器等等，提供了一些接口，这样方便集中管理和维护代码，但似乎又违背了单一原则？所有配置参数都可以通过Configuration来设置，意味着所有的配置都是可替代的，这样就非常灵活了。 参考文章 Java框架篇—Mybatis 入门 【mybatis源码解析】整体架构解析]]></content>
      <categories>
        <category>mybatis</category>
      </categories>
      <tags>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot Profile使用]]></title>
    <url>%2F2016%2F02%2F22%2Fprofile-usage-in-spring-boot%2F</url>
    <content type="text"><![CDATA[Spring Boot使用@Profile注解可以实现不同环境下配置参数的切换，任何@Component或@Configuration注解的类都可以使用@Profile注解。 例如： 12345@Configuration@Profile("production")public class ProductionConfiguration &#123; // ...&#125; 通常，一个项目中可能会有多个profile场景，例如下面为test场景： 12345@Configuration@Profile("test")public class TestConfiguration &#123; // ...&#125; 在存在多个profile情况下，你可以使用spring.profiles.active来设置哪些profile被激活。spring.profiles.include属性用来设置无条件的激活哪些profile。 例如，你可以在application.properties中设置： 1spring.profiles.active=dev,hsqldb 或者在application.yaml中设置： 1spring.profiles.active:dev,hsqldb spring.profiles.active属性可以通过命令行参数或者资源文件来设置，其查找顺序，请参考Spring Boot特性。 自定义Profile注解@Profile注解需要接受一个字符串，作为场景名。这样每个地方都需要记住这个字符串。Spring的@Profile注解支持定义在其他注解之上，以创建自定义场景注解。 12345@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Profile("dev")public @interface Dev &#123;&#125; 这样就创建了一个@Dev注解，该注解可以标识bean使用于@Dev这个场景。后续就不再需要使用@Profile(&quot;dev&quot;)的方式。这样即可以简化代码，同时可以利用IDE的自动补全:) 多个Profile例子下面是一个例子： 12345package com.javachen.example.service;public interface MessageService &#123; String getMessage();&#125; 对于MessageService接口，我们可以有生产和测试两种实现： 123456789101112131415161718package com.javachen.example.service;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Profile;import org.springframework.stereotype.Component;@Component@Profile(&#123; "dev" &#125;)public class HelloWorldService implements MessageService&#123; @Value("$&#123;name:World&#125;") private String name; public String getMessage() &#123; return "Hello " + this.name; &#125;&#125; 12345678910111213141516171819202122package com.javachen.example.service;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Profile;import org.springframework.stereotype.Component;@Component@Profile(&#123; "prod" &#125;)public class GenericService implements MessageService &#123; @Value("$&#123;hello:Hello&#125;") private String hello; @Value("$&#123;name:World&#125;") private String name; @Override public String getMessage() &#123; return this.hello + " " + this.name; &#125;&#125; Application类为： 12345678910111213141516171819@SpringBootApplicationpublic class Application implements CommandLineRunner &#123; private static final Logger logger = LoggerFactory.getLogger(Application.class); @Autowired private MessageService messageService; @Override public void run(String... args) &#123; logger.info(this.messageService.getMessage()); if (args.length &gt; 0 &amp;&amp; args[0].equals("exitcode")) &#123; throw new ExitException(); &#125; &#125; public static void main(String[] args) throws Exception &#123; SpringApplication.run(Application.class, args); &#125;&#125; 实际使用中，使用哪个profile由spring.profiles.active控制，你在resources/application.properties中定义spring.profiles.active=XXX，或者通过-Dspring.profiles.active=XXX。XXX可以是dev或者prod或者dev,prod。需要注意的是：本例中是将@Profile用在Service类上，一个Service接口不能同时存在超过两个实现类，故本例中不能同时使用dev和prod。 通过不同的profile，可以有对应的资源文件application-{profile}.properties。例如，application-dev.properties内容如下： 1name=JavaChen-dev application-prod.properties内容如下： 1name=JavaChen-prod 接下来进行测试。spring.profiles.active=dev时，运行Application类，查看日志输出。 12016-02-22 15:45:18,470 [main] INFO com.javachen.example.Application - Hello JavaChen-dev spring.profiles.active=prod时，运行Application类，查看日志输出。 12016-02-22 15:47:21,270 [main] INFO com.javachen.example.Application - Hello JavaChen-prod logback配置多Profile在resources目录下添加logback-spring.xml，并分别对dev和prod进行配置： 1234567891011121314151617181920212223242526272829303132&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration&gt; &lt;!--&lt;include resource="org/springframework/boot/logging/logback/base.xml" /&gt;--&gt; &lt;springProfile name="dev"&gt; &lt;logger name="com.javachen.example" level="TRACE" /&gt; &lt;appender name="LOGFILE" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;/springProfile&gt; &lt;springProfile name="prod"&gt; &lt;appender name="LOGFILE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;File&gt;log/server.log&lt;/File&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;FileNamePattern&gt;log/server_%d&#123;yyyy-MM-dd&#125;.log.zip&lt;/FileNamePattern&gt; &lt;/rollingPolicy&gt; &lt;layout class="ch.qos.logback.classic.PatternLayout"&gt; &lt;pattern&gt;%date [%thread] %-5level %logger&#123;80&#125; - %msg%n&lt;/pattern&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;/springProfile&gt; &lt;root level="info"&gt; &lt;appender-ref ref="LOGFILE" /&gt; &lt;/root&gt; &lt;logger name="com.javachen.example" level="DEBUG" /&gt;&lt;/configuration&gt; 这样，就可以做到不同profile场景下的日志输出不一样。 maven中的场景配置使用maven的resource filter可以实现多场景切换。 123456789101112131415161718192021222324252627282930&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;prod&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;properties&gt; &lt;build.profile.id&gt;prod&lt;/build.profile.id&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;properties&gt; &lt;build.profile.id&gt;dev&lt;/build.profile.id&gt; &lt;/properties&gt; &lt;/profile&gt;&lt;/profiles&gt;&lt;build&gt; &lt;filters&gt; &lt;filter&gt;application-$&#123;build.profile.id&#125;.properties&lt;/filter&gt; &lt;/filters&gt; &lt;resources&gt; &lt;resource&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;/resource&gt; &lt;/resources&gt;&lt;/build&gt; 这样在maven编译时，可以通过-P参数指定maven profile即可。 总结使用Spring Boot的Profile注解可以实现多场景下的配置切换，方便开发中进行测试和部署生产环境。 本文中相关代码在github上面。 参考文章 Spring 3.1 M1: Introducing @Profile spring boot profile试用]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring boot</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DevTools in Spring Boot]]></title>
    <url>%2F2016%2F02%2F22%2Fdevtools-in-spring-boot%2F</url>
    <content type="text"><![CDATA[本文主要了解Spring Boot 1.3.0新添加的spring-boot-devtools模块的使用，该模块主要是为了提高开发者开发Spring Boot应用的用户体验。 要想使用该模块需要在Maven中添加： 123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 或者在Gradle配置文件中添加： 123dependencies &#123; compile(&quot;org.springframework.boot:spring-boot-devtools&quot;)&#125; 1、默认属性在Spring Boot集成Thymeleaf时，spring.thymeleaf.cache属性设置为false可以禁用模板引擎编译的缓存结果。 现在，devtools会自动帮你做到这些，禁用所有模板的缓存，包括Thymeleaf, Freemarker, Groovy Templates, Velocity, Mustache等。 更多的属性，请参考DevToolsPropertyDefaultsPostProcessor。 2、自动重启你可能使用过 JRebel 或者 Spring Loaded来自动重启应用，现在只需要引入devtools就可以了，当代码变动时，它会自动进行重启应用。 当然，你也可以使用插件来实现，例如在maven中配置插件： 1234567891011&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;fork&gt;true&lt;/fork&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 在gradle中，配置： 123bootRun &#123; addResources = true&#125; DevTools在重启过程中依赖于application context的shutdown hook，如果设置SpringApplication.setRegisterShutdownHook(false)，则自动重启将不起作用。 排除静态资源文件静态资源文件在改变之后有时候没必要触发应用程序重启，例如，Thymeleaf的模板会被变量替代。默认的，/META-INF/maven、/META-INF/resources、/resources、/static、/public或者/templates这些目录下的文件修改之后不会触发重启但是会触发LiveReload。可以通过spring.devtools.restart.exclude属性来修改默认值。 如果你想保留默认配置，并添加一些额外的路径，可以使用spring.devtools.restart.additional-exclude属性。 观察额外的路径如果你想观察不在classpath中的路径的文件变化并触发重启，则可以配置 spring.devtools.restart.additional-paths 属性。 关闭自动重启设置 spring.devtools.restart.enabled 属性为false，可以关闭该特性。可以在application.properties中设置，也可以通过设置环境变量的方式。 1234public static void main(String[] args) &#123; System.setProperty("spring.devtools.restart.enabled", "false"); SpringApplication.run(MyApp.class, args);&#125; 使用一个触发文件通过设置spring.devtools.restart.trigger-file属性指定一个文件，当该文件被修改时，则触发自动重启。 自定义自动重启类加载器Spring Boot自动重启使用的是两个类加载器，大多数情况下工作良好，有时候会出现问题。 默认的，IDE中打开的项目会使用 restart 类加载器进行加载，而任何其他的 .jar 文件会使用 base 类加载器进行加载。如果你使用的是多模块的项目，并且有些模块没有被导入到IDE，你需要创建并编辑META-INF/spring-devtools.properties文件来自定义一些配置。 spring-devtools.properties文件包含有 restart.exclude. 和 restart.include. 前缀的属性。include属性文件的都会被加入到restart类加载器，exclude属性文件的都会被加入到base类加载器，他们的值是正则表达式，所有的属性值必须是唯一的。 例如： 12restart.include.companycommonlibs=/mycorp-common-[\\w-]+\.jarrestart.include.projectcommon=/mycorp-myproj-[\\w-]+\.jar classpath中的所有META-INF/spring-devtools.properties文件都会被加载。 已知的限制如果对象是使用ObjectInputStream进行反序列化，则自动重启将不可用。如果你需要反序列化对象，则你需要使用spring的ConfigurableObjectInputStream并配合Thread.currentThread().getContextClassLoader()进行反序列化。 3、LiveReload在浏览器方面，DevTools内置了一个LiveReload服务，可以自动刷新浏览器。如果你使用JRebel，则自动重启将会失效，取而代之的是使用动态加载类文件。当然，其他的DevTools（例如LiveReload和属性覆盖）特性还能使用。 该特性可以通过spring.devtools.livereload.enabled属性来设置是否开启。 4、全局设置可以在 $HOME 目录下创建一个.spring-boot-devtools.properties文件来设置全局的配置。 例如，设置一个触发文件类触发重启： 1spring.devtools.reload.trigger-file=.reloadtrigger 5、远程应用DevTools不仅可以用于本地应用，也可以用于远程应用。通过设置 spring.devtools.remote.secret 属性可以开启远程应用。 远程DevTools支持包括两个部分，服务端应用和你IDE中运行的本地客户端应用。当设置spring.devtools.remote.secret属性之后，服务端应用自动开启DevTools特性，客户端程序需要手动启动。 运行远程客户端应用远程客户端应用被设计来运行在你的IDE中。你需要使用和你连接的远程应用相同的classpath来运行org.springframework.boot.devtools.RemoteSpringApplication类，传递给该类的必选参数是你连接的应用的url。 例如，如果你在使用Eclipse或者STS，并且你有一个 my-app 应用部署在Cloud Foundry，你可以按照以下步骤操作： 从Run菜单运行`Run Configurations…`` 创建一个Java Application的启动配置 浏览my-app项目 使用org.springframework.boot.devtools.RemoteSpringApplication作为main类 添加https://myapp.cfapps.io到the Program arguments 一个运行中的远程客户端将会是这样子： 12345678910111213 . ____ _ __ _ _ /\\ / ___'_ __ _ _(_)_ __ __ _ ___ _ \ \ \ \( ( )\___ | '_ | '_| | '_ \/ _` | | _ \___ _ __ ___| |_ ___ \ \ \ \ \\/ ___)| |_)| | | | | || (_| []::::::[] / -_) ' \/ _ \ _/ -_) ) ) ) ) ' |____| .__|_| |_|_| |_\__, | |_|_\___|_|_|_\___/\__\___|/ / / / =========|_|==============|___/===================================/_/_/_/ :: Spring Boot Remote :: 1.3.2.RELEASE2015-06-10 18:25:06.632 INFO 14938 --- [ main] o.s.b.devtools.RemoteSpringApplication : Starting RemoteSpringApplication on pwmbp with PID 14938 (/Users/pwebb/projects/spring-boot/code/spring-boot-devtools/target/classes started by pwebb in /Users/pwebb/projects/spring-boot/code/spring-boot-samples/spring-boot-sample-devtools)2015-06-10 18:25:06.671 INFO 14938 --- [ main] s.c.a.AnnotationConfigApplicationContext : Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@2a17b7b6: startup date [Wed Jun 10 18:25:06 PDT 2015]; root of context hierarchy2015-06-10 18:25:07.043 WARN 14938 --- [ main] o.s.b.d.r.c.RemoteClientConfiguration : The connection to http://localhost:8080 is insecure. You should use a URL starting with 'https://'.2015-06-10 18:25:07.074 INFO 14938 --- [ main] o.s.b.d.a.OptionalLiveReloadServer : LiveReload server is running on port 357292015-06-10 18:25:07.130 INFO 14938 --- [ main] o.s.b.devtools.RemoteSpringApplication : Started RemoteSpringApplication in 0.74 seconds (JVM running for 1.105) 说明： 因为远程客户端和远程应用使用的是相同的classpath，所以远程客户端可以直接读取应用的配置文件。所以spring.devtools.remote.secret属性能够被读取到并传递给服务端进行校验。 远程url建议使用更加安全的https://协议 如果需要使用代理，则需要设置spring.devtools.remote.proxy.host和spring.devtools.remote.proxy.port属性。 远程更新远程客户端会监控你的应用的classpath的改变和本地重启的方式一样。任何更新的资源将会被推送到远程应用并且（如果有必要）触发一个重启。这在你集成一个使用云服务的本地不存在的特性时会是非常有用的。通常远程的更新和重启比一个完整的重新编译和部署周期会快的多。 只有在远程客户端运行的过程中，文件才会被监控。如果你在启动远程客户端之前，修改一个文件，其将不会被推送到远程应用。 远程调试远程调试默认使用的端口是8000，你可以通过spring.devtools.remote.debug.local-port来修改。 你可以通过查看JAVA_OPTS来看远程调试是否被启用，主要是观察是否有-Xdebug -Xrunjdwp:server=y,transport=dt_socket,suspend=n参数 参考文章 Spring Boot Developer tools Spring Boot新模块devtools maven工程使用spring-boot-devtools进行热部署，更改代码避免重启web容器 spring-boot下热部署热启动方案测试汇总]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring boot</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala Reading List]]></title>
    <url>%2F2016%2F01%2F23%2Fscala-reading-list%2F</url>
    <content type="text"><![CDATA[学习教程 为 Java程序员准备的Scala教程 Scala 初学指南：本书是 The Neophyte’s Guide to Scala 的中文翻译，是 Daniel Westheide 写的一系列有关 Scala 的文章。 http://ifeve.com/tag/scala/ Scala 指南：开始精彩的Scala旅程 Scala 课堂 Scala社区 Scala中文社区 Scala Google中文社区 Scala 开发教程 http://dirlt.com/scala.html Scala Tutorial: Getting Started with Scala Scala边练边学 Databricks Scala 编程风格指南 How to get started with Akka Streams? http://www.infoq.com/cn/scala/：Infoq上关于Scala的文章 博客： http://hongjiang.info/：有关jvm,scala与后端架构 鸟窝 Scala.js Play中文文档]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bash内部变量]]></title>
    <url>%2F2015%2F07%2F09%2Fbash-internal-variables%2F</url>
    <content type="text"><![CDATA[Bash中存在一些内部变量。 $BASHBash的二进制程序文件的路径。 12$ echo $BASH/bin/bash $BASH_ENV这个环境变量会指向一个Bash的启动文件，当一个脚本被调用的时候，这个启动文件将会被读取。 $BASH_SUBSHELL这个变量用来提示子shell的层次。这是一个Bash的新特性，直到版本3的Bash才被引入近来。 123456789101112131415161718192021222324#!/bin/bash# subshell.shecho "Subshell level OUTSIDE subshell = $BASH_SUBSHELL"outer_variable=Outer(echo "Subshell level INSIDE subshell = $BASH_SUBSHELL"inner_variable=Inner echo "From subshell, \"inner_variable\" = $inner_variable"echo "From subshell, \"outer\" = $outer_variable")echo "Subshell level OUTSIDE subshell = $BASH_SUBSHELL"if [ -z "$inner_variable" ]then echo "inner_variable undefined in main body of shell"else echo "inner_variable defined in main body of shell"fiexit 0 $BASH_VERSINFO[n]这是一个含有6个元素的数组，它包含了所安装的Bash的版本信息。这与下边的$BASH_VERSION很相像，但是这个更加详细一些。 1234567891011for n in 0 1 2 3 4 5do echo "BASH_VERSINFO[$n] = $&#123;BASH_VERSINFO[$n]&#125;"done # BASH_VERSINFO[0] = 3 # 主版本号.# BASH_VERSINFO[1] = 2 # 次版本号.# BASH_VERSINFO[2] = 25 # 补丁次数.# BASH_VERSINFO[3] = 1 # 编译版本.# BASH_VERSINFO[4] = release # 发行状态.# BASH_VERSINFO[5] = x86_64-redhat-linux-gnu # 结构体系 $BASH_VERSION安装在系统上的Bash版本号。 12345bash$ echo $BASH_VERSION3.2.25(1)-releasetcsh% echo $BASH_VERSIONBASH_VERSION: Undefined variable. 检查$BASH_VERSION对于判断系统上到底运行的是哪个shell来说是一种非常好的方法。变量$SHELL有时候不能够给出正确的答案。 $DIRSTACK在目录栈中最顶端的值(将会受到pushd和popd的影响)。这个内建变量与dirs命令相符，但是dirs命令会显示目录栈的整个内容。 $EDITOR脚本所调用的默认编辑器，通常情况下是vi或者是emacs。 $EUID有效用户ID。不管当前用户被假定成什么用户，这个数都用来表示当前用户的标识号，也可能使用su命令来达到假定的目的。 $EUID并不一定与$UID相同。 $FUNCNAME当前函数的名字。 123xyz23 ()&#123; echo "$FUNCNAME now executing." # 打印: xyz23 now executing.&#125; $GLOBIGNORE一个文件名的模式匹配列表，如果在通配中匹配到的文件包含有这个列表中的某个文件，那么这个文件将被从匹配到的结果中去掉。 $GROUPS目前用户所属的组。这是一个当前用户的组id列表(数组)，与记录在/etc/passwd文件中的内容一样。 12345678root# echo $GROUPS0root# echo $&#123;GROUPS[1]&#125;1root# echo $&#123;GROUPS[5]&#125;6 $HOME用户的home目录。 $HOSTNAMEhostname放在一个初始化脚本中，在系统启动的时候分配一个系统名字。然而，gethostname()函数可以用来设置这个Bash内部变量$HOSTNAME。 1234567$ hostnamelocalhost$ cat /etc/sysconfig/networkNETWORKING=yesNETWORKING_IPV6=noHOSTNAME=localhost $HOSTTYPE主机类型，就像$MACHTYPE，用来识别系统硬件。 12bash$ echo $HOSTTYPEx86_64 $IFS内部域分隔符，这个变量用来决定Bash在解释字符串时如何识别域或者单词边界。 $IFS默认为空白(空格、制表符和换行符)，但这是可以修改的，比如，在分析逗号分隔的数据文件时，就可以设置为逗号。注意，$*使用的是保存在$IFS中的第一个字符。 123456789bash$ echo $IFS | cat -vte$bash$ bash -c 'set w x y z;echo "$*"'w x y zbash$ bash -c 'set w x y z; IFS=":-;"; echo "$*"'w:x:y:z#从字符串中读取命令，并分配参数给位置参数 $IFS处理其他字符与处理空白字符不同。 12345678910111213141516171819202122232425262728output_args_one_per_line()&#123; for arg do echo "[$arg]" done&#125;IFS=" "#空白做分隔符var=" a b c "output_args_one_per_line $var #[a]#[b]#[c]IFS=:#:做分隔符var=":a::b:c:::" output_args_one_per_line $var#[]#[a]#[]#[b]#[c]#[]#[] 上面，同样的事情也会发生在awk的”FS”域中。 $IGNOREEOF忽略EOF：告诉shell在log out之前要忽略多少文件结束符(control-D)。 $LC_COLLATE常在.bashrc或/etc/profile中设置，这个变量用来控制文件名扩展和模式匹配的展开顺序。如果$LC_COLLATE设置得不正确的话，LC_COLLATE会在文件名匹配中产生不可预料的结果。 $LC_CTYPE这个内部变量用来控制通配和模式匹配中的字符串解释。 $LINENO这个变量用来记录自身在脚本中所在的行号。这个变量只有在脚本使用这个变量的时候才有意义，并且这个变量一般用于调试目的。 123456# *** 调试代码块开始 ***last_cmd_arg=$_ # Save it.echo "At line number $LINENO, variable \"v1\" = $v1"echo "Last command argument processed = $last_cmd_arg"# *** 调试代码块结束 *** $MACHTYPE机器类型，标识系统的硬件。 12bash$ echo $MACHTYPEx86_64-redhat-linux-gnu $OLDPWD之前的工作目录。 $OSTYPE操作系统类型。 12bash$ echo $OSTYPElinux $PATH可执行文件的搜索路径，一般为/usr/bin/, /usr/X11R6/bin/, /usr/local/bin等等。 当给出一个命令时，shell会自动生成一张哈希表，并且在这张哈希表中按照path变量中所列出的路径来搜索这个可执行命令。路径会存储在环境变量中，$PATH变量本身就一个以冒号分隔的目录列表。通常情况下，系统都是在/etc/profile和~/.bashrc中存储$PATH的定义。 12echo $PATH/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin://usr/java/latest/bin:/root/bin 当前的工作目录，通常是不会出现在$PATH中的，这样做的目的是出于安全的考虑。 $PIPESTATUS这个数组变量将保存最后一个运行的前台管道的退出状态码。相当有趣的是，这个退出状态码和最后一个命令运行的退出状态码并不一定相同。 1234567891011121314bash$ echo $PIPESTATUS0bash$ ls -al | bogus_command-bash: bogus_command: command not foundbash$ echo $PIPESTATUS141bash$ ls -al | bogus_command-bash: bogus_command: command not foundbash$ echo $?127 $PIPESTATUS数组的每个成员都保存了运行在管道中的相应命令的退出状态码。$PIPESTATUS[0]保存管道中第一个命令的退出状态码，$PIPESTATUS[1]保存第二个命令的退出状态码，依此类推。 123bash$ who | grep nobody | sortbash$ echo $&#123;PIPESTATUS[*]&#125;0 1 0 在某些上下文中，变量$PIPESTATUS可能不会给出期望的结果。 123456bash$ $ ls | bogus_command | wcbash: bogus_command: command not found 0 0 0bash$ echo $&#123;PIPESTATUS[@]&#125;141 127 0 上边输出不正确的原因归咎于ls的行为。因为如果把ls的结果放到管道上，并且这个输出并没有被读取，那么SIGPIPE将会杀掉它，同时退出状态码变为141，而不是我们所期望的0。这种情况也会发生在tr命令中。 $PPID进程的$PPID就是这个进程的父进程的进程ID。 $PROMPT_COMMAND这个变量保存了在主提示符$PS1显示之前需要执行的命令。 $PS1这是主提示符，可以在命令行中见到它。 $PS2第二提示符，当你需要额外输入的时候，你就会看到它，默认显示&gt;。 $PS3第三提示符，它在一个select循环中显示。 12345678910111213#!/bin/bashPS3='Choose your favorite vegetable: ' # 设置提示符字串.select vegetable in "beans" "carrots" "potatoes" "onions" "rutabagas"do echo "Your favorite veggie is $vegetable." echo "Yuck!" break # 如果这里没有 'break' 会发生什么?doneexit 0 $PS4第四提示符，当你使用-x选项来调用脚本时，这个提示符会出现在每行输出的开头。默认显示+。 $PWD工作目录，你当前所在的目录。 $REPLY当没有参数变量提供给read命令的时候，这个变量会作为默认变量提供给read命令，也可以用于select菜单，但是只提供所选择变量的编号，而不是变量本身的值。 12345678910111213141516171819#!/bin/bash# reply.sh# REPLY是提供给'read'命令的默认变量.echo -n "What is your favorite vegetable? "readecho "Your favorite vegetable is $REPLY."# 当且仅当没有变量提供给"read"命令时，REPLY才保存最后一个"read"命令读入的值echo -n "What is your favorite fruit? "read fruitecho "Your favorite fruit is $fruit."echo "but..."echo "Value of \$REPLY is still $REPLY."# $REPLY还是保存着上一个read命令的值，因为变量$fruit被传入到了这个新的"read"命令中exit 0 $SECONDS这个脚本已经运行的时间(以秒为单位)。 12345678910111213141516171819202122232425#!/bin/bashTIME_LIMIT=10INTERVAL=1echo "Hit Control-C to exit before $TIME_LIMIT seconds."while [ "$SECONDS" -le "$TIME_LIMIT" ]do if [ "$SECONDS" -eq 1 ] then units=second else units=seconds fi echo "This script has been running $SECONDS $units." # 在一台比较慢或者是附载过大的机器上, #+ 在单次循环中, 脚本可能会忽略计数. sleep $INTERVALdoneecho -e "\a" # Beep!(哔哔声!)exit 0 $SHELLOPTSshell中已经激活的选项的列表，这是一个只读变量。 12bash$ echo $SHELLOPTSbraceexpand:emacs:hashall:histexpand:history:interactive-comments:monitor $SHLVLShell级别，就是Bash被嵌套的深度。如果是在命令行中，那么$SHLVL为1，如果在脚本中那么$SHLVL为2。 $TMOUT如果$TMOUT环境变量被设置为非零值time的话，那么经过time秒后，shell提示符将会超时，这将会导致登出(logout)。 在2.05b版本的Bash中, $TMOUT变量与命令read可以在脚本中结合使用. 12345678910111213141516# 只能够在Bash脚本中使用, 必须使用2.05b或之后版本的Bash.TMOUT=3 # 提示输入时间为3秒.echo "What is your favorite song?"echo "Quickly now, you only have $TMOUT seconds to answer!"read songif [ -z "$song" ]then song="(no answer)" # 默认响应.fi echo "Your favorite song is $song." 有更加复杂的办法可以在脚本中实现定时输入。一种办法就是建立一个定式循环，当超时的时候给脚本发个信号。不过这也需要有一个信号处理例程能够捕捉由定时循环所产生的中断。 定时输入的例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445#!/bin/bash# timed-input.shTIMELIMIT=3 # 这个例子中设置的是3秒. 也可以设置为其他的时间值.PrintAnswer()&#123; if [ "$answer" = TIMEOUT ] then echo $answer else # 别和上边的例子弄混了. echo "Your favorite veggie is $answer" kill $! # 不再需要后台运行的TimerOn函数了, kill了吧. # $! 变量是上一个在后台运行的作业的PID. fi&#125; TimerOn()&#123; sleep $TIMELIMIT &amp;&amp; kill -s 14 $$ &amp; # 等待3秒, 然后给脚本发送一个信号.&#125; Int14Vector()&#123; answer="TIMEOUT" PrintAnswer exit 14&#125; trap Int14Vector 14 # 定时中断(14)会暗中给定时间限制. echo "What is your favorite vegetable "TimerOnread answerPrintAnswer# 无可否认, 这是一个定时输入的复杂实现, #+ 然而"read"命令的"-t"选项可以简化这个任务. # 参考后边的"t-out.sh".# 如果你需要一个真正优雅的写法...#+ 建议你使用C或C++来重写这个应用, #+ 你可以使用合适的函数库, 比如'alarm'和'setitimer'来完成这个任务.exit 0 另一种选择是使用stty。 可能最简单的办法就是使用-t选项来read了。 1234567891011121314151617#!/bin/bash# t-out.shTIMELIMIT=4 # 4秒read -t $TIMELIMIT variable &lt;&amp;1# ^^^# 在这个例子中，对于Bash 1.x和2.x就需要"&lt;&amp;1"了，但是Bash 3.x就不需要.if [ -z "$variable" ] # 值为null?then echo "Timed out, variable still unset."else echo "variable = $variable"fi exit 0 $UID用户ID号，当前用户的用户标识号，记录在/etc/passwd文件中。 这是当前用户的真实id，即使只是通过使用su命令来临时改变为另一个用户标识，这个id也不会被改变。$UID是一个只读变量，不能在命令行或者脚本中修改它，并且和id内建命令很相像。 变量$ENV、$LOGNAME、$MAIL、$TERM、$USER和$USERNAME都不是Bash的内建变量。然而这些变量经常在Bash的启动文件中被当作环境变量来设置。$SHELL是用户登陆shell的名字，它可以在/etc/passwd中设置，或者也可以在”init”脚本中设置，并且它也不是Bash内建的。 位置参数$0, $1, $2位置参数，从命令行传递到脚本，或者传递给函数，或者set给变量。 $#命令行参数或者位置参数的个数。 $*所有的位置参数都被看作为一个单词。$*必须被引用起来。 $@与$*相同，但是每个参数都是一个独立的引用字符串，这就意味着，参数是被完整传递的，并没有被解释或扩展。这也意味着，参数列表中每个参数都被看作为单独的单词。当然，$@应该被引用起来。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/bin/bash# arglist.sh# 多使用几个参数来调用这个脚本，比如"one two three"E_BADARGS=65if [ ! -n "$1" ]then echo "Usage: `basename $0` argument1 argument2 etc." exit $E_BADARGSfiechoindex=1 echo "Listing args with \"\$*\":"for arg in "$*" # 如果"$*"不被""引用，那么将不能正常地工作do echo "Arg #$index = $arg" let "index+=1"done# $* 将所有的参数看成一个单词echoindex=1 echo "Listing args with \"\$@\":"for arg in "$@" # 如果"$*"不被""引用，那么将不能正常地工作do echo "Arg #$index = $arg" let "index+=1"done # $@ 把每个参数都看成是单独的单词echoindex=1 echo "Listing args with \$*:"for arg in $*do echo "Arg #$index = $arg" let "index+=1"done# 未引用的$*将会把参数看成单独的单词exit 0 shift命令执行以后, $@将会保存命令行中剩余的参数, 但是没有之前的$1, 因为被丢弃了. 1234567891011#!/bin/bash# 使用 ./scriptname 1 2 3 4 5 来调用这个脚本echo "$@" # 1 2 3 4 5shiftecho "$@" # 2 3 4 5shiftecho "$@" # 3 4 5# 每次"shift"都会丢弃$1.# "$@" 将包含剩下的参数. $@也可以作为工具使用，用来过滤传递给脚本的输入。cat &quot;$@&quot;结构既可以接受从stdin传递给脚本的输入，也可以接受从参数中指定的文件中传递给脚本的输入。 1234567891011#!/bin/bash# rot13.sh: 典型的rot13算法# 用法: ./rot13.sh filename# 或 ./rot13.sh &lt;filename# 或 ./rot13.sh and supply keyboard input (stdin)cat "$@" | tr 'a-zA-Z' 'n-za-mN-ZA-M' # "a"变为"n"，"b"变为"o"，等等# 'cat "$@"'结构允许从stdin或者从文件中获得输入. exit 0 $*和$@中的参数有时候会表现出不一致而且令人迷惑的行为，这都依赖于$IFS的设置。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137#!/bin/bash# 内部Bash变量"$*"和"$@"的古怪行为,#+ 都依赖于它们是否被双引号引用起来.# 单词拆分与换行的不一致的处理.set -- "First one" "second" "third:one" "" "Fifth: :one"# 设置这个脚本的参数, $1, $2, 等等echo 'IFS unchanged, using "$*"'c=0for i in "$*" # 引用起来do echo "$((c+=1)): [$i]" # 这行在下边每个例子中都一样doneecho ---echo 'IFS unchanged, using $*'c=0for i in $* # 未引用do echo "$((c+=1)): [$i]" doneecho ---echo 'IFS unchanged, using "$@"'c=0for i in "$@" do echo "$((c+=1)): [$i]" doneecho ---echo 'IFS unchanged, using $@'c=0for i in $@ do echo "$((c+=1)): [$i]" doneecho ---IFS=:echo 'IFS=":", using "$*"'c=0for i in "$*" do echo "$((c+=1)): [$i]" doneecho ---echo 'IFS=":", using $*'c=0for i in $* do echo "$((c+=1)): [$i]" doneecho ---var=$*echo 'IFS=":", using "$var" (var=$*)'c=0for i in "$var"do echo "$((c+=1)): [$i]"doneecho ---echo 'IFS=":", using $var (var=$*)'c=0for i in $vardo echo "$((c+=1)): [$i]"doneecho ---var="$*"echo 'IFS=":", using $var (var="$*")'c=0for i in $vardo echo "$((c+=1)): [$i]"doneecho ---echo 'IFS=":", using "$var" (var="$*")'c=0for i in "$var"do echo "$((c+=1)): [$i]"doneecho ---echo 'IFS=":", using "$@"'c=0for i in "$@"do echo "$((c+=1)): [$i]"doneecho ---echo 'IFS=":", using $@'c=0for i in $@do echo "$((c+=1)): [$i]"doneecho --- var=$@echo 'IFS=":", using $var (var=$@)'c=0for i in $vardo echo "$((c+=1)): [$i]"doneecho --- echo 'IFS=":", using "$var" (var=$@)'c=0for i in "$var"do echo "$((c+=1)): [$i]"doneecho ---var="$@"echo 'IFS=":", using "$var" (var="$@")'c=0for i in "$var"do echo "$((c+=1)): [$i]"doneecho ---echo 'IFS=":", using $var (var="$@")'c=0for i in $vardo echo "$((c+=1)): [$i]"doneecho# 使用ksh或者zsh -y来试试这个脚本.exit 0 $@与$*中的参数只有在被双引号引用起来的时候才会不同。 当$IFS值为空时, $*和$@的行为依赖于正在运行的Bash或者sh的版本。 12345678910111213141516#!/bin/bash# 如果$IFS被设置，但其值为空，那么"$*"和"$@"将不会像期望的那样显示位置参数.mecho () # 打印位置参数&#123; echo "$1,$2,$3";&#125;IFS="" # 设置了，但值为空set a b c # 位置参数mecho "$*" # abc,,mecho $* # a,b,cmecho $@ # a,b,cmecho "$@" # a,b,c 其他的特殊参数$-传递给脚本的标记(使用set命令)。 $!运行在后台的最后一个作业的PID。 1sleep $&#123;TIMEOUT&#125;; eval 'kill -9 $!' &amp;&gt; /dev/null; $_这个变量保存之前执行的命令的最后一个参数的值。 12345678910111213#!/bin/bashecho $_ # /bin/bash # 只是调用/bin/bash来运行这个脚本du &gt;/dev/null # 这么做命令行上将没有输出echo $_ # duls -al &gt;/dev/null # 这么做命令行上将没有输出.echo $_ # -al (这是最后的参数):echo $_ # : $?命令、函数或者是脚本本身的退出状态码。 $$脚本自身的进程ID。$$变量在脚本中经常用来构造”唯一的”临时文件名。这么做通常比调用mktemp命令来的简单。 123456789#!/bin/bash# temp.shmktempTMPFILE=/tmp/ftp.$$echo $TMPFILEexit 0 运行脚本，会输出结果： 12/tmp/tmp.lrRiA13050/tmp/ftp.13049 参考文章 高级Bash脚本编程指南-中文版]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bash条件判断]]></title>
    <url>%2F2015%2F07%2F08%2Fbash-if-else%2F</url>
    <content type="text"><![CDATA[每个完整并且合理的程序语言都具有条件判断的功能，并且可以根据条件测试的结果做下一步的处理。Bash有test命令、各种中括号和圆括号操作，和if/then结构。 条件测试if/then结构用来判断命令列表的退出状态码是否为0。 有一个专有命令[(左中括号，特殊字符)。这个命令与test命令等价，并且出于效率上的考虑，这是一个内建命令。这个命令把它的参数作为比较表达式或者作为文件测试，并且根据比较的结果来返回一个退出状态码(0 表示真，1表示假)。 1234567if [ 0 ] then echo "0 is true."else echo "0 is false."fi # 0 is true. 在版本2.02的Bash中，引入了[[ ... ]]扩展测试命令，因为这种表现形式可能对某些语言的程序员来说更容易熟悉一些。 注意：[[是一个关键字，并不是一个命令。 123[[ 1 &lt; 3 ]]echo $? # 0 Bash把[[ $a -lt $b ]]看作一个单独的元素，并且返回一个退出状态码。(( ... ))和let ...结构也能够返回退出状态码，当它们所测试的算术表达式的结果为非零的时候，将会返回退出状态码0。这些算术扩展结构被用来做算术比较。 1234567$ let "1&lt;2"$ echo $?0$ (( 0 &amp;&amp; 1 ))$ echo $?1 if命令能够测试任何命令，并不仅仅是中括号中的条件。 123456789101112131415161718192021222324252627282930313233cmp a b &amp;&gt; /dev/null echo $? # 2if cmp a b &amp;&gt; /dev/null # 禁止输出.then echo "Files a and b are identical."else echo "Files a and b differ."fi# 非常有用的"if-grep"结构:if grep -q bash /etc/profilethen echo "File contains at least one occurrence of Bash."fiword=Linuxletter_sequence=inuif echo "$word" | grep -q "$letter_sequence"# "-q" 选项是用来禁止输出的.then echo "$letter_sequence found in $word"else echo "$letter_sequence not found in $word"fiif COMMAND_WHOSE_EXIT_STATUS_IS_0_UNLESS_ERROR_OCCURREDthen echo "Command succeeded."else echo "Command failed."fi 条件判断主要判断的是条件是否为真或者假。那么，在什么情况下才为真呢？ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# 0 为真if [ 0 ] then echo "0 is true."else echo "0 is false."fi# 1 为真if [ 1 ] then echo "1 is true."else echo "1 is false."fi# -1 为真if [ -1 ] then echo "-1 is true."else echo "-1 is false."fi# NULL 为假if [ ] then echo "NULL is true."else echo "NULL is false."fi# 随便的一串字符为真if [ xyz ] then echo "Random string is true."else echo "Random string is false."fi# 未初始化的变量为假if [ $xyz ] then echo "Uninitialized variable is true."else echo "Uninitialized variable is false."fi# 更加正规的条件检查if [ -n "$xyz" ] then echo "Uninitialized variable is true."else echo "Uninitialized variable is false."fixyz= # 初始化了, 但是赋null值# null变量为假if [ -n "$xyz" ] then echo "Null variable is true."else echo "Null variable is false."fi# "false" 为真if [ "false" ] then echo "\"false\" is true." else echo "\"false\" is false." fi# 再来一个, 未初始化的变量# "$false" 为假if [ "$false" ] then echo "\"\$false\" is true." else echo "\"\$false\" is false." fi [这个命令与test命令等价，把它的参数作为比较表达式或者作为文件测试，并且根据比较的结果来返回一个退出状态码(0 表示真，1表示假)，如果参数为确定的一个值或者有初始化，则退出状态码为0；否者为空值或者未初始化，则为1。上面例子也可以通过状态码来验证： 1234567891011121314151617181920$ [ 0 ] ; echo $?0$ [ 1 ] ; echo $?0$ [ -1 ] ; echo $?0$ [ ] ; echo $?1$ [ xyz ] ; echo $?0$ [ $xyz ] ; echo $?1$ [ -n "$xyz" ] ; echo $?1$ xyz= ; [ -n "$xyz" ] ; echo $?1$ [ "false" ] ; echo $?0$ [ "$false" ] ; echo $?1 如果if和then在条件判断的同一行上的话，必须使用分号来结束if表达式。if和then都是关键字，关键字(或者命令)如果作为表达式的开头，并且如果想在同一行上再写一个新的表达式的话，那么必须使用分号来结束上一句表达式。 if语句里还可以加上elif分支，elif是else if的缩写形式，作用是在外部的判断结构中再嵌入一个内部的if/then结构。 12345678910111213if [ condition1 ]then command1 command2 command3elif [ condition2 ]# 与else if一样then command4 command5else default-commandfi if test condition-true结构与if [ condition-true ]完全相同. 就像我们前面所看到的，左中括号是调用test命令的标识，而关闭条件判断用的的右中括号在if/test结构中并不是严格必需的，但是在Bash的新版本中必须要求使用。 注意：test命令在Bash中是内建命令，用来测试文件类型，或者用来比较字符串。因此，在Bash脚本中，test命令并不会调用外部的/usr/bin/test中的test命令，这是sh-utils工具包中的一部分。同样的，[也并不会调用/usr/bin/[，这是/usr/bin/test的符号链接。 下面测试type命令： 12345678910bash$ type testtest is a shell builtinbash$ type '['[ is a shell builtinbash$ type '[['[[ is a shell keywordbash$ type ']]']] is a shell keywordbash$ type ']'bash: type: ]: not found test、/usr/bin/test、[ ]和/usr/bin/[都是等价命令。 1234567891011121314151617181920212223242526272829if test -z "$1"then echo "No command-line arguments."else echo "First command-line argument is $1."fiif /usr/bin/test -z "$1"then echo "No command-line arguments."else echo "First command-line argument is $1."fiif [ -z "$1" ] # if [ -z "$1" 应该能够运行，但是Bash报错, 提示缺少关闭条件测试的右中括号then echo "No command-line arguments."else echo "First command-line argument is $1."fiif /usr/bin/[ -z "$1" ] # if /usr/bin/[ -z "$1" # 能够工作，但是还是给出一个错误消息。注意：在版本3.x的Bash中, 这个bug已经被修正了then echo "No command-line arguments."else echo "First command-line argument is $1."fi [[ ]]结构比[ ]结构更加通用。这是一个扩展的test命令，是从ksh88中引进的。在[[和]]之间所有的字符都不会发生文件名扩展或者单词分割，但是会发生参数扩展和命令替换。 123456file=/etc/passwdif [[ -e $file ]]then echo "Password file exists."fi 使用[[ ... ]]条件判断结构而不是[ ... ]，能够防止脚本中的许多逻辑错误。比如&amp;&amp;、||、&lt;和&gt;操作符能够正常存在于[[ ]]条件判断结构中,，但是如果出现在[ ]结构中的话，会报错。 在if后面也不一定非得是test命令或者是用于条件判断的中括号结构。 1234567dir=/home/bozoif cd "$dir" 2&gt;/dev/null; then # "2&gt;/dev/null" 会隐藏错误信息. echo "Now in $dir."else echo "Can't change to $dir."fi “if COMMAND”结构将会返回COMMAND的退出状态码。与此相似，在中括号中的条件判断也不一定非得要if不可，也可以使用列表结构。 123456var1=20var2=22[ "$var1" -ne "$var2" ] &amp;&amp; echo "$var1 is not equal to $var2"home=/home/bozo[ -d "$home" ] || echo "$home directory does not exist." (( ))结构扩展并计算一个算术表达式的值。如果表达式的结果为0，那么返回的退出状态码为1，或者是”假”。而一个非零值的表达式所返回的退出状态码将为0，或者是”true”。这种情况和先前所讨论的test命令和[ ]结构的行为正好相反。 1234567891011121314151617181920212223242526272829303132333435363738#!/bin/bash# 算术测试.# (( ... ))结构可以用来计算并测试算术表达式的结果. # 退出状态将会与[ ... ]结构完全相反!(( 0 ))echo "Exit status of \"(( 0 ))\" is $?." # 1(( 1 ))echo "Exit status of \"(( 1 ))\" is $?." # 0(( 5 &gt; 4 )) # 真echo "Exit status of \"(( 5 &gt; 4 ))\" is $?." # 0 (( 5 &gt; 9 )) # 假echo "Exit status of \"(( 5 &gt; 9 ))\" is $?." # 1(( 5 - 5 )) # 0echo "Exit status of \"(( 5 - 5 ))\" is $?." # 1(( 5 / 4 )) # 除法也可以.echo "Exit status of \"(( 5 / 4 ))\" is $?." # 0(( 1 / 2 )) # 除法的计算结果 &lt; 1.echo "Exit status of \"(( 1 / 2 ))\" is $?." # 截取之后的结果为 0. # 1(( 1 / 0 )) 2&gt;/dev/null # 除数为0, 非法计算. # ^^^^^^^^^^^echo "Exit status of \"(( 1 / 0 ))\" is $?." # 1# "2&gt;/dev/null"起了什么作用?# 如果这句被删除会怎样?(( 1 / 0 )) echo "Exit status of \"(( 1 / 0 ))\" is $?." # 2exit 0 就如文章开头所言，(( ... ))和let ...结构也能够返回退出状态码，当它们所测试的算术表达式的结果为非零的时候，将会返回退出状态码0。 测试操作符文件测试操作符： -e 文件存在 -a 文件存在，这个选项的效果与-e相同. 但是它已经被”弃用”了, 并且不鼓励使用. -f 表示这个文件是一个一般文件(并不是目录或者设备文件) -s 文件大小不为零 -d 表示这是一个目录 -b 表示这是一个块设备(软盘、光驱等。) -c 表示这是一个字符设备(键盘, modem, 声卡, 等等.) -p 这个文件是一个管道 -h 这是一个符号链接 -L 这是一个符号链接 -S 表示这是一个socket -t 文件(描述符)被关联到一个终端设备上。这个测试选项一般被用来检测脚本中的stdin([ -t 0 ]) 或者stdout([ -t 1 ])是否来自于一个终端. -r 文件是否具有可读权限(指的是正在运行这个测试命令的用户是否具有读权限) -w 文件是否具有可写权限(指的是正在运行这个测试命令的用户是否具有写权限) -x 文件是否具有可执行权限(指的是正在运行这个测试命令的用户是否具有可执行权限) -g set-group-id(sgid)标记被设置到文件或目录上。如果目录具有sgid标记的话, 那么在这个目录下所创建的文件将属于拥有这个目录的用户组，而不必是创建这个文件的用户组. 这个特性对于在一个工作组中共享目录非常有用。 -u set-user-id (suid)标记被设置到文件上。如果一个root用户所拥有的二进制可执行文件设置了set-user-id标记位的话，那么普通用户也会以root权限来运行这个文件。这对于需要访问系统硬件的执行程序非常有用。如果没有suid标志的话，这些二进制执行程序是不能够被非root用户调用的。对于设置了suid标志的文件，在它的权限列中将会以s表示。 -k 设置粘贴位。粘贴位设置在目录中，它将限制写权限，在它们的权限标记列中将会显示t。如果用户并不拥有这个设置了粘贴位的目录，但是他在这个目录下具有写权限，那么这个用户只能在这个目录下删除自己所拥有的文件。这将有效的防止用户在一个公共目录中不慎覆盖或者删除别人的文件，比如说/tmp目录。 -O 判断你是否是文件的拥有者 -G 文件的group-id是否与你的相同 -N 从文件上一次被读取到现在为止，文件是否被修改过 f1 -nt f2 文件f1比文件f2新 f1 -ot f2 文件f1比文件f2旧 f1 -ef f2 文件f1和文件f2是相同文件的硬链接 ! 反转上边所有测试的结果(如果没给出条件，那么返回真)。 二元比较操作符，用来比较两个变量或数字： -eq 等于 if [ &quot;$a&quot; -eq &quot;$b&quot; ] -ne 不等于 if [ &quot;$a&quot; -ne &quot;$b&quot; ] -gt 大于 if [ &quot;$a&quot; -gt &quot;$b&quot; ] -ge 大于等于 if [ &quot;$a&quot; -ge &quot;$b&quot; ] -lt 小于 if [ &quot;$a&quot; -lt &quot;$b&quot; ] -le 小于等于 if [ &quot;$a&quot; -le &quot;$b&quot; ] &lt; 小于(在双括号中使用) ((&quot;$a&quot; &lt; &quot;$b&quot;)) &lt;= 小于等于(在双括号中使用) ((&quot;$a&quot; &lt;= &quot;$b&quot;)) &gt; 大于(在双括号中使用) ((&quot;$a&quot; &gt; &quot;$b&quot;)) &gt;= 大于等于(在双括号中使用) ((&quot;$a&quot; &gt;= &quot;$b&quot;)) -a 逻辑与，一般都是和test命令或者是单中括号结构一起使用的 if [ &quot;$exp1&quot; -a &quot;$exp2&quot; ] -o 逻辑或，一般都是和test命令或者是单中括号结构一起使用的 if [ &quot;$exp1&quot; -o &quot;$exp2&quot; ] 字符串比较： = 等于 if [ &quot;$a&quot; = &quot;$b&quot; ] ==等于 if [ &quot;$a&quot; == &quot;$b&quot; ] 与=等价 != 不等号 if [ &quot;$a&quot; != &quot;$b&quot; ] 这个操作符将在[[ ... ]]结构中使用模式匹配 &lt; 小于，按照ASCII字符进行排序 if [[ &quot;$a&quot; &lt; &quot;$b&quot; ]] if [ &quot;$a&quot; \&lt; &quot;$b&quot; ] 注意使用在[ ]结构中的时候需要被转义。 &gt; 大于，按照ASCII字符进行排序 if [[ &quot;$a&quot; &gt; &quot;$b&quot; ]] if [ &quot;$a&quot; \&gt; &quot;$b&quot; ] 注意使用在[ ]结构中的时候需要被转义。 -z 字符串为”null”，意思就是字符串长度为零 -n 字符串不为”null”，当-n使用在中括号中进行条件测试的时候，必须要把字符串用双引号引用起来。 注意：==比较操作符在双中括号对和单中括号对中的行为是不同的。 12345678910111213141516171819if [[ $a == z* ]] # 如果$a以"z"开头(模式匹配)，那么结果将为真then echo trueelse echo falsefiif [[ $a == "z*" ]] # 如果$a与z*相等，就是字面意思完全一样，那么结果为真。then echo trueelse echo falsefiif [ $a == z* ] # 如果$a与z*相等，就是字面意思完全一样，那么结果为真。不存在模式匹配then echo trueelse echo falsefiif [ "$a" == "z*" ] # 如果$a与z*相等，就是字面意思完全一样，那么结果为真。then echo trueelse echo falsefi 检查字符串是否为null，有以下几种方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/bin/bash# str-test.sh: 检查null字符串和未引用的字符串if [ -n $string1 ] # $string1 没有被声明和初始化then echo "String \"string1\" is not null."else echo "String \"string1\" is null."fiif [ -n "$string1" ] # 这次$string1被引号扩起来了.then echo "String \"string1\" is not null."else echo "String \"string1\" is null."fiif [ $string1 ] # 这次，就一个$string1，什么都不加then echo "String \"string1\" is not null."else echo "String \"string1\" is null."fi# [ ] 测试操作符能够独立检查string是否为null，然而，使用("$string1")是一种非常好的习惯# if [ $string1 ] 只有一个参数 "]"# if [ "$string1" ] 有两个参数，一个是空的"$string1"，另一个是"]" string1=initializedif [ $string1 ] # 再来，还是只有$string1，什么都不加then echo "String \"string1\" is not null."else echo "String \"string1\" is null."fi# 这个例子运行还是给出了正确的结果，但是使用引用的("$string1")还是更好一些string1="a = b"if [ $string1 ] # 再来，还是只有$string1，什么都不加then echo "String \"string1\" is not null."else echo "String \"string1\" is null."fi# 未引用的"$string1"，这回给出了错误的结果exit 0 总结本篇文章介绍了如何使用Bash的条件判断，涉及到test命令、各种中括号和圆括号操作，以及if/then结构。需要知道，test、/usr/bin/test、[ ]和/usr/bin/[都是等价命令，使用type命令可以分区内建命令和关键字。另外，需要区分单中括号和双中括号以及双圆括号的含义是不同的。 单中括号[，与test命令等价，是个内建命令 双中括号[[，是扩展测试命令，是个关键字 双圆括号(())，是根据算术表达式运行结果来返回不同状态码 使用[[ ... ]]条件判断结构而不是[ ... ]，能够防止脚本中的许多逻辑错误。比如&amp;&amp;、||、&lt;和&gt;操作符能够正常存在于[[ ]]条件判断结构中,，但是如果出现在[ ]结构中的话，会报错。 参考文章 高级Bash脚本编程指南-中文版]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bash中的变量]]></title>
    <url>%2F2015%2F07%2F07%2Fbash-variable%2F</url>
    <content type="text"><![CDATA[变量是脚本编程中进行数据表现的一种方法。说白了，变量不过是计算机为了保留数据项，而在内存中分配的一个位置或一组位置的标识或名字。变量既可以出现在算术操作中，也可以出现在字符串分析过程中。 变量赋值变量使用=来实现赋值操作，前后都不能有空白。例如： 12a=314echo "The value of \"a\" is $a." 也可以使用let来赋值： 12let a=16+5echo "The value of \"a\" is now $a." 使用read命令进行赋值： 123echo -n "Enter \"a\" "read aecho "The value of \"a\" is now $a." 下面是复杂一点的赋值： 1234a=23 echo $ab=$aecho $b 我们也可以使用命令替换来赋值： 123456a=`echo Hello!` # 把'echo'命令的结果传给变量'a'echo $aa=`ls -l` # 把'ls -l'的结果赋值给'a'echo $a # 然而, 如果没有引号的话将会删除ls结果中多余的tab和换行符.echo "$a" # 如果加上引号的话, 那么就会保留ls结果中的空白符. 注意：上面的代码有无引号，在linux系统中，运行结果是不一样的；在mac系统上，结果是一致的。 也可以使用$(...)机制来进行变量赋值，这其实还是命令替换的一种形式。例如： 123# From /etc/rc.d/rc.localR=$(cat /etc/redhat-release)arch=$(uname -m) 变量的类型不像其他程序语言一样，Bash并不对变量区分”类型”。本质上，Bash变量都是字符串。但是依赖于具体的上下文，Bash也允许比较操作和整数操作。其中的关键因素就是，变量中的值是否只有数字。 整型的变量： 123a=2334 # 整型.let "a += 1"echo "a = $a " # a = 2335 把变量编程字符串： 12b=$&#123;a/23/BB&#125; # 将"23"替换成"BB"echo "b = $b" # b = BB35 对字符串类型变量计算加法： 12let "b += 1" # BB35 + 1 =echo "b = $b" # b = 1 将上面过程反过来操作： 12345678c=BB34echo "c = $c" # c = BB34d=$&#123;c/BB/23&#125; # 将"BB"替换成"23".# 这使得变量$d变为一个整形.echo "d = $d" # d = 2334let "d += 1" # 2334 + 1 =echo "d = $d" # d = 2335 null变量可以用在算术操作中，下面例子将null变量转换成一个整型变量： 1234e=""echo "e = $e" # e =let "e += 1" # 算术操作允许一个null变量?echo "e = $e" # e = 1 对于没有声明的变量，也可以转换成一个整型变量： 123echo "f = $f" # f =let "f += 1" # 算术操作能通过么?echo "f = $f" # f = 1 不区分变量的类型既是幸运的事情也是悲惨的事情。它允许你在编写脚本的时候更加的灵活(但是也足够把你搞晕!)，并且可以让你能够更容易的编写代码。然而，这也很容易产生错误，并且让你养成糟糕的编程习惯。 另外，变量还分局部变量和环境变量。局部变量只在代码块或者函数中可见；环境变量将影响用户接口和shell行为。 在通常情况下，每个进程都有自己的”环境”，这个环境是由一组变量组成的，这些变量中存有进程可能需要引用的信息。在这种情况下，shell与一个一般的进程没什么区别。 但是，分配给环境变量的空间是有限的。创建太多环境变量，或者给一个环境变量分配太多的空间都会引起错误。 如果一个脚本要设置一个环境变量，那么需要将这些变量export出来，也就是需要通知到脚本本地的环境。但是，子进程是不能够export变量来影响产生自己的父进程的环境的。 位置参数变量是有位置参数的，从命令行传递到脚本的参数: $0, $1, $2, $3 . . . $0就是脚本文件自身的名字，$1是第一个参数，$2是第二个参数，$3是第三个参数，然后是第四个。$9之后的位置参数就必须用大括号括起来了，比如：${10}, ${11}, ${12}。 两个比较特殊的变量$*和$@表示所有的位置参数。 {}标记法提供了一种提取从命令行传递到脚本的最后一个位置参数的简单办法，但是这种方法同时还需要使用间接引用。 1234args=$# # 位置参数的个数.lastarg=$&#123;!args&#125;# 或: lastarg=$&#123;!#&#125;# 注意，不能直接使用 lastarg=$&#123;!$#&#125; ，这会产生错误 如果脚本需要一个命令行参数，而在调用的时候，这个参数没被提供，那么这就可能造成给这个参数赋一个null变量，通常情况下，这都会产生问题。一种解决这个问题的办法就是使用添加额外字符的方法，在使用这个位置参数的变量和位置参数本身的后边全部添加同样的额外字符。 1234567variable1_=$1_ # 而不是 variable1=$1# 这将阻止报错, 即使在调用时没提供这个位置参数critical_argument01=$variable1_#使用正则表达式替换_为空，得到输入的变量variable1=$&#123;variable1_/_/&#125; 另外，一种方法是判断是否存在： 1234if [ -z $1 ]then exit $E_MISSING_POS_PARAMfi 更好的方法是使用参数替换： 12DefaultVal=xxxx$&#123;1:-$DefaultVal&#125; shift命令会重新分配位置参数，其实就是把所有的位置参数都向左移动一个位置。这样的话，原来的$1就消失了，但是$0(脚本名)是不会改变的。如果传递了大量的位置参数到脚本中，那么shift命令允许你访问的位置参数的数量超过10个，当然{}标记法也提供了这样的功能。 下面是使用shift命令的例子： 123456789101112131415#!/bin/bash# 使用'shift'来逐步存取所有的位置参数. # 给脚本命个名，比如shft，然后给脚本传递一些位置参数, 比如: # ./shft a b c def 23 skidoountil [ -z "$1" ] # 直到所有的位置参数都被存取完...do echo -n "$1 " shiftdone echo # 额外的换行. exit 0 当然，shift命令也可以用在函数当中。 12345678910111213multiply () # 将乘数作为参数传递进来. &#123; # 可以接受多个参数. local product=1until [ -z "$1" ] # 直到处理完所有的参数...do let "product *= $1" shiftdoneecho $product # 不会echo到stdout&#125; 变量替换变量的名字就是变量保存值的地方，引用变量的值就叫做变量替换。 如果variable1是一个变量的名字，那么$variable1就是引用这变量的值，即这边变量所包含的数据。 没有$前缀的时候，变量可能存在如下几种情况： 变量被声明或被赋值 变量被unset 变量被export 变量代表一种信号 变量赋值可以使用=，也可以在read命令中或者循环头进行赋值，例如for var2 in 1 2 3。 注意，$variable事实上只是${variable}的简写形式。在某些上下文中$variable可能会引起错误，这时候你就需要用${variable}了。 被一对双引号括起来的变量替换是不会被禁止的，所以双引号被称为部分引用，有时候又被称为”弱引用”。但是，如果使用单引号的话，那么变量替换就会被禁止了，变量名只会被解释成字面的意思，不会发生变量替换，所以单引号被称为全引用，有时候也被称为”强引用”。 变量赋值和替换，举例如下： 12345678910111213a=375hello=$aecho hello # 没有变量引用，只是个hello字符串echo $hello # 375echo $&#123;hello&#125; # 375echo "$hello" # 375echo "$&#123;hello&#125;" # 375# 全引用的作用将会导致"$"被解释为单独的字符，而不是变量前缀echo '$hello' # $hello 引用一个变量将保留其中的空白，但如果是变量替换，就不会保留了。 123hello="A B C D"echo $hello # A B C Decho "$hello" # A B C D 一个未初始化的变量是没有值的，但是在做算术操作的时候，这个未初始化的变量看起来值为0。这是一个未文档化(并且可能不具可移植性)的行为。 123echo "$uninitialized" # (blank line)let "uninitialized += 5" # Add 5 to it.echo "$uninitialized" # 5 变量引用在一个双引号中通过直接使用变量名的方法来引用变量，一般情况下都是没问题的。这么做将阻止所有在引号中的特殊字符被重新解释，包括变量名，但是$、`(后置引用)、和\除外。保留$作为特殊字符的意义是为了能够在双引号中也能够正常的引用变量，也就是说，这个变量将被它的值所取代。 使用双引号还能够阻止单词分割，如果一个参数被双引号扩起来的话，那么这个参数将认为是一个单元，即使这个参数包含有空白，那里面的单词也不会被分隔开。 123hello="A B C D"echo $hello # A B C Decho "$hello" # A B C D 在echo语句中，只有在单词分割或者需要保留空白的时候，才需要把参数用双引号括起来。 谈到空白，我们可以通过IFS变量修改默认的空白符，例如： 12345678910var="'(]\\&#123;&#125;\$\""# 下面输出一样echo $var # '(]\&#123;&#125;$"echo "$var" # '(]\&#123;&#125;$"IFS='\'# 下面输出不一样了，因为这里空白字符定义为\了，变量替换的时候，会将其替换为一个空格echo $var # '(] &#123;&#125;$" echo "$var" # '(]\&#123;&#125;$" 单引号操作与双引号基本一样，但是不允许引用变量，因为$的特殊意义被关闭了。在单引号中，任何特殊字符都按照字面的意思进行解释，除了’,所以说单引号是一种比双引号更严格的引用方法。 因为即使是转义符在单引号中也是按照字面意思解释的，所以如果想在一对单引号中显示一个单引号是不行的。对于下面的例子： 1echo "Why can't I write 's between single quotes" 使用单引号来引用，可以这样做: 1echo 'Why can'\''t I write '"'"'s between single quotes' # Why can't I write 's between single quotes 三个被单引号引用的字符串，在这三个字符串之间有一个用转义符转义的单引号，和一个用双引号括起来的单引号。 参考文章 高级Bash脚本编程指南-中文版]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bash中的特殊字符]]></title>
    <url>%2F2015%2F07%2F06%2Fbash-special-characters%2F</url>
    <content type="text"><![CDATA[Bash中，用在脚本和其他地方的字符叫做特殊字符。下面依次举例介绍每个字符的用途。 #行首以#(#!是个例外)开头是注释。 1# This line is a comment. 注释也可以放在于本行命令的后边。 1echo "A comment will follow." # 注释在这里。 命令是不能放在同一行上注释的后边的。因为没有办法把注释结束掉，好让同一行上后边的”代码生效”，只能够另起一行来使用下一个命令。 在echo中转义的#是不能作为注释的，同样也可以出现在特定的参数替换结构中，或者是出现在数字常量表达式中。 123456echo "The # here does not begin a comment." echo 'The # here does not begin a comment.' echo The \# here does not begin a comment. #转义字符\echo The # 这里开始一个注释.echo $&#123;PATH#*:&#125; # 参数替换, 不是一个注释echo $(( 2#101011 )) # 数制转换, 不是一个注释 标准的引用和转义字符&quot; &#39; \可以用来转义#，某些特定的模式匹配操作也可以使用#。 ;分号作为命令行分隔符，可以在同一行上写两个或两个以上的命令。 1234567echo hello; echo thereif [ -x "$filename" ]; then echo "File $filename exists.";else echo "File $filename not found."; touch $filename fi; echo "File test complete." 在某些情况下，;也可以被转义。 ;;终止case选项。 1234case "$variable" in abc) echo "\$variable = abc" ;; xyz) echo "\$variable = xyz" ;;esac .点命令等价于source命令，这是一个bash的内建命令。 如果点放在文件名的开头的话，那么这个文件将会成为”隐藏”文件，并且ls命令将不会正常的显示出这个文件。 如果作为目录名的话，一个单独的点代表当前的工作目录，而两个点表示上一级目录。 点也可以表示当前目录。 1cp /home/javachen/current_work/* . 当用作匹配字符的作用时，通常都是作为正则表达式的一部分来使用，点用来匹配任何的单个字符。 &quot;,&#39;双引号为部分引用，单引号为全引用。 12echo "The # here does not begin a comment." echo 'The # here does not begin a comment.' ,逗号操作费，链接了一系列的算术操作。 虽然里边所有的内容都被运行了，但只有最后一项被返回。 1let "t2 = ((a = 9, 15 / 3))" # a = 9 t2 = 15 / 3 也可以这样使用： 1mkdir -p &#123;a,b,c&#125; #创建三个目录a、b、c \转义符，一种对单字符的引用机制，通常是用于对单字符进行转义。\通常用来转义&quot;和&#39;，这样双引号和但引号就不会被解释成特殊含义了。 /文件名路径分隔符，分隔文件名不同的部分，也可以用来作为除法算术操作符。 12/home/bozo/projects/Makefilelet "t2 = ((a = 9, 15 / 3))" # a = 9 t2 = 15 / 3 `命令替换，command结构可以将命令的输出赋值到一个变量中去。 1date=`date` :空命令，等价于”NOP”，什么都不做，也可以被认为与shell的内建命令true作用相同。”:”命令是一个bash的内建命令，它的退出码是”true”，即为0。 12:echo $? # 0 死循环： 1234while : # while truedo datedone 在 if/then 中的占位符，什么都不做，引出分支。 12345if conditionthen : # 什么都不做，引出分支else take-some-actionfi 在一个二元命令中提供一个占位符。 123n=1: $((n = $n + 1)) # 如果没有":"的话，Bash 将会尝试把 $((n = $n + 1)) 解释为一个命令，运行时会报错echo -n "$n " 在here document中提供一个命令所需的占位符或者用于注释代码。 12345678910111213: &lt;&lt;TESTVARIABLES$&#123;HOSTNAME?&#125;$&#123;USER?&#125;$&#123;MAIL?&#125; # 如果其中某个变量没被设置, 那么就打印错误信息. TESTVARIABLES: &lt;&lt;COMMENTBLOCKecho "This line will not echo."This is a comment line missing the "#" prefix.This is another comment line missing the "#" prefix.&amp;*@!!++=The above line will cause no error message,because the Bash interpreter will ignore it.COMMENTBLOCK 使用参数替换来评估字符串变量。 1: $&#123;HOSTNAME ?&#125; $&#123;USER?&#125; $&#123;MAIL?&#125; #如果一个或多个必要的环境变量没被设置的话，就打印错误信息. 在与&gt;重定向操作符结合使用时，将会把一个文件清空，但是并不会修改这个文件的权限。如果之前这个文件并不存在，那么就创建这个文件。 123: &gt; data.xxx # 文件"data.xxx" 现在被清空了# 与 cat /dev/null &gt;data.xxx 的作用相同# 然而，这并不会产生一个新的进程，因为":"是一个内建命令 在与&gt;&gt;重定向操作符结合使用时，将不会对预先存在的目标文件产生任何影响。如果这个文件之前并不存在，那么就创建它。这只适用于正规文件,，而不适用于管道、符号连接和某些特殊文件。 1: &gt;&gt; target_file 也可能用来作为注释行，虽然我们不推荐这么做。使用#来注释的话，将关闭剩余行的错误检查，所以可以在注释行中写任何东西。然而，使用:的话将不会这样。 1: This is a comment that generates an error, ( if [ $x -eq 3] fi ). “:”还用来在/etc/passwd和$PATH变量中做分隔符。 12echo $PATH/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin !取反操作符。!操作符将会反转命令的退出码的结果： 12345true # "true" 是内建命令echo "exit status of \"true\" = $?" # 0! trueecho "exit status of \"! true\" = $?" # 1 如果一个命令以!开头，那么会启用Bash的历史机制。 123true!true# 这次就没有错误了, 也没有反转结果.它只是重复了之前的命令(true). 在另一种上下文中，如命令行模式下，!还能反转bash的历史机制。需要注意 的是，在一个脚本中，历史机制是被禁用的。 12345678910111213$ history |head -n 10 18 date 19 ls 20 cd 21 pwd 22 jps 23 java 24 ll 25 ps 26 history#执行bash历史中的一条命令$ !25 !操作符还是Bash的关键字。 在一个不同的上下文中，!也会出现在变量的间接引用中。 123456789101112a=letter_of_alphabetletter_of_alphabet=z# 直接引用.echo "a = $a" # a = letter_of_alphabet# 间接引用.eval a=\$$a echo "Now a=$a" #Now a = z# 间接引用.echo $&#123;!a&#125; # a = z *用来做文件名匹配： 12$ echo *abs-book.sgml add-drive.sh agram.sh alias.sh 也可以用在正则表达式中，用来匹配任意个数(包含0个)的字符。 在算术操作符的上下文中， *号表示乘法运算。如果要做幂运算，使用**，这是求幂操作符。 ？测试操作符。在一个特定的表达式中，?用来测试一个条件的结果。 在一个双括号结构中，?就是C语言的三元操作符。 1(( t = a&lt;45?7:11 )) # C语言风格的三元操作 在参数替换表达式中，?用来测试一个变量是否set。 在通配中，用来做匹配单个字符的”通配符”，在正则表达式中，也是用来表示一个字符。 $在变量替换中，用于引用变量的内容。 12var1=5echo $var1 在一个变量前面加上$用来引用这个变量的值。 在正则表达式中，表示行结束符。 ${} 是参数替换，$*, $@是位置参数，$? 是退出状态码变量，$$是进程id变量，保存所在脚本的进程 ID。 ()命令组： 1(a=hello; echo $a) 在括号中的命令列表，将会作为一个子shell来运行。 例外: 在pipe中的一个大括号中的代码段可能运行在一个 子shell中。 1234ls | &#123; read firstline; read secondline; &#125;#错误. 在大括号中的代码段, 将运行到子shell中, 所以"ls"的输出将不能传递到代码块中echo "First line is $firstline; second line is $secondline" # 不能工作 初始化数组： 1Array=(element1 element2 element3) {xxx,yyy,zzz,...}大括号扩展： 123cat &#123;file1,file2,file3&#125; &gt; combined_filecp file22.&#123;txt,backup&#125; # 拷贝"file22.txt"到"file22.backup"中 一个命令可能会对大括号中的以逗号分的文件列表起作用。在通配符中，将对大括号中的文件名做扩展。 在大括号中，不允许有空白，除非这个空白引用或转义。 12$ echo &#123;file1,file2&#125;\ :&#123;\ A," B",' C'&#125;file1 : A file1 : B file1 : C file2 : A file2 : B file2 : C {}代码块，又被称为内部组，这个结构事实上创建一个匿名函数。与”标准”函数不同的是，在其中的变量，对于脚本其他部分的代码来还是可见的。 12&#123; local a; a=1; &#125;-bash: local: can only be used in a function 123a=123&#123; a=321; &#125;echo "a = $a" # a = 321 (说明在代码块中对变量a所作的修改影响了外边的变量) 下边的代码展示在大括号结构中代码的I/O 重定向。 123456789101112#!/bin/bash# 从/etc/fstab中读行. 3File=/etc/fstab&#123; read line1 read line2&#125; &lt; $Fileecho "First line in $File is:" "$line1"echo "Second line in $File is:" "$line2"exit 与上面所讲到的()中的命令组不同的是，大括号中的代码块将不会开一个新的子shell。 &#39;{}&#39; \;路径名。一般都在find命令中使用，这不是一个shell内建命令。;用来结束find命令序列的-exec选项，它需要被保护以防止被shell所解释。 1find . -mtime -1 -type f -exec tar rvf archive.tar '&#123;&#125;' \; []条件测试。 在一个array结构的上下文中，中括号用来引用数组中每个元素的编号。 12Array[1]=a echo $&#123;Array[1]&#125; 用作正则表达式的一部分，方括号描一个匹配的字符范围。例如，正则表达式中，”[xyz]” 将会匹配字符x, y, 或z。 [[ ]]测试表达式在[[ ]]中。 (( ))双圆括号结构，扩展并计算在(( ))中的整数表达式。 与let命令很相似，((...))结构允许算术扩展和赋值。举个简单的例子，a=$(( 5 + 3 ))，将把变量”a”设为”5 + 3”或者8。 &gt; &amp;&gt; &gt;&amp; &gt;&gt; &lt; &lt;&gt;重定向。 重定向scriptname的输出到文件filename中。如果filename存在的，那么将会被覆盖： 1scriptname &gt;filename 也可以清空文件内容： 1&gt; a.log 重定向command的stdout和stderr到filename中: 1command &amp;&gt;filename 重定向command的stdout到stderr中: 1command &gt;&amp;2 把scriptname的输出加到文件filename中。如果filename不存在的话，将被创建。 1scriptname &gt;&gt; filename 打开文件filename用来读写，并且分配文件描述符i给这个文件。如果filename不存在，这个文件将会创建： 1[i]&lt;&gt;filename “&lt;”或”&gt;”还可以用于进程替换。 在一种不同的上下文中，”&lt;”和”&gt;”可用来做字符串比较操作或者整数比较。 &lt;&lt;用在here document中的重定向。 &lt;&lt;&lt;用在here string中的重定向。 \&lt;,\&gt;正则表达式中的单词边： 1grep '\&lt;the\&gt;' textfile |管道。分析前边命令的输出，并将输出作为后边命令的输入。这是一种产生命令链的好方法。 12345# 与一个简单的"ls -l"结果相同echo ls -l | sh# 合并和排序所有的".lst"文件, 然后删除所有重复的行.cat *.lst | sort | uniq 输出的命令也可以传递到脚本中： 123456#!/bin/bash# uppercase.sh : 修改输入, 全部转换为大写tr 'a-z' 'A-Z'exit 0 现在我们输送ls -l的输出到该脚本中： 12345$ ls -l | ./uppercase.shDRWXR-X--- 2 ROOT ROOT 4096 05-28 10:06 ML-1M-RW-R--R-- 1 ROOT ROOT 0 2014-11-21 MONITOR_DISK.TXTDRWXR-XR-X 2 ROOT ROOT 4096 2014-11-21 SCRIPT-RW-R--R-- 1 ROOT ROOT 16 07-06 16:09 UPPERCASE.SH 管道中的每个进程的stdout比下一个进程作为stdin来读入，否则，数据流会阻塞，并且管道将产生一些非预期的行为： 12# 从"cat file1 file2"中的输出并没出现cat file1 file2 | ls -l | sort 作为子进程的运行的管道，不能够改变脚本的变量： 123variable="initial_value"echo "new_value" | read variableecho "variable = $variable" # variable =initial_value 如果管道中的这个命令产生一个异常，并中途失败，那么这个管道将过早的终止，这种行为叫做broken pipe，并且这种状态下将发送一个SIGPIPE信号。 &gt;|强制重定向(使设置noclobber选项，就是-C选项)，这将强制的覆盖一个现存文件。 ||或操作，在一个条件测试结构中，如果条件测试结构两边中的任意一边结果为true的话，||操作就会返回0(代表执行成功)。 &amp;后台运行命令，一个命令后边跟一个&amp;表示在后台运行。 1sleep 10 &amp; 在一个脚本中，命令和循环都可能运行在后台： 1234for i in1 2 3 4 5 6 7 8 9 10do echo -n "$i "done &amp; 在一个脚本中，使用后台运行命令可能会使这个脚本挂起，直到敲ENTER 键，挂起的脚本才会恢复。看起来只有在这个命令的结果需要输出到stdout的时候，这种现象才会出现。 只要在后台运行命令的后边加上一个wait命令就会解决这个问题。 123456#!/bin/bash # test.shls -l &amp;echo "Done."wait 如果将后台运行命令的输出重定向到文件中或/dev/null中，也能解决这个问题。 &amp;&amp;与逻辑操作。在一个条件测试结构中，只有在条件测试结构的两边结果都为true的时，&amp;&amp;操作才会返回0(代表sucess)。 -选项，前缀。在所有的命令内如果使用选项参数的话，前边都要加上-。 12345678910ls -alsort -dfu $filenameset -- $variableif [ $file1 -ot $file2 ]then echo "File $file1 is older than $file2." fi 用于重定向stdin或stdout： 12345678# 从一个目录移动整个目录树到另一个目录(cd /source/directory &amp;&amp; tar cf - . ) | (cd /dest/directory &amp;&amp; tar xpvf -)# 当然也可以这样写：cp -a /source/directory/* /dest/directorycp -a /source/directory/* /source/directory/.[^.]* /dest/directory # 如果在/source/directory中有隐藏文件的话bunzip2 -c linux-2.6.16.tar.bz2 | tar xvf - 注意，在这个上下文中，-本身并不是一个Bash操作，而是一个可以被特定的UNIX工具识别的选项，这些特定的UNIX工具特指那些可以写输出到stdout的工具，比如tar、cat等等。 12$ echo "whatever" | cat -whatever 使用diff命令来和另一个文件的一段进行比较： 1grep Linux file1 | diff file2 - 一个更真实的例子是备份最后一天所有修改的文件： 123456789101112131415161718#!/bin/bashBACKUPFILE=backup-$(date +%m-%d-%Y)# 如果在命令行中没有指定备份文件的文件名，那么将默认使用"backup-MM-DD-YYYY.tar.gz"archive=$&#123;1:-$BACKUPFILE&#125;tar cvf - `find . -mtime -1 -type f -print` &gt; $archive.tar# 还有两种简单写法：# find . -mtime -1 -type f -print0 | xargs -0 tar rvf "$archive.tar"# find . -mtime -1 -type f -exec tar rvf "$archive.tar" '&#123;&#125;' \;gzip $archive.tarecho "Directory $PWD backed up in archive file \"$archive.tar.gz\"."exit 0 -还可以用来指先前的工作目录，cd -将会回到当前的工作目录，它使用了 $OLDPWD 环境变量。 另外，还可以当做减号来使用。 =等号，赋值操作。 +加号，也可以用在正则表达式中。某些内建命令使用+来打开特定的选项，用-来用这些特定的选项。 %取模操作，也可以用于正则表达式。 ~home目录。 ~+当前工作目录，相当于$PWD内部变量。 ~-当前的工作目录，相当于$OLDPWD内部变量。 ^行首，在正则表达式中，^表示定位到文本行的行首。 控制字符修改终端或文本显示的行为。控制字符以CONTROL + key这种方式进行组合(同时按下)。控制字符也可以使用8进制或16进制表示法来进行表示，但是前边必须要加上转义符。 控制字符比较多，这里不一一列出了。 空白用来分隔函数，命令或变量。空白包含空格、tab、空行，或者是它们之间任意的组合体。在某些上下文中，比如变量赋值，空白是不被允许的，会产生语法错误。 空行不会影响脚本的行为，因此使用空行可以很好的划分独立的函数段以增加可读性。 特殊变量$IFS用来做一些输入命令的分隔符，默认情况下是空白。 如果想在字符串或变量中使用空白，那么应该使用引用。例如下面例子： 123hello="A B C D"echo $hello # A B C Decho "$hello" # A B C D 参考文章 高级Bash脚本编程指南-中文版]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jekyll kramdown配置]]></title>
    <url>%2F2015%2F06%2F30%2Fjekyll-kramdown-config%2F</url>
    <content type="text"><![CDATA[之前博客是使用的redcarpet的markdown语法，其在_config.yml中的配置方式为： 123markdown: redcarpetredcarpet: extensions: [ "fenced_code_blocks", "hard_wrap","autolink", "tables", "strikethrough", "superscript", "with_toc_data", "highlight", "prettify","no_intra_emphasis"] 这种配置支持使用 1234567891011现在，想尝试使用karkdown的语法。kramdown是一个Markdown解析器，它能够正确解释公式内部的符号，不会与Markdown语法冲突，比如不会将^符号变成&lt;sup&gt;&lt;/sup&gt;标签。kramdown支持MathJax，见[Jekyll中使用MathJax](http://www.pkuwwt.tk/linux/2013-12-03-jekyll-using-mathjax/)。kramdown默认是支持TOC，你可以进一步设置TOC相关的参数，见 [为 Octopress 添加 TOC](http://loudou.info/blog/2014/08/01/wei-octopress-tian-jia-toc/)。安装kramdown：~~~bash$ gem install kramdown 在_config.yml中的配置方式为： 1234markdown: kramdownkramdown: input: GFM use_coderay: true 在编写文章时，插入下面代码，渲染之后就可以生成TOC了： 12* TOC&#123;:toc&#125; krmadown支持和github一样的语法高亮，用三个 pages上不支持coderay，所以该方式无法搞定，可行的解决方法是上传本地编译好的html。如果是本地或者自己的空间，可以安装coderay。123~~~bash$ gem install coderay 使用 1234567891011~~~pythonclass AdView (object): def __init__ (self, name = None): self.name = name def test (self): if self.name == &apos;admin&apos;: return False else return True 更多语法，见kramdown语法小记。 最后的配置为： 12345678910kramdown: input: GFM extensions: - autolink - footnotes - smart use_coderay: true syntax_highlighter: rouge coderay: coderay_line_numbers: nil coderay支持的语言有限，并且rouge兼容Pygments，故这里使用rouge： 1$ gem install rouge 参考文章 jekyll kramdown 语法高亮配置 jekyll kramdown 语法高亮 Octopress 精益修改 (4) Kramdown 语法文档翻译（一） Markdown的各种扩展]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>jekyll</tag>
        <tag>kramdown</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高级Bash脚本编程入门]]></title>
    <url>%2F2015%2F06%2F29%2Fadvanced-bash-script-programming%2F</url>
    <content type="text"><![CDATA[最近在看《Advanced Bash Scripting Guide》这本书，第二章举了一个清除日志的例子，来讲述如何使用Bash进行编程并聊到了一些编程规范。本文主要是基于这部分内容记录我的读书笔记并整理一些相关知识点。 说到清除日志，你可以使用下面命令来完成清除/var/log下的log文件这件事情： 1234cd /var/logcat /dev/null &gt; messages cat /dev/null &gt; wtmpecho "Logs cleaned up." 更简单的清除日志方法是： 123echo "" &gt;messages #或者&gt;messages 注意：/var/log/messages 记录系统报错信息/var/log/wtmp 记录系统登录信息 在Bash编程时，脚本通常都是放到一个文件里面，该文件可以有后缀名也可以没有，例如，你可以将该文件命名为cleanlog，然后在文件头声明一个命令解释器，这里是#!/bin/bash： 1234567#!/bin/bashLOG_DIR=/var/logcd $LOG_DIRcat /dev/null &gt; messagescat /dev/null &gt; wtmpecho "Logs cleaned up."exit 当然，还可以使用其他的命令行解释器，例如： 123456789#!/bin/sh#!/bin/bash#!/usr/bin/perl #!/usr/bin/tcl #!/bin/sed -f#!/usr/awk -f#自删除脚本#!/bin/rm 说明： #! 后面的路径必须真实存在，否则运行时会提示Command not found的错误。 在UNIX系统中，在!后边需要一个空格。 如果脚本中还包含有其他的#!行，那么bash将会把它看成是一个一般的注释行。 上面代码将/var/log定义为变量，这样会比把代码写死好很多，因为如果你可能想修改为其他目录，只需要修改变量的值就可以。 对于/var/log目录，一般用户没有访问权限，故需要使用root用户来运行上面脚本，另外，用户不一定有修改目录的权限，所以需要增强代码，做一些判断。 1234567891011121314151617181920212223242526272829#!/bin/bashLOG_DIR=/var/logROOT_UID=0LINES=50E_XCD=66E_NOTROOT=67# 当然要使用root 用户来运行.if [ "$UID" -ne "$ROOT_UID" ]then echo "Must be root to run this script." exit $E_NOTROOTficd $LOG_DIRif[ "$PWD" != "$LOG_DIR" ]then echo "Can't change to $LOG_DIR." exit $E_XCDficat /dev/null &gt; messagescat /dev/null &gt; wtmpecho "Logs cleaned up."#返回0表示成功exit 0 上面代码一样定义了一些变量，然后加了两个判断，去检查脚本运行中可能出现的错误并打印错误说明。如果脚本运行错误，则程序会退出并返回一个错误码，不同类型的错误对应的错误码不一样，这样便于识别错误原因；如果脚本运行正常，则正常退出，默认返回码为0。 对于cd $LOG_DIR操作判断是否执行成功，更有效的做法是： 12345#使用或操作替代if else判断cd /var/log || &#123; echo "Cannot change to necessary directory." &gt;&amp;2 exit $E_XCD&#125; 通常，我们可能不想全部清除日志，而是保留最后几行日志，这样就需要给脚本传入参数： 12345678910111213141516171819202122232425262728293031323334353637383940#!/bin/bashLOG_DIR=/var/logROOT_UID=0LINES=50E_XCD=66E_NOTROOT=67# 当然要使用root 用户来运行.if [ "$UID" -ne "$ROOT_UID" ]then echo "Must be root to run this script." exit $E_NOTROOTficd $LOG_DIRif[ "$PWD" != "$LOG_DIR" ]then echo "Can't change to $LOG_DIR." exit $E_XCDfi# 测试是否有命令行参数，非空判断if [ -n "$1" ]then lines=$1else lines=$LINES # 默认，如果不在命令行中指定fi# 保存log file消息的最后部分tail -$lines messages &gt; mesg.tempmv mesg.temp messagescat /dev/null &gt; wtmpecho "Logs cleaned up."#返回0表示成功exit 0 上面使用if else来判断是否有输入参数，一个更好的检测命令行参数的方式是使用正则表达式做判断，以检查输入参数的合法性： 1234567E_WRONGARGS=65 # 非数值参数(错误的参数格式)case "$1" in "" ) lines=50;; *[!0-9]*) echo "Usage: `basename $0` file-to-cleanup"; exit $E_WRONGARGS;; * ) lines=$1;;esac 编写完脚本之后，你可以使用sh scriptname或者bash scriptname来调用这个脚本。不推荐使用sh &lt;scriptname，因为这禁用了脚本从stdin中读数据的功能。更方便的方法是让脚本本身就具有 可执行权限，通过chmod命令可以修改。比如: 1chmod 555 scriptname #允许任何人都具有可读和执行权限 或者： 12chmod +rx scriptname #允许任何人都具有可读和执行权限 chmod u+rx scriptname #只给脚本的所有者可读和执行权限 既然脚本已经具有了可执行权限，现在你可以使用./scriptname来测试这个脚本了。如果这个脚本以一个#!行开头，那么脚本将会调用合适的命令解释器来运行。 这样一个简单的脚本就编写完成并能运行了，从这个例子中，我们可以学到bash编程的一些代码规范： 使用变量 脚本运行中，需要做一些异常判断 除此之外，google公司还定义了一份Shell Style Guide，可以仔细阅读并约束自己去遵循这些规范。 参考文章 高级Bash脚本编程指南-中文版]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-shell脚本分析]]></title>
    <url>%2F2015%2F06%2F26%2Fspark-shell-command%2F</url>
    <content type="text"><![CDATA[本文主要分析spark-shell脚本的运行逻辑，涉及到spark-submit、spark-class等脚本的分析，希望通过分析脚本以了解spark中各个进程的参数、JVM参数和内存大小如何设置。 spark-shell使用yum安装spark之后，你可以直接在终端运行spark-shell命令，或者在spark的home目录/usr/lib/spark下运行bin/spark-shell命令，这样就可以进入到spark命令行交互模式。 spark-shell 脚本是如何运行的呢？该脚本代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586## Shell script for starting the Spark Shell REPLcygwin=falsecase "`uname`" in CYGWIN*) cygwin=true;;esac# Enter posix mode for bashset -o posix## Global script variablesFWDIR="$(cd "`dirname "$0"`"/..; pwd)"function usage() &#123; echo "Usage: ./bin/spark-shell [options]" "$FWDIR"/bin/spark-submit --help 2&gt;&amp;1 | grep -v Usage 1&gt;&amp;2 exit 0&#125;if [[ "$@" = *--help ]] || [[ "$@" = *-h ]]; then usagefisource "$FWDIR"/bin/utils.shSUBMIT_USAGE_FUNCTION=usagegatherSparkSubmitOpts "$@"# SPARK-4161: scala does not assume use of the java classpath,# so we need to add the "-Dscala.usejavacp=true" flag mnually. We# do this specifically for the Spark shell because the scala REPL# has its own class loader, and any additional classpath specified# through spark.driver.extraClassPath is not automatically propagated.SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true"function main() &#123; if $cygwin; then # Workaround for issue involving JLine and Cygwin # (see http://sourceforge.net/p/jline/bugs/40/). # If you're using the Mintty terminal emulator in Cygwin, may need to set the # "Backspace sends ^H" setting in "Keys" section of the Mintty options # (see https://github.com/sbt/sbt/issues/562). stty -icanon min 1 -echo &gt; /dev/null 2&gt;&amp;1 export SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Djline.terminal=unix" "$FWDIR"/bin/spark-submit --class org.apache.spark.repl.Main "$&#123;SUBMISSION_OPTS[@]&#125;" spark-shell "$&#123;APPLICATION_OPTS[@]&#125;" stty icanon echo &gt; /dev/null 2&gt;&amp;1 else export SPARK_SUBMIT_OPTS "$FWDIR"/bin/spark-submit --class org.apache.spark.repl.Main "$&#123;SUBMISSION_OPTS[@]&#125;" spark-shell "$&#123;APPLICATION_OPTS[@]&#125;" fi&#125;# Copy restore-TTY-on-exit functions from Scala script so spark-shell exits properly even in# binary distribution of Spark where Scala is not installedexit_status=127saved_stty=""# restore stty settings (echo in particular)function restoreSttySettings() &#123; stty $saved_stty saved_stty=""&#125;function onExit() &#123; if [[ "$saved_stty" != "" ]]; then restoreSttySettings fi exit $exit_status&#125;# to reenable echo if we are interrupted before completing.trap onExit INT# save terminal settingssaved_stty=$(stty -g 2&gt;/dev/null)# clear on error so we don't later try to restore themif [[ ! $? ]]; then saved_stty=""fimain "$@"# record the exit status lest it be overwritten:# then reenable echo and propagate the code.exit_status=$?onExit 从上往下一步步分析，首先是判断是否为cygwin，这里用到了bash中的case语法： 1234cygwin=falsecase "`uname`" in CYGWIN*) cygwin=true;;esac 在linux系统中，uname命令的运行结果为linux，其值不等于CYGWIN*，故cygwin=false。 开启bash的posix模式： 1set -o posix 获取上级目录绝对路径，这里使用到了dirname命令： 1FWDIR="$(cd "`dirname "$0"`"/..; pwd)" 提示：bash 中，$0 是获取脚本名称 判断输入参数中是否有--help或者-h，如果有，则打印使用说明，实际上运行的是/bin/spark-submit --help命令： 123456789function usage() &#123; echo "Usage: ./bin/spark-shell [options]" "$FWDIR"/bin/spark-submit --help 2&gt;&amp;1 | grep -v Usage 1&gt;&amp;2 exit 0&#125;if [[ "$@" = *--help ]] || [[ "$@" = *-h ]]; then usagefi 提示： 2&gt;&amp;1 的意思是将标准错误也输出到标准输出当中；1&gt;&amp;2是将标准输出输出到标准错误当中 bash 中，$@ 是获取脚本所有的输入参数 再往后面是定义了一个main方法，并将spark-shell的输入参数传给该方法运行，main方法中判断是否是cygwin模式，如果不是，则运行 12345SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true"export SPARK_SUBMIT_OPTS"$FWDIR"/bin/spark-submit --class org.apache.spark.repl.Main "$&#123;SUBMISSION_OPTS[@]&#125;" spark-shell "$&#123;APPLICATION_OPTS[@]&#125;" 提示：”${SUBMISSION_OPTS[@]}” 这是什么意思？ 从上面可以看到，其实最后调用的是spark-submit命令，并指定--class参数为org.apache.spark.repl.Main类，后面接的是spark-submit的提交参数，再后面是spark-shell，最后是传递应用的参数。 最后，是获取main方法运行结果： 12exit_status=$?onExit 提示： bash 中，$?是获取上个命令运行结束返回的状态码 如果以调试模式运行spark-shell，在不加参数的情况下，输出内容为： 123456789101112131415161718192021222324252627282930+ cygwin=false+ case &quot;`uname`&quot; in++ uname+ set -o posix+++ dirname /usr/lib/spark/bin/spark-shell++ cd /usr/lib/spark/bin/..++ pwd+ FWDIR=/usr/lib/spark+ [[ &apos;&apos; = *--help ]]+ [[ &apos;&apos; = *-h ]]+ source /usr/lib/spark/bin/utils.sh+ SUBMIT_USAGE_FUNCTION=usage+ gatherSparkSubmitOpts+ &apos;[&apos; -z usage &apos;]&apos;+ SUBMISSION_OPTS=()+ APPLICATION_OPTS=()+ (( 0 ))+ export SUBMISSION_OPTS+ export APPLICATION_OPTS+ SPARK_SUBMIT_OPTS=&apos; -Dscala.usejavacp=true&apos;+ exit_status=127+ saved_stty=+ trap onExit INT++ stty -g+ saved_stty=500:5:bf:8a3b:3:1c:7f:15:4:0:1:0:11:13:1a:0:12:f:17:16:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0+ [[ ! -n 0 ]]+ main+ false+ export SPARK_SUBMIT_OPTS+ /usr/lib/spark/bin/spark-submit --class org.apache.spark.repl.Main spark-shell 提示：通过运行set -x可以开启bash调试代码的特性。 接下来就涉及到spark-submit命令的逻辑了。 spark-submit完整的spark-submit脚本内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# NOTE: Any changes in this file must be reflected in SparkSubmitDriverBootstrapper.scala!export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"ORIG_ARGS=("$@")# Set COLUMNS for progress barexport COLUMNS=`tput cols`while (($#)); do if [ "$1" = "--deploy-mode" ]; then SPARK_SUBMIT_DEPLOY_MODE=$2 elif [ "$1" = "--properties-file" ]; then SPARK_SUBMIT_PROPERTIES_FILE=$2 elif [ "$1" = "--driver-memory" ]; then export SPARK_SUBMIT_DRIVER_MEMORY=$2 elif [ "$1" = "--driver-library-path" ]; then export SPARK_SUBMIT_LIBRARY_PATH=$2 elif [ "$1" = "--driver-class-path" ]; then export SPARK_SUBMIT_CLASSPATH=$2 elif [ "$1" = "--driver-java-options" ]; then export SPARK_SUBMIT_OPTS=$2 elif [ "$1" = "--master" ]; then export MASTER=$2 fi shiftdoneif [ -z "$SPARK_CONF_DIR" ]; then export SPARK_CONF_DIR="$SPARK_HOME/conf"fiDEFAULT_PROPERTIES_FILE="$SPARK_CONF_DIR/spark-defaults.conf"if [ "$MASTER" == "yarn-cluster" ]; then SPARK_SUBMIT_DEPLOY_MODE=clusterfiexport SPARK_SUBMIT_DEPLOY_MODE=$&#123;SPARK_SUBMIT_DEPLOY_MODE:-"client"&#125;export SPARK_SUBMIT_PROPERTIES_FILE=$&#123;SPARK_SUBMIT_PROPERTIES_FILE:-"$DEFAULT_PROPERTIES_FILE"&#125;# For client mode, the driver will be launched in the same JVM that launches# SparkSubmit, so we may need to read the properties file for any extra class# paths, library paths, java options and memory early on. Otherwise, it will# be too late by the time the driver JVM has started.if [[ "$SPARK_SUBMIT_DEPLOY_MODE" == "client" &amp;&amp; -f "$SPARK_SUBMIT_PROPERTIES_FILE" ]]; then # Parse the properties file only if the special configs exist contains_special_configs=$( grep -e "spark.driver.extra*\|spark.driver.memory" "$SPARK_SUBMIT_PROPERTIES_FILE" | \ grep -v "^[[:space:]]*#" ) if [ -n "$contains_special_configs" ]; then export SPARK_SUBMIT_BOOTSTRAP_DRIVER=1 fifiexec "$SPARK_HOME"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$&#123;ORIG_ARGS[@]&#125;" 首先是设置SPARK_HOME，并保留原始输入参数： 12export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"ORIG_ARGS=("$@") 接下来，使用while语句配合shift命令，依次判断输入参数。 说明：shift是将输入参数位置向左移位 设置SPARK_CONF_DIR变量，并判断spark-submit部署模式。 如果$SPARK_CONF_DIR/spark-defaults.conf文件存在，则检查是否设置spark.driver.extra开头的和spark.driver.memory变量，如果设置了，则SPARK_SUBMIT_BOOTSTRAP_DRIVER设为1。 最后，执行的是spark-class命令，输入参数为org.apache.spark.deploy.SparkSubmit类名和原始参数。 spark-class该脚本首先还是判断是否是cygwin，并设置SPARK_HOME和SPARK_CONF_DIR变量。 运行bin/load-spark-env.sh，加载spark环境变量。 spark-class至少需要传递一个参数，如果没有，则会打印脚本使用说明Usage: spark-class &lt;class&gt; [&lt;args&gt;]。 如果设置了SPARK_MEM变量，则提示SPARK_MEM变量过时，应该使用spark.executor.memory或者spark.driver.memory变量。 设置默认内存DEFAULT_MEM为512M，如果SPARK_MEM变量存在，则使用SPARK_MEM的值。 使用case语句判断spark-class传入的第一个参数的值： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152SPARK_DAEMON_JAVA_OPTS="$SPARK_DAEMON_JAVA_OPTS -Dspark.akka.logLifecycleEvents=true"# Add java opts and memory settings for master, worker, history server, executors, and repl.case "$1" in # Master, Worker, and HistoryServer use SPARK_DAEMON_JAVA_OPTS (and specific opts) + SPARK_DAEMON_MEMORY. 'org.apache.spark.deploy.master.Master') OUR_JAVA_OPTS="$SPARK_DAEMON_JAVA_OPTS $SPARK_MASTER_OPTS" OUR_JAVA_MEM=$&#123;SPARK_DAEMON_MEMORY:-$DEFAULT_MEM&#125; ;; 'org.apache.spark.deploy.worker.Worker') OUR_JAVA_OPTS="$SPARK_DAEMON_JAVA_OPTS $SPARK_WORKER_OPTS" OUR_JAVA_MEM=$&#123;SPARK_DAEMON_MEMORY:-$DEFAULT_MEM&#125; ;; 'org.apache.spark.deploy.history.HistoryServer') OUR_JAVA_OPTS="$SPARK_DAEMON_JAVA_OPTS $SPARK_HISTORY_OPTS" OUR_JAVA_MEM=$&#123;SPARK_DAEMON_MEMORY:-$DEFAULT_MEM&#125; ;; # Executors use SPARK_JAVA_OPTS + SPARK_EXECUTOR_MEMORY. 'org.apache.spark.executor.CoarseGrainedExecutorBackend') OUR_JAVA_OPTS="$SPARK_JAVA_OPTS $SPARK_EXECUTOR_OPTS" OUR_JAVA_MEM=$&#123;SPARK_EXECUTOR_MEMORY:-$DEFAULT_MEM&#125; ;; 'org.apache.spark.executor.MesosExecutorBackend') OUR_JAVA_OPTS="$SPARK_JAVA_OPTS $SPARK_EXECUTOR_OPTS" OUR_JAVA_MEM=$&#123;SPARK_EXECUTOR_MEMORY:-$DEFAULT_MEM&#125; export PYTHONPATH="$FWDIR/python:$PYTHONPATH" export PYTHONPATH="$FWDIR/python/lib/py4j-0.8.2.1-src.zip:$PYTHONPATH" ;; # Spark submit uses SPARK_JAVA_OPTS + SPARK_SUBMIT_OPTS + # SPARK_DRIVER_MEMORY + SPARK_SUBMIT_DRIVER_MEMORY. 'org.apache.spark.deploy.SparkSubmit') OUR_JAVA_OPTS="$SPARK_JAVA_OPTS $SPARK_SUBMIT_OPTS" OUR_JAVA_MEM=$&#123;SPARK_DRIVER_MEMORY:-$DEFAULT_MEM&#125; if [ -n "$SPARK_SUBMIT_LIBRARY_PATH" ]; then if [[ $OSTYPE == darwin* ]]; then export DYLD_LIBRARY_PATH="$SPARK_SUBMIT_LIBRARY_PATH:$DYLD_LIBRARY_PATH" else export LD_LIBRARY_PATH="$SPARK_SUBMIT_LIBRARY_PATH:$LD_LIBRARY_PATH" fi fi if [ -n "$SPARK_SUBMIT_DRIVER_MEMORY" ]; then OUR_JAVA_MEM="$SPARK_SUBMIT_DRIVER_MEMORY" fi ;; *) OUR_JAVA_OPTS="$SPARK_JAVA_OPTS" OUR_JAVA_MEM=$&#123;SPARK_DRIVER_MEMORY:-$DEFAULT_MEM&#125; ;;esac 可能存在以下几种情况： org.apache.spark.deploy.master.Master org.apache.spark.deploy.worker.Worker org.apache.spark.deploy.history.HistoryServer org.apache.spark.executor.CoarseGrainedExecutorBackend org.apache.spark.executor.MesosExecutorBackend org.apache.spark.deploy.SparkSubmit 并分别设置每种情况下的Java运行参数和使用内存大小，以表格形式表示如下： OUR_JAVA_OPTS OUR_JAVA_MEM Master $SPARK_DAEMON_JAVA_OPTS $SPARK_MASTER_OPTS ${SPARK_DAEMON_MEMORY:-$DEFAULT_MEM} Worker $SPARK_DAEMON_JAVA_OPTS $SPARK_WORKER_OPTS ${SPARK_DAEMON_MEMORY:-$DEFAULT_MEM} HistoryServer $SPARK_DAEMON_JAVA_OPTS $SPARK_HISTORY_OPTS ${SPARK_DAEMON_MEMORY:-$DEFAULT_MEM} CoarseGrainedExecutorBackend $SPARK_JAVA_OPTS $SPARK_EXECUTOR_OPTS ${SPARK_EXECUTOR_MEMORY:-$DEFAULT_MEM} MesosExecutorBackend $SPARK_JAVA_OPTS $SPARK_EXECUTOR_OPTS ${SPARK_EXECUTOR_MEMORY:-$DEFAULT_MEM} SparkSubmit $SPARK_JAVA_OPTS $SPARK_SUBMIT_OPTS ${SPARK_DRIVER_MEMORY:-$DEFAULT_MEM} 通过上表就可以知道每一个spark中每个进程如何设置JVM参数和内存大小。 接下来是查找JAVA_HOME并检查Java版本。 设置SPARK_TOOLS_JAR变量。 运行bin/compute-classpath.sh计算classpath。 判断SPARK_SUBMIT_BOOTSTRAP_DRIVER变量值，如果该值为1，则运行org.apache.spark.deploy.SparkSubmitDriverBootstrapper类，以替换原来的org.apache.spark.deploy.SparkSubmit的类，执行的脚本为exec &quot;$RUNNER&quot; org.apache.spark.deploy.SparkSubmitDriverBootstrapper &quot;$@&quot;；否则，运行java命令exec &quot;$RUNNER&quot; -cp &quot;$CLASSPATH&quot; $JAVA_OPTS &quot;$@&quot;。 从最后运行的脚本可以看到，spark-class脚本的作用主要是查找java命令、计算环境变量、设置JAVA_OPTS等，至于运行的是哪个java类的main方法，取决于SPARK_SUBMIT_BOOTSTRAP_DRIVER变量的值。 接下来，就是要分析org.apache.spark.deploy.SparkSubmitDriverBootstrapper和org.apache.spark.deploy.SparkSubmit类的运行逻辑以及两者之间的区别，这部分内容见下篇文章。]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala中的对象]]></title>
    <url>%2F2015%2F06%2F19%2Fscala-object%2F</url>
    <content type="text"><![CDATA[Scala中没有静态方法或静态字段，但可以使用object这个语法结构来实现相同的功能。对象与类在语法层面上很相似，除了不能提供构造器参数外，对象可以拥有类的所有特性。 Scala的object定义了单个实例，其可以用来存放工具函数或常量等： 12345678object Timer &#123; var count = 0 def currentCount(): Long = &#123; count += 1 count &#125;&#125; 使用object中的常量或方法，通过object名称直接调用，对象构造器在对象第一次被使用时调用（如果某对象一直未被使用，那么其构造器也不会被调用）。 12345678scala&gt; Timer.currentCount()res52: Long = 1scala&gt; Timer.currentCountres53: Long = 2scala&gt; Timer.countres54: Int = 2 object的构造器不接受参数传递。 伴生对象对象如果与某个类同名，那么它就是一个伴生对象。类和它的伴生对象必须在同一个源文件中，可以将在Java类中定义的静态常量、方法等放置到Scala的类的伴生对象中。 类可以访问伴生对象私有属性，但是必须通过伴生对象.属性名 或 伴生对象.方法 调用，伴生对象也可以访问类的私有属性。 伴生对象是类的一个特殊实例。 12345678class Counter&#123; def getTotalCounter()= Counter.getCount&#125;object Counter&#123; private var cnt = 0 private def getCount()= cnt&#125; 对象可以继承类，以及一个或多个特质，其结果是一个继承了指定类以及特质的类的对象，同时拥有在对象定义中给出的所有特性。 123456789abstract class Person(var name:String, var age:Int)&#123; def info():Unit&#125;object XiaoMing extends Person("XiaoMing", 5)&#123; def info()&#123; println(" name is "+name+", age is "+age) &#125;&#125; Java程序通常从一个public类的main方法开始。而在Scala中，程序从对象的main方法开始，方法的类型是 Array[String] =&gt; Unit。 12345object Hello &#123; def main(args: Array[String]) &#123; println("Hello, world!") &#125;&#125; 除此之外，还可以扩展App特质，然后将程序代码放在构造器内即可，命令行参数从args属性获取。 123456object Hello extends App &#123; if (args.length &gt; 0) println("Hello, " + args(0)) else println("Hello, world!")&#125; apply 方法apply方法是对象的一类特有方法，一般可用于创建伴生类。apply方法可以用简洁的方式调用，形如Object(args..)， 当然，你也可以跟其他方法一样调用，Object.apply(args...)，这两种写法的结果是一样的。 现在，当你看到List(1,2,3)这样的语句就不会感到奇怪了，这只是List.apply(1,2,3)的简写而已。 使用apply方法的好处是，在创建对象时，可以省去使用new关键字。 当类或对象有一个主要用途的时候，apply方法为你提供了一个很好的语法糖。 12345678910scala&gt; class Foo &#123;&#125;defined class Fooscala&gt; object FooMaker &#123; | def apply() = new Foo | &#125;defined module FooMakerscala&gt; val newFoo = FooMaker() //没有newnewFoo: Foo = Foo@5b83f762 或者： 12345678910scala&gt; class Bar &#123; | def apply() = 0 | &#125;defined class Barscala&gt; val bar = new Barbar: Bar = Bar@47711479scala&gt; bar()res8: Int = 0 枚举在Scala中并没有枚举类型，但在标准类库中提供了Enumeration类来获得枚举。扩展Enumeration类后，调用Value方法来初始化枚举中的可能值。 123object TrafficLightColor extends Enumeration &#123; val Red, Yellow, Green = Value&#125; 上述实例中代码可以改为下面这样，区别是：Value方法每次返回内部类Value的新实例。 123val Red = Valueval Yellow = Valueval Green = Value 用Value方法初始化枚举类变量时，Value方法会返回内部类的新实例，且该内部类也叫Value。另外，在调用Value方法时，也可传入ID、名称两参数。如果未指定ID，默认从零开始，后面参数的ID是前一参数ID值加1。如果未指定名称，默认与属性字段同名。 12345object TrafficLight extends Enumeration&#123; val Red = Value(1, "Stop") val Yellow = Value("Wait") //可以单独传名称 val Green = Value(4) //可以单独传ID&#125; 上例中，Yellow属性就仅定义了名称，Green仅定义ID。 参数在不指定名称时，默认参数的Value为字段名。枚举类型的值是 对象名.Value ，如上例中的枚举类型是 TrafficLight.Value。 12TrafficLight.Green//TrafficLight.Value = Green 通过id方法获取枚举类型值的ID: 12TrafficLight.Yellow.id//Int = 2 通过values方法获取所有枚举值的集合: 12TrafficLight.values//TrafficLight.ValueSet = TrafficLight.ValueSet(Stop, Wait, Green) 通过ID来获取对应的枚举对象: 12TrafficLight(1)//TrafficLight.Value = Stop 参考文章 Scala学习——对象]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala中的类]]></title>
    <url>%2F2015%2F06%2F19%2Fscala-class%2F</url>
    <content type="text"><![CDATA[阅读《Programming in Scala》，整理Scala类、继承、重载相关的一些知识点。 类Scala使用class来定义类。 12345class Counter &#123; private var value = 0 // 必须初始化字段 def increment() &#123; value += 1 &#125; // 方法默认公有 def current() = value //空括号方法&#125; Scala中的类不能声明为public，一个Scala源文件中可以有多个类。 类的初始化和调用： 12val myCounter = new Counter // 或new Counter()myCounter.increment() Scala在遇到混合了无参数和空括号方法的情况时很大度。特别是，你可以用空括号方法重载无参数方法，并且反之亦可。你还可以在调用任何不带参数的方法时省略空的括号： 1234myCounter.incrementmyCounter.current()myCounter.current 原则上 Scala 的函数调用中可以省略所有的空括号，但在可能产生副作用的情况下，推荐仍然写一对空的括号。 如果将current方法的声明改为下面这种无参方法的形式，则调用时不能带( )： 1234567891011class Counter &#123; private var value = 0 // 必须初始化字段 def increment() &#123; value += 1 &#125; // 方法默认公有 def current = value //无参方法&#125;val myCounter = new Counter myCounter.current() // 调用必须是myCounter.current这种风格&lt;console&gt;:10: error: Int does not take parameters myCounter.current() 我们还可以选择把current作为字段而不是方法来实现，只要简单地在每个实现里把 def 修改成 val 即可： 12345class Counter &#123; private var value = 0 // 必须初始化字段 def increment() &#123; value += 1 &#125; // 方法默认公有 val current = value &#125; 唯一的差别是字段的访问或许稍微比方法调用要快，因为字段值在类被初始化的时候被预计算，而方法调用在每次调用的时候都要计算。换句话说，字 段在每个 Element 对象上需要更多的内存空间。 getter和setterScala对每个字段都提供了getter和setter方法。 123class Person &#123; var age = 0&#125; 上面例子中，getter和setter分别叫做age和age_=。 12println(fred.age) // 调用方法fred.age()fred.age = 21 // 调用方法fred.age_=(21) 将这个简单的Person编译后，使用javap查看生成的字节码，可以验证这一点。 12// -private选项说明显示所有的类和成员javap -private Person.class Person字节码如下： 123456public class Person implements scala.ScalaObject &#123; private int age; public int age(); public void age_$eq(int); // =号被翻译成了$eq public Person();&#125; Scala中，字段和getter/setter间的关系，还有其他几种情况。 使用val声明的字段，是只有getter，因为val声明的是不可变的。Scala中不能实现只有setter的字段。 还有种对象私有字段。Scala中，方法可以访问该类的所有对象的私有字段，这一点与Java一样。如果通过private[this]来字段来修饰，那么这个字段是对象私有的，这种情况下，不会生成getter和setter。对象私有字段，只能由当前对象的方法访问，而该类的其他对象的方法是无法访问的。 接下来是一种与private[this]相似的访问控制。Scala中可以使用private[class-name]来指定可以访问该字段的类，class-name必须是当前定义的类，或者是当前定义的类的外部类。这种情况会生成getter和setter方法。 没有初始值的字段即是抽象字段，关于抽象类的说明，后面再讨论。根据是val声明还是var声明，会生成相应的抽象的setter/getter，但是不生成字段。 1234abstract class Person &#123; val id: Int var name: String&#125; 查看编译后的字节码，可以得知，JVM类只生成了setter/getter，但没有生成字段。 123456public abstract class Person implements scala.ScalaObject &#123; public abstract int id(); public abstract java.lang.String name(); public abstract void name_$eq(java.lang.String); public Person();&#125; Bean属性使用 @BeanProperty 注解来为字段生成符合JavaBeans规范的getter/setter方法。使用该注解后，将会生成4个方法：Scala的getter/setter和JavaBeans规范的getter/setter（如果是val声明，就没有setter部分了）。 123456import scala.reflect.BeanProperty// 在Scala 2.10.0之后已被废弃// 使用scala.beans.BeanProperty代替class Person &#123; @BeanProperty var name: String = _&#125; 构造器在Scala中，有两种构造器，主构造器（primary constructor）和辅助构造器（auxiliary constructor）。 辅助构造器辅助构造器与Java构造器很相似，但有两点不同： 名字是this（Java中构造器名称与类名相同） 辅助构造器必须以对已经定义的辅助构造器或主构造器的调用开始 1234567891011121314class Person &#123; private var name = "" private var age = 0 def this(name: String) &#123; this() // 调用主构造器 this.name = name &#125; def this(name: String, age: Int) &#123; this(name) // 调用辅助构造器 this.age = age &#125;&#125; 调用： 123val p1 = new Person // 主构造器val p2 = new Person("Fred") // 第一个辅助构造器val p3 = new Person("Fred", 42) // 第二个辅助构造器 主构造器在scala中每个类都有主构造器，主构造器并不是以this方法定义，而是与类定义交织在一起。 1、主构造器参数直接放在类名之后，指的是()中的参数，主构造器参数会被编译成字段，其值被初始化成构造时传入的参数。 123class Person(a: String) &#123; val name:String =a&#125; 2、主构造器会执行类定义中的所有语句，这里是是{}中的语句。 123456class Person &#123; println(0) def printNum(num: Int) &#123; println(num) &#125; println(1) printNum(2)&#125; 3、如果主构造器参数不带val或var，那么会根据是否被方法使用来决定。如果不带val或var的参数被方法使用了，它会变为对象私有字段；如果没有被方法使用，则被当成一个普通的参数，不升级成字段。这部分说明请看参数化字段。 4、可以将主构造器变为私有的，将private关键字放在圆括号前： 12class Person private(var name: String,val age: Int)&#123;&#125; 编译之后： 1234567891011//javappublic class Person &#123; private java.lang.String name; private final int age; public java.lang.String name(); public void name_$eq(java.lang.String); public int age(); //注意私有构造方法 private Person(java.lang.String, int);&#125; 参数化字段1、构造参数不带var或val，被类中函数使用： 123class Person(a: String) &#123; def name:String =a&#125; 则构造参数被升格为私有字段，效果类似private[this] val，反编译该类为： 1234567891011public class Person extends java.lang.Object&#123; //私有final private final java.lang.String a; public static java.lang.String $lessinit$greater$default$1(); //函数 public java.lang.String name(); public Person(java.lang.String);&#125; 2、构造参数不带var或val，未在类中使用，则该参数为普通参数： 12class Person(a: String) &#123;&#125; 反编译为： 123public class Person extends java.lang.Object&#123; public Person(java.lang.String);&#125; 3、构造参数带var或val。Person类的定义中有一个构造参数a，并在name方法中被使用，如果你想避免这种参数和方法混合在一起的定义方式，你可以使用参数化字段来定义类，如下： 1234// 请注意小括号 class Person( val name: String) &#123;&#125; 这是在同一时间使用相同的名称定义参数和属性的一个简写方式。尤其特别的是，类 Person 现在拥有一个可以从类外部访问的不能重新赋值的属性name。 同样也可以使用var前缀类参数，这种情况下相应的字段将能重新被赋值。最终，还有可能添加 如private、protected、或override这类的修饰符到这些参数化字段上，就好象你可以在其他类成员上做的事情。 嵌套类在Scala中，几乎可以在任何的语法结构中内嵌任何语法结构。可以类中定义类，也可以在方法中定义方法。 Scala中每个实例都有自己的内部类。 12345678910111213141516171819202122232425import scala.collection.mutable.ArrayBuffer class NetWork &#123; class Member(val name: String) &#123; &#125; private val members = new ArrayBuffer[Member] def join(m: Member) = &#123; members += m m &#125;&#125; val chatter = new NetWorkval myFace = new NetWorkval m1=new chatter.Member("m1")val m2=new chatter.Member("m2")chatter.join(m1)chatter.join(m2)val m3=new myFace.Member("m3")chatter.join(m3) 往chatter中加入m3时，会出现编译错误： 1234&lt;console&gt;:14: error: type mismatch; found : myFace.Member required: chatter.Member chatter.join(m3) 这是因为，chatter.Member类和myFace.Member类是不同的两个类。这一点与Java中内部类是不同的。 如果想产生类似Java中的内部类特性，可以将Member声明到Network的外部，或者使用类型投影，这里是将内部类中的Member换成NetWork#Member，代码如下。 12345678910111213141516171819202122232425import scala.collection.mutable.ArrayBuffer class NetWork &#123; class Member(val name: String) &#123; &#125; private val members = new ArrayBuffer[NetWork#Member] def join(m: NetWork#Member) = &#123; members += m m &#125;&#125; val chatter = new NetWorkval myFace = new NetWorkval m1=new chatter.Member("m1")val m2=new chatter.Member("m2")chatter.join(m1)chatter.join(m2)val m3=new myFace.Member("m3")chatter.join(m3) 与Java中一样，如果需要在内部类中使用外部类的引用，使用 外部类名.class 的语法即可。不过Scala中有一个为这种情况服务的语法： 12345678910111213class NetWork(val name: String) &#123; outer =&gt; class Member(val name: String) &#123; def description = name + " inside " + outer.name &#125;&#125;val work = new NetWork("work")work.name//workval m1=new work.Member("m1")m1.description//m1 inside work 抽象类和Java一样，Scala用abstract修饰抽象类，抽象类没有具体实例方法。具有抽象成员的类本身必须被声明为抽象的。抽象类定义如下： 123abstract class Element &#123; def contents: Array[String]&#125; 请注意类 Element 的 contents 方法并没带有 abstract 修饰符，如果方法没有实现，也就是说没有等号或方法体，它就是抽象的。类 Element 声明了抽象方法 contents，但当前没有定义具体方法。 抽象类不能实例化，否则会得到编译器错误： 123scala&gt; new Element&lt;console&gt;:9: error: class Element is abstract; cannot be instantiated new Element 我们可以向 Element 添加显示宽度和高度的方法： 12345abstract class Element &#123; def contents: Array[String] def height: Int = contents.length def width: Int = if (height == 0) 0 else contents(0).length&#125; 请注意 Element 的三个方法没一个有参数列表，甚至连个空列表都没有。这种无参数方法在Scala里是非常普通的，带有空括号的方法，被称为空括号方法。 继承继承一个抽象类使用extends关键字，如果你省略 extends，Scala 编译器隐式地假设你的类扩展自 scala.AnyRef，在 Java 平台上与 java.lang.Object 一致。 123class ArrayElement(conts: Array[String]) extends Element &#123; def contents: Array[String] = conts&#125; 如果将类声明为final的，则这个类不能被继承。如果将类的方法和字段声明为final，则它们不能被重写。 子类继承超类中所有非私有的成员，如果子类中的成员与超类中成员具有相同名称和参数，则成为重载；如果子类中的成员是具体的而超类中的是抽象的，我们还可以说子类实现了超类中的成员。 上面例子中，ArrayElement类的contents方法重载或者说实现了超类的contents方法，并继承了width和height方法。我们可以实例一个ArrayElement对象，然后调用方法： 1234scala&gt; val ae = new ArrayElement(Array("hello", "world"))ae: ArrayElement = ArrayElement@d94e60scala&gt; ae.widthres1: Int = 5 上面ae变量的类型是ArrayElement，其实我们也可以将其声明为超类类型，这叫做子类型化：是指子类的值可以被用在需要其超类的值的任何地方。 1val e: Element = new ArrayElement(Array("hello")) 这样的话，e变量是声明为Element类型，但是是初始化为ArrayElement类型。这个涉及到多态的概念。 如果子类要调用超类的构造器，则需要这样定义： 1234class LineElement(s: String) extends ArrayElement(Array(s)) &#123; override def width = s.length override def height = 1&#125; LineElement类继承自ArrayElement，并且LineElement类的构造器中传入了一个参数s，LineElement类想要调用超类的构造器，需要把要传递的参数或参数列表放在超类名之后的括号里即可： 1... extends ArrayElement(Array(s)) ... 这样，就完成了子类调用父类的构造器进行初始化父类。 重载统一访问原则只是 Scala 在对待字段和方法方面比 Java 更统一的一个方面；另一个差异是 Scala 里，字段和方法属于相同的命名空间。这使得字段重载无参数方法成为可能。比如说，你可以改变类 ArrayElement 中 contents 的实现，从一个方法变为一个字段，而无需修改类 Element 中 contents 的抽象方法定义： 123class ArrayElement(conts: Array[String]) extends Element &#123; val contents: Array[String] = conts&#125; Scala里禁止在同一个类里用同样的名称定义字段和方法，例如，下面的代码在Scala中无法通过编译： 1234class WontCompile &#123; private var f = 0 // 编译不过，因为字段和方法重名 def f = 1&#125; 通常情况下，Scala 仅为定义准备了两个命名空间：值(字段、方法、包还有单例对象)、类型(类和特质名)，而 Java 有四个：字 段、方法、类型和包。 Scala把字段和方法放进同一个命名空间，这样你就可以使用val重载无参数的方法。 重写在Scala中重写一个非抽象方法必须使用override修饰符，重写超类的抽象方法时，不需要使用override关键字。调用超类的方法就如Java一样，使用super关键字。 请注意 LineElement 里 width 和 height 的定义带着 override 修饰符。Scala里所有重载了父类具体成员的成员都需要这样的修饰符。如果成员实现的 是同名的抽象成员则这个修饰符是可选的。而如果成员并未重载或实现什么其它基类里的成员则禁用这个修饰符。由于类 LineElement 的 height 和 width 重载了类 Element 的具体成员定义，override 是需要的。 这条规则给编译器提供了有用的信息来帮助避免某些难以捕捉的错误并使得系统的改进更加安全。 构造顺序和提前定义现有如下的类： 12345678class Creature &#123; val range: Int = 10 val env: Array[Int] = new Array[Int](range)&#125; class Ant extends Creature &#123; override val range = 2&#125; 在构造时，发生的过程如下： Ant构造器在构造自己之前，调用超类构造器； Creature的构造器将range字段设为10； Creature的构造器初始化env数组，调用range字段的getter； range的getter被Ant类重写了，返回的Ant类中的range，但是Ant类还未初始化，所以返回了0； env被设置成长度为0的数组 Ant构造器继续执行，将range字段设为2。 在Java中也会出现碰见相似的问题，被调用的方法被子类所重写，有可能结果不是预期的。在构造器中，不应该依赖val的值。（只能重写超类抽象的var声明字段，所以没有这个问题；如果是def，也一样会出现这种问题。） 这个问题的根本原因来自于Java语言的设计决定——允许在超类的构造方法中调用子类的方法。而在C++中，构造前后会更改指向虚函数的指针，所以不会出现这类问题。 这个问题有几种解决方法： 将val声明为final，安全但不灵活； 在超类中将val声明为lazy，安全但不高效； 使用提前定义语法。 提前定义语法是将需要提前定义的成员放在extends关键字后的一个语法块中，还需要使用with关键字： 123class Ant extends &#123; override val range = 2&#125; with Creature 提前定义的等号右侧只能引用之前已经有的提前定义，不能出现类中其他的成员（因为都还没初始化呢）。 使用-Xcheckinit编译器标志来调试构造顺序问题。这个标志会在有未初始化的字段被访问时抛出异常。 参考文章 Scala学习——类]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决固定导航时锚点偏移问题]]></title>
    <url>%2F2015%2F06%2F18%2Ffix-anchor-offset-when-using-bootstrap-navbar-fixed-top%2F</url>
    <content type="text"><![CDATA[最近Bootstrap修改了博客主题，使其支持响应式布局，并且将导航菜单固定住，不随滚到条滚动，这样做带来的影响是Categories和Tags页面点击某一个分类或者标签链接时，锚点定位必然定位于页面顶部，这样一来就会被固定住的导航遮挡，例如，我在Categories页面，点击hbase分类，锚点定位最后如下图： 网上查找了一些资料，找到一篇文章点击锚点让定位偏移顶部，这篇文章提到几种解决办法： 第一种，使用css将锚点偏移： 12&lt;a class="target-fix" &gt;&lt;/a&gt;&lt;artivle&gt;主体内容...&lt;/article&gt; css如下： 1234567.target-fix &#123; position: relative; top: -44px; /*偏移值*/ display: block; height: 0; overflow: hidden;&#125; 对于现代浏览器如果支持css的:target声明，可以这么设置： 123article.a-post:target&#123; padding-top:44px;&#125; 第二种，使用JavaScript去调整scroll值： 1234567891011$(function()&#123; if(location.hash)&#123; var target = $(location.hash); if(target.length==1)&#123; var top = target.offset().top-44; if(top &gt; 0)&#123; $('html,body').animate(&#123;scrollTop:top&#125;, 1000); &#125; &#125; &#125;&#125;); 注意：上面代码中的44为固定的导航所占的像素高度，根据你的实际情况做修改。 当然，你也可以使用jquery-hashchange插件去实现上面的功能，但是需要注意jquery-hashchange是否支持你使用的JQuery版本。 1234567891011121314$(function()&#123; /* 绑定事件*/ $(window).hashchange(function()&#123; var target = $(location.hash); if(target.length==1)&#123; var top = target.offset().top-44; if(top &gt; 0)&#123; $('html,body').animate(&#123;scrollTop:top&#125;, 1000); &#125; &#125; &#125;); /* 触发事件 */ $(window).hashchange();&#125;); 分析上面两种方法，我最后使用的是第二种方法，在core.js文件中添加如下代码： 1234567$('a[href^=#][href!=#]').click(function() &#123; var target = document.getElementById(this.hash.slice(1)); if (!target) return; var targetOffset = $(target).offset().top-70; $('html,body').animate(&#123;scrollTop: targetOffset&#125;, 400); return false;&#125;); 这里，我是在链接上监听单击事件，获取目标对象的偏移，上面减去70是因为下面的css代码： 123456#wrap &#123; min-height: 100%; height: auto; margin: 0 auto -60px; padding: 70px 0 60px;&#125; 刷新页面，再次点击目录或者标签，就可以正常的跳到锚点位置了。你可以点击分类hbase 试试效果。 但是，还没有结束。如果是从其他页面，例如，在文章页面点击分类或标签时，页面却不会跳转到正确的锚点位置。这是因为上面的javascript代码只是考虑了当前页面，是在当前页面获取目标的偏离，而没有考虑在另外一个页面单击链接跳到目标页面的锚点的情况。 所以，我们需要修改代码： 123456789101112var handler=function(hash)&#123; var target = document.getElementById(hash.slice(1)); if (!target) return; var targetOffset = $(target).offset().top-70; $('html,body').animate(&#123;scrollTop: targetOffset&#125;, 400);&#125;$('a[href^=#][href!=#]').click(function()&#123; handler(this.hash)&#125;);if(location.hash)&#123; handler(location.hash) &#125; 这样，就大功告成了，希望这篇文章对你有所帮助。 参考文章 点击锚点让定位偏移顶部]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Scala高价函数简化代码]]></title>
    <url>%2F2015%2F06%2F18%2Fsimplify-code-using-scala-higher-order-function%2F</url>
    <content type="text"><![CDATA[在Scala里，带有其他函数做参数的函数叫做高阶函数，使用高阶函数可以简化代码。 减少重复代码有这样一段代码，查找当前目录样以某一个字符串结尾的文件： 123456object FileMatcher &#123; private def filesHere = (new java.io.File(".")).listFiles def filesEnding(query: String) = for (file &lt;- filesHere; if file.getName.endsWith(query)) yield file&#125; 如果，我们想查找包含某一个字符串的文件，则代码需要修改为： 123def filesContaining(query: String) = for (file &lt;- filesHere; if file.getName.contains(query)) yield file 上面的改动只是使用了 contains 替代 endsWith，但是随着需求越来越复杂，我们要不停地去修改这段代码。例如，我想实现正则匹配的查找，则代码会是下面这个样子： 123def filesRegex(query: String) = for (file &lt;- filesHere; if file.getName.matches(query)) yield file 为了应变复杂的需求，我们可以进行重构代码，抽象出变化的代码部分，将其声明为一个方法： 1234def filesMatching(query: String,matcher: (String, String) =&gt; Boolean) = &#123; for (file &lt;- filesHere; if matcher(file.getName, query)) yield file&#125; 这样，针对不同的需求，我们可以编写不同的matcher方法实现，该方法返回一个布尔值。 有了这个新的 filesMatching 帮助方法，你可以通过让三个搜索方法调用它，并传入合适的函数 来简化它们： 12345def filesEnding(query: String) = filesMatching(query, _.endsWith(_))def filesContaining(query: String) = filesMatching(query, _.contains(_))def filesRegex(query: String) = filesMatching(query, _.matches(_)) 上面的例子使用了占位符，例如， filesEnding 方法里的函数文本 _.endsWith(_) 其实就是： 1(fileName: String, query: String) =&gt; fileName.endsWith(query) 因为，已经确定了参数类型为字符串，故上面可以省略参数类型。由于第一个参数 fileName 在方法体中被第一个使用，第二个参数 query 第二个使用，你也可以使用占位符语法：_.endsWith(_)。第一个下划线是第一个参数文件名的占位符，第二个下划线是第二个参数查询字串的占位符。 因为query参数是从外部传过来的，其可以直接传递给matcher函数，故filesMatching可以只需要一个参数： 12345678910111213object FileMatcher &#123; private def filesHere = (new java.io.File(".")).listFiles private def filesMatching(matcher: String =&gt; Boolean) = for (file &lt;- filesHere; if matcher(file.getName)) yield file def filesEnding(query: String) = filesMatching(_.endsWith(query)) def filesContaining(query: String) = filesMatching(_.contains(query)) def filesRegex(query: String) = filesMatching(_.matches(query))&#125; 上面的例子使用了函数作为第一类值帮助你减少代码重复的方式，另外还演示了闭包是如何能帮助你减少代码重复的。前面一个例子里用到的函数文本，如 _.endsWith(_)和_.contains(_)都是在运行期实例化成函数值而不是闭包，因为它们没有捕 获任何自由变量。 举例来说，表达式_.endsWith(_)里用的两个变量都是用下划线代表的，也就是说它们都是从传递给函数的参数获得的。因此，_.endsWith(_)使用了两个绑定变量，而不是自由变量。 相对的，最近的例子里面用到的函数文本_.endsWith(query)包含一个绑定变量，下划线代表的参数和一个名为 query 的自由变量。仅仅因为 Scala 支持闭包才使得你可以在最近的这个例子里从 filesMatching 中去掉 query 参数，从而更进一步简化了代码。 另外一个例子，是循环集合时可以使用exists方法来简化代码。以下是使用了这种方式的方法去判断是否传入的 List 包含了负数的例子: 1234567def containsNeg(nums: List[Int]): Boolean = &#123; var exists = false for (num &lt;- nums) if (num &lt; 0) exists = true exists&#125; 采用和上面例子同样的方法，我们可以抽象代码，将重要的逻辑抽离到一个独立的方法中去实现。对于上面的查找判断是否存在的逻辑，Scala中提供了高阶函数 exists 来实现，代码如下： 1def containsNeg(nums: List[Int]) = nums.exists(_ &lt; 0) 同样，如果你要查找集合中是否存在偶数，则可以使用下面的代码： 1def containsOdd(nums: List[Int]) = nums.exists(_ % 2 == 1) 柯里化当函数有多个参数列表时，可以使用柯里化函数来简化代码调用。例如，对下面的函数，它实现两个 Int 型参数，x 和 y 的加法： 12345scala&gt; def plainOldSum(x: Int, y: Int) = x + yplainOldSum: (Int,Int)Intscala&gt; plainOldSum(1, 2)res4: Int = 3 我们可以将其柯里化，代之以一个列表的两个Int参数，实现如下： 12345scala&gt; def curriedSum(x: Int)(y: Int) = x + ycurriedSum: (Int)(Int)Intscala&gt; curriedSum(1)(2)res5: Int = 3 当你调用 curriedSum，你实际上背靠背地调用了两个传统函数。第一个函数调 用带单个的名为 x 的 Int 参数，并返回第二个函数的函数值，第二个函数带 Int 参数 y。 你可以使用偏函数，填上第一个参数并且部分应用第二个参数。 12scala&gt; val onePlus = curriedSum(1)_onePlus: (Int) =&gt; Int = &lt;function&gt; curriedSum(1)_里的下划线是第二个参数列表的占位符。结果就是指向一个函数的参考，这个函数在被调用的时候，对它唯一的Int参数加1并返回结果： 12scala&gt; onePlus(2)res7: Int = 3 可变长度参数类似柯里化函数，对于同类型的多参数列表，我们还可以使用可变长度参数，这部分内容，请参考《Scala基本语法和概念》中的可变长度参数。 贷出模式前面的例子提到了使用函数作为参数，我们可以将这个函数的执行结果再次作为参数传入函数，即双倍控制结构：能够重复一个操作两次并返回结果。 下面是一个例子： 12345scala&gt; def twice(op: Double =&gt; Double, x: Double) = op(op(x))twice: ((Double) =&gt; Double,Double)Doublescala&gt; twice(_ + 1, 5)res9: Double = 7.0 上面例子中 op 的类型是 Double =&gt; Double，就是说它是带一个 Double 做参数并返回另一个 Double 的函数。这里，op函数等同于： 1def add(x:Int)=x+1 op函数会执行两次，第一次是执行add(5)=6，第二次是执行add(add(5))=add(6)=6+1=7。 任何时候，你发现你的代码中多个地方有重复的代码块，你就应该考虑把它实现为这种双重控制结构。 考虑这样一种需求：打开一个资源，对它进行操作，然后关闭资源，你可以这样实现： 12345678def withPrintWriter(file: File, op: PrintWriter =&gt; Unit) &#123; val writer = new PrintWriter(file) try &#123; op(writer) &#125; finally &#123; writer.close() &#125;&#125; 有了这个方法，你就可以这样使用: 1withPrintWriter(new File("date.txt"), writer =&gt; writer.println(new java.util.Date) ) 注意：这里和上面的例子一样，使用了=&gt; 来映射式定义函数，其可以看成是没有参数的函数，返回一个匿名函数；调用的时候是调用这个返回的匿名函数。 使用这个方法的好处是，调用这个方法只需要关注如何操作资源，而不用去关心资源的打开和关闭。这个技巧被称为贷出模式：loan pattern，因为该函数要个模板方法一样，实现了资源的打开和关闭，而将使用 PrintWriter 操作资源贷出给函数，交由调用者来实现。 例子里的 withPrintWriter 把 PrintWriter 借给函数 op。当函数完成的时候，它发出信号说明它不再需要“借”的资源。于是资源被关闭在 finally 块中，以确信其确实被关闭，而忽略函数是正常结束返回还是抛出了异常。 因为，这个函数有两个参数，所以你可以将该函数柯里化： 12345678def withPrintWriter(file: File)(op: PrintWriter =&gt; Unit) &#123; val writer = new PrintWriter(file) try &#123; op(writer) &#125; finally &#123; writer.close() &#125; &#125; 这样的话，你可以如下方式调用： 1234val file = new File("date.txt")withPrintWriter(file) &#123; writer =&gt; writer.println(new java.util.Date)&#125; 这个例子里，第一个参数列表，包含了一个 File 参数，被写成包围在小括号中。第二个参数列表，包含了一个函数参数，被包围在大括号中。 当一个函数只有一个参数时，可以使用大括号代替小括号。 传名参数 by-name parameter《Programming in Scala》的第九章提到了传名参数这个概念。其中举的例子是：实现一个称为myAssert的断言函数，该函数将带一个函数值做输入并参考一个标志位来决定该做什么。 如果没有传名参数，你可以这样写myAssert： 1234var assertionsEnabled = true def myAssert(predicate: () =&gt; Boolean) = if (assertionsEnabled &amp;&amp; !predicate()) throw new AssertionError 这个定义是正确的，但使用它会有点儿难看： 1myAssert(() =&gt; 5 &gt; 3) 你或许很想省略函数文本里的空参数列表和=&gt;符号，写成如下形式： 1myAssert(5 &gt; 3) // 不会有效，因为缺少() =&gt; 传名函数恰好为了实现你的愿望而出现。要实现一个传名函数，要定义参数的类型开始于=&gt;而不是() =&gt;。例如，你可以通过改变其类型() =&gt; Boolean为=&gt; Boolean，把myAssert的predicate参数改为传名参数。 123def byNameAssert(predicate: =&gt; Boolean) = if (assertionsEnabled &amp;&amp; !predicate) throw new AssertionError 现在你可以在需要断言的属性里省略空的参数了。使用byNameAssert的结果看上去就好象使用了内建控制结构： 1byNameAssert(5 &gt; 3) 传名类型中，空的参数列表()被省略，它仅在参数中被允许。没有什么传名变量或传名字段这样的东西。 现在，你或许想知道为什么你不能简化myAssert的编写，使用陈旧的Boolean作为它参数的类型，如： 123def boolAssert(predicate: Boolean) = if (assertionsEnabled &amp;&amp; !predicate) throw new AssertionError 当然这种格式同样合法，并且使用这个版本boolAssert的代码看上去仍然与前面的一样： 1boolAssert(5 &gt; 3) 虽然如此，这两种方式之间存在一个非常重要的差别须指出。因为boolAssert的参数类型是Boolean，在boolAssert(5 &gt; 3)里括号中的表达式先于boolAssert的调用被评估。表达式5 &gt; 3产生true，被传给boolAssert。相对的，因为byNameAssert的predicate参数的类型是=&gt; Boolean，byNameAssert(5 &gt; 3)里括号中的表达式不是先于byNameAssert的调用被评估的。而是代之以先创建一个函数值，其apply方法将评估5 &gt; 3，而这个函数值将被传递给byNameAssert。 因此这两种方式之间的差别，在于如果断言被禁用，你会看到boolAssert括号里的表达式的某些副作用，而byNameAssert却没有。例如，如果断言被禁用，boolAssert的例子里尝试对x / 0 == 0的断言将产生一个异常： 12345678scala&gt; var assertionsEnabled = false assertionsEnabled: Boolean = false scala&gt; boolAssert(x / 0 == 0) java.lang.ArithmeticException: / by zero at .&lt; init&gt;(&lt; console&gt;:8) at .&lt; clinit&gt;(&lt; console&gt;) at RequestResult$.&lt; init&gt;(&lt; console&gt;:3) at RequestResult$.&lt; clinit&gt;(&lt; console&gt;)... 但在byNameAssert的例子里尝试同样代码的断言将不产生异常： 1scala&gt; byNameAssert(x / 0 == 0) 总结本文主要总结了几种使用Scala高阶函数简化代码的方法，涉及到的知识点有：柯里化、偏函数、函数映射式定义、可变长度参数、贷出模式以及传名参数。需要意识到的是，灵活使用高阶函数可以简化代码，但也可能会增加代码阅读的复杂度。]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统笔记]]></title>
    <url>%2F2015%2F06%2F15%2Fnote-about-recommendation-system%2F</url>
    <content type="text"><![CDATA[1、产生原因 信息过载 无明确需求 2、什么是推荐？在信息过载又没有明确需求的情况下，找到用户感兴趣的东西。 《Mahout实战》上的定义是：推荐就是通过对喜好的这些模式进行预测，借以发现你尚未知晓，却合乎心意的新事物。 3、推荐和搜索区别： 相同点：快速发现有用信息的工具 不同点：搜索引擎是用户找信息；推荐系统是信息找用户 为了解决信息过载的问题，代表性的解决方案是分类目录和搜索引擎。和搜索引擎一样，推荐系统也是一种帮助用户快速发现有用信息的工具。和搜索引擎不同的是，推荐系统不需要用户提供明确的需求，而是通过分析用户的历史行为给用户的兴趣建模，从而主动给用户推荐能够满足他们兴趣和需求的信息。 因此，从某种意义上说，推荐系统和搜索引擎对于用户来说是两个互补的工具。搜索引擎满足了用户有明确目的时的主动查找需求，而推荐系统能够在用户没有明确目的的时候帮助他们发现感兴趣的新内容。 从物品的角度出发，推荐系统可以更好地发掘物品的长尾。长尾商品往往代表了一小部分用户的个性化需求，发掘这类信息正是推荐系统的长项。 4、推荐系统定义推荐系统广泛存在于各类网站中，作为一个应用为用户提供个性化推荐。它需要依赖用户的行为数据，因此一般都由后台日志系统、推荐算法系统和前台展示页面3部分构成。 5、推荐系统任务： 发现有价值的信息 增加曝光度 推荐系统的任务就是联系用户和信息，一方面帮助用户发现对自己有价值的信息，另一方面让信息能够展现在对它感兴趣的用户面前，从而实现信息消费者和信息生产者的双赢。 6、推荐系统的模块： 获取用户偏好，计算用户模型，需求信息 推荐对象模型，特征信息 推荐算法 推荐评估 7、推荐系统原理： 基于社交网络推荐 基于内容推荐：基于物品的属性进行推荐。 热门推荐：推荐热门的商品 协同过滤：仅仅通过了解用户与物品之间的关系进行推荐。 8、常见形式： 猜你喜欢 买了又买 精品推荐 关联互补 9、十大挑战 数据稀疏：基于物品协同 冷启动 增量计算 多样性与精确性的选择 推荐系统的脆弱性 用户行为的挖掘和利用 推荐系统的评估 用户界面及用户体验 多维数据交叉利用 社交网络推荐 10、用户建模流程：用户–&gt;获取用户信息–&gt;建立用户模型–&gt;模型的更新–&gt;利用模型进行推荐–&gt;提供推荐结果–&gt;用户反馈–&gt;更新模型 11、创建一个推荐引擎过程： 1、创建输入。输入的数据是结构化的，包括用户、商品和喜好，或者还有时间戳，用于拆分数据 2、创建推荐程序。选择推荐算法编写推荐程序。 3、训练数据与评分。一般会将数据随机拆分为训练数据和测试数据，一般比例设置为8比2。 4、评估结果。使用查全率和查准率评估推荐程序，或者统计MAE、SMSE、UOC等 5、输出结果。取Top K个结果。 输入的数据模型包括：用户、商品和偏好值，没有偏好值的关联成为布尔型偏好，这表示用户和物品的关联具备三种可能状态：喜好、不喜欢或无所谓。 12、推荐系统评测主要有3种评测推荐效果的实验方法： 离线实验：划分训练集和测试集，在训练集训练用户兴趣模型，在测试集预测 优点：快速方便 缺点：无法用真实的商业指标来衡量 用户调查：用抽样的方法找部分用户试验效果 优点：指标比较真实 缺点：规模受限，统计意义不够 在线实验：AB测试 优点：指标真实 缺点：测试时间长，设计复杂 实际中，这三种方法在推荐算法上线前都要完成。 评测指标： 用户满意度。用户作为推荐系统的重要参与者，其满意度是评测推荐系统的最重要指标。但是，用户满意度没有办法离线计算，只能通过用户调查问卷或者在线实验获得。 预测准确度。预测准确度度量一个推荐系统或者推荐算法预测用户行为的能力。这个指标是最重要的推荐系统离线评测指标，可以通过离线实验计算。在计算该指标时需要有一个离线的数据集，该数据集包含用户的历史行为记录。然后，将该数据集通过时间分成训练集和测试集。最后，通过在训练集上建立用户的行为和兴趣模型预测用户在测试集上的行为，并计算预测行为和测试集上实际行为的重合度作为预测准确度。预测用户对物品评分的行为称为评分预测。评分预测的预测准确度一般通过均方根误差（RMSE）和平均绝对误差（MAE）计算。TopN推荐的预测准确率一般通过准确率（precision）/召回率（recall）度量。 覆盖率。覆盖率（coverage）描述一个推荐系统对物品长尾的发掘能力。覆盖率有不同的定义方法，最简单的定义为推荐系统能够推荐出来的物品占总物品集合的比例。覆盖率是一个内容提供商会关心的指标。 多样性。为了满足用户广泛的兴趣，推荐列表需要能够覆盖用户不同的兴趣领域，即推荐结果需要具有多样性。 新颖性。新颖的推荐是指给用户推荐那些他们以前没有听说过的物品。在一个网站中实现新颖性的最简单办法是，把那些用户之前在网站中对其有过行为的物品从推荐列表中过滤掉。评测新颖度的最简单方法是利用推荐结果的平均流行度，因为越不热门的物品越可能让用户觉得新颖。因此，如果推荐结果中物品的平均热门程度较低，那么推荐结果就可能有比较高的新颖性。 惊喜度。如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高，而推荐的新颖性仅仅取决于用户是否听说过这个推荐结果。 信任度。度量推荐系统的信任度只能通过问卷调查的方式，询问用户是否信任推荐系统的推荐结果。提高推荐系统的信任度主要有两种方法。首先需要增加推荐系统的透明度，而增加推荐系统透明度的主要办法是提供推荐解释。只有让用户了解推荐系统的运行机制，让用户认同推荐系统的运行机制，才会提高用户对推荐系统的信任度。其次是考虑用户的社交网络信息，利用用户的好友信息给用户做推荐，并且用好友进行推荐解释。这是因为用户对他们的好友一般都比较信任，因此如果推荐的商品是好友购买过的，那么他们对推荐结果就会相对比较信任。 实时性。推荐系统的实时性包括两个方面。首先，推荐系统需要实时地更新推荐列表来满足用户新的行为变化。实时性的第二个方面是推荐系统需要能够将新加入系统的物品推荐给用户。这主要考验了推荐系统处理物品冷启动的能力。 健壮性（反作弊）。算法健壮性的评测主要利用模拟攻击。首先，给定一个数据集和一个算法，可以用这个算法给这个数据集中的用户生成推荐列表。然后，用常用的攻击方法向数据集中注入噪声数据，然后利用算法在注入噪声后的数据集上再次给用户生成推荐列表。最后，通过比较攻击前后推荐列表的相似度评测算法的健壮性。如果攻击后的推荐列表相对于攻击前没有发生大的变化，就说明算法比较健壮。 设计推荐系统时尽量使用代价比较高的用户行为。比如，如果有用户购买行为和用户浏览行为，那么主要应该使用用户购买行为，因为购买需要付费，所以攻击购买行为的代价远远大于攻击浏览行为。 在使用数据前，进行攻击检测，从而对数据进行清理。 下图直观地描述了准确率和召回率的含义: 一般来说，评测维度分为如下3种。 用户维度。主要包括用户的人口统计学信息、活跃度以及是不是新用户等。 物品维度。包括物品的属性信息、流行度、平均分以及是不是新加入的物品等。 时间维度。包括季节，是工作日还是周末，是白天还是晚上等。 如果能够在推荐系统评测报告中包含不同维度下的系统评测指标，就能帮我们全面地了解推荐系统性能，找到一个看上去比较弱的算法的优势，发现一个看上去比较强的算法的缺点。 13、协同过滤协同过滤分为：基于用户的推荐和基于物品的推荐。 基于用户的推荐是找出于该用户邻近的用户，然后将这些用户最感兴趣的物品推荐给该用户。通常包括以下几个组件： 数据模型 用户间的相似性度量 用户邻域的定义：N个最相似用户构成的邻域和基于阈值的邻域 推荐引擎 相似性度量的说明，请参考 Mahout推荐引擎介绍。 《Mahout实战》上提到基于用户的推荐，Mahout中最佳方案为：使用两个最近的邻域，欧式距离相似性度量。 基于物品的推荐，就是给目标用户推荐与他喜欢的物品相似度较高的物品。 14、冷启动问题冷启动分几种： 用户冷启动，即用户刚刚来，还没有对物品做出行为，比如你刚注册豆瓣电影，但没有标记过任何一部电影，所以豆瓣很难根据你的行为来做出推荐； 物品冷启动，新的物品一进入网站，还没有用户给出过对它的行为，那么如何将新物品推荐给可能会对它感兴趣的用户； 系统冷启动， 即新开发的网站如何让用户体验到个性化服务的问题。 解决冷启动的方法： 1、利用用户注册信息。我们注册一个网站的账号的时候很可能会填写性别、年龄、职业等人口统计学特征；还有的网站会让用户描述兴趣；以及用户可能通过其他网站比如新浪微博、腾讯账号来登录，这时就可以采用用户的社交数据。利用的人口统计学特征越多，对用户的兴趣描述就越准确。 2、要求用户在注册时对一些物品进行反馈，通过这些反馈来推测用户兴趣。使用决策树来选择待测试的物品。 对于一个物品i，用户们对i的行为可以分为3类（喜欢、不喜欢、无感觉），然后看这三类人的兴趣是不一致，如果这三类人兴趣都差不多，那么i的区分度可能就不那么高，也就是说如果喜欢i和不喜欢i的人是兴趣类似的，那么i就没啥区分度，不能够用来度量新用户的兴趣。这个算法会从区分度最高的物品开始，将用户分成3类，又在每类用户找找到区分度最高的物品继续询问，一层层下去直到最后的叶子节点。 3、利用物品的内容信息。通过向量空间模型来表示物品的内容，将其表示成一个关键词向量，计算权重，再通过计算相似度来计算物品的相似度。这里涉及到文本挖掘工作。 4、发挥专家的作用，即利用专家来对数据进行标注。这个对绝大多数情况并不现实。 UserCF对物品冷启动问题并不敏感，因为总有用户会访问到新物品，此时给那些相似的用户推荐这个物品，就会有越来越多的人来访问这个物品，形成良性循环。在ItemCF中，物品冷启动就是比较严重的问题了，物品冷启动必须频繁更新物品相似度表，时间复杂度高。]]></content>
      <categories>
        <category>mahout</category>
      </categories>
      <tags>
        <tag>recommendation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Mahout实现协同过滤]]></title>
    <url>%2F2015%2F06%2F10%2Fcollaborative-filtering-using-mahout%2F</url>
    <content type="text"><![CDATA[Mahout算法框架自带的推荐器有下面这些： GenericUserBasedRecommender：基于用户的推荐器，用户数量少时速度快； GenericItemBasedRecommender：基于商品推荐器，商品数量少时速度快，尤其当外部提供了商品相似度数据后效率更好； SlopeOneRecommender：基于slope-one算法的推荐器，在线推荐或更新较快，需要事先大量预处理运算，物品数量少时较好； SVDRecommender：奇异值分解，推荐效果较好，但之前需要大量预处理运算； KnnRecommender：基于k近邻算法(KNN)，适合于物品数量较小时； TreeClusteringRecommender：基于聚类的推荐器，在线推荐较快，之前需要大量预处理运算，用户数量较少时效果好； Mahout最常用的三个推荐器是上述的前三个，本文主要讨论前两种的使用。 接口相关介绍基于用户或物品的推荐器主要包括以下几个接口： DataModel 是用户喜好信息的抽象接口，它的具体实现支持从任意类型的数据源抽取用户喜好信息。Taste 默认提供 JDBCDataModel 和 FileDataModel，分别支持从数据库和文件中读取用户的喜好信息。 UserSimilarity 和 ItemSimilarity。UserSimilarity 用于定义两个用户间的相似度，它是基于协同过滤的推荐引擎的核心部分，可以用来计算用户的“邻居”，这里我们将与当前用户口味相似的用户称为他的邻居。ItemSimilarity 类似的，计算内容之间的相似度。 UserNeighborhood 用于基于用户相似度的推荐方法中，推荐的内容是基于找到与当前用户喜好相似的邻居用户的方式产生的。UserNeighborhood 定义了确定邻居用户的方法，具体实现一般是基于 UserSimilarity 计算得到的。 Recommender 是推荐引擎的抽象接口，Taste 中的核心组件。程序中，为它提供一个 DataModel，它可以计算出对不同用户的推荐内容。实际应用中，主要使用它的实现类 GenericUserBasedRecommender 或者 GenericItemBasedRecommender，分别实现基于用户相似度的推荐引擎或者基于内容的推荐引擎。 RecommenderEvaluator：评分器。 RecommenderIRStatsEvaluator：搜集推荐性能相关的指标，包括准确率、召回率等等。 目前，Mahout为DataModel提供了以下几种实现： org.apache.mahout.cf.taste.impl.model.GenericDataModel org.apache.mahout.cf.taste.impl.model.GenericBooleanPrefDataModel org.apache.mahout.cf.taste.impl.model.PlusAnonymousUserDataModel org.apache.mahout.cf.taste.impl.model.file.FileDataModel org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel org.apache.mahout.cf.taste.impl.model.cassandra.CassandraDataModel org.apache.mahout.cf.taste.impl.model.mongodb.MongoDBDataModel org.apache.mahout.cf.taste.impl.model.jdbc.SQL92JDBCDataModel org.apache.mahout.cf.taste.impl.model.jdbc.MySQLJDBCDataModel org.apache.mahout.cf.taste.impl.model.jdbc.PostgreSQLJDBCDataModel org.apache.mahout.cf.taste.impl.model.jdbc.GenericJDBCDataModel org.apache.mahout.cf.taste.impl.model.jdbc.SQL92BooleanPrefJDBCDataModel org.apache.mahout.cf.taste.impl.model.jdbc.MySQLBooleanPrefJDBCDataModel org.apache.mahout.cf.taste.impl.model.jdbc.PostgreBooleanPrefSQLJDBCDataModel org.apache.mahout.cf.taste.impl.model.jdbc.ReloadFromJDBCDataModel 从类名上就可以大概猜出来每个DataModel的用途，奇怪的是竟然没有HDFS的DataModel，有人实现了一个，请参考MAHOUT-1579。 UserSimilarity 和 ItemSimilarity 相似度实现有以下几种： CityBlockSimilarity：基于Manhattan距离相似度 EuclideanDistanceSimilarity：基于欧几里德距离计算相似度 LogLikelihoodSimilarity：基于对数似然比的相似度 PearsonCorrelationSimilarity：基于皮尔逊相关系数计算相似度 SpearmanCorrelationSimilarity：基于皮尔斯曼相关系数相似度 TanimotoCoefficientSimilarity：基于谷本系数计算相似度 UncenteredCosineSimilarity：计算 Cosine 相似度 以上相似度的说明，请参考Mahout推荐引擎介绍。 UserNeighborhood 主要实现有两种： NearestNUserNeighborhood：对每个用户取固定数量N个最近邻居 ThresholdUserNeighborhood：对每个用户基于一定的限制，取落在相似度限制以内的所有用户为邻居 Recommender分为以下几种实现： GenericUserBasedRecommender：基于用户的推荐引擎 GenericBooleanPrefUserBasedRecommender：基于用户的无偏好值推荐引擎 GenericItemBasedRecommender：基于物品的推荐引擎 GenericBooleanPrefItemBasedRecommender：基于物品的无偏好值推荐引擎 RecommenderEvaluator有以下几种实现： AverageAbsoluteDifferenceRecommenderEvaluator：计算平均差值 RMSRecommenderEvaluator：计算均方根差 RecommenderIRStatsEvaluator的实现类是GenericRecommenderIRStatsEvaluator。 单机运行首先，需要在maven中加入对mahout的依赖： 1234567891011121314151617181920212223&lt;dependency&gt; &lt;groupId&gt;org.apache.mahout&lt;/groupId&gt; &lt;artifactId&gt;mahout-core&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.mahout&lt;/groupId&gt; &lt;artifactId&gt;mahout-integration&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.mahout&lt;/groupId&gt; &lt;artifactId&gt;mahout-math&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.mahout&lt;/groupId&gt; &lt;artifactId&gt;mahout-examples&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt;&lt;/dependency&gt; 基于用户的推荐，以FileDataModel为例： 123456789101112131415161718File modelFile modelFile = new File("intro.csv");DataModel model = new FileDataModel(modelFile);//用户相似度，使用基于皮尔逊相关系数计算相似度UserSimilarity similarity = new PearsonCorrelationSimilarity(model);//选择邻居用户，使用NearestNUserNeighborhood实现UserNeighborhood接口，选择邻近的4个用户UserNeighborhood neighborhood = new NearestNUserNeighborhood(4, similarity, model);Recommender recommender = new GenericUserBasedRecommender(model, neighborhood, similarity);//给用户1推荐4个物品List&lt;RecommendedItem&gt; recommendations = recommender.recommend(1, 4);for (RecommendedItem recommendation : recommendations) &#123; System.out.println(recommendation);&#125; 注意：FileDataModel要求输入文件中的字段分隔符为逗号或者制表符，如果你想使用其他分隔符，你可以扩展一个FileDataModel的实现，例如，mahout中已经提供了一个解析MoiveLens的数据集（分隔符为::）的实现GroupLensDataModel。 GenericUserBasedRecommender是基于用户的简单推荐器实现类，推荐主要参照传入的DataModel和UserNeighborhood，总体是三个步骤： (1) 从UserNeighborhood获取当前用户Ui最相似的K个用户集合{U1, U2, …Uk}； (2) 从这K个用户集合排除Ui的偏好商品，剩下的Item集合为{Item0, Item1, …Itemm}； (3) 对Item集合里每个Itemj计算Ui可能偏好程度值pref(Ui, Itemj)，并把Item按此数值从高到低排序，前N个item推荐给用户Ui。 对相同用户重复获得推荐结果，我们可以改用CachingRecommender来包装GenericUserBasedRecommender对象，将推荐结果缓存起来： 1Recommender cachingRecommender = new CachingRecommender(recommender); 上面代码可以在main方法中直接运行，然后，我们可以获取推荐模型的评分： 1234567891011121314//使用平均绝对差值获得评分RecommenderEvaluator evaluator = new AverageAbsoluteDifferenceRecommenderEvaluator();// 用RecommenderBuilder构建推荐引擎RecommenderBuilder recommenderBuilder = new RecommenderBuilder() &#123; @Override public Recommender buildRecommender(DataModel model) throws TasteException &#123; UserSimilarity similarity = new PearsonCorrelationSimilarity(model); UserNeighborhood neighborhood = new NearestNUserNeighborhood(4, similarity, model); return new GenericUserBasedRecommender(model, neighborhood, similarity); &#125;&#125;;// Use 70% of the data to train; test using the other 30%.double score = evaluator.evaluate(recommenderBuilder, null, model, 0.7, 1.0);System.out.println(score); 接下来，可以获取推荐结果的查准率和召回率： 123456789101112131415RecommenderIRStatsEvaluator statsEvaluator = new GenericRecommenderIRStatsEvaluator();// Build the same recommender for testing that we did last time:RecommenderBuilder recommenderBuilder = new RecommenderBuilder() &#123; @Override public Recommender buildRecommender(DataModel model) throws TasteException &#123; UserSimilarity similarity = new PearsonCorrelationSimilarity(model); UserNeighborhood neighborhood = new NearestNUserNeighborhood(4, similarity, model); return new GenericUserBasedRecommender(model, neighborhood, similarity); &#125;&#125;;// 计算推荐4个结果时的查准率和召回率IRStatistics stats = statsEvaluator.evaluate(recommenderBuilder,null, model, null, 4, GenericRecommenderIRStatsEvaluator.CHOOSE_THRESHOLD,1.0);System.out.println(stats.getPrecision());System.out.println(stats.getRecall()); 如果是基于物品的推荐，代码大体相似，只是没有了UserNeighborhood，然后将上面代码中的User换成Item即可，完整代码如下： 12345678910111213141516171819202122232425262728293031323334353637File modelFile modelFile = new File("intro.csv");DataModel model = new FileDataModel(new File(file));// Build the same recommender for testing that we did last time:RecommenderBuilder recommenderBuilder = new RecommenderBuilder() &#123; @Override public Recommender buildRecommender(DataModel model) throws TasteException &#123; ItemSimilarity similarity = new PearsonCorrelationSimilarity(model); return new GenericItemBasedRecommender(model, similarity); &#125;&#125;;//获取推荐结果List&lt;RecommendedItem&gt; recommendations = recommenderBuilder.buildRecommender(model).recommend(1, 4);for (RecommendedItem recommendation : recommendations) &#123; System.out.println(recommendation);&#125;//计算评分RecommenderEvaluator evaluator = new AverageAbsoluteDifferenceRecommenderEvaluator();// Use 70% of the data to train; test using the other 30%.double score = evaluator.evaluate(recommenderBuilder, null, model, 0.7, 1.0);System.out.println(score);//计算查全率和查准率RecommenderIRStatsEvaluator statsEvaluator = new GenericRecommenderIRStatsEvaluator();// Evaluate precision and recall "at 2":IRStatistics stats = statsEvaluator.evaluate(recommenderBuilder, null, model, null, 4, GenericRecommenderIRStatsEvaluator.CHOOSE_THRESHOLD, 1.0);System.out.println(stats.getPrecision());System.out.println(stats.getRecall()); 在Spark中运行在Spark中运行，需要将Mahout相关的jar添加到Spark的classpath中，修改/etc/spark/conf/spark-env.sh，添加下面两行代码： 12SPARK_DIST_CLASSPATH=&quot;$SPARK_DIST_CLASSPATH:/usr/lib/mahout/lib/*&quot;SPARK_DIST_CLASSPATH=&quot;$SPARK_DIST_CLASSPATH:/usr/lib/mahout/*&quot; 然后，以本地模式在spark-shell中运行下面代码交互测试： 12345678910111213141516171819202122//注意：这里是本地目录val model = new FileDataModel(new File("intro.csv"))val evaluator = new RMSRecommenderEvaluator()val recommenderBuilder = new RecommenderBuilder &#123; override def buildRecommender(dataModel: DataModel): Recommender = &#123; val similarity = new LogLikelihoodSimilarity(dataModel) new GenericItemBasedRecommender(dataModel, similarity) &#125;&#125;val score = evaluator.evaluate(recommenderBuilder, null, model, 0.95, 0.05)println(s"Score=$score")val recommender=recommenderBuilder.buildRecommender(model)val users=trainingRatings.map(_.user).distinct().take(20)import scala.collection.JavaConversions._val result=users.par.map&#123;user=&gt; user+","+recommender.recommend(user,40).map(_.getItemID).mkString(",")&#125; https://github.com/sujitpal/mia-scala-examples上面有一个评估基于物品或是用户的各种相似度下的评分的类，叫做 RecommenderEvaluator，供大家学习参考。 分布式运行Mahout提供了org.apache.mahout.cf.taste.hadoop.item.RecommenderJob类以MapReduce的方式来实现基于物品的协同过滤，查看该类的使用说明： 123456789101112131415161718192021222324252627$ hadoop jar /usr/lib/mahout/mahout-examples-0.9-cdh5.4.0-job.jar org.apache.mahout.cf.taste.hadoop.item.RecommenderJob15/06/10 16:19:34 ERROR common.AbstractJob: Missing required option --similarityClassnameMissing required option --similarityClassnameUsage: [--input &lt;input&gt; --output &lt;output&gt; --numRecommendations &lt;numRecommendations&gt;--usersFile &lt;usersFile&gt; --itemsFile &lt;itemsFile&gt; --filterFile &lt;filterFile&gt;--booleanData &lt;booleanData&gt; --maxPrefsPerUser &lt;maxPrefsPerUser&gt;--minPrefsPerUser &lt;minPrefsPerUser&gt; --maxSimilaritiesPerItem&lt;maxSimilaritiesPerItem&gt; --maxPrefsInItemSimilarity &lt;maxPrefsInItemSimilarity&gt;--similarityClassname &lt;similarityClassname&gt; --threshold &lt;threshold&gt;--outputPathForSimilarityMatrix &lt;outputPathForSimilarityMatrix&gt; --randomSeed&lt;randomSeed&gt; --sequencefileOutput --help --tempDir &lt;tempDir&gt; --startPhase&lt;startPhase&gt; --endPhase &lt;endPhase&gt;]--similarityClassname (-s) similarityClassname Name of distributed similarity measures class to instantiate, alternatively use one of the predefined similarities ([SIMILARITY_COOCCURRENCE, SIMILARITY_LOGLIKELIHOOD, SIMILARITY_TANIMOTO_COEFFICIEN T, SIMILARITY_CITY_BLOCK, SIMILARITY_COSINE, SIMILARITY_PEARSON_CORRELATION , SIMILARITY_EUCLIDEAN_DISTANCE] ) 可见，该类可以接收的命令行参数如下： --input(path): 存储用户偏好数据的目录，该目录下可以包含一个或多个存储用户偏好数据的文本文件； --output(path): 结算结果的输出目录 --numRecommendations (integer): 为每个用户推荐的item数量，默认为10 --usersFile (path): 指定一个包含了一个或多个存储userID的文件路径，仅为该路径下所有文件包含的userID做推荐计算 (该选项可选) --itemsFile (path): 指定一个包含了一个或多个存储itemID的文件路径，仅为该路径下所有文件包含的itemID做推荐计算 (该选项可选) --filterFile (path): 指定一个路径，该路径下的文件包含了[userID,itemID]值对，userID和itemID用逗号分隔。计算结果将不会为user推荐[userID,itemID]值对中包含的item (该选项可选) --booleanData (boolean): 如果输入数据不包含偏好数值，则将该参数设置为true，默认为false --maxPrefsPerUser (integer): 在最后计算推荐结果的阶段，针对每一个user使用的偏好数据的最大数量，默认为10 --minPrefsPerUser (integer): 在相似度计算中，忽略所有偏好数据量少于该值的用户，默认为1 --maxSimilaritiesPerItem (integer): 针对每个item的相似度最大值，默认为100 --maxPrefsPerUserInItemSimilarity (integer): 在item相似度计算阶段，针对每个用户考虑的偏好数据最大数量，默认为1000 --similarityClassname (classname): 向量相似度计算类 outputPathForSimilarityMatrix：SimilarityMatrix输出目录 --randomSeed：随机种子 -sequencefileOutput：序列文件输出路径 --tempDir (path): 存储临时文件的目录，默认为当前用户的home目录下的temp目录 --startPhase --endPhase --threshold (double): 忽略相似度低于该阀值的item对 一个例子如下，使用SIMILARITY_LOGLIKELIHOOD相似度推荐物品： 1$ hadoop jar /usr/lib/mahout/mahout-examples-0.9-cdh5.4.0-job.jar org.apache.mahout.cf.taste.hadoop.item.RecommenderJob --input /tmp/mahout/part-00000 --output /tmp/mahout-out -s SIMILARITY_LOGLIKELIHOOD 默认情况下，mahout使用的reduce数目为1，这样造成大数据处理时效率较低，可以通过参数mahout执行脚本中的MAHOUT_OPTS中的-Dmapred.reduce.tasks参数指定reduce数目。 上面命令运行完成之后，会在当前用户的hdfs主目录生成temp目录，该目录可由--tempDir (path)参数设置： 123456789101112$ hadoop fs -ls tempFound 10 items-rw-r--r-- 3 root hadoop 7 2015-06-10 14:42 temp/maxValues.bin-rw-r--r-- 3 root hadoop 5522717 2015-06-10 14:42 temp/norms.bindrwxr-xr-x - root hadoop 0 2015-06-10 14:41 temp/notUsed-rw-r--r-- 3 root hadoop 7 2015-06-10 14:42 temp/numNonZeroEntries.bin-rw-r--r-- 3 root hadoop 3452222 2015-06-10 14:41 temp/observationsPerColumn.bindrwxr-xr-x - root hadoop 0 2015-06-10 14:47 temp/pairwiseSimilaritydrwxr-xr-x - root hadoop 0 2015-06-10 14:52 temp/partialMultiplydrwxr-xr-x - root hadoop 0 2015-06-10 14:39 temp/preparePreferenceMatrixdrwxr-xr-x - root hadoop 0 2015-06-10 14:50 temp/similarityMatrixdrwxr-xr-x - root hadoop 0 2015-06-10 14:42 temp/weights 观察yarn的管理界面，该命令会生成9个任务，任务名称依次是： PreparePreferenceMatrixJob-ItemIDIndexMapper-Reducer PreparePreferenceMatrixJob-ToItemPrefsMapper-Reducer PreparePreferenceMatrixJob-ToItemVectorsMapper-Reducer RowSimilarityJob-CountObservationsMapper-Reducer RowSimilarityJob-VectorNormMapper-Reducer RowSimilarityJob-CooccurrencesMapper-Reducer RowSimilarityJob-UnsymmetrifyMapper-Reducer partialMultiply RecommenderJob-PartialMultiplyMapper-Reducer 从任务名称，大概可以知道每个任务在做什么，如果你的输入参数不一样，生成的任务数可能不一样，这个需要测试一下才能确认。 在hdfs上查看输出的结果，用户和推荐结果用\t分隔，推荐结果中物品之间用逗号分隔，物品后面通过冒号连接评分： 12843 [10709679:4.8334665,8389878:4.833426,9133835:4.7503786,10366169:4.7503185,9007487:4.750272,8149253:4.7501993,10366165:4.750115,9780049:4.750108,8581254:4.750071,10456307:4.7500467]6253 [10117445:3.0375953,10340299:3.0340924,8321090:3.0340924,10086615:3.032164,10436801:3.0187714,9668385:3.0141575,8502110:3.013954,10476325:3.0074399,10318667:3.0004222,8320987:3.0003839] 使用Java API方式执行，请参考Mahout分步式程序开发 基于物品的协同过滤ItemCF。 在Scala或者Spark中，可以以Java API或者命令方式运行，最后还可以通过Spark来处理推荐的结果，例如：过滤、去重、补足数据，这部分内容不做介绍。 参考文章 协同过滤原理与Mahout实现]]></content>
      <categories>
        <category>mahout</category>
      </categories>
      <tags>
        <tag>mahout</tag>
        <tag>recommendation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark On YARN内存分配]]></title>
    <url>%2F2015%2F06%2F09%2Fmemory-in-spark-on-yarn%2F</url>
    <content type="text"><![CDATA[本文主要了解Spark On YARN部署模式下的内存分配情况，因为没有深入研究Spark的源代码，所以只能根据日志去看相关的源代码，从而了解“为什么会这样，为什么会那样”。 说明按照Spark应用程序中的driver分布方式不同，Spark on YARN有两种模式： yarn-client模式、yarn-cluster模式。 当在YARN上运行Spark作业，每个Spark executor作为一个YARN容器运行。Spark可以使得多个Tasks在同一个容器里面运行。 下图是yarn-cluster模式的作业执行图，图片来源于网络： 关于Spark On YARN相关的配置参数，请参考Spark配置参数。本文主要讨论内存分配情况，所以只需要关注以下几个内心相关的参数： spark.driver.memory：默认值512m spark.executor.memory：默认值512m spark.yarn.am.memory：默认值512m spark.yarn.executor.memoryOverhead：值为executorMemory * 0.07, with minimum of 384 spark.yarn.driver.memoryOverhead：值为driverMemory * 0.07, with minimum of 384 spark.yarn.am.memoryOverhead：值为AM memory * 0.07, with minimum of 384 注意： --executor-memory/spark.executor.memory 控制 executor 的堆的大小，但是 JVM 本身也会占用一定的堆空间，比如内部的 String 或者直接 byte buffer，spark.yarn.XXX.memoryOverhead属性决定向 YARN 请求的每个 executor 或dirver或am 的额外堆内存大小，默认值为 max(384, 0.07 * spark.executor.memory) 在 executor 执行的时候配置过大的 memory 经常会导致过长的GC延时，64G是推荐的一个 executor 内存大小的上限。 HDFS client 在大量并发线程时存在性能问题。大概的估计是每个 executor 中最多5个并行的 task 就可以占满写入带宽。 另外，因为任务是提交到YARN上运行的，所以YARN中有几个关键参数，参考YARN的内存和CPU配置： yarn.app.mapreduce.am.resource.mb：AM能够申请的最大内存，默认值为1536MB yarn.nodemanager.resource.memory-mb：nodemanager能够申请的最大内存，默认值为8192MB yarn.scheduler.minimum-allocation-mb：调度时一个container能够申请的最小资源，默认值为1024MB yarn.scheduler.maximum-allocation-mb：调度时一个container能够申请的最大资源，默认值为8192MB 测试Spark集群测试环境为： master：64G内存，16核cpu worker：128G内存，32核cpu worker：128G内存，32核cpu worker：128G内存，32核cpu worker：128G内存，32核cpu 注意：YARN集群部署在Spark集群之上的，每一个worker节点上同时部署了一个NodeManager，并且YARN集群中的配置如下： 12345678910111213141516&lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;106496&lt;/value&gt; &lt;!-- 104G --&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;106496&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; 将spark的日志基本调为DEBUG，并将log4j.logger.org.apache.hadoop设置为WARN建设不必要的输出，修改/etc/spark/conf/log4j.properties： 12345678910111213# Set everything to be logged to the consolelog4j.rootCategory=DEBUG, consolelog4j.appender.console=org.apache.log4j.ConsoleAppenderlog4j.appender.console.target=System.errlog4j.appender.console.layout=org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern=%d&#123;yy/MM/dd HH:mm:ss&#125; %p %c&#123;1&#125;: %m%n# Settings to quiet third party logs that are too verboselog4j.logger.org.eclipse.jetty=WARNlog4j.logger.org.apache.hadoop=WARNlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERRORlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFOlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO 接下来是运行测试程序，以官方自带的SparkPi例子为例，下面主要测试client模式，至于cluster模式请参考下面的过程。运行下面命令： 12345678spark-submit --class org.apache.spark.examples.SparkPi \ --master yarn-client \ --num-executors 4 \ --driver-memory 2g \ --executor-memory 3g \ --executor-cores 4 \ /usr/lib/spark/lib/spark-examples-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar \ 100000 观察输出日志（无关的日志被略去）： 1234567891011121314151617181920212223242526272829303115/06/08 13:57:01 INFO SparkContext: Running Spark version 1.3.015/06/08 13:57:02 INFO SecurityManager: Changing view acls to: root15/06/08 13:57:02 INFO SecurityManager: Changing modify acls to: root15/06/08 13:57:03 INFO MemoryStore: MemoryStore started with capacity 1060.3 MB15/06/08 13:57:04 DEBUG YarnClientSchedulerBackend: ClientArguments called with: --arg bj03-bi-pro-hdpnamenn:51568 --num-executors 4 --num-executors 4 --executor-memory 3g --executor-memory 3g --executor-cores 4 --executor-cores 4 --name Spark Pi15/06/08 13:57:04 DEBUG YarnClientSchedulerBackend: [actor] handled message (24.52531 ms) ReviveOffers from Actor[akka://sparkDriver/user/CoarseGrainedScheduler#864850679]15/06/08 13:57:05 INFO Client: Requesting a new application from cluster with 4 NodeManagers15/06/08 13:57:05 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (106496 MB per container)15/06/08 13:57:05 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead15/06/08 13:57:05 INFO Client: Setting up container launch context for our AM15/06/08 13:57:07 DEBUG Client: ===============================================================================15/06/08 13:57:07 DEBUG Client: Yarn AM launch context:15/06/08 13:57:07 DEBUG Client: user class: N/A15/06/08 13:57:07 DEBUG Client: env:15/06/08 13:57:07 DEBUG Client: CLASSPATH -&gt; &#123;&#123;PWD&#125;&#125;&lt;CPS&gt;&#123;&#123;PWD&#125;&#125;/__spark__.jar&lt;CPS&gt;$HADOOP_CONF_DIR&lt;CPS&gt;$HADOOP_COMMON_HOME/*&lt;CPS&gt;$HADOOP_COMMON_HOME/lib/*&lt;CPS&gt;$HADOOP_HDFS_HOME/*&lt;CPS&gt;$HADOOP_HDFS_HOME/lib/*&lt;CPS&gt;$HADOOP_MAPRED_HOME/*&lt;CPS&gt;$HADOOP_MAPRED_HOME/lib/*&lt;CPS&gt;$HADOOP_YARN_HOME/*&lt;CPS&gt;$HADOOP_YARN_HOME/lib/*&lt;CPS&gt;$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*&lt;CPS&gt;$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*&lt;CPS&gt;:/usr/lib/spark/lib/spark-assembly.jar::/usr/lib/hadoop/lib/*:/usr/lib/hadoop/*:/usr/lib/hadoop-hdfs/lib/*:/usr/lib/hadoop-hdfs/*:/usr/lib/hadoop-mapreduce/lib/*:/usr/lib/hadoop-mapreduce/*:/usr/lib/hadoop-yarn/lib/*:/usr/lib/hadoop-yarn/*:/usr/lib/hive/lib/*:/usr/lib/flume-ng/lib/*:/usr/lib/paquet/lib/*:/usr/lib/avro/lib/*15/06/08 13:57:07 DEBUG Client: SPARK_DIST_CLASSPATH -&gt; :/usr/lib/spark/lib/spark-assembly.jar::/usr/lib/hadoop/lib/*:/usr/lib/hadoop/*:/usr/lib/hadoop-hdfs/lib/*:/usr/lib/hadoop-hdfs/*:/usr/lib/hadoop-mapreduce/lib/*:/usr/lib/hadoop-mapreduce/*:/usr/lib/hadoop-yarn/lib/*:/usr/lib/hadoop-yarn/*:/usr/lib/hive/lib/*:/usr/lib/flume-ng/lib/*:/usr/lib/paquet/lib/*:/usr/lib/avro/lib/*15/06/08 13:57:07 DEBUG Client: SPARK_YARN_CACHE_FILES_FILE_SIZES -&gt; 9723720815/06/08 13:57:07 DEBUG Client: SPARK_YARN_STAGING_DIR -&gt; .sparkStaging/application_1433742899916_000115/06/08 13:57:07 DEBUG Client: SPARK_YARN_CACHE_FILES_VISIBILITIES -&gt; PRIVATE15/06/08 13:57:07 DEBUG Client: SPARK_USER -&gt; root15/06/08 13:57:07 DEBUG Client: SPARK_YARN_MODE -&gt; true15/06/08 13:57:07 DEBUG Client: SPARK_YARN_CACHE_FILES_TIME_STAMPS -&gt; 143374302739915/06/08 13:57:07 DEBUG Client: SPARK_YARN_CACHE_FILES -&gt; hdfs://mycluster:8020/user/root/.sparkStaging/application_1433742899916_0001/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar#__spark__.jar15/06/08 13:57:07 DEBUG Client: resources:15/06/08 13:57:07 DEBUG Client: __spark__.jar -&gt; resource &#123; scheme: &quot;hdfs&quot; host: &quot;mycluster&quot; port: 8020 file: &quot;/user/root/.sparkStaging/application_1433742899916_0001/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar&quot; &#125; size: 97237208 timestamp: 1433743027399 type: FILE visibility: PRIVATE15/06/08 13:57:07 DEBUG Client: command:15/06/08 13:57:07 DEBUG Client: &#123;&#123;JAVA_HOME&#125;&#125;/bin/java -server -Xmx512m -Djava.io.tmpdir=&#123;&#123;PWD&#125;&#125;/tmp &apos;-Dspark.eventLog.enabled=true&apos; &apos;-Dspark.executor.instances=4&apos; &apos;-Dspark.executor.memory=3g&apos; &apos;-Dspark.executor.cores=4&apos; &apos;-Dspark.driver.port=51568&apos; &apos;-Dspark.serializer=org.apache.spark.serializer.KryoSerializer&apos; &apos;-Dspark.driver.appUIAddress=http://bj03-bi-pro-hdpnamenn:4040&apos; &apos;-Dspark.executor.id=&lt;driver&gt;&apos; &apos;-Dspark.kryo.classesToRegister=scala.collection.mutable.BitSet,scala.Tuple2,scala.Tuple1,org.apache.spark.mllib.recommendation.Rating&apos; &apos;-Dspark.driver.maxResultSize=8g&apos; &apos;-Dspark.jars=file:/usr/lib/spark/lib/spark-examples-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar&apos; &apos;-Dspark.driver.memory=2g&apos; &apos;-Dspark.eventLog.dir=hdfs://mycluster:8020/user/spark/applicationHistory&apos; &apos;-Dspark.app.name=Spark Pi&apos; &apos;-Dspark.fileserver.uri=http://X.X.X.X:49172&apos; &apos;-Dspark.tachyonStore.folderName=spark-81ae0186-8325-40f2-867b-65ee7c922357&apos; -Dspark.yarn.app.container.log.dir=&lt;LOG_DIR&gt; org.apache.spark.deploy.yarn.ExecutorLauncher --arg &apos;bj03-bi-pro-hdpnamenn:51568&apos; --executor-memory 3072m --executor-cores 4 --num-executors 4 1&gt; &lt;LOG_DIR&gt;/stdout 2&gt; &lt;LOG_DIR&gt;/stderr15/06/08 13:57:07 DEBUG Client: =============================================================================== 从Will allocate AM container, with 896 MB memory including 384 MB overhead日志可以看到，AM占用了896 MB内存，除掉384 MB的overhead内存，实际上只有512 MB，即spark.yarn.am.memory的默认值，另外可以看到YARN集群有4个NodeManager，每个container最多有106496 MB内存。 Yarn AM launch context启动了一个Java进程，设置的JVM内存为512m，见/bin/java -server -Xmx512m。 这里为什么会取默认值呢？查看打印上面这行日志的代码，见org.apache.spark.deploy.yarn.Client： 123456789101112131415161718private def verifyClusterResources(newAppResponse: GetNewApplicationResponse): Unit = &#123; val maxMem = newAppResponse.getMaximumResourceCapability().getMemory() logInfo("Verifying our application has not requested more than the maximum " + s"memory capability of the cluster ($maxMem MB per container)") val executorMem = args.executorMemory + executorMemoryOverhead if (executorMem &gt; maxMem) &#123; throw new IllegalArgumentException(s"Required executor memory ($&#123;args.executorMemory&#125;" + s"+$executorMemoryOverhead MB) is above the max threshold ($maxMem MB) of this cluster!") &#125; val amMem = args.amMemory + amMemoryOverhead if (amMem &gt; maxMem) &#123; throw new IllegalArgumentException(s"Required AM memory ($&#123;args.amMemory&#125;" + s"+$amMemoryOverhead MB) is above the max threshold ($maxMem MB) of this cluster!") &#125; logInfo("Will allocate AM container, with %d MB memory including %d MB overhead".format( amMem, amMemoryOverhead))&#125; args.amMemory来自ClientArguments类，这个类中会校验输出参数： 12345678910111213141516171819202122232425262728293031private def validateArgs(): Unit = &#123; if (numExecutors &lt;= 0) &#123; throw new IllegalArgumentException( "You must specify at least 1 executor!\n" + getUsageMessage()) &#125; if (executorCores &lt; sparkConf.getInt("spark.task.cpus", 1)) &#123; throw new SparkException("Executor cores must not be less than " + "spark.task.cpus.") &#125; if (isClusterMode) &#123; for (key &lt;- Seq(amMemKey, amMemOverheadKey, amCoresKey)) &#123; if (sparkConf.contains(key)) &#123; println(s"$key is set but does not apply in cluster mode.") &#125; &#125; amMemory = driverMemory amCores = driverCores &#125; else &#123; for (key &lt;- Seq(driverMemOverheadKey, driverCoresKey)) &#123; if (sparkConf.contains(key)) &#123; println(s"$key is set but does not apply in client mode.") &#125; &#125; sparkConf.getOption(amMemKey) .map(Utils.memoryStringToMb) .foreach &#123; mem =&gt; amMemory = mem &#125; sparkConf.getOption(amCoresKey) .map(_.toInt) .foreach &#123; cores =&gt; amCores = cores &#125; &#125;&#125; 从上面代码可以看到当 isClusterMode 为true时，则args.amMemory值为driverMemory的值；否则，则从spark.yarn.am.memory中取，如果没有设置该属性，则取默认值512m。isClusterMode 为true的条件是 userClass 不为空，def isClusterMode: Boolean = userClass != null，即输出参数需要有--class参数，而从下面日志可以看到ClientArguments的输出参数中并没有该参数。 115/06/08 13:57:04 DEBUG YarnClientSchedulerBackend: ClientArguments called with: --arg bj03-bi-pro-hdpnamenn:51568 --num-executors 4 --num-executors 4 --executor-memory 3g --executor-memory 3g --executor-cores 4 --executor-cores 4 --name Spark Pi 故，要想设置AM申请的内存值，要么使用cluster模式，要么在client模式中，是有--conf手动设置spark.yarn.am.memory属性，例如： 123456789spark-submit --class org.apache.spark.examples.SparkPi \ --master yarn-client \ --num-executors 4 \ --driver-memory 2g \ --executor-memory 3g \ --executor-cores 4 \ --conf spark.yarn.am.memory=1024m \ /usr/lib/spark/lib/spark-examples-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar \ 100000 打开YARN管理界面，可以看到： a. Spark Pi 应用启动了5个Container，使用了18G内存、5个CPU core b. YARN为AM启动了一个Container，占用内存为2048M c. YARN启动了4个Container运行任务，每一个Container占用内存为4096M 为什么会是2G +4G *4=18G呢？第一个Container只申请了2G内存，是因为我们的程序只为AM申请了512m内存，而yarn.scheduler.minimum-allocation-mb参数决定了最少要申请2G内存。至于其余的Container，我们设置了executor-memory内存为3G，为什么每一个Container占用内存为4096M呢？ 为了找出规律，多测试几组数据，分别测试并收集executor-memory为3G、4G、5G、6G时每个executor对应的Container内存申请情况： executor-memory=3g：2G+4G * 4=18G executor-memory=4g：2G+6G * 4=26G executor-memory=5g：2G+6G * 4=26G executor-memory=6g：2G+8G * 4=34G 关于这个问题，我是查看源代码，根据org.apache.spark.deploy.yarn.ApplicationMaster -&gt; YarnRMClient -&gt; YarnAllocator的类查找路径找到YarnAllocator中有这样一段代码： 123456789// Executor memory in MB.protected val executorMemory = args.executorMemory// Additional memory overhead.protected val memoryOverhead: Int = sparkConf.getInt("spark.yarn.executor.memoryOverhead", math.max((MEMORY_OVERHEAD_FACTOR * executorMemory).toInt, MEMORY_OVERHEAD_MIN))// Number of cores per executor.protected val executorCores = args.executorCores// Resource capability requested for each executorsprivate val resource = Resource.newInstance(executorMemory + memoryOverhead, executorCores) 因为没有具体的去看YARN的源代码，所以这里猜测Container的大小是根据executorMemory + memoryOverhead计算出来的，大概的规则是每一个Container的大小必须为yarn.scheduler.minimum-allocation-mb值的整数倍，当executor-memory=3g时，executorMemory + memoryOverhead为3G+384M=3456M，需要申请的Container大小为yarn.scheduler.minimum-allocation-mb * 2 =4096m=4G，其他依此类推。 注意： Yarn always rounds up memory requirement to multiples of yarn.scheduler.minimum-allocation-mb, which by default is 1024 or 1GB. Spark adds an overhead to SPARK_EXECUTOR_MEMORY/SPARK_DRIVER_MEMORY before asking Yarn for the amount. 另外，需要注意memoryOverhead的计算方法，当executorMemory的值很大时，memoryOverhead的值相应会变大，这个时候就不是384m了，相应的Container申请的内存值也变大了，例如：当executorMemory设置为90G时，memoryOverhead值为math.max(0.07 * 90G, 384m)=6.3G，其对应的Container申请的内存为98G。 回头看看给AM对应的Container分配2G内存原因，512+384=896，小于2G，故分配2G，你可以在设置spark.yarn.am.memory的值之后再来观察。 打开Spark的管理界面 http://ip:4040 ，可以看到driver和Executor中内存的占用情况： 从上图可以看到Executor占用了1566.7 MB内存，这是怎样计算出来的？参考Spark on Yarn: Where Have All the Memory Gone?这篇文章，totalExecutorMemory的计算方式为： 123456789101112//yarn/common/src/main/scala/org/apache/spark/deploy/yarn/YarnSparkHadoopUtil.scala val MEMORY_OVERHEAD_FACTOR = 0.07 val MEMORY_OVERHEAD_MIN = 384//yarn/common/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala protected val memoryOverhead: Int = sparkConf.getInt("spark.yarn.executor.memoryOverhead", math.max((MEMORY_OVERHEAD_FACTOR * executorMemory).toInt, MEMORY_OVERHEAD_MIN))...... val totalExecutorMemory = executorMemory + memoryOverhead numPendingAllocate.addAndGet(missing) logInfo(s"Will allocate $missing executor containers, each with $totalExecutorMemory MB " + s"memory including $memoryOverhead MB overhead") 这里我们给executor-memory设置的3G内存，memoryOverhead的值为math.max(0.07 * 3072, 384)=384，其最大可用内存通过下面代码来计算： 1234567//core/src/main/scala/org/apache/spark/storage/BlockManager.scala/** Return the total amount of storage memory available. */private def getMaxMemory(conf: SparkConf): Long = &#123; val memoryFraction = conf.getDouble("spark.storage.memoryFraction", 0.6) val safetyFraction = conf.getDouble("spark.storage.safetyFraction", 0.9) (Runtime.getRuntime.maxMemory * memoryFraction * safetyFraction).toLong&#125; 即，对于executor-memory设置3G时，executor内存占用大约为 3072m * 0.6 * 0.9 = 1658.88m，注意：实际上是应该乘以Runtime.getRuntime.maxMemory的值，该值小于3072m。 上图中driver占用了1060.3 MB，此时driver-memory的值是位2G，故driver中存储内存占用为：2048m * 0.6 * 0.9 =1105.92m，注意：实际上是应该乘以Runtime.getRuntime.maxMemory的值，该值小于2048m。 这时候，查看worker节点CoarseGrainedExecutorBackend进程启动脚本： 12345678910111213$ jps46841 Worker21894 CoarseGrainedExecutorBackend934521816 ExecutorLauncher4336924300 NodeManager38012 JournalNode36929 QuorumPeerMain22909 Jps$ ps -ef|grep 21894nobody 21894 21892 99 17:28 ? 00:04:49 /usr/java/jdk1.7.0_71/bin/java -server -XX:OnOutOfMemoryError=kill %p -Xms3072m -Xmx3072m -Djava.io.tmpdir=/data/yarn/local/usercache/root/appcache/application_1433742899916_0069/container_1433742899916_0069_01_000003/tmp -Dspark.driver.port=60235 -Dspark.yarn.app.container.log.dir=/data/yarn/logs/application_1433742899916_0069/container_1433742899916_0069_01_000003 org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url akka.tcp://sparkDriver@bj03-bi-pro-hdpnamenn:60235/user/CoarseGrainedScheduler --executor-id 2 --hostname X.X.X.X --cores 4 --app-id application_1433742899916_0069 --user-class-path file:/data/yarn/local/usercache/root/appcache/application_1433742899916_0069/container_1433742899916_0069_01_000003/__app__.jar 可以看到每个CoarseGrainedExecutorBackend进程分配的内存为3072m，如果我们想查看每个executor的jvm运行情况，可以开启jmx。在/etc/spark/conf/spark-defaults.conf中添加下面一行代码： 1spark.executor.extraJavaOptions -Dcom.sun.management.jmxremote.port=1099 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false 然后，通过jconsole监控jvm堆内存运行情况，这样方便调试内存大小。 总结由上可知，在client模式下，AM对应的Container内存由spark.yarn.am.memory加上spark.yarn.am.memoryOverhead来确定，executor加上spark.yarn.executor.memoryOverhead的值之后确定对应Container需要申请的内存大小，driver和executor的内存加上spark.yarn.driver.memoryOverhead或spark.yarn.executor.memoryOverhead的值之后再乘以0.54确定storage memory内存大小。在YARN中，Container申请的内存大小必须为yarn.scheduler.minimum-allocation-mb的整数倍。 下面这张图展示了Spark on YARN 内存结构，图片来自How-to: Tune Your Apache Spark Jobs (Part 2)： 至于cluster模式下的分析，请参考上面的过程。希望这篇文章对你有所帮助！ 参考文章 Spark1.0.0 on YARN 模式部署 Spark on Yarn: Where Have All the Memory Gone? Apache Spark Jobs 性能调优（二）]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark配置参数]]></title>
    <url>%2F2015%2F06%2F07%2Fspark-configuration%2F</url>
    <content type="text"><![CDATA[以下是整理的Spark中的一些配置参数，官方文档请参考Spark Configuration。 Spark提供三个位置用来配置系统： Spark属性：控制大部分的应用程序参数，可以用SparkConf对象或者Java系统属性设置 环境变量：可以通过每个节点的conf/spark-env.sh脚本设置。例如IP地址、端口等信息 日志配置：可以通过log4j.properties配置 Spark属性Spark属性控制大部分的应用程序设置，并且为每个应用程序分别配置它。这些属性可以直接在SparkConf上配置，然后传递给SparkContext。SparkConf允许你配置一些通用的属性（如master URL、应用程序名称等等）以及通过set()方法设置的任意键值对。例如，我们可以用如下方式创建一个拥有两个线程的应用程序。 12345val conf = new SparkConf() .setMaster("local[2]") .setAppName("CountingSheep") .set("spark.executor.memory", "1g")val sc = new SparkContext(conf) 动态加载Spark属性在一些情况下，你可能想在SparkConf中避免硬编码确定的配置。例如，你想用不同的master或者不同的内存数运行相同的应用程序。Spark允许你简单地创建一个空conf。 1val sc = new SparkContext(new SparkConf()) 然后你在运行时设置变量： 12./bin/spark-submit --name "My app" --master local[4] --conf spark.shuffle.spill=false --conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" myApp.jar Spark shell和spark-submit工具支持两种方式动态加载配置。第一种方式是命令行选项，例如--master，如上面shell显示的那样。spark-submit可以接受任何Spark属性，用--conf参数表示。但是那些参与Spark应用程序启动的属性要用特定的参数表示。运行./bin/spark-submit --help将会显示选项的整个列表。 bin/spark-submit也会从conf/spark-defaults.conf中读取配置选项，这个配置文件中，每一行都包含一对以空格或者等号分开的键和值。例如： 1234spark.master spark://5.6.7.8:7077spark.executor.memory 512mspark.eventLog.enabled truespark.serializer org.apache.spark.serializer.KryoSerializer 任何标签指定的值或者在配置文件中的值将会传递给应用程序，并且通过SparkConf合并这些值。在SparkConf上设置的属性具有最高的优先级，其次是传递给spark-submit或者spark-shell的属性值，最后是spark-defaults.conf文件中的属性值。 优先级顺序： 1SparkConf &gt; CLI &gt; spark-defaults.conf 查看Spark属性在http://&lt;driver&gt;:4040上的应用程序Web UI在Environment标签中列出了所有的Spark属性。这对你确保设置的属性的正确性是很有用的。 注意：只有通过spark-defaults.conf, SparkConf以及命令行直接指定的值才会显示。对于其它的配置属性，你可以认为程序用到了默认的值。 可用的属性控制内部设置的大部分属性都有合理的默认值，一些最通用的选项设置如下： 应用程序属性 属性名称 默认值 含义 spark.app.name (none) 你的应用程序的名字。这将在UI和日志数据中出现 spark.driver.cores 1 driver程序运行需要的cpu内核数 spark.driver.maxResultSize 1g 每个Spark action(如collect)所有分区的序列化结果的总大小限制。设置的值应该不小于1m，0代表没有限制。如果总大小超过这个限制，程序将会终止。大的限制值可能导致driver出现内存溢出错误（依赖于spark.driver.memory和JVM中对象的内存消耗）。 spark.driver.memory 512m driver进程使用的内存数 spark.executor.memory 512m 每个executor进程使用的内存数。和JVM内存串拥有相同的格式（如512m,2g） spark.extraListeners (none) 注册监听器，需要实现SparkListener spark.local.dir /tmp Spark中暂存空间的使用目录。在Spark1.0以及更高的版本中，这个属性被SPARK_LOCAL_DIRS(Standalone, Mesos)和LOCAL_DIRS(YARN)环境变量覆盖。 spark.logConf false 当SparkContext启动时，将有效的SparkConf记录为INFO。 spark.master (none) 集群管理器连接的地方 运行环境 属性名称 默认值 含义 spark.driver.extraClassPath (none) 附加到driver的classpath的额外的classpath实体。 spark.driver.extraJavaOptions (none) 传递给driver的JVM选项字符串。例如GC设置或者其它日志设置。注意，在这个选项中设置Spark属性或者堆大小是不合法的。Spark属性需要用--driver-class-path设置。 spark.driver.extraLibraryPath (none) 指定启动driver的JVM时用到的库路径 spark.driver.userClassPathFirst false (实验性)当在driver中加载类时，是否用户添加的jar比Spark自己的jar优先级高。这个属性可以降低Spark依赖和用户依赖的冲突。它现在还是一个实验性的特征。 spark.executor.extraClassPath (none) 附加到executors的classpath的额外的classpath实体。这个设置存在的主要目的是Spark与旧版本的向后兼容问题。用户一般不用设置这个选项 spark.executor.extraJavaOptions (none) 传递给executors的JVM选项字符串。例如GC设置或者其它日志设置。注意，在这个选项中设置Spark属性或者堆大小是不合法的。Spark属性需要用SparkConf对象或者spark-submit脚本用到的spark-defaults.conf文件设置。堆内存可以通过spark.executor.memory设置 spark.executor.extraLibraryPath (none) 指定启动executor的JVM时用到的库路径 spark.executor.logs.rolling.maxRetainedFiles (none) 设置被系统保留的最近滚动日志文件的数量。更老的日志文件将被删除。默认没有开启。 spark.executor.logs.rolling.size.maxBytes (none) executor日志的最大滚动大小。默认情况下没有开启。值设置为字节 spark.executor.logs.rolling.strategy (none) 设置executor日志的滚动(rolling)策略。默认情况下没有开启。可以配置为time和size。对于time，用spark.executor.logs.rolling.time.interval设置滚动间隔；对于size，用spark.executor.logs.rolling.size.maxBytes设置最大的滚动大小 spark.executor.logs.rolling.time.interval daily executor日志滚动的时间间隔。默认情况下没有开启。合法的值是daily, hourly, minutely以及任意的秒。 spark.files.userClassPathFirst false (实验性)当在Executors中加载类时，是否用户添加的jar比Spark自己的jar优先级高。这个属性可以降低Spark依赖和用户依赖的冲突。它现在还是一个实验性的特征。 spark.python.worker.memory 512m 在聚合期间，每个python worker进程使用的内存数。在聚合期间，如果内存超过了这个限制，它将会将数据塞进磁盘中 spark.python.profile false 在Python worker中开启profiling。通过sc.show_profiles()展示分析结果。或者在driver退出前展示分析结果。可以通过sc.dump_profiles(path)将结果dump到磁盘中。如果一些分析结果已经手动展示，那么在driver退出前，它们再不会自动展示 spark.python.profile.dump (none) driver退出前保存分析结果的dump文件的目录。每个RDD都会分别dump一个文件。可以通过ptats.Stats()加载这些文件。如果指定了这个属性，分析结果不会自动展示 spark.python.worker.reuse true 是否重用python worker。如果是，它将使用固定数量的Python workers，而不需要为每个任务fork()一个Python进程。如果有一个非常大的广播，这个设置将非常有用。因为，广播不需要为每个任务从JVM到Python worker传递一次 spark.executorEnv.[EnvironmentVariableName] (none) 通过EnvironmentVariableName添加指定的环境变量到executor进程。用户可以指定多个EnvironmentVariableName，设置多个环境变量 spark.mesos.executor.home driver side SPARK_HOME 设置安装在Mesos的executor上的Spark的目录。默认情况下，executors将使用driver的Spark本地（home）目录，这个目录对它们不可见。注意，如果没有通过spark.executor.uri指定Spark的二进制包，这个设置才起作用 spark.mesos.executor.memoryOverhead executor memory * 0.07, 最小384m 这个值是spark.executor.memory的补充。它用来计算mesos任务的总内存。另外，有一个7%的硬编码设置。最后的值将选择spark.mesos.executor.memoryOverhead或者spark.executor.memory的7%二者之间的大者 Shuffle行为 属性名称 默认值 含义 spark.reducer.maxMbInFlight 48 从递归任务中同时获取的map输出数据的最大大小（mb）。因为每一个输出都需要我们创建一个缓存用来接收，这个设置代表每个任务固定的内存上限，所以除非你有更大的内存，将其设置小一点 spark.shuffle.blockTransferService netty 实现用来在executor直接传递shuffle和缓存块。有两种可用的实现：netty和nio。基于netty的块传递在具有相同的效率情况下更简单 spark.shuffle.compress true 是否压缩map操作的输出文件。一般情况下，这是一个好的选择。 spark.shuffle.consolidateFiles false 如果设置为”true”，在shuffle期间，合并的中间文件将会被创建。创建更少的文件可以提供文件系统的shuffle的效率。这些shuffle都伴随着大量递归任务。当用ext4和dfs文件系统时，推荐设置为”true”。在ext3中，因为文件系统的限制，这个选项可能机器（大于8核）降低效率 spark.shuffle.file.buffer.kb 32 每个shuffle文件输出流内存内缓存的大小，单位是kb。这个缓存减少了创建只中间shuffle文件中磁盘搜索和系统访问的数量 spark.shuffle.io.maxRetries 3 Netty only，自动重试次数 spark.shuffle.io.numConnectionsPerPeer 1 Netty only spark.shuffle.io.preferDirectBufs true Netty only spark.shuffle.io.retryWait 5 Netty only spark.shuffle.manager sort 它的实现用于shuffle数据。有两种可用的实现：sort和hash。基于sort的shuffle有更高的内存使用率 spark.shuffle.memoryFraction 0.2 如果spark.shuffle.spill为true，shuffle中聚合和合并组操作使用的java堆内存占总内存的比重。在任何时候，shuffles使用的所有内存内maps的集合大小都受这个限制的约束。超过这个限制，spilling数据将会保存到磁盘上。如果spilling太过频繁，考虑增大这个值 spark.shuffle.sort.bypassMergeThreshold 200 (Advanced) In the sort-based shuffle manager, avoid merge-sorting data if there is no map-side aggregation and there are at most this many reduce partitions spark.shuffle.spill true 如果设置为”true”，通过将多出的数据写入磁盘来限制内存数。通过spark.shuffle.memoryFraction来指定spilling的阈值 spark.shuffle.spill.compress true 在shuffle时，是否将spilling的数据压缩。压缩算法通过spark.io.compression.codec指定。 Spark UI 属性名称 默认值 含义 spark.eventLog.compress false 是否压缩事件日志。需要spark.eventLog.enabled为true spark.eventLog.dir file:///tmp/spark-events Spark事件日志记录的基本目录。在这个基本目录下，Spark为每个应用程序创建一个子目录。各个应用程序记录日志到直到的目录。用户可能想设置这为统一的地点，像HDFS一样，所以历史文件可以通过历史服务器读取 spark.eventLog.enabled false 是否记录Spark的事件日志。这在应用程序完成后，重新构造web UI是有用的 spark.ui.killEnabled true 运行在web UI中杀死stage和相应的job spark.ui.port 4040 你的应用程序dashboard的端口。显示内存和工作量数据 spark.ui.retainedJobs 1000 在垃圾回收之前，Spark UI和状态API记住的job数 spark.ui.retainedStages 1000 在垃圾回收之前，Spark UI和状态API记住的stage数 压缩和序列化 属性名称 默认值 含义 spark.broadcast.compress true 在发送广播变量之前是否压缩它 spark.closure.serializer org.apache.spark.serializer.JavaSerializer 闭包用到的序列化类。目前只支持java序列化器 spark.io.compression.codec snappy 压缩诸如RDD分区、广播变量、shuffle输出等内部数据的编码解码器。默认情况下，Spark提供了三种选择：lz4、lzf和snappy，你也可以用完整的类名来制定。 spark.io.compression.lz4.block.size 32768 LZ4压缩中用到的块大小。降低这个块的大小也会降低shuffle内存使用率 spark.io.compression.snappy.block.size 32768 Snappy压缩中用到的块大小。降低这个块的大小也会降低shuffle内存使用率 spark.kryo.classesToRegister (none) 如果你用Kryo序列化，给定的用逗号分隔的自定义类名列表表示要注册的类 spark.kryo.referenceTracking true 当用Kryo序列化时，跟踪是否引用同一对象。如果你的对象图有环，这是必须的设置。如果他们包含相同对象的多个副本，这个设置对效率是有用的。如果你知道不在这两个场景，那么可以禁用它以提高效率 spark.kryo.registrationRequired false 是否需要注册为Kyro可用。如果设置为true，然后如果一个没有注册的类序列化，Kyro会抛出异常。如果设置为false，Kryo将会同时写每个对象和其非注册类名。写类名可能造成显著地性能瓶颈。 spark.kryo.registrator (none) 如果你用Kryo序列化，设置这个类去注册你的自定义类。如果你需要用自定义的方式注册你的类，那么这个属性是有用的。否则spark.kryo.classesToRegister会更简单。它应该设置一个继承自KryoRegistrator的类 spark.kryoserializer.buffer.max.mb 64 Kryo序列化缓存允许的最大值。这个值必须大于你尝试序列化的对象 spark.kryoserializer.buffer.mb 0.064 Kyro序列化缓存的大小。这样worker上的每个核都有一个缓存。如果有需要，缓存会涨到spark.kryoserializer.buffer.max.mb设置的值那么大。 spark.rdd.compress true 是否压缩序列化的RDD分区。在花费一些额外的CPU时间的同时节省大量的空间 spark.serializer org.apache.spark.serializer.JavaSerializer 序列化对象使用的类。默认的Java序列化类可以序列化任何可序列化的java对象但是它很慢。所有我们建议用org.apache.spark.serializer.KryoSerializer spark.serializer.objectStreamReset 100 当用org.apache.spark.serializer.JavaSerializer序列化时，序列化器通过缓存对象防止写多余的数据，然而这会造成这些对象的垃圾回收停止。通过请求’reset’，你从序列化器中flush这些信息并允许收集老的数据。为了关闭这个周期性的reset，你可以将值设为-1。默认情况下，每一百个对象reset一次 运行时行为 属性名称 默认值 含义 spark.broadcast.blockSize 4096 TorrentBroadcastFactory传输的块大小，太大值会降低并发，太小的值会出现性能瓶颈 spark.broadcast.factory org.apache.spark.broadcast.TorrentBroadcastFactory broadcast实现类 spark.cleaner.ttl (infinite) spark记录任何元数据（stages生成、task生成等）的持续时间。定期清理可以确保将超期的元数据丢弃，这在运行长时间任务是很有用的，如运行7*24的sparkstreaming任务。RDD持久化在内存中的超期数据也会被清理 spark.default.parallelism 本地模式：机器核数；Mesos：8；其他：max(executor的core，2) 如果用户不设置，系统使用集群中运行shuffle操作的默认任务数（groupByKey、 reduceByKey等） spark.executor.heartbeatInterval 10000 executor 向 the driver 汇报心跳的时间间隔，单位毫秒 spark.files.fetchTimeout 60 driver 程序获取通过SparkContext.addFile()添加的文件时的超时时间，单位秒 spark.files.useFetchCache true 获取文件时是否使用本地缓存 spark.files.overwrite false 调用SparkContext.addFile()时候是否覆盖文件 spark.hadoop.cloneConf false 每个task是否克隆一份hadoop的配置文件 spark.hadoop.validateOutputSpecs true 是否校验输出 spark.storage.memoryFraction 0.6 Spark内存缓存的堆大小占用总内存比例，该值不能大于老年代内存大小，默认值为0.6，但是，如果你手动设置老年代大小，你可以增加该值 spark.storage.memoryMapThreshold 2097152 内存块大小 spark.storage.unrollFraction 0.2 Fraction of spark.storage.memoryFraction to use for unrolling blocks in memory. spark.tachyonStore.baseDir System.getProperty(“java.io.tmpdir”) Tachyon File System临时目录 spark.tachyonStore.url tachyon://localhost:19998 Tachyon File System URL 网络 属性名称 默认值 含义 spark.driver.host (local hostname) driver监听的主机名或者IP地址。这用于和executors以及独立的master通信 spark.driver.port (random) driver监听的接口。这用于和executors以及独立的master通信 spark.fileserver.port (random) driver的文件服务器监听的端口 spark.broadcast.port (random) driver的HTTP广播服务器监听的端口 spark.replClassServer.port (random) driver的HTTP类服务器监听的端口 spark.blockManager.port (random) 块管理器监听的端口。这些同时存在于driver和executors spark.executor.port (random) executor监听的端口。用于与driver通信 spark.port.maxRetries 16 当绑定到一个端口，在放弃前重试的最大次数 spark.akka.frameSize 10 在”control plane”通信中允许的最大消息大小。如果你的任务需要发送大的结果到driver中，调大这个值 spark.akka.threads 4 通信的actor线程数。当driver有很多CPU核时，调大它是有用的 spark.akka.timeout 100 Spark节点之间的通信超时。单位是秒 spark.akka.heartbeat.pauses 6000 This is set to a larger value to disable failure detector that comes inbuilt akka. It can be enabled again, if you plan to use this feature (Not recommended). Acceptable heart beat pause in seconds for akka. This can be used to control sensitivity to gc pauses. Tune this in combination of spark.akka.heartbeat.interval and spark.akka.failure-detector.threshold if you need to. spark.akka.failure-detector.threshold 300.0 This is set to a larger value to disable failure detector that comes inbuilt akka. It can be enabled again, if you plan to use this feature (Not recommended). This maps to akka’s akka.remote.transport-failure-detector.threshold. Tune this in combination of spark.akka.heartbeat.pauses and spark.akka.heartbeat.interval if you need to. spark.akka.heartbeat.interval 1000 This is set to a larger value to disable failure detector that comes inbuilt akka. It can be enabled again, if you plan to use this feature (Not recommended). A larger interval value in seconds reduces network overhead and a smaller value ( ~ 1 s) might be more informative for akka’s failure detector. Tune this in combination of spark.akka.heartbeat.pauses and spark.akka.failure-detector.threshold if you need to. Only positive use case for using failure detector can be, a sensistive failure detector can help evict rogue executors really quick. However this is usually not the case as gc pauses and network lags are expected in a real Spark cluster. Apart from that enabling this leads to a lot of exchanges of heart beats between nodes leading to flooding the network with those. 调度相关属性 属性名称 默认值 含义 spark.task.cpus 1 为每个任务分配的内核数 spark.task.maxFailures 4 Task的最大重试次数 spark.scheduler.mode FIFO Spark的任务调度模式，还有一种Fair模式 spark.cores.max 当应用程序运行在Standalone集群或者粗粒度共享模式Mesos集群时，应用程序向集群请求的最大CPU内核总数（不是指每台机器，而是整个集群）。如果不设置，对于Standalone集群将使用spark.deploy.defaultCores中数值，而Mesos将使用集群中可用的内核 spark.mesos.coarse False 如果设置为true，在Mesos集群中运行时使用粗粒度共享模式 spark.speculation False 以下几个参数是关于Spark推测执行机制的相关参数。此参数设定是否使用推测执行机制，如果设置为true则spark使用推测执行机制，对于Stage中拖后腿的Task在其他节点中重新启动，并将最先完成的Task的计算结果最为最终结果 spark.speculation.interval 100 Spark多长时间进行检查task运行状态用以推测，以毫秒为单位 spark.speculation.quantile 推测启动前，Stage必须要完成总Task的百分比 spark.speculation.multiplier 1.5 比已完成Task的运行速度中位数慢多少倍才启用推测 spark.locality.wait 3000 以下几个参数是关于Spark数据本地性的。本参数是以毫秒为单位启动本地数据task的等待时间，如果超出就启动下一本地优先级别的task。该设置同样可以应用到各优先级别的本地性之间（本地进程 -&gt; 本地节点 -&gt; 本地机架 -&gt; 任意节点 ），当然，也可以通过spark.locality.wait.node等参数设置不同优先级别的本地性 spark.locality.wait.process spark.locality.wait 本地进程级别的本地等待时间 spark.locality.wait.node spark.locality.wait 本地节点级别的本地等待时间 spark.locality.wait.rack spark.locality.wait 本地机架级别的本地等待时间 spark.scheduler.revive.interval 1000 复活重新获取资源的Task的最长时间间隔（毫秒），发生在Task因为本地资源不足而将资源分配给其他Task运行后进入等待时间，如果这个等待时间内重新获取足够的资源就继续计算 Dynamic Allocation 属性名称 默认值 含义 spark.dynamicAllocation.enabled false 是否开启动态资源搜集 spark.dynamicAllocation.executorIdleTimeout 600 spark.dynamicAllocation.initialExecutors spark.dynamicAllocation.minExecutors spark.dynamicAllocation.maxExecutors Integer.MAX_VALUE spark.dynamicAllocation.minExecutors 0 spark.dynamicAllocation.schedulerBacklogTimeout 5 spark.dynamicAllocation.sustainedSchedulerBacklogTimeout schedulerBacklogTimeout 安全 属性名称 默认值 含义 spark.authenticate false 是否Spark验证其内部连接。如果不是运行在YARN上，请看spark.authenticate.secret spark.authenticate.secret None 设置Spark两个组件之间的密匙验证。如果不是运行在YARN上，但是需要验证，这个选项必须设置 spark.core.connection.auth.wait.timeout 30 连接时等待验证的实际。单位为秒 spark.core.connection.ack.wait.timeout 60 连接等待回答的时间。单位为秒。为了避免不希望的超时，你可以设置更大的值 spark.ui.filters None 应用到Spark web UI的用于过滤类名的逗号分隔的列表。过滤器必须是标准的javax servlet Filter。通过设置java系统属性也可以指定每个过滤器的参数。spark.&lt;class name of filter&gt;.params=&#39;param1=value1,param2=value2&#39;。例如-Dspark.ui.filters=com.test.filter1、-Dspark.com.test.filter1.params=&#39;param1=foo,param2=testing&#39; spark.acls.enable false 是否开启Spark acls。如果开启了，它检查用户是否有权限去查看或修改job。UI利用使用过滤器验证和设置用户 spark.ui.view.acls empty 逗号分隔的用户列表，列表中的用户有查看Spark web UI的权限。默认情况下，只有启动Spark job的用户有查看权限 spark.modify.acls empty 逗号分隔的用户列表，列表中的用户有修改Spark job的权限。默认情况下，只有启动Spark job的用户有修改权限 spark.admin.acls empty 逗号分隔的用户或者管理员列表，列表中的用户或管理员有查看和修改所有Spark job的权限。如果你运行在一个共享集群，有一组管理员或开发者帮助debug，这个选项有用 加密 属性名称 默认值 含义 spark.ssl.enabled false 是否开启ssl spark.ssl.enabledAlgorithms Empty JVM支持的加密算法列表，逗号分隔 spark.ssl.keyPassword None spark.ssl.keyStore None spark.ssl.keyStorePassword None spark.ssl.protocol None spark.ssl.trustStore None spark.ssl.trustStorePassword None Spark Streaming 属性名称 默认值 含义 spark.streaming.blockInterval 200 在这个时间间隔（ms）内，通过Spark Streaming receivers接收的数据在保存到Spark之前，chunk为数据块。推荐的最小值为50ms spark.streaming.receiver.maxRate infinite 每秒钟每个receiver将接收的数据的最大记录数。有效的情况下，每个流将消耗至少这个数目的记录。设置这个配置为0或者-1将会不作限制 spark.streaming.receiver.writeAheadLogs.enable false Enable write ahead logs for receivers. All the input data received through receivers will be saved to write ahead logs that will allow it to be recovered after driver failures spark.streaming.unpersist true 强制通过Spark Streaming生成并持久化的RDD自动从Spark内存中非持久化。通过Spark Streaming接收的原始输入数据也将清除。设置这个属性为false允许流应用程序访问原始数据和持久化RDD，因为它们没有被自动清除。但是它会造成更高的内存花费 集群管理Spark On YARN 属性名称 默认值 含义 spark.yarn.am.memory 512m client 模式时，am的内存大小；cluster模式时，使用spark.driver.memory变量 spark.driver.cores 1 claster模式时，driver使用的cpu核数，这时候driver运行在am中，其实也就是am和核数；client模式时，使用spark.yarn.am.cores变量 spark.yarn.am.cores 1 client 模式时，am的cpu核数 spark.yarn.am.waitTime 100000 启动时等待时间 spark.yarn.submit.file.replication 3 应用程序上传到HDFS的文件的副本数 spark.yarn.preserve.staging.files False 若为true，在job结束后，将stage相关的文件保留而不是删除 spark.yarn.scheduler.heartbeat.interval-ms 5000 Spark AppMaster发送心跳信息给YARN RM的时间间隔 spark.yarn.max.executor.failures 2倍于executor数，最小值3 导致应用程序宣告失败的最大executor失败次数 spark.yarn.applicationMaster.waitTries 10 RM等待Spark AppMaster启动重试次数，也就是SparkContext初始化次数。超过这个数值，启动失败 spark.yarn.historyServer.address Spark history server的地址（不要加 http://）。这个地址会在Spark应用程序完成后提交给YARN RM，然后RM将信息从RM UI写到history server UI上。 spark.yarn.dist.archives (none) spark.yarn.dist.files (none) spark.executor.instances 2 executor实例个数 spark.yarn.executor.memoryOverhead executorMemory * 0.07, with minimum of 384 executor的堆内存大小设置 spark.yarn.driver.memoryOverhead driverMemory * 0.07, with minimum of 384 driver的堆内存大小设置 spark.yarn.am.memoryOverhead AM memory * 0.07, with minimum of 384 am的堆内存大小设置，在client模式时设置 spark.yarn.queue default 使用yarn的队列 spark.yarn.jar (none) spark.yarn.access.namenodes (none) spark.yarn.appMasterEnv.[EnvironmentVariableName] (none) 设置am的环境变量 spark.yarn.containerLauncherMaxThreads 25 am启动executor的最大线程数 spark.yarn.am.extraJavaOptions (none) spark.yarn.maxAppAttempts yarn.resourcemanager.am.max-attempts in YARN am重试次数 Spark on Mesos使用较少，参考Running Spark on Mesos。 Spark Standalone Mode参考Spark Standalone Mode。 Spark History Server当你运行Spark Standalone Mode或者Spark on Mesos模式时，你可以通过Spark History Server来查看job运行情况。 Spark History Server的环境变量： 属性名称 含义 SPARK_DAEMON_MEMORY Memory to allocate to the history server (default: 512m). SPARK_DAEMON_JAVA_OPTS JVM options for the history server (default: none). SPARK_PUBLIC_DNS SPARK_HISTORY_OPTS 配置 spark.history.* 属性 Spark History Server的属性： 属性名称 默认 含义 spark.history.provider org.apache.spark.deploy.history.FsHistoryProvide 应用历史后端实现的类名。 目前只有一个实现, 由Spark提供, 它查看存储在文件系统里面的应用日志 spark.history.fs.logDirectory file:/tmp/spark-events spark.history.updateInterval 10 以秒为单位，多长时间Spark history server显示的信息进行更新。每次更新都会检查持久层事件日志的任何变化。 spark.history.retainedApplications 50 在Spark history server上显示的最大应用程序数量，如果超过这个值，旧的应用程序信息将被删除。 spark.history.ui.port 18080 官方版本中，Spark history server的默认访问端口 spark.history.kerberos.enabled false 是否使用kerberos方式登录访问history server，对于持久层位于安全集群的HDFS上是有用的。如果设置为true，就要配置下面的两个属性。 spark.history.kerberos.principal 空 用于Spark history server的kerberos主体名称 spark.history.kerberos.keytab 空 用于Spark history server的kerberos keytab文件位置 spark.history.ui.acls.enable false 授权用户查看应用程序信息的时候是否检查acl。如果启用，只有应用程序所有者和spark.ui.view.acls指定的用户可以查看应用程序信息;如果禁用，不做任何检查。 环境变量通过环境变量配置确定的Spark设置。环境变量从Spark安装目录下的conf/spark-env.sh脚本读取（或者windows的conf/spark-env.cmd）。在独立的或者Mesos模式下，这个文件可以给机器确定的信息，如主机名。当运行本地应用程序或者提交脚本时，它也起作用。 注意，当Spark安装时，conf/spark-env.sh默认是不存在的。你可以复制conf/spark-env.sh.template创建它。 可以在spark-env.sh中设置如下变量： 环境变量 含义 JAVA_HOME Java安装的路径 PYSPARK_PYTHON PySpark用到的Python二进制执行文件路径 SPARK_LOCAL_IP 机器绑定的IP地址 SPARK_PUBLIC_DNS 你Spark应用程序通知给其他机器的主机名 除了以上这些，Spark standalone cluster scripts也可以设置一些选项。例如每台机器使用的核数以及最大内存。 因为spark-env.sh是shell脚本，其中的一些可以以编程方式设置。例如，你可以通过特定的网络接口计算SPARK_LOCAL_IP。 配置日志Spark用log4j logging。你可以通过在conf目录下添加log4j.properties文件来配置。一种方法是复制log4j.properties.template文件。]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YARN的内存和CPU配置]]></title>
    <url>%2F2015%2F06%2F05%2Fyarn-memory-and-cpu-configuration%2F</url>
    <content type="text"><![CDATA[Hadoop YARN同时支持内存和CPU两种资源的调度，本文介绍如何配置YARN对内存和CPU的使用。 YARN作为一个资源调度器，应该考虑到集群里面每一台机子的计算资源，然后根据application申请的资源进行分配Container。Container是YARN里面资源分配的基本单位，具有一定的内存以及CPU资源。 在YARN集群中，平衡内存、CPU、磁盘的资源的很重要的，根据经验，每两个container使用一块磁盘以及一个CPU核的时候可以使集群的资源得到一个比较好的利用。 内存配置关于内存相关的配置可以参考hortonwork公司的文档Determine HDP Memory Configuration Settings来配置你的集群。 YARN以及MAPREDUCE所有可用的内存资源应该要除去系统运行需要的以及其他的hadoop的一些程序，总共保留的内存=系统内存+HBASE内存。 可以参考下面的表格确定应该保留的内存： 每台机子内存 系统需要的内存 HBase需要的内存 4GB 1GB 1GB 8GB 2GB 1GB 16GB 2GB 2GB 24GB 4GB 4GB 48GB 6GB 8GB 64GB 8GB 8GB 72GB 8GB 8GB 96GB 12GB 16GB 128GB 24GB 24GB 255GB 32GB 32GB 512GB 64GB 64GB 计算每台机子最多可以拥有多少个container，可以使用下面的公式: containers = min (2*CORES, 1.8*DISKS, (Total available RAM) / MIN_CONTAINER_SIZE) 说明： CORES为机器CPU核数 DISKS为机器上挂载的磁盘个数 Total available RAM为机器总内存 MIN_CONTAINER_SIZE是指container最小的容量大小，这需要根据具体情况去设置，可以参考下面的表格： 每台机子可用的RAM container最小值 小于4GB 256MB 4GB到8GB之间 512MB 8GB到24GB之间 1024MB 大于24GB 2048MB 每个container的平均使用内存大小计算方式为： RAM-per-container = max(MIN_CONTAINER_SIZE, (Total Available RAM) / containers)) 通过上面的计算，YARN以及MAPREDUCE可以这样配置： 配置文件 配置设置 默认值 计算值 yarn-site.xml yarn.nodemanager.resource.memory-mb 8192 MB = containers * RAM-per-container yarn-site.xml yarn.scheduler.minimum-allocation-mb 1024MB = RAM-per-container yarn-site.xml yarn.scheduler.maximum-allocation-mb 8192 MB = containers * RAM-per-container yarn-site.xml (check) yarn.app.mapreduce.am.resource.mb 1536 MB = 2 * RAM-per-container yarn-site.xml (check) yarn.app.mapreduce.am.command-opts -Xmx1024m = 0.8 * 2 * RAM-per-container mapred-site.xml mapreduce.map.memory.mb 1024 MB = RAM-per-container mapred-site.xml mapreduce.reduce.memory.mb 1024 MB = 2 * RAM-per-container mapred-site.xml mapreduce.map.java.opts = 0.8 * RAM-per-container mapred-site.xml mapreduce.reduce.java.opts = 0.8 * 2 * RAM-per-container 举个例子：对于128G内存、32核CPU的机器，挂载了7个磁盘，根据上面的说明，系统保留内存为24G，不适应HBase情况下，系统剩余可用内存为104G，计算containers值如下： containers = min (2*32, 1.8* 7 , (128-24)/2) = min (64, 12.6 , 51) = 13 计算RAM-per-container值如下： RAM-per-container = max (2, (124-24)/13) = max (2, 8) = 8 你也可以使用脚本yarn-utils.py来计算上面的值： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131#!/usr/bin/env pythonimport optparsefrom pprint import pprintimport loggingimport sysimport mathimport ast''' Reserved for OS + DN + NM, Map: Memory =&gt; Reservation '''reservedStack = &#123; 4:1, 8:2, 16:2, 24:4, 48:6, 64:8, 72:8, 96:12, 128:24, 256:32, 512:64&#125;''' Reserved for HBase. Map: Memory =&gt; Reservation ''' reservedHBase = &#123;4:1, 8:1, 16:2, 24:4, 48:8, 64:8, 72:8, 96:16, 128:24, 256:32, 512:64&#125;GB = 1024def getMinContainerSize(memory): if (memory &lt;= 4): return 256 elif (memory &lt;= 8): return 512 elif (memory &lt;= 24): return 1024 else: return 2048 passdef getReservedStackMemory(memory): if (reservedStack.has_key(memory)): return reservedStack[memory] if (memory &lt;= 4): ret = 1 elif (memory &gt;= 512): ret = 64 else: ret = 1 return retdef getReservedHBaseMem(memory): if (reservedHBase.has_key(memory)): return reservedHBase[memory] if (memory &lt;= 4): ret = 1 elif (memory &gt;= 512): ret = 64 else: ret = 2 return ret def main(): log = logging.getLogger(__name__) out_hdlr = logging.StreamHandler(sys.stdout) out_hdlr.setFormatter(logging.Formatter(' %(message)s')) out_hdlr.setLevel(logging.INFO) log.addHandler(out_hdlr) log.setLevel(logging.INFO) parser = optparse.OptionParser() memory = 0 cores = 0 disks = 0 hbaseEnabled = True parser.add_option('-c', '--cores', default = 16, help = 'Number of cores on each host') parser.add_option('-m', '--memory', default = 64, help = 'Amount of Memory on each host in GB') parser.add_option('-d', '--disks', default = 4, help = 'Number of disks on each host') parser.add_option('-k', '--hbase', default = "True", help = 'True if HBase is installed, False is not') (options, args) = parser.parse_args() cores = int (options.cores) memory = int (options.memory) disks = int (options.disks) hbaseEnabled = ast.literal_eval(options.hbase) log.info("Using cores=" + str(cores) + " memory=" + str(memory) + "GB" + " disks=" + str(disks) + " hbase=" + str(hbaseEnabled)) minContainerSize = getMinContainerSize(memory) reservedStackMemory = getReservedStackMemory(memory) reservedHBaseMemory = 0 if (hbaseEnabled): reservedHBaseMemory = getReservedHBaseMem(memory) reservedMem = reservedStackMemory + reservedHBaseMemory usableMem = memory - reservedMem memory -= (reservedMem) if (memory &lt; 2): memory = 2 reservedMem = max(0, memory - reservedMem) memory *= GB containers = int (min(2 * cores, min(math.ceil(1.8 * float(disks)), memory/minContainerSize))) if (containers &lt;= 2): containers = 3 log.info("Profile: cores=" + str(cores) + " memory=" + str(memory) + "MB" + " reserved=" + str(reservedMem) + "GB" + " usableMem=" + str(usableMem) + "GB" + " disks=" + str(disks)) container_ram = abs(memory/containers) if (container_ram &gt; GB): container_ram = int(math.floor(container_ram / 512)) * 512 log.info("Num Container=" + str(containers)) log.info("Container Ram=" + str(container_ram) + "MB") log.info("Used Ram=" + str(int (containers*container_ram/float(GB))) + "GB") log.info("Unused Ram=" + str(reservedMem) + "GB") log.info("yarn.scheduler.minimum-allocation-mb=" + str(container_ram)) log.info("yarn.scheduler.maximum-allocation-mb=" + str(containers*container_ram)) log.info("yarn.nodemanager.resource.memory-mb=" + str(containers*container_ram)) map_memory = container_ram reduce_memory = 2*container_ram if (container_ram &lt;= 2048) else container_ram am_memory = max(map_memory, reduce_memory) log.info("mapreduce.map.memory.mb=" + str(map_memory)) log.info("mapreduce.map.java.opts=-Xmx" + str(int(0.8 * map_memory)) +"m") log.info("mapreduce.reduce.memory.mb=" + str(reduce_memory)) log.info("mapreduce.reduce.java.opts=-Xmx" + str(int(0.8 * reduce_memory)) + "m") log.info("yarn.app.mapreduce.am.resource.mb=" + str(am_memory)) log.info("yarn.app.mapreduce.am.command-opts=-Xmx" + str(int(0.8*am_memory)) + "m") log.info("mapreduce.task.io.sort.mb=" + str(int(0.4 * map_memory))) passif __name__ == '__main__': try: main() except(KeyboardInterrupt, EOFError): print("\nAborting ... Keyboard Interrupt.") sys.exit(1) 执行下面命令： 1python yarn-utils.py -c 32 -m 128 -d 7 -k False 返回结果如下： 12345678910111213141516Using cores=32 memory=128GB disks=7 hbase=FalseProfile: cores=32 memory=106496MB reserved=24GB usableMem=104GB disks=7Num Container=13Container Ram=8192MBUsed Ram=104GBUnused Ram=24GByarn.scheduler.minimum-allocation-mb=8192yarn.scheduler.maximum-allocation-mb=106496yarn.nodemanager.resource.memory-mb=106496mapreduce.map.memory.mb=8192mapreduce.map.java.opts=-Xmx6553mmapreduce.reduce.memory.mb=8192mapreduce.reduce.java.opts=-Xmx6553myarn.app.mapreduce.am.resource.mb=8192yarn.app.mapreduce.am.command-opts=-Xmx6553mmapreduce.task.io.sort.mb=3276 这样的话，每个container内存为8G，似乎有点多，我更愿意根据集群使用情况任务将其调整为2G内存，则集群中下面的参数配置值如下： 配置文件 配置设置 计算值 yarn-site.xml yarn.nodemanager.resource.memory-mb = 52 * 2 =104 G yarn-site.xml yarn.scheduler.minimum-allocation-mb = 2G yarn-site.xml yarn.scheduler.maximum-allocation-mb = 52 * 2 = 104G yarn-site.xml (check) yarn.app.mapreduce.am.resource.mb = 2 * 2=4G yarn-site.xml (check) yarn.app.mapreduce.am.command-opts = 0.8 * 2 * 2=3.2G mapred-site.xml mapreduce.map.memory.mb = 2G mapred-site.xml mapreduce.reduce.memory.mb = 2 * 2=4G mapred-site.xml mapreduce.map.java.opts = 0.8 * 2=1.6G mapred-site.xml mapreduce.reduce.java.opts = 0.8 * 2 * 2=3.2G 对应的xml配置为： 1234567891011121314151617181920&lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;106496&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;106496&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;/name&gt; &lt;value&gt;-Xmx3276m&lt;/value&gt; &lt;/property&gt; 另外，还有一下几个参数： yarn.nodemanager.vmem-pmem-ratio：任务每使用1MB物理内存，最多可使用虚拟内存量，默认是2.1。 yarn.nodemanager.pmem-check-enabled：是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true。 yarn.nodemanager.vmem-pmem-ratio：是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true。 第一个参数的意思是当一个map任务总共分配的物理内存为2G的时候，该任务的container最多内分配的堆内存为1.6G，可以分配的虚拟内存上限为2*2.1=4.2G。另外，照这样算下去，每个节点上YARN可以启动的Map数为104/2=52个。 CPU配置YARN中目前的CPU被划分成虚拟CPU（CPU virtual Core），这里的虚拟CPU是YARN自己引入的概念，初衷是，考虑到不同节点的CPU性能可能不同，每个CPU具有的计算能力也是不一样的，比如某个物理CPU的计算能力可能是另外一个物理CPU的2倍，这时候，你可以通过为第一个物理CPU多配置几个虚拟CPU弥补这种差异。用户提交作业时，可以指定每个任务需要的虚拟CPU个数。 在YARN中，CPU相关配置参数如下： yarn.nodemanager.resource.cpu-vcores：表示该节点上YARN可使用的虚拟CPU个数，默认是8，注意，目前推荐将该值设值为与物理CPU核数数目相同。如果你的节点CPU核数不够8个，则需要调减小这个值，而YARN不会智能的探测节点的物理CPU总数。 yarn.scheduler.minimum-allocation-vcores：单个任务可申请的最小虚拟CPU个数，默认是1，如果一个任务申请的CPU个数少于该数，则该对应的值改为这个数。 yarn.scheduler.maximum-allocation-vcores：单个任务可申请的最多虚拟CPU个数，默认是32。 对于一个CPU核数较多的集群来说，上面的默认配置显然是不合适的，在我的测试集群中，4个节点每个机器CPU核数为31，留一个给操作系统，可以配置为： 12345678&lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;31&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt; &lt;value&gt;124&lt;/value&gt;&lt;/property&gt; 参考文章 Determine HDP Memory Configuration Settings Hadoop YARN如何调度内存和CPU]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何使用Spark ALS实现协同过滤]]></title>
    <url>%2F2015%2F06%2F01%2Fhow-to-implement-collaborative-filtering-using-spark-als%2F</url>
    <content type="text"><![CDATA[本文主要记录最近一段时间学习和实现Spark MLlib中的协同过滤的一些总结，希望对大家熟悉Spark ALS算法有所帮助。 更新： 【2016.06.12】Spark1.4.0中MatrixFactorizationModel提供了recommendForAll方法实现离线批量推荐，见SPARK-3066。 测试环境为了测试简单，在本地以local方式运行Spark，你需要做的是下载编译好的压缩包解压即可，可以参考Spark本地模式运行。 测试数据使用MovieLens的MovieLens 10M数据集，下载之后解压到data目录。数据的格式请参考README中的说明，需要注意的是ratings.dat中的数据被处理过，每个用户至少访问了20个商品。 下面的代码均在spark-shell中运行，启动时候可以根据你的机器内存设置JVM参数，例如： 1bin/spark-shell --executor-memory 3g --driver-memory 3g --driver-java-options '-Xms2g -Xmx2g -XX:+UseCompressedOops' 预测评分这个例子主要演示如何训练数据、评分并计算根均方差。 准备工作首先，启动spark-shell，然后引入mllib包，我们需要用到ALS算法类和Rating评分类： 1import org.apache.spark.mllib.recommendation.&#123;ALS, Rating&#125; Spark的日志级别默认为INFO，你可以手动设置为WARN级别，同样先引入log4j依赖： 1import org.apache.log4j.&#123;Logger,Level&#125; 然后，运行下面代码： 12Logger.getLogger("org.apache.spark").setLevel(Level.WARN)Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF) 加载数据spark-shell启动成功之后，sc为内置变量，你可以通过它来加载测试数据： 1val data = sc.textFile("data/ml-1m/ratings.dat") 接下来解析文件内容，获得用户对商品的评分记录： 123val ratings = data.map(_.split("::") match &#123; case Array(user, item, rate, ts) =&gt; Rating(user.toInt, item.toInt, rate.toDouble)&#125;).cache() 查看第一条记录： 12scala&gt; ratings.firstres81: org.apache.spark.mllib.recommendation.Rating = Rating(1,1193,5.0) 我们可以统计文件中用户和商品数量： 123val users = ratings.map(_.user).distinct()val products = ratings.map(_.product).distinct()println("Got "+ratings.count()+" ratings from "+users.count+" users on "+products.count+" products.") 可以看到如下输出： 1//Got 1000209 ratings from 6040 users on 3706 products. 你可以对评分数据生成训练集和测试集，例如：训练集和测试集比例为8比2： 123val splits = ratings.randomSplit(Array(0.8, 0.2), seed = 111l)val training = splits(0).repartition(numPartitions)val test = splits(1).repartition(numPartitions) 这里，我们是将评分数据全部当做训练集，并且也为测试集。 训练模型接下来调用ALS.train()方法，进行模型训练： 1234val rank = 12val lambda = 0.01val numIterations = 20val model = ALS.train(ratings, rank, numIterations, lambda) 训练完后，我们看看model中的用户和商品特征向量： 1234567891011model.userFeatures//res82: org.apache.spark.rdd.RDD[(Int, Array[Double])] = users MapPartitionsRDD[400] at mapValues at ALS.scala:218model.userFeatures.count//res84: Long = 6040model.productFeatures//res85: org.apache.spark.rdd.RDD[(Int, Array[Double])] = products MapPartitionsRDD[401] at mapValues at ALS.scala:222model.productFeatures.count//res86: Long = 3706 评测我们要对比一下预测的结果，注意：我们将训练集当作测试集来进行对比测试。从训练集中获取用户和商品的映射： 123val usersProducts= ratings.map &#123; case Rating(user, product, rate) =&gt; (user, product)&#125; 显然，测试集的记录数等于评分总记录数，验证一下： 1usersProducts.count //Long = 1000209 使用推荐模型对用户商品进行预测评分，得到预测评分的数据集： 123var predictions = model.predict(usersProducts).map &#123; case Rating(user, product, rate) =&gt; ((user, product), rate)&#125; 查看其记录数： 1predictions.count //Long = 1000209 将真实评分数据集与预测评分数据集进行合并，这样得到用户对每一个商品的实际评分和预测评分： 12345val ratesAndPreds = ratings.map &#123; case Rating(user, product, rate) =&gt; ((user, product), rate)&#125;.join(predictions)ratesAndPreds.count //Long = 1000209 然后计算根均方差： 123456val rmse= math.sqrt(ratesAndPreds.map &#123; case ((user, product), (r1, r2)) =&gt; val err = (r1 - r2) err * err&#125;.mean())println(s"RMSE = $rmse") 上面这段代码其实就是对测试集进行评分预测并计算相似度，这段代码可以抽象为一个方法，如下： 12345678910111213141516171819/** Compute RMSE (Root Mean Squared Error). */def computeRmse(model: MatrixFactorizationModel, data: RDD[Rating]) = &#123; val usersProducts = data.map &#123; case Rating(user, product, rate) =&gt; (user, product) &#125; val predictions = model.predict(usersProducts).map &#123; case Rating(user, product, rate) =&gt; ((user, product), rate) &#125; val ratesAndPreds = data.map &#123; case Rating(user, product, rate) =&gt; ((user, product), rate) &#125;.join(predictions) math.sqrt(ratesAndPreds.map &#123; case ((user, product), (r1, r2)) =&gt; val err = (r1 - r2) err * err &#125;.mean())&#125; 除了RMSE指标，我们还可以及时AUC以及Mean average precision at K (MAPK)，关于AUC的计算方法，参考RunRecommender.scala，关于MAPK的计算方法可以参考《Packt.Machine Learning with Spark.2015.pdf》一书第四章节内容，或者你可以看本文后面内容。 保存真实评分和预测评分我们还可以保存用户对商品的真实评分和预测评分记录到本地文件： 123ratesAndPreds.sortByKey().repartition(1).sortBy(_._1).map(&#123; case ((user, product), (rate, pred)) =&gt; (user + "," + product + "," + rate + "," + pred)&#125;).saveAsTextFile("/tmp/result") 上面这段代码先按用户排序，然后重新分区确保目标目录中只生成一个文件。如果你重复运行这段代码，则需要先删除目标路径： 12import scala.sys.process._"rm -r /tmp/result".! 我们还可以对预测的评分结果按用户进行分组并按评分倒排序： 12345predictions.map &#123; case ((user, product), rate) =&gt; (user, (product, rate))&#125;.groupByKey(numPartitions).map&#123;case (user_id,list)=&gt; (user_id,list.toList.sortBy &#123;case (goods_id,rate)=&gt; - rate&#125;)&#125; 给一个用户推荐商品这个例子主要是记录如何给一个或大量用户进行推荐商品，例如，对用户编号为384的用户进行推荐，查出该用户在测试集中评分过的商品。 找出5个用户： 12users.take(5) //Array[Int] = Array(384, 1084, 4904, 3702, 5618) 查看用户编号为384的用户的预测结果中预测评分排前10的商品： 1234567891011121314val userId = users.take(1)(0) //384val K = 10val topKRecs = model.recommendProducts(userId, K)println(topKRecs.mkString("\n"))// Rating(384,2545,8.354966018818265)// Rating(384,129,8.113083736094676)// Rating(384,184,8.038113395650853)// Rating(384,811,7.983433591425284)// Rating(384,1421,7.912044967873945)// Rating(384,1313,7.719639594879865)// Rating(384,2892,7.53667094600392)// Rating(384,2483,7.295378004543803)// Rating(384,397,7.141158013610967)// Rating(384,97,7.071089782695754) 查看该用户的评分记录： 123456789101112131415val goodsForUser=ratings.keyBy(_.user).lookup(384)// Seq[org.apache.spark.mllib.recommendation.Rating] = WrappedArray(Rating(384,2055,2.0), Rating(384,1197,4.0), Rating(384,593,5.0), Rating(384,599,3.0), Rating(384,673,2.0), Rating(384,3037,4.0), Rating(384,1381,2.0), Rating(384,1610,4.0), Rating(384,3074,4.0), Rating(384,204,4.0), Rating(384,3508,3.0), Rating(384,1007,3.0), Rating(384,260,4.0), Rating(384,3487,3.0), Rating(384,3494,3.0), Rating(384,1201,5.0), Rating(384,3671,5.0), Rating(384,1207,4.0), Rating(384,2947,4.0), Rating(384,2951,4.0), Rating(384,2896,2.0), Rating(384,1304,5.0))productsForUser.size //Int = 22productsForUser.sortBy(-_.rating).take(10).map(rating =&gt; (rating.product, rating.rating)).foreach(println)// (593,5.0)// (1201,5.0)// (3671,5.0)// (1304,5.0)// (1197,4.0)// (3037,4.0)// (1610,4.0)// (3074,4.0)// (204,4.0)// (260,4.0) 可以看到该用户对22个商品评过分以及浏览的商品是哪些。 我们可以该用户对某一个商品的实际评分和预测评分方差为多少： 123456val actualRating = productsForUser.take(1)(0)//actualRating: org.apache.spark.mllib.recommendation.Rating = Rating(384,2055,2.0) val predictedRating = model.predict(789, actualRating.product)val predictedRating = model.predict(384, actualRating.product)//predictedRating: Double = 1.9426030777174637val squaredError = math.pow(predictedRating - actualRating.rating, 2.0)//squaredError: Double = 0.0032944066875075172 如何找出和一个已知商品最相似的商品呢？这里，我们可以使用余弦相似度来计算： 123456import org.jblas.DoubleMatrix/* Compute the cosine similarity between two vectors */def cosineSimilarity(vec1: DoubleMatrix, vec2: DoubleMatrix): Double = &#123; vec1.dot(vec2) / (vec1.norm2() * vec2.norm2())&#125; 以2055商品为例，计算实际评分和预测评分相似度 12345678val itemId = 2055val itemFactor = model.productFeatures.lookup(itemId).head//itemFactor: Array[Double] = Array(0.3660752773284912, 0.43573060631752014, -0.3421429991722107, 0.44382765889167786, -1.4875195026397705, 0.6274569630622864, -0.3264533579349518, -0.9939845204353333, -0.8710321187973022, -0.7578890323638916, -0.14621856808662415, -0.7254264950752258)val itemVector = new DoubleMatrix(itemFactor)//itemVector: org.jblas.DoubleMatrix = [0.366075; 0.435731; -0.342143; 0.443828; -1.487520; 0.627457; -0.326453; -0.993985; -0.871032; -0.757889; -0.146219; -0.725426]cosineSimilarity(itemVector, itemVector)// res99: Double = 0.9999999999999999 找到和该商品最相似的10个商品： 123456789101112131415161718val sims = model.productFeatures.map&#123; case (id, factor) =&gt; val factorVector = new DoubleMatrix(factor) val sim = cosineSimilarity(factorVector, itemVector) (id, sim)&#125;val sortedSims = sims.top(K)(Ordering.by[(Int, Double), Double] &#123; case (id, similarity) =&gt; similarity &#125;)//sortedSims: Array[(Int, Double)] = Array((2055,0.9999999999999999), (2051,0.9138311231145874), (3520,0.8739823400539756), (2190,0.8718466671129721), (2050,0.8612639515847019), (1011,0.8466911667526461), (2903,0.8455764332511272), (3121,0.8227325520485377), (3674,0.8075743004357392), (2016,0.8063817280259447))println(sortedSims.mkString("\n"))// (2055,0.9999999999999999)// (2051,0.9138311231145874)// (3520,0.8739823400539756)// (2190,0.8718466671129721)// (2050,0.8612639515847019)// (1011,0.8466911667526461)// (2903,0.8455764332511272)// (3121,0.8227325520485377)// (3674,0.8075743004357392)// (2016,0.8063817280259447) 显然第一个最相似的商品即为该商品本身，即2055，我们可以修改下代码，取前k+1个商品，然后排除第一个： 1234567891011121314val sortedSims2 = sims.top(K + 1)(Ordering.by[(Int, Double), Double] &#123; case (id, similarity) =&gt; similarity &#125;)//sortedSims2: Array[(Int, Double)] = Array((2055,0.9999999999999999), (2051,0.9138311231145874), (3520,0.8739823400539756), (2190,0.8718466671129721), (2050,0.8612639515847019), (1011,0.8466911667526461), (2903,0.8455764332511272), (3121,0.8227325520485377), (3674,0.8075743004357392), (2016,0.8063817280259447), (3672,0.8016276723120674))sortedSims2.slice(1, 11).map&#123; case (id, sim) =&gt; (id, sim) &#125;.mkString("\n")// (2051,0.9138311231145874)// (3520,0.8739823400539756)// (2190,0.8718466671129721)// (2050,0.8612639515847019)// (1011,0.8466911667526461)// (2903,0.8455764332511272)// (3121,0.8227325520485377)// (3674,0.8075743004357392)// (2016,0.8063817280259447)// (3672,0.8016276723120674) 接下来，我们可以计算给该用户推荐的前K个商品的平均准确度MAPK，该算法定义如下（该算法是否正确还有待考证）： 123456789101112131415161718/* Function to compute average precision given a set of actual and predicted ratings */// Code for this function is based on: https://github.com/benhamner/Metricsdef avgPrecisionK(actual: Seq[Int], predicted: Seq[Int], k: Int): Double = &#123; val predK = predicted.take(k) var score = 0.0 var numHits = 0.0 for ((p, i) &lt;- predK.zipWithIndex) &#123; if (actual.contains(p)) &#123; numHits += 1.0 score += numHits / (i.toDouble + 1.0) &#125; &#125; if (actual.isEmpty) &#123; 1.0 &#125; else &#123; score / scala.math.min(actual.size, k).toDouble &#125;&#125; 给该用户推荐的商品为： 12val actualProducts = productsForUser.map(_.product)//actualProducts: Seq[Int] = ArrayBuffer(2055, 1197, 593, 599, 673, 3037, 1381, 1610, 3074, 204, 3508, 1007, 260, 3487, 3494, 1201, 3671, 1207, 2947, 2951, 2896, 1304) 给该用户预测的商品为： 12 val predictedProducts = topKRecs.map(_.product)//predictedProducts: Array[Int] = Array(2545, 129, 184, 811, 1421, 1313, 2892, 2483, 397, 97) 最后的准确度为： 12val apk10 = avgPrecisionK(actualProducts, predictedProducts, 10)// apk10: Double = 0.0 批量推荐你可以评分记录中获得所有用户然后依次给每个用户推荐： 12345val users = ratings.map(_.user).distinct()users.collect.flatMap &#123; user =&gt; model.recommendProducts(user, 10)&#125; 这种方式是遍历内存中的一个集合然后循环调用RDD的操作，运行会比较慢，另外一种方式是直接操作model中的userFeatures和productFeatures，代码如下： 1234567891011121314151617181920val itemFactors = model.productFeatures.map &#123; case (id, factor) =&gt; factor &#125;.collect()val itemMatrix = new DoubleMatrix(itemFactors)println(itemMatrix.rows, itemMatrix.columns)//(3706,12)// broadcast the item factor matrixval imBroadcast = sc.broadcast(itemMatrix)//获取商品和索引的映射var idxProducts=model.productFeatures.map &#123; case (prodcut, factor) =&gt; prodcut &#125;.zipWithIndex().map&#123;case (prodcut, idx) =&gt; (idx,prodcut)&#125;.collectAsMap()val idxProductsBroadcast = sc.broadcast(idxProducts)val allRecs = model.userFeatures.map&#123; case (user, array) =&gt; val userVector = new DoubleMatrix(array) val scores = imBroadcast.value.mmul(userVector) val sortedWithId = scores.data.zipWithIndex.sortBy(-_._1) //根据索引取对应的商品id val recommendedProducts = sortedWithId.map(_._2).map&#123;idx=&gt;idxProductsBroadcast.value.get(idx).get&#125; (user, recommendedProducts) &#125; 这种方式其实还不是最优方法，更好的方法可以参考Personalised recommendations using Spark，当然这篇文章中的代码还可以继续优化一下。我修改后的代码如下，供大家参考： 1234567891011121314151617181920212223242526272829303132333435363738val productFeatures = model.productFeatures.collect()var productArray = ArrayBuffer[Int]()var productFeaturesArray = ArrayBuffer[Array[Double]]()for ((product, features) &lt;- productFeatures) &#123; productArray += product productFeaturesArray += features&#125;val productArrayBroadcast = sc.broadcast(productArray)val productFeatureMatrixBroadcast = sc.broadcast(new DoubleMatrix(productFeaturesArray.toArray).transpose())start = System.currentTimeMillis()val allRecs = model.userFeatures.mapPartitions &#123; iter =&gt; // Build user feature matrix for jblas var userFeaturesArray = ArrayBuffer[Array[Double]]() var userArray = new ArrayBuffer[Int]() while (iter.hasNext) &#123; val (user, features) = iter.next() userArray += user userFeaturesArray += features &#125; var userFeatureMatrix = new DoubleMatrix(userFeaturesArray.toArray) var userRecommendationMatrix = userFeatureMatrix.mmul(productFeatureMatrixBroadcast.value) var productArray=productArrayBroadcast.value var mappedUserRecommendationArray = new ArrayBuffer[String](params.topk) // Extract ratings from the matrix for (i &lt;- 0 until userArray.length) &#123; var ratingSet = mutable.TreeSet.empty(Ordering.fromLessThan[(Int,Double)](_._2 &gt; _._2)) for (j &lt;- 0 until productArray.length) &#123; var rating = (productArray(j), userRecommendationMatrix.get(i,j)) ratingSet += rating &#125; mappedUserRecommendationArray += userArray(i)+","+ratingSet.take(params.topk).mkString(",") &#125; mappedUserRecommendationArray.iterator&#125; 2015.06.12 更新： 悲哀的是，上面的方法还是不能解决问题，因为矩阵相乘会撑爆集群内存；可喜的是，如果你关注Spark最新动态，你会发现Spark1.4.0中MatrixFactorizationModel提供了recommendForAll方法实现离线批量推荐，详细说明见SPARK-3066。因为，我使用的Hadoop版本是CDH-5.4.0，其中Spark版本还是1.3.0，所以暂且不能在集群上测试Spark1.4.0中添加的新方法。 如果上面结果跑出来了，就可以验证推荐结果是否正确。还是以384用户为例： 1234allRecs.lookup(384).head.take(10)//res50: Array[Int] = Array(1539, 219, 1520, 775, 3161, 2711, 2503, 771, 853, 759)topKRecs.map(_.product)//res49: Array[Int] = Array(1539, 219, 1520, 775, 3161, 2711, 2503, 771, 853, 759) 接下来，我们可以计算所有推荐结果的准确度了，首先，得到每个用户评分过的所有商品： 1val userProducts = ratings.map&#123; case Rating(user, product, rating) =&gt; (user, product) &#125;.groupBy(_._1) 然后，预测的商品和实际商品关联求准确度： 1234567// finally, compute the APK for each user, and average them to find MAPKval MAPK = allRecs.join(userProducts).map&#123; case (userId, (predicted, actualWithIds)) =&gt; val actual = actualWithIds.map(_._2).toSeq avgPrecisionK(actual, predicted, K)&#125;.reduce(_ + _) / allRecs.countprintln("Mean Average Precision at K = " + MAPK)//Mean Average Precision at K = 0.018827551771260383 其实，我们也可以使用Spark内置的算法计算RMSE和MAE： 12345678910111213141516171819// MSE, RMSE and MAEimport org.apache.spark.mllib.evaluation.RegressionMetricsval predictedAndTrue = ratesAndPreds.map &#123; case ((user, product), (actual, predicted)) =&gt; (actual, predicted) &#125;val regressionMetrics = new RegressionMetrics(predictedAndTrue)println("Mean Squared Error = " + regressionMetrics.meanSquaredError)println("Root Mean Squared Error = " + regressionMetrics.rootMeanSquaredError)// Mean Squared Error = 0.5490153087908566// Root Mean Squared Error = 0.7409556726220918// MAPKimport org.apache.spark.mllib.evaluation.RankingMetricsval predictedAndTrueForRanking = allRecs.join(userProducts).map&#123; case (userId, (predicted, actualWithIds)) =&gt; val actual = actualWithIds.map(_._2) (predicted.toArray, actual.toArray)&#125;val rankingMetrics = new RankingMetrics(predictedAndTrueForRanking)println("Mean Average Precision = " + rankingMetrics.meanAveragePrecision)// Mean Average Precision = 0.04417535679520426 计算推荐2000个商品时的准确度为： 123456val MAPK2000 = allRecs.join(userProducts).map&#123; case (userId, (predicted, actualWithIds)) =&gt; val actual = actualWithIds.map(_._2).toSeq avgPrecisionK(actual, predicted, 2000)&#125;.reduce(_ + _) / allRecs.countprintln("Mean Average Precision = " + MAPK2000)//Mean Average Precision = 0.025228311843069083 保存和加载推荐模型对与实时推荐，我们需要启动一个web server，在启动的时候生成或加载训练模型，然后提供API接口返回推荐接口，需要调用的相关方法为： 12save(model: MatrixFactorizationModel, path: String)load(sc: SparkContext, path: String) model中的userFeatures和productFeatures也可以保存起来： 123val outputDir="/tmp"model.userFeatures.map&#123; case (id, vec) =&gt; id + "\t" + vec.mkString(",") &#125;.saveAsTextFile(outputDir + "/userFeatures")model.productFeatures.map&#123; case (id, vec) =&gt; id + "\t" + vec.mkString(",") &#125;.saveAsTextFile(outputDir + "/productFeatures") 总结本文主要记录如何使用ALS算法实现协同过滤并给用户推荐商品，以上代码在Github仓库中的ScalaLocalALS.scala文件。 如果你想更加深入了解Spark MLlib算法的使用，可以看看Packt.Machine Learning with Spark.2015.pdf这本电子书并下载书中的源码，本文大部分代码参考自该电子书。 参考资料 Spark MLlib中的协同过滤 Packt.Machine Learning with Spark.2015.pdf SPARK-3066]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>recommendation</tag>
        <tag>als</tag>
        <tag>mlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测试Hive集成Sentry]]></title>
    <url>%2F2015%2F04%2F30%2Ftest-hive-with-sentry%2F</url>
    <content type="text"><![CDATA[本文在安装和配置Sentry基础之上测试Hive集成Sentry。注意：这里Hive中并没有配置Kerberos认证。 关于配置了Kerberos的Hive集群如何集成Sentry，请参考配置安全的Hive集群集成Sentry。 1. 配置Sentry见安装和配置Sentry。 2. 配置HiveHive Metastore集成Sentry需要在 /etc/hive/conf/hive-site.xml中添加： 12345678&lt;property&gt; &lt;name&gt;hive.metastore.pre.event.listeners&lt;/name&gt; &lt;value&gt;org.apache.sentry.binding.metastore.MetastoreAuthzBinding&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.event.listeners&lt;/name&gt; &lt;value&gt;org.apache.sentry.binding.metastore.SentryMetastorePostEventListener&lt;/value&gt;&lt;/property&gt; Hive-server2集成Sentry修改 /etc/hive/conf/hive-site.xml，添加以下内容： 12345678910111213141516&lt;property&gt; &lt;name&gt;hive.server2.enable.impersonation&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.security.authorization.task.factory&lt;/name&gt; &lt;value&gt;org.apache.sentry.binding.hive.SentryHiveAuthorizationTaskFactoryImpl&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.session.hook&lt;/name&gt; &lt;value&gt;org.apache.sentry.binding.hive.HiveAuthzBindingSessionHook&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.sentry.conf.url&lt;/name&gt; &lt;value&gt;file:///etc/hive/conf/sentry-site.xml&lt;/value&gt;&lt;/property&gt; 参考模板sentry-site.xml.hive-client.template在 /etc/hive/conf/ 目录创建 sentry-site.xml： 12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;sentry.service.client.server.rpc-port&lt;/name&gt; &lt;value&gt;8038&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.service.client.server.rpc-address&lt;/name&gt; &lt;value&gt;cdh1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.service.client.server.rpc-connection-timeout&lt;/name&gt; &lt;value&gt;200000&lt;/value&gt; &lt;/property&gt; &lt;!--以下是客户端配置--&gt; &lt;property&gt; &lt;name&gt;sentry.provider&lt;/name&gt; &lt;value&gt;org.apache.sentry.provider.file.HadoopGroupResourceAuthorizationProvider&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.hive.provider.backend&lt;/name&gt; &lt;value&gt;org.apache.sentry.provider.db.SimpleDBProviderBackend&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.metastore.service.users&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt;&lt;!--queries made by hive user (beeline) skip meta store check--&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.hive.server&lt;/name&gt; &lt;value&gt;server1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.hive.testing.mode&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3. 重启Hive在cdh1上启动或重启hiveserver2： 1$ /etc/init.d/hive-server2 restart 4. 准备测试数据参考 Securing Impala for analysts，准备测试数据： 123456$ cat /tmp/events.csv10.1.2.3,US,android,createNote10.200.88.99,FR,windows,updateNote10.1.2.3,US,android,updateNote10.200.88.77,FR,ios,createNote10.1.4.5,US,windows,updateTag 然后，在hive中运行下面 sql 语句： 12345678910create database sensitive;create table sensitive.events ( ip STRING, country STRING, client STRING, action STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';load data local inpath '/tmp/events.csv' overwrite into table sensitive.events;create database filtered;create view filtered.events as select country, client, action from sensitive.events;create view filtered.events_usonly as select * from filtered.events where country = 'US'; 在 cdh1上通过 beeline 连接 hiveserver2，运行下面命令创建角色和组： 12# 注意：我设置了hiveserver2的jdbc端口为10001$ beeline -u "jdbc:hive2://cdh1:10001/" -n hive -p hive -d org.apache.hive.jdbc.HiveDriver 执行下面的 sql 语句创建 role、group等： 12345678create role admin_role;GRANT ALL ON SERVER server1 TO ROLE admin_role;GRANT ROLE admin_role TO GROUP admin;GRANT ROLE admin_role TO GROUP hive;create role test_role;GRANT ALL ON DATABASE filtered TO ROLE test_role;GRANT ROLE test_role TO GROUP test; 上面创建了两个角色： admin_role，具有管理员权限，可以读写所有数据库，并授权给 admin 和 hive 组（对应操作系统上的组） test_role，只能读写 filtered 数据库，并授权给 test 组。 因为系统上没有test用户和组，所以需要手动创建： 1$ useradd test 5. 测试测试admin_role角色使用hive用户访问beeline： 1$ beeline -u "jdbc:hive2://cdh1:10000/" -n hive -p hive -d org.apache.hive.jdbc.HiveDriver 查看当前系统用户是谁： 12345670: jdbc:hive2://cdh1:10000/&gt; set system:user.name;+------------------------+--+| set |+------------------------+--+| system:user.name=hive |+------------------------+--+1 row selected (0.188 seconds) hive属于admin_role组，具有管理员权限，可以查看所有角色： 123456780: jdbc:hive2://cdh1:10000/&gt; show roles;+-------------+--+| role |+-------------+--+| test_role || admin_role |+-------------+--+2 rows selected (0.199 seconds) 查看所有权限： 12345678910111213140: jdbc:hive2://cdh1:10000/&gt; SHOW GRANT ROLE test_role;+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+---------------+| database | table | partition | column | principal_name | principal_type | privilege | grant_option | grant_time | +-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+---------------+| filtered | | | | test_role | ROLE | * | false | 1430293474047000 | +-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+---------------+0: jdbc:hive2://cdh1:10000/&gt; SHOW GRANT ROLE admin_role;+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+---------------+| database | table | partition | column | principal_name | principal_type | privilege | grant_option | grant_time | +-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+---------------+| * | | | | admin_role | ROLE | * | false | 1430293473308000 +-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+---------------+1 row selected (0.16 seconds) hive用户可以查看所有数据库、访问所有表： 1234567891011121314151617181920212223242526272829303132330: jdbc:hive2://cdh1:10000/&gt; show databases;+----------------+--+| database_name |+----------------+--+| default || filtered || sensitive |+----------------+--+3 rows selected (0.391 seconds)0: jdbc:hive2://cdh1:10000/&gt; use filtered;No rows affected (0.101 seconds)0: jdbc:hive2://cdh1:10000/&gt; select * from filtered.events;+-----------------+----------------+----------------+--+| events.country | events.client | events.action |+-----------------+----------------+----------------+--+| US | android | createNote || FR | windows | updateNote || US | android | updateNote || FR | ios | createNote || US | windows | updateTag |+-----------------+----------------+----------------+--+5 rows selected (0.431 seconds)0: jdbc:hive2://cdh1:10000/&gt; select * from sensitive.events;+---------------+-----------------+----------------+----------------+--+| events.ip | events.country | events.client | events.action |+---------------+-----------------+----------------+----------------+--+| 10.1.2.3 | US | android | createNote || 10.200.88.99 | FR | windows | updateNote || 10.1.2.3 | US | android | updateNote || 10.200.88.77 | FR | ios | createNote || 10.1.4.5 | US | windows | updateTag |+---------------+-----------------+----------------+----------------+--+5 rows selected (0.247 seconds) 测试test_role角色使用test用户访问beeline： 1$ beeline -u "jdbc:hive2://cdh1:10000/" -n test -p test -d org.apache.hive.jdbc.HiveDriver 查看当前系统用户是谁： 12345670: jdbc:hive2://cdh1:10000/&gt; set system:user.name;+------------------------+--+| set |+------------------------+--+| system:user.name=hive |+------------------------+--+1 row selected (0.188 seconds) test用户不是管理员，是不能查看所有角色的： 120: jdbc:hive2://cdh1:10000/&gt; show roles;ERROR : Error processing Sentry command: Access denied to test. Server Stacktrace: org.apache.sentry.provider.db.SentryAccessDeniedException: Access denied to test test用户可以列出所有数据库： 1234567890: jdbc:hive2://cdh1:10000/&gt; show databases;+----------------+--+| database_name |+----------------+--+| default || filtered || sensitive |+----------------+--+3 rows selected (0.079 seconds) test用户可以filtered库： 123456789101112130: jdbc:hive2://cdh1:10000/&gt; use filtered;No rows affected (0.206 seconds)0: jdbc:hive2://cdh1:10000/&gt; select * from events;+-----------------+----------------+----------------+--+| events.country | events.client | events.action |+-----------------+----------------+----------------+--+| US | android | createNote || FR | windows | updateNote || US | android | updateNote || FR | ios | createNote || US | windows | updateTag |+-----------------+----------------+----------------+--+5 rows selected (0.361 seconds) 但是，test用户没有权限访问sensitive库： 1230: jdbc:hive2://cdh1:10000/&gt; use sensitive;Error: Error while compiling statement: FAILED: SemanticException No valid privileges Required privileges for this query: Server=server1-&gt;Db=sensitive-&gt;Table=*-&gt;action=insert;Server=server1-&gt;Db=sensitive-&gt;Table=*-&gt;action=select; (state=42000,code=40000) 6. 排错在CDH5的高版本中，hive cli 不建议使用，在hive集成sentry之后，再运行hive cli 会提示找不到sentry的类的遗产，解决办法是，将sentry相关的jar包链接到hive的home目录下的lib目录下： 12345678910ln -s /usr/lib/sentry/lib/sentry-binding-hive.jar /usr/lib/hive/lib/ln -s /usr/lib/sentry/lib/sentry-core-common.jar /usr/lib/hive/libln -s /usr/lib/sentry/lib/sentry-core-common-db.jar /usr/lib/hive/libln -s /usr/lib/sentry/lib/sentry-policy-common.jar /usr/lib/hive/libln -s /usr/lib/sentry/lib/sentry-policy-db.jar /usr/lib/hive/libln -s /usr/lib/sentry/lib/sentry-policy-cache.jar /usr/lib/hive/libln -s /usr/lib/sentry/lib/sentry-provider-common.jar /usr/lib/hive/libln -s /usr/lib/sentry/lib/sentry-provider-db.jar /usr/lib/hive/libln -s /usr/lib/sentry/lib/sentry-provider-cache.jar /usr/lib/hive/libln -s /usr/lib/sentry/lib/sentry-provider-file.jar /usr/lib/hive/lib 7. 参考文章 Securing Impala for analysts]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>sentry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装和配置Sentry]]></title>
    <url>%2F2015%2F04%2F30%2Finstall-and-config-sentry%2F</url>
    <content type="text"><![CDATA[本文主要记录安装和配置Sentry的过程，关于Sentry的介绍，请参考Apache Sentry架构介绍。 1. 环境说明系统环境： 操作系统：CentOs 6.6 Hadoop版本：CDH5.4 运行用户：root 这里，我参考使用yum安装CDH Hadoop集群一文搭建了一个测试集群，并选择cdh1节点来安装sentry服务。 2. 安装在cdh1节点上运行下面命令查看Sentry的相关组件有哪些: 12345$ yum list sentry*sentry.noarch 1.4.0+cdh5.4.0+155-1.cdh5.4.0.p0.47.el6 @cdhsentry-hdfs-plugin.noarch 1.4.0+cdh5.4.0+155-1.cdh5.4.0.p0.47.el6 @cdhsentry-store.noarch 1.4.0+cdh5.4.0+155-1.cdh5.4.0.p0.47.el6 @cdh 以上组件说明： sentry：sentry的基本包 sentry-hdfs-plugin：hdfs插件 sentry-store：sentry store组件 这里安装以上所有组件： 1$ yum install sentry* -y 3. 配置参考sentry-site.xml.service.template，来修改Sentry的配置文件 /etc/sentry/conf/sentry-site.xml。 配置 sentry service 相关的参数12345678910111213141516171819202122232425262728&lt;property&gt; &lt;name&gt;sentry.service.admin.group&lt;/name&gt; &lt;value&gt;impala,hive,solr,hue&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;sentry.service.allow.connect&lt;/name&gt; &lt;value&gt;impala,hive,solr,hue&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;sentry.verify.schema.version&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;sentry.service.reporting&lt;/name&gt; &lt;value&gt;JMX&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;sentry.service.server.rpc-address&lt;/name&gt; &lt;value&gt;cdh1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;sentry.service.server.rpc-port&lt;/name&gt; &lt;value&gt;8038&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;sentry.service.web.enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 如果需要使用kerberos认证，则还需要配置以下参数： 123456789101112&lt;property&gt; &lt;name&gt;sentry.service.security.mode&lt;/name&gt; &lt;value&gt;kerberos&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;sentry.service.server.principal&lt;/name&gt; &lt;value&gt;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;sentry.service.server.keytab&lt;/name&gt; &lt;value&gt;&lt;/value&gt;&lt;/property&gt; 配置 sentry store 相关参数sentry store可以使用两种方式，如果使用基于SimpleDbProviderBackend的方式，则需要设置jdbc相关的参数： 12345678910111213141516&lt;property&gt; &lt;name&gt;sentry.store.jdbc.url&lt;/name&gt; &lt;value&gt;jdbc:postgresql://cdh1:5432/sentry&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;sentry.store.jdbc.driver&lt;/name&gt; &lt;value&gt;org.postgresql.Driver&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;sentry.store.jdbc.user&lt;/name&gt; &lt;value&gt;sentry&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;sentry.store.jdbc.password&lt;/name&gt; &lt;value&gt;sentry&lt;/value&gt;&lt;/property&gt; Sentry store的组映射sentry.store.group.mapping有些两种配置方式：org.apache.sentry.provider.common.HadoopGroupMappingService或者org.apache.sentry.provider.file.LocalGroupMapping，当使用后者的时候，还需要配置sentry.store.group.mapping.resource参数，即设置Policy file的路径。 123456789 &lt;property&gt; &lt;name&gt;sentry.store.group.mapping&lt;/name&gt; &lt;value&gt;org.apache.sentry.provider.common.HadoopGroupMappingService&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.store.group.mapping.resource&lt;/name&gt; &lt;value&gt; &lt;/value&gt; &lt;description&gt; Policy file for group mapping. Policy file path for local group mapping, when sentry.store.group.mapping is set to LocalGroupMapping Service class.&lt;/description&gt;&lt;/property&gt; 配置客户端的参数：配置Sentry和hive集成时的服务名称，默认值为HS2，这里设置为server1： 1234&lt;property&gt; &lt;name&gt;sentry.hive.server&lt;/name&gt; &lt;value&gt;server1&lt;/value&gt;&lt;/property&gt; 初始化数据库如果配置 sentry store 使用 posrgres 数据库，当然你也可以使用其他的数据库，则需要创建并初始化数据库。数据库的创建过程，请参考 Hadoop自动化安装shell脚本，下面列出关键脚本。 1234567891011yum install postgresql-server postgresql-jdbc -yln -s /usr/share/java/postgresql-jdbc.jar /usr/lib/hive/lib/postgresql-jdbc.jarln -s /usr/share/java/postgresql-jdbc.jar /usr/lib/sentry/lib/postgresql-jdbc.jarsu -c "cd ; /usr/bin/pg_ctl start -w -m fast -D /var/lib/pgsql/data" postgressu -c "cd ; /usr/bin/psql --command \"create user sentry with password 'sentry'; \" " postgressu -c "cd ; /usr/bin/psql --command \"drop database sentry;\" " postgressu -c "cd ; /usr/bin/psql --command \"CREATE DATABASE sentry owner=sentry;\" " postgressu -c "cd ; /usr/bin/psql --command \"GRANT ALL privileges ON DATABASE sentry TO sentry;\" " postgressu -c "cd ; /usr/bin/pg_ctl restart -w -m fast -D /var/lib/pgsql/data" postgres 然后，修改 /var/lib/pgsql/data/pg_hba.conf 内容如下： 12345678910# TYPE DATABASE USER CIDR-ADDRESS METHOD# &quot;local&quot; is for Unix domain socket connections onlylocal all all md5# IPv4 local connections:#host all all 0.0.0.0/0 trusthost all all 127.0.0.1/32 md5# IPv6 local connections:#host all all ::1/128 nd5 如果是第一次安装，则初始化 sentry 的元数据库： 12345678910111213141516$ sentry --command schema-tool --conffile /etc/sentry/conf/sentry-site.xml --dbType postgres --initSchemaSentry store connection URL: jdbc:postgresql://cdh1/sentrySentry store Connection Driver : org.postgresql.DriverSentry store connection User: sentryStarting sentry store schema initialization to 1.4.0-cdh5-2Initialization script sentry-postgres-1.4.0-cdh5-2.sqlConnecting to jdbc:postgresql://cdh1/sentryConnected to: PostgreSQL (version 8.4.18)Driver: PostgreSQL Native Driver (version PostgreSQL 9.0 JDBC4 (build 801))Transaction isolation: TRANSACTION_REPEATABLE_READAutocommit status: true1 row affected (0.002 seconds)No rows affected (0.004 seconds)Closing: 0: jdbc:postgresql://cdh1/sentryInitialization script completedSentry schemaTool completed 如果是更新，则执行： 1$ sentry --command schema-tool --conffile /etc/sentry/conf/sentry-site.xml --dbType postgres --upgradeSchema 4. 启动在cdh1上启动sentry-store服务： 1$ /etc/init.d/sentry-store start 查看日志： 1$ cat /var/log/sentry/sentry-store.out 查看sentry的web监控界面http://cdh1:51000/。 5. 参考文章 Setting Up Hive Authorization with Sentry]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>sentry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Sentry架构介绍]]></title>
    <url>%2F2015%2F04%2F29%2Fapache-sentry-architecture%2F</url>
    <content type="text"><![CDATA[介绍Apache Sentry是Cloudera公司发布的一个Hadoop开源组件，截止目前还是Apache的孵化项目，它提供了细粒度级、基于角色的授权以及多租户的管理模式。Sentry当前可以和Hive/Hcatalog、Apache Solr 和Cloudera Impala集成，未来会扩展到其他的Hadoop组件，例如HDFS和HBase。 特性Apache Sentry为Hadoop使用者提供了以下便利： 能够在Hadoop中存储更敏感的数据 使更多的终端用户拥有Hadoop数据访问权 创建更多的Hadoop使用案例 构建多用户应用程序 符合规范（例如SOX，PCI，HIPAA，EAL3） 在Sentry诞生之前，对于授权有两种备选解决方案：粗粒度级的HDFS授权和咨询授权，但它们并不符合典型的规范和数据安全需求，原因如下： 粗粒度级的HDFS授权：安全访问和授权的基本机制被HDFS文件模型的粒度所限制。五级授权是粗粒度的，因为没有对文件内数据的访问控制：用户要么可以访问整个文件，要么什么都看不到。另外，HDFS权限模式不允许多个组对同一数据集有不同级别的访问权限。 咨询授权：咨询授权在Hive中是一个很少使用的机制，旨在使用户能够自我监管，以防止意外删除或重写数据。这是一种“自服务”模式，因为用户可以为自己授予任何权限。因此，一旦恶意用户通过认证，它不能阻止其对敏感数据的访问。 通过引进Sentry，Hadoop目前可在以下方面满足企业和政府用户的RBAC需求： 安全授权：Sentry可以控制数据访问，并对已通过验证的用户提供数据访问特权。 细粒度访问控制：Sentry支持细粒度的Hadoop数据和元数据访问控制。在Hive和Impala中Sentry的最初发行版本中，Sentry在服务器、数据库、表和视图范围提供了不同特权级别的访问控制，包括查找、插入等，允许管理员使用视图限制对行或列的访问。管理员也可以通过Sentry和带选择语句的视图或UDF，根据需要在文件内屏蔽数据。 基于角色的管理：Sentry通过基于角色的授权简化了管理，你可以轻易将访问同一数据集的不同特权级别授予多个组。 多租户管理：Sentry允许为委派给不同管理员的不同数据集设置权限。在Hive/Impala的情况下，Sentry可以在数据库/schema级别进行权限管理。 统一平台：Sentry为确保数据安全，提供了一个统一平台，使用现有的Hadoop Kerberos实现安全认证。同时，通过Hive或Impala访问数据时可以使用同样的Sentry协议。未来，Sentry协议会被扩展到其它组件。 如何工作Apache Sentry的目标是实现授权管理，它是一个策略引擎，被数据处理工具用来验证访问权限。它也是一个高度扩展的模块，可以支持任何的数据模型。当前，它支持Apache Hive和Cloudera Impala的关系数据模型，以及Apache中的有继承关系的数据模型。 Sentry提供了定义和持久化访问资源的策略的方法。目前，这些策略可以存储在文件里或者是能使用RPC服务访问的数据库后端存储里。数据访问工具，例如Hive，以一定的模式辨认用户访问数据的请求，例如从一个表读一行数据或者删除一个表。这个工具请求Sentry验证访问是否合理。Sentry构建请求用户被允许的权限的映射并判断给定的请求是否允许访问。请求工具这时候根据Sentry的判断结果来允许或者禁止用户的访问请求。 Sentry授权包括以下几种角色： 资源。可能是Server、Database、Table、或者URL（例如：HDFS或者本地路径）。Sentry1.5中支持对列进行授权。 权限。授权访问某一个资源的规则。 角色。角色是一系列权限的集合。 用户和组。一个组是一系列用户的集合。Sentry 的组映射是可以扩展的。默认情况下，Sentry使用Hadoop的组映射（可以是操作系统组或者LDAP中的组）。Sentry允许你将用户和组进行关联，你可以将一系列的用户放入到一个组中。Sentry不能直接给一个用户或组授权，需要将权限授权给角色，角色可以授权给一个组而不是一个用户。 架构下面是Sentry架构图，图片来自《Apache Sentry architecture overview》。 Sentry的体系结构中有三个重要的组件：一是Binding；二是Policy Engine；三是Policy Provider。 BindingBinding实现了对不同的查询引擎授权，Sentry将自己的Hook函数插入到各SQL引擎的编译、执行的不同阶段。这些Hook函数起两大作用：一是起过滤器的作用，只放行具有相应数据对象访问权限的SQL查询；二是起授权接管的作用，使用了Sentry之后，grant/revoke管理的权限完全被Sentry接管，grant/revoke的执行也完全在Sentry中实现；对于所有引擎的授权信息也存储在由Sentry设定的统一的数据库中。这样所有引擎的权限就实现了集中管理。 Policy Engine这是Sentry授权的核心组件。Policy Engine判定从binding层获取的输入的权限要求与服务提供层已保存的权限描述是否匹配。 Policy ProviderPolicy Provider负责从文件或者数据库中读取出原先设定的访问权限。Policy Engine以及Policy Provider其实对于任何授权体系来说都是必须的，因此是公共模块，后续还可服务于别的查询引擎。 基于文件的提供者使用的是ini格式的文件保存元数据信息，这个文件可以是一个本地文件或者HDFS文件。下面是一个例子： 12345678910111213[groups]# Assigns each Hadoop group to its set of rolesmanager = analyst_role, junior_analyst_roleanalyst = analyst_roleadmin = admin_role[roles]analyst_role = server=server1-&gt;db=analyst1, \ server=server1-&gt;db=jranalyst1-&gt;table=*-&gt;action=select, \ server=server1-&gt;uri=hdfs://ha-nn-uri/landing/analyst1, \ server=server1-&gt;db=default-&gt;table=tab2# Implies everything on server1.admin_role = server=server1 基于文件的方式不易于使用程序修改，在修改过程中会存在资源竞争，不利于维护。Hive和Impala需要提供工业标准的SQL接口来管理授权策略，要求能够使用编程的方式进行管理。 Sentry策略存储和服务将角色和权限以及组合角色的映射持久化到一个关系数据库并提供编程的API接口方便创建、查询、更新和删除。这允许Sentry的客户端并行和安全地获取和修改权限。 Sentry策略存储可以使用很多后端的数据库，例如MySQL、Postgres等等，它使用ORM库DataNucleus来完成持久化操作。Sentry支持kerberos认证，也可以扩展代码支持其他认证方式。 使用和Hive集成Sentry策略引擎通过hook的方式插入到hive中，hiveserver2在查询成功编译之后执行这个hook。 这个hook获得这个查询需要以读和写的方式访问的对象，然后Sentry的Hive binding基于SQL授权模型将他们转换为授权的请求。 策略维护： 策略维护包括两个步骤。在查询编译期间，hive调用Sentry的授权任务工厂来生产会在查询过程中执行的Sentry的特定任务行。这个任务调用Sentry存储客户端发送RPC请求给Sentry服务要求改变授权策略。 和HCatalog集成 Sentry通过pre-listener hook集成到Hive Metastore。metastore在执行metadata维护请求之前执行这个hook。metastore binding为提交给metastore和HCatalog客户端的metadata修改请求创建一个Sentry授权请求。 网上关于sentry配置和使用的例子： Securing Impala for analysts 安装和配置Sentry 测试Hive集成Sentry 配置安全的Hive集群集成Sentry 配置安全的Impala集群集成Sentry 参考文章 Apache Sentry architecture overview 为什么Cloudera要创建Hadoop安全组件Sentry？ Cloudera发布Hadoop开源组件Sentry：提供细粒度基于角色的安全控制]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>impala</tag>
        <tag>sentry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译CDH Spark源代码]]></title>
    <url>%2F2015%2F04%2F28%2Fcompile-cdh-spark-source-code%2F</url>
    <content type="text"><![CDATA[本文以Cloudera维护的Spark分支项目为例，记录跟新Spark分支以及编译Spark源代码的过程。 下载代码在Github上fork Cloudera维护的Spark项目到自己的github账号里，对应的地址为https://github.com/javachen/spark。 下载代码： 1$ git clone https://github.com/javachen/spark 然后，切换到最新的分支，当前为 cdh5-1.3.0_5.4.0。 12$ cd spark$ git checkout cdh5-1.3.0_5.4.0 查看当前分支： 123⇒ git branch* cdh5-1.3.0_5.4.0 master 如果spark发布了新的版本，需要同步到我自己维护的spark项目中，可以按以下步骤进行操作: 1234567891011121314# 添加远程仓库地址$ git remote add cdh git@github.com:cloudera/spark.git# 抓取远程仓库更新：$ git fetch cdh# 假设cloudera发布了新的版本 cdh/cdh5-1.3.0_5.4.X$ git checkout -b cdh5-1.3.0_5.4.X cdh/cdh5-1.3.0_5.4.X# 切换到新下载的分支 $ git checkout cdh5-1.3.0_5.4.X# 将其提交到自己的远程仓库：$ git push origin cdh5-1.3.0_5.4.X:cdh5-1.3.0_5.4.X 编译安装 zinc在mac上安装zinc： 1$ brew install zinc 使用maven编译指定hadoop版本为2.6.0-cdh5.4.0，并集成yarn和hive： 12$ export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"$ mvn -Pyarn -Dhadoop.version=2.6.0-cdh5.4.0 -Phive -DskipTests clean package 在CDH的spark中，要想集成hive-thriftserver进行编译，需要修改 pom.xml 文件，添加一行 sql/hive-thriftserver： 123456789101112131415161718192021&lt;modules&gt; &lt;module&gt;core&lt;/module&gt; &lt;module&gt;bagel&lt;/module&gt; &lt;module&gt;graphx&lt;/module&gt; &lt;module&gt;mllib&lt;/module&gt; &lt;module&gt;tools&lt;/module&gt; &lt;module&gt;streaming&lt;/module&gt; &lt;module&gt;sql/catalyst&lt;/module&gt; &lt;module&gt;sql/core&lt;/module&gt; &lt;module&gt;sql/hive&lt;/module&gt; &lt;module&gt;sql/hive-thriftserver&lt;/module&gt; &lt;!--添加的一行--&gt; &lt;module&gt;repl&lt;/module&gt; &lt;module&gt;assembly&lt;/module&gt; &lt;module&gt;external/twitter&lt;/module&gt; &lt;module&gt;external/kafka&lt;/module&gt; &lt;module&gt;external/flume&lt;/module&gt; &lt;module&gt;external/flume-sink&lt;/module&gt; &lt;module&gt;external/zeromq&lt;/module&gt; &lt;module&gt;external/mqtt&lt;/module&gt; &lt;module&gt;examples&lt;/module&gt; &lt;/modules&gt; 然后，再执行： 12$ export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"$ mvn -Pyarn -Dhadoop.version=2.6.0-cdh5.4.0 -Phive -Phive-thriftserver -DskipTests clean package 运行测试用例： 1$ mvn -Pyarn -Dhadoop.version=2.6.0-cdh5.4.0 -Phive test 运行java8测试： 1$ mvn install -DskipTests -Pjava8-tests 使用sbt编译1$ build/sbt -Pyarn -Dhadoop.version=2.6.0-cdh5.4.0 -Phive assembly 生成压缩包1$ ./make-distribution.sh 排错 Unable to find configuration file at location scalastyle-config.xml 异常 在idea中使用maven对examples模块运行package或者install命令会出现Unable to find configuration file at location scalastyle-config.xml异常，解决办法是将根目录下的scalastyle-config.xml拷贝到examples目录下去，这是因为pom.xml中定义的是scalastyle-maven-plugin插件从maven运行的当前目录查找该文件。 123456789101112131415161718192021222324&lt;plugin&gt; &lt;groupId&gt;org.scalastyle&lt;/groupId&gt; &lt;artifactId&gt;scalastyle-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.4.0&lt;/version&gt; &lt;configuration&gt; &lt;verbose&gt;false&lt;/verbose&gt; &lt;failOnViolation&gt;true&lt;/failOnViolation&gt; &lt;includeTestSourceDirectory&gt;false&lt;/includeTestSourceDirectory&gt; &lt;failOnWarning&gt;false&lt;/failOnWarning&gt; &lt;sourceDirectory&gt;$&#123;basedir&#125;/src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;$&#123;basedir&#125;/src/test/scala&lt;/testSourceDirectory&gt; &lt;configLocation&gt;scalastyle-config.xml&lt;/configLocation&gt; &lt;outputFile&gt;scalastyle-output.xml&lt;/outputFile&gt; &lt;outputEncoding&gt;UTF-8&lt;/outputEncoding&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;check&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 参考 Building Spark]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala中下划线的用途]]></title>
    <url>%2F2015%2F04%2F23%2Fall-the-uses-of-an-underscore-in-scala%2F</url>
    <content type="text"><![CDATA[存在性类型： 123def foo(l: List[Option[_]]) = def f(m: M[_]) 高阶类型参数： 123case class A[K[_],T](a: K[T])def f[M[_]] 临时变量： 1val _ = 5 临时参数： 1List(1, 2, 3) foreach &#123; _ =&gt; println("Hi") &#125; //List(1, 2, 3) foreach &#123; t =&gt; println("Hi") &#125; 通配模式： 1234567891011Some(5) match &#123; case Some(_) =&gt; println("Yes") &#125;match &#123; case List(1,_,_) =&gt; " a list with three element and the first element is 1" case List(_*) =&gt; " a list with zero or more elements " case Map[_,_] =&gt; " matches a map with any key type and any value type " case _ =&gt; &#125;val (a, _) = (1, 2)for (_ &lt;- 1 to 10) 通配导入： 12345// Imports all the classes in the package matchingimport scala.util.matching._// Imports all the members of the object Fun (static import in Java).import com.test.Fun._ 隐藏导入： 12345// Imports all the members of the object Fun but renames Foo to Barimport com.test.Fun.&#123; Foo =&gt; Bar , _ &#125;// Imports all the members except Foo. To exclude a member rename it to _import com.test.Fun.&#123; Foo =&gt; _ , _ &#125; 连接字母和标点符号： 1def bang_!(x: Int) = 5 占位符： 123456789101112( (_: Int) + (_: Int) )(2,3)val nums = List(1,2,3,4,5,6,7,8,9,10)nums map (_ + 2)nums sortWith(_&gt;_)nums filter (_ % 2 == 0)nums reduceLeft(_+_)nums reduce (_ + _)nums reduceLeft(_ max _)nums.exists(_ &gt; 5)nums.takeWhile(_ &lt; 8) 偏函数： 1234567891011def fun = &#123; // Some code&#125;val funLike = fun _List(1, 2, 3) foreach println _1 to 5 map (10 * _)//List("foo", "bar", "baz").map(_.toUpperCase())List("foo", "bar", "baz").map(n =&gt; n.toUpperCase()) 初始化默认值: 12var d:Double = _ var i:Int = _ 参数序列： 123456789101112131415//Range转换为ListList(1 to 5:_*)//Range转换为VectorVector(1 to 5: _*)//可变参数中def capitalizeAll(args: String*) = &#123; args.map &#123; arg =&gt; arg.capitalize &#125;&#125;val arr = Array("what's", "up", "doc?")capitalizeAll(arr: _*) 作为参数名： 12345678910//访问mapvar m3 = Map((1,100), (2,200))for(e&lt;-m3) println(e._1 + ": " + e._2)m3 filter (e=&gt;e._1&gt;1)m3 filterKeys (_&gt;1)m3.map(e=&gt;(e._1*10, e._2))m3 map (e=&gt;e._2)//元组(1,2)._2 参考资料 [1] What are all the uses of an underscore in Scala?]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala集合]]></title>
    <url>%2F2015%2F04%2F22%2Fscala-collections%2F</url>
    <content type="text"><![CDATA[Scala有一个非常通用，丰富，强大，可组合的集合库；集合是高阶的(high level)并暴露了一大套操作方法。很多集合的处理和转换可以被表达的简洁又可读，但不审慎地用它们的功能也会导致相反的结果。每个Scala程序员应该阅读 集合设计文档；通过它可以很好地洞察集合库，并了解设计动机。 scala集合API：http://www.scala-lang.org/docu/files/collections-api/collections.html。 怎样使用集合，请参考 Effective Scala。 架构Scala的所有的集合类都可以在包 scala.collection 包中找到，其中的集合类都是高级抽象类或特性。 Scala 集合类系统地区分了可变的和不可变的集合。可变集合可以在适当的地方被更新或扩展。这意味着你可以修改，添加，移除一个集合的元素。而不可变集合类，相比之下，永远不会改变。不过，你仍然可以模拟添加，移除或更新操作。但是这些操作将在每一种情况下都返回一个新的集合，同时使原来的集合不发生改变。 可变的集合类位于 scala.collection.mutable 包中，而不可变的集合位于 scala.collection.immutable 。scala.collection 包中的集合，既可以是可变的，也可以是不可变的。例如：collection.IndexedSeq[T] 就是 collection.immutable.IndexedSeq[T] 和 collection.mutable.IndexedSeq[T] 这两类的超类。scala.collection 包中的根集合类中定义了相同的接口作为不可变集合类，同时，scala.collection.mutable 包中的可变集合类代表性的添加了一些有辅助作用的修改操作到这个 immutable 接口。 下面的图表显示 scala.collection.immutable 中的所有集合类。 下面的图表显示 scala.collection.mutable 中的所有集合类。 默认情况下，Scala 一直采用不可变集合类。例如，如果你仅写了 Set 而没有任何加前缀也没有从其它地方导入 Set，你会得到一个不可变的 set，另外如果你写迭代，你也会得到一个不可变的迭代集合类，这是由于这些类在从 scala 中导入的时候都是默认绑定的。为了得到可变的默认版本，你需要显式的声明collection.mutable.Set或collection.mutable.Iterable。 一个有用的约定，如果你想要同时使用可变和不可变集合类，只导入 collection.mutable 包即可。 1import scala.collection.mutable //导入包scala.collection.mutable 然而，像没有前缀的 Set 这样的关键字， 仍然指的是一个不可变集合，然而 mutable.Set 指的是可变的副本（可变集合）。 为了方便和向后兼容性，一些导入类型在包 scala 中有别名，所以你能通过简单的名字使用它们而不需要 import。这有一个例子是 List类型，它可以用以下两种方法使用，如下： 123scala.collection.immutable.List // 这是它的定义位置scala.List //通过scala 包中的别名List // 因为scala._ 总是是被自动导入。 其它类型的别名有： Traversable, Iterable, Seq, IndexedSeq, Iterator, Stream, Vector, StringBuilder, Range。 不可变（collection.immutable._） 可变（collection.mutable._） Array ArrayBuffer List ListBuffer String StringBuilder / LinkedList, DoubleLinkedList List MutableList / Queue Array ArraySeq Stack ArrayStack HashMap HashSet HashMap HashSet TraversableTraversable 是容器类的最高级别特性，它唯一的抽象操作是 foreach： 1def foreach[U](f: Elem =&gt; U) Traversable 同时定义的很多具体方法： 相加操作++ Map 操作有 map，flatMap 和 collect xs map f 通过函数xs中的每一个元素调用函数f来生成一个容器。 xs flatMap f 通过对容器xs中的每一个元素调用作为容器的值函数f，在把所得的结果连接起来作为一个新的容器。 xs collect f 通过对每个xs中的符合定义的元素调用偏函数f，并把结果收集起来生成一个集合。 转换操作包括 toArray，toList，toIterable，toSeq，toIndexedSeq，toStream，toSet，和 toMap xs.toArray 把容器转换为一个数组 xs.toList 把容器转换为一个list xs.toIterable 把容器转换为一个迭代器。 xs.toSeq 把容器转换为一个序列 xs.toIndexedSeq 把容器转换为一个索引序列 xs.toStream 把容器转换为一个延迟计算的流。 xs.toSet 把容器转换为一个Set。 xs.toMap 把由键/值对组成的容器转换为一个映射表。如果该容器并不是以键/值对作为元素的，那么调用这个操作将会导致一个静态类型的错误。 拷贝操作有 copyToBuffer 和 copyToArray xs copyToBuffer buf 把容器的所有元素拷贝到buf缓冲区。 xs copyToArray(arr, s, n) 拷贝最多n个元素到数组arr的坐标s处。参数s，n是可选项。 Size 操作包括有 isEmpty，nonEmpty，size 和 hasDefiniteSize xs.isEmpty 测试容器是否为空。 xs.nonEmpty 测试容器是否包含元素。 xs.size 计算容器内元素的个数。 xs.hasDefiniteSize 如果xs的大小是有限的，则为true。 元素检索操作有 head，last，headOption，lastOption 和 find xs.head 返回容器内第一个元素（或其他元素，若当前的容器无序）。 xs.headOption xs选项值中的第一个元素，若xs为空则为None。 xs.last 返回容器的最后一个元素（或某个元素，如果当前的容器无序的话）。 xs.lastOption xs选项值中的最后一个元素，如果xs为空则为None。 xs find p 查找xs中满足p条件的元素，若存在则返回第一个元素；若不存在，则为空。 子容器检索操作有 tail，init，slice，take，drop，takeWhilte，dropWhile，filter，filteNot 和 withFilter xs.tail 返回由除了xs.head外的其余部分。 xs.init 返回除xs.last外的其余部分。 xs slice (from, to) 返回由xs的一个片段索引中的元素组成的容器（从from到to，但不包括to）。 xs take n 由xs的第一个到第n个元素（或当xs无序时任意的n个元素）组成的容器。 xs drop n 由除了xs take n以外的元素组成的容器。 xs takeWhile p 容器xs中最长能够满足断言p的前缀。 xs dropWhile p 容器xs中除了xs takeWhile p以外的全部元素。 xs filter p 由xs中满足条件p的元素组成的容器。 xs withFilter p 这个容器是一个不太严格的过滤器。子容器调用map，flatMap，foreach和withFilter只适用于xs中那些的满足条件p的元素。 xs filterNot p 由xs中不满足条件p的元素组成的容器。 拆分操作有 splitAt，span，partition 和 groupBy xs splitAt n 把xs从指定位置的拆分成两个容器（xs take n和xs drop n）。 xs span p 根据一个断言p将xs拆分为两个容器（xs takeWhile p, xs.dropWhile p）。 xs partition p 把xs分割为两个容器，符合断言p的元素赋给一个容器，其余的赋给另一个(xs filter p, xs.filterNot p)。 xs groupBy f 根据判别函数f把xs拆分一个到容器的map中。 元素测试包括有 exists，forall 和 count xs forall p 返回一个布尔值表示用于表示断言p是否适用xs中的所有元素。 xs exists p 返回一个布尔值判断xs中是否有部分元素满足断言p。 xs count p 返回xs中符合断言p条件的元素个数。 折叠操作有 foldLeft，foldRight，/:，:\，reduceLeft 和 reduceRight (z /: xs)(op) 在xs中，对由z开始从左到右的连续元素应用二进制运算op。 (xs :\ z)(op) 在xs中，对由z开始从右到左的连续元素应用二进制运算op xs.foldLeft(z)(op) 与 (z /: xs)(op)相同。 xs.foldRight(z)(op) 与 (xs :\ z)(op)相同。 xs reduceLeft op 非空容器xs中的连续元素从左至右调用二进制运算op。 xs reduceRight op 非空容器xs中的连续元素从右至左调用二进制运算op。 特殊折叠包括 sum, product, min, max xs.sum 返回容器xs中数字元素的和。 xs.product xs返回容器xs中数字元素的积。 xs.min 容器xs中有序元素值中的最小值。 xs.max 容器xs中有序元素值中的最大值。 字符串操作有 mkString，addString 和 stringPrefix xs addString (b, start, sep, end) 把一个字符串加到StringBuilder对象b中，该字符串显示为将xs中所有元素用分隔符sep连接起来并封装在start和end之间。其中start，end和sep都是可选的。 xs mkString (start, sep, end) 把容器xs转换为一个字符串，该字符串显示为将xs中所有元素用分隔符sep连接起来并封装在start和end之间。其中start，end和sep都是可选的。 xs.stringPrefix 返回一个字符串，该字符串是以容器名开头的 xs.toString。 视图操作包含两个view方法的重载体 xs.view 通过容器xs生成一个视图。 xs view (from, to) 生成一个表示在指定索引范围内的xs元素的视图。 Iterable继承 Traversable 的特性是 Iterable，该类实现了 foreach 方法，定义了一个迭代器。 1234def foreach[U](f: Elem =&gt; U): Unit = &#123; val it = iterator while (it.hasNext) f(it.next())&#125; Iterable 有两个方法返回迭代器：grouped 和 sliding。grouped 方法返回元素的增量分块，sliding 方法生成一个滑动元素的窗口。两者的差异见下面代码： 12345678910111213141516scala&gt; val xs = List(1, 2, 3, 4, 5)xs: List[Int] = List(1, 2, 3, 4, 5)scala&gt; val git = xs grouped 3git: Iterator[List[Int]] = non-empty iteratorscala&gt; git.next()res3: List[Int] = List(1, 2, 3)scala&gt; git.next()res4: List[Int] = List(4, 5)scala&gt; val sit = xs sliding 3sit: Iterator[List[Int]] = non-empty iteratorscala&gt; sit.next()res5: List[Int] = List(1, 2, 3)scala&gt; sit.next()res6: List[Int] = List(2, 3, 4)scala&gt; sit.next()res7: List[Int] = List(3, 4, 5) Iterable 增加了一些其他方法： xs takeRight n 一个容器由xs的最后n个元素组成（若定义的元素是无序，则由任意的n个元素组成）。 xs dropRight n 一个容器由除了xs 被取走的（执行过takeRight方法）n个元素外的其余元素组成。 xs zip ys 把一对容器 xs和ys的包含的元素合成到一个iterabale。 xs zipAll (ys, x, y) 一对容器 xs 和ys的相应的元素合并到一个iterable ，实现方式是通过附加的元素x或y，把短的序列被延展到相对更长的一个上。 xs.zip WithIndex 把一对容器xs和它的序列，所包含的元素组成一个iterable 。 xs sameElements ys 测试 xs 和 ys 是否以相同的顺序包含相同的元素。 Seq序列，指的是一类具有一定长度的可迭代访问的对象，其中每个元素均带有一个从0开始计数的固定索引位置。 序列的操作有以下几种，如下表所示： 索引和长度 xs(i) (或者为xs apply i)。xs的第i个元素 xs isDefinedAt i 测试xs.indices中是否包含i。 xs.length 序列的长度（同size）。 xs.lengthCompare ys 如果xs的长度小于ys的长度，则返回-1。如果xs的长度大于ys的长度，则返回+1，如果它们长度相等，则返回0。即使其中一个序列是无限的，也可以使用此方法。 xs.indices xs的索引范围，从0到xs.length - 1。 索引搜索 xs indexOf x 返回序列xs中等于x的第一个元素的索引（存在多种变体）。 xs lastIndexOf x 返回序列xs中等于x的最后一个元素的索引（存在多种变体）。 xs indexOfSlice ys 查找子序列ys，返回xs中匹配的第一个索引。 xs indexOfSlice ys 查找子序列ys，返回xs中匹配的倒数一个索引。 xs indexWhere p xs序列中满足p的第一个元素。（有多种形式） xs segmentLength (p, i) xs中，从xs(i)开始并满足条件p的元素的最长连续片段的长度。 xs prefixLength p xs序列中满足p条件的先头元素的最大个数。 加法 x +: xs 由序列xs的前方添加x所得的新序列。 xs :+ x 由序列xs的后方追加x所得的新序列。 xs padTo (len, x) 在xs后方追加x，直到长度达到len后得到的序列。 更新 xs patch (i, ys, r) 将xs中第i个元素开始的r个元素，替换为ys所得的序列。 xs updated (i, x) 将xs中第i个元素替换为x后所得的xs的副本。 xs(i) = x （或写作 xs.update(i, x)，仅适用于可变序列）将xs序列中第i个元素修改为x。 排序 xs.sorted 通过使用xs中元素类型的标准顺序，将xs元素进行排序后得到的新序列。 xs sortWith lt 将lt作为比较操作，并以此将xs中的元素进行排序后得到的新序列。 xs sortBy f 将序列xs的元素进行排序后得到的新序列。参与比较的两个元素各自经f函数映射后得到一个结果，通过比较它们的结果来进行排序。 反转 xs.reverse 与xs序列元素顺序相反的一个新序列。 xs.reverseIterator 产生序列xs中元素的反序迭代器。 xs reverseMap f 以xs的相反顺序，通过f映射xs序列中的元素得到的新序列。 比较 xs startsWith ys 测试序列xs是否以序列ys开头（存在多种形式）。 xs endsWith ys 测试序列xs是否以序列ys结束（存在多种形式）。 xs contains x 测试xs序列中是否存在一个与x相等的元素。 xs containsSlice ys 测试xs序列中是否存在一个与ys相同的连续子序列。 (xs corresponds ys)(p) 测试序列xs与序列ys中对应的元素是否满足二元的判断式p。 多集操作 xs intersect ys 序列xs和ys的交集，并保留序列xs中的顺序。 xs diff ys 序列xs和ys的差集，并保留序列xs中的顺序。 xs union ys 并集；同xs ++ ys。 xs.distinct不含重复元素的xs的子序列。 Seq 具有两个子特征 LinearSeq 和 IndexedSeq。它们不添加任何新的操作，但都提供不同的性能特点：线性序列具有高效的 head 和 tail 操作，而索引序列具有高效的apply, length, 和 (如果可变) update操作。 缓冲器Buffers是可变序列一个重要的种类。它们不仅允许更新现有的元素，而且允许元素的插入、移除和在buffer尾部高效地添加新元素。buffer 支持的主要新方法有：用于在尾部添加元素的 += 和 ++=；用于在前方添加元素的 +=: 和 ++=:；用于插入元素的 insert 和 insertAll；以及用于删除元素的 remove 和 -=。 Buffer类的操作： 加法 buf += x 将元素x追加到buffer，并将buf自身作为结果返回。 buf += (x, y, z) 将给定的元素追加到buffer。 buf ++= xs 将xs中的所有元素追加到buffer。 x +=: buf 将元素x添加到buffer的前方。 xs ++=: buf 将xs中的所有元素都添加到buffer的前方。 buf insert (i, x) 将元素x插入到buffer中索引为i的位置。 buf insertAll (i, xs) 将xs的所有元素都插入到buffer中索引为i的位置。 移除 buf -= x 将元素x从buffer中移除。 buf remove i 将buffer中索引为i的元素移除。 buf remove (i, n) 将buffer中从索引i开始的n个元素移除。 buf trimStart n 移除buffer中的前n个元素。 buf trimEnd n 移除buffer中的后n个元素。 buf.clear() 移除buffer中的所有元素。 克隆 buf.clone 与buf具有相同元素的新buffer。 ListBuffer 和 ArrayBuffer 是常用的 buffer 实现 。顾名思义，ListBuffe r依赖列表，支持高效地将它的元素转换成列表。而ArrayBuffer依赖数组，能快速地转换成数组。 SetSet 是不包含重复元素的可迭代对象。 不可变 Set 类的操作： 测试 xs contains x 测试x是否是xs的元素。 xs(x) 与xs contains x相同。 xs subsetOf ys 测试xs是否是ys的子集。 加法： xs + x 包含xs中所有元素以及x的集合。 xs + (x, y, z) 包含xs中所有元素及附加元素的集合 xs ++ ys 包含xs中所有元素及ys中所有元素的集合 减法： xs - x 包含xs中除x以外的所有元素的集合。 xs - x 包含xs中除去给定元素以外的所有元素的集合。 xs -- ys 集合内容为：xs中所有元素，去掉ys中所有元素后剩下的部分。 xs.empty 与xs同类的空集合。 二进制操作： xs &amp; ys 集合xs和ys的交集。 xs intersect ys 等同于 xs &amp; ys。 xs union ys 等同于xs xs &amp;~ ys 集合xs和ys的差集。 xs diff ys 等同于 xs &amp;~ ys。 可变 Set 类的操作 加法： xs += x 把元素x添加到集合xs中。该操作有副作用，它会返回左操作符，这里是xs自身。 xs += (x, y, z) 添加指定的元素到集合xs中，并返回xs本身。（同样有副作用） xs ++= ys 添加集合ys中的所有元素到集合xs中，并返回xs本身。（表达式有副作用） xs add x 把元素x添加到集合xs中，如集合xs之前没有包含x，该操作返回true，否则返回false。 移除： xs -= x 从集合xs中删除元素x，并返回xs本身。（表达式有副作用） xs -= (x, y, z) 从集合xs中删除指定的元素，并返回xs本身。（表达式有副作用） xs --= ys 从集合xs中删除所有属于集合ys的元素，并返回xs本身。（表达式有副作用） xs remove x 从集合xs中删除元素x。如之前xs中包含了x元素，返回true，否则返回false。 xs retain p 只保留集合xs中满足条件p的元素。 xs.clear() 删除集合xs中的所有元素。 更新： xs(x) = b （ 同 xs.update(x, b) ）参数b为布尔类型，如果值为true就把元素x加入集合xs，否则从集合xs中删除x。 克隆： xs.clone 产生一个与xs具有相同元素的可变集合。 与不变集合一样，可变集合也提供了+和++操作符来添加元素，-和--用来删除元素。但是这些操作在可变集合中通常很少使用，因为这些操作都要通过集合的拷贝来实现。可变集合提供了更有效率的更新方法，+=和-=。 s += elem，添加元素elem到集合s中，并返回产生变化后的集合作为运算结果。同样的，s -= elem执行从集合s中删除元素elem的操作，并返回产生变化后的集合作为运算结果。除了+=和-=之外还有从可遍历对象集合或迭代器集合中添加和删除所有元素的批量操作符++=和--=。 Set集合的两个特质是SortedSet和 BitSet。 SortedSetSortedSet 是指以特定的顺序（这一顺序可以在创建集合之初自由的选定）排列其元素（使用iterator或foreach）的集合。 SortedSet 的默认表示是有序二叉树，即左子树上的元素小于所有右子树上的元素。这样，一次简单的顺序遍历能按增序返回集合中的所有元素。Scala的类 immutable.TreeSet 使用红黑树实现，它在维护元素顺序的同时，也会保证二叉树的平衡，即叶节点的深度差最多为1。 创建一个空的 TreeSet ，可以先定义排序规则： 12scala&gt; val myOrdering = Ordering.fromLessThan[String](_ &gt; _)myOrdering: scala.math.Ordering[String] = scala.math.Ordering$$anon$9@6bd5a0fa 然后，用这一排序规则创建一个空的树集： 12scala&gt; TreeSet.empty(myOrdering)res1: scala.collection.immutable.TreeSet[String] = TreeSet() 或者，你也可以不指定排序规则参数，只需要给定一个元素类型或空集合。在这种情况下，将使用此元素类型默认的排序规则。 12scala&gt; TreeSet.empty[String]res2: scala.collection.immutable.TreeSet[String] = TreeSet() 如果通过已有的TreeSet来创建新的集合（例如，通过串联或过滤操作），这些集合将和原集合保持相同的排序规则。例如， 12scala&gt; res2 + ("one", "two", "three", "four")res3: scala.collection.immutable.TreeSet[String] = TreeSet(four, one, three, two) 有序集合同样支持元素的范围操作。例如，range方法返回从指定起始位置到结束位置（不含结束元素）的所有元素，from方法返回大于等于某个元素的所有元素。调用这两种方法的返回值依然是有序集合。例如： 1234scala&gt; res3 range ("one", "two")res4: scala.collection.immutable.TreeSet[String] = TreeSet(one, three)scala&gt; res3 from "three"res5: scala.collection.immutable.TreeSet[String] = TreeSet(three, two) Bitset位集合是由单字或多字的紧凑位实现的非负整数的集合。其内部使用Long型数组来表示。第一个Long元素表示的范围为0到63，第二个范围为64到127，以此类推（值为0到127的非可变位集合通过直接将值存储到第一个或第两个Long字段的方式，优化掉了数组处理的消耗）。对于每个Long，如果有相应的值包含于集合中则它对应的位设置为1，否则该位为0。这里遵循的规律是，位集合的大小取决于存储在该集合的最大整数的值的大小。假如N是为集合所要表示的最大整数，则集合的大小就是N/64个长整形字，或者N/8个字节，再加上少量额外的状态信息字节。 因此当位集合包含的元素值都比较小时，它比其他的集合类型更紧凑。位集合的另一个优点是它的contains方法（成员测试）、+=运算（添加元素）、-=运算（删除元素）都非常的高效。 1234val bs = collection.mutable.BitSet()bs += (1,3,5) // BitSet(1, 5, 3)bs ++= List(7,9) // BitSet(1, 9, 7, 5, 3)bs.clear // BitSet() MapMap是一种可迭代的键值对结构（也称映射或关联）。Scala的Predef类提供了隐式转换，允许使用另一种语法：key -&gt; value，来代替(key, value)。如：Map(&quot;x&quot; -&gt; 24, &quot;y&quot; -&gt; 25, &quot;z&quot; -&gt; 26) 等同于 Map((&quot;x&quot;, 24), (&quot;y&quot;, 25), (&quot;z&quot;, 26))，却更易于阅读。 不可变Map类的操作: 查询： ms get k 返回一个Option，其中包含和键k关联的值。若k不存在，则返回None。 ms(k) （完整写法是ms apply k）返回和键k关联的值。若k不存在，则抛出异常。 ms getOrElse (k, d)返回和键k关联的值。若k不存在，则返回默认值d。 ms contains k 检查ms是否包含与键k相关联的映射。 ms isDefinedAt k 同contains。 添加及更新: ms + (k -&gt; v) 返回一个同时包含ms中所有键值对及从k到v的键值对k -&gt; v的新映射。 ms + (k -&gt; v, l -&gt; w) 返回一个同时包含ms中所有键值对及所有给定的键值对的新映射。 ms ++ kvs 返回一个同时包含ms中所有键值对及kvs中的所有键值对的新映射。 ms updated (k, v) 同ms + (k -&gt; v)。 移除： ms - k 返回一个包含ms中除键k以外的所有映射关系的映射。 ms - (k, 1, m) 返回一个滤除了ms中与所有给定的键相关联的映射关系的新映射。 ms -- ks 返回一个滤除了ms中与ks中给出的键相关联的映射关系的新映射。 子容器： ms.keys 返回一个用于包含ms中所有键的iterable对象 ms.keySet 返回一个包含ms中所有的键的集合。 ms.keyIterator 返回一个用于遍历ms中所有键的迭代器。 ms.values 返回一个包含ms中所有值的iterable对象。 ms.valuesIterator 返回一个用于遍历ms中所有值的迭代器。 变换： ms filterKeys p 一个映射视图，其包含一些ms中的映射，且这些映射的键满足条件p。用条件谓词p过滤ms中所有的键，返回一个仅包含与过滤出的键值对的映射视图。 ms mapValues f 用f将ms中每一个键值对的值转换成一个新的值，进而返回一个包含所有新键值对的映射视图。 可变Map类中的操作： 添加及更新： ms(k) = v （完整形式为ms.update(x, v)）。向映射ms中新增一个以k为键、以v为值的映射关系，ms先前包含的以k为值的映射关系将被覆盖。 ms += (k -&gt; v) 向映射ms增加一个以k为键、以v为值的映射关系，并返回ms自身。 ms += (k -&gt; v, l -&gt; w) 向映射ms中增加给定的多个映射关系，并返回ms自身。 ms ++= kvs 向映射ms增加kvs中的所有映射关系，并返回ms自身。 ms put (k, v) 向映射ms增加一个以k为键、以v为值的映射，并返回一个Option，其中可能包含此前与k相关联的值。 ms getOrElseUpdate (k, d) 如果ms中存在键k，则返回键k的值。否则向ms中新增映射关系k -&gt; v并返回d。 移除： ms -= k 从映射ms中删除以k为键的映射关系，并返回ms自身。 ms -= (k, l, m) 从映射ms中删除与给定的各个键相关联的映射关系，并返回ms自身。 ms --= ks 从映射ms中删除与ks给定的各个键相关联的映射关系，并返回ms自身。 ms remove k 从ms中移除以k为键的映射关系，并返回一个Option，其可能包含之前与k相关联的值。 ms retain p 仅保留ms中键满足条件谓词p的映射关系。 ms.clear() 删除ms中的所有映射关系 变换： ms transform f 以函数f转换ms中所有键值对，transform中参数f的类型是(A, B) =&gt; B，即对ms中的所有键值对调用f，得到一个新的值，并用该值替换原键值对中的值。 克隆： ms.clone 返回一个新的可变映射，其中包含与ms相同的映射关系。 Map的添加和删除操作与Set的相关操作相同。同Set操作一样，可变映射也支持非破坏性修改操作+、-、和 updated。但是这些操作涉及到可变映射的复制，因此较少被使用。而利用两种变形m(key) = value和m += (key -&gt; value)， 我们可以“原地”修改可变映射m。此外，存还有一种变形m put (key, value)，该调用返回一个Option值，其中包含此前与键相关联的值，如果不存在这样的值，则返回None。 同步的Map，使用SychronizedMap，同步Set，使用SynchronizedSet。 不可变Map的定义： 1234567891011121314//创建map并指定类型scala&gt; var m1 = Map[Int, Int]()m: scala.collection.immutable.Map[Int,Int] = Map() //缺醒是不可变map//创建map并初始化scala&gt; var m2 = Map(1-&gt;100, 2-&gt;200)m: scala.collection.immutable.Map[Int,Int] = Map(1 -&gt; 100, 2 -&gt; 200)scala&gt; var m3 = Map((1,100), (2,200))m: scala.collection.immutable.Map[Int,Int] = Map(1 -&gt; 100, 2 -&gt; 200)//创建map并指定类型、初始化scala&gt; val m4:Map[Int,String] = Map(1-&gt;"a",2-&gt;"b")m4: Map[Int,String] = Map(1 -&gt; a, 2 -&gt; b) 读取元素： 123456789101112131415161718192021222324scala&gt; m3(1)res0: Int = 100scala&gt; m3.get(1)res1: Option[Int] = Some(100)scala&gt; m3.getOrElse(4, -1)res2: Int = -1//读取所有元素scala&gt; for(e&lt;-m3) println(e._1 + ": " + e._2)1: 1002: 2003: 300scala&gt; m3.foreach(e=&gt;println(e._1 + ": " + e._2))1: 1002: 2003: 300scala&gt; for ((k,v)&lt;-m3) println(k + ": " + v)1: 1002: 2003: 300 也可以进行filter、map操作： 12345678910111213141516171819scala&gt; m3 filter (e=&gt;e._1&gt;1)res46: scala.collection.immutable.Map[Int,Int] = Map(2 -&gt; 200, 3 -&gt; 300)scala&gt; m3 filterKeys (_&gt;1)res47: scala.collection.immutable.Map[Int,Int] = Map(2 -&gt; 200, 3 -&gt; 300)scala&gt; m3.map(e=&gt;(e._1*10, e._2))res48: scala.collection.immutable.Map[Int,Int] = Map(10 -&gt; 100, 20 -&gt; 200, 30 -&gt; 300)scala&gt; m3 map (e=&gt;e._2)res49: scala.collection.immutable.Iterable[Int] = List(100, 200, 300)//相当于：scala&gt; m3.values.toListres50: List[Int] = List(100, 200, 300)//按照key来取对应的value值：scala&gt; 2 to 100 flatMap m3.getres52: scala.collection.immutable.IndexedSeq[Int] = Vector(200, 300) 增加、删除、更新： 123456789101112131415161718192021222324//Map本身不可改变，即使定义为var，更新操作也是返回一个新的不可变Mapscala&gt; var m4 = Map(1-&gt;100)m4: scala.collection.immutable.Map[Int,Int] = Map(1 -&gt; 100)scala&gt; m4 += (2-&gt;200) // m4指向新的(1-&gt;100,2-&gt;200), (1-&gt;100)应该被回收//另一种更新方式scala&gt; m4.updated(1,1000)res7: scala.collection.immutable.Map[Int,Int] = Map(1 -&gt; 1000, 2 -&gt; 200)//增加多个元素：scala&gt; Map(1-&gt;100,2-&gt;200) + (3-&gt;300, 4-&gt;400)res8: scala.collection.immutable.Map[Int,Int] = Map(1 -&gt; 100, 2 -&gt; 200, 3 -&gt; 300, 4 -&gt; 400)//删除元素：scala&gt; Map(1-&gt;100,2-&gt;200,3-&gt;300) - (2,3) res9: scala.collection.immutable.Map[Int,Int] = Map(1 -&gt; 100)scala&gt; Map(1-&gt;100,2-&gt;200,3-&gt;300) -- List(2,3) res10: scala.collection.immutable.Map[Int,Int] = Map(1 -&gt; 100)//合并Map：scala&gt; Map(1-&gt;100,2-&gt;200) ++ Map(3-&gt;300) res11: scala.collection.immutable.Map[Int,Int] = Map(1 -&gt; 100, 2 -&gt; 200, 3 -&gt; 300) 对于可变Map的定义和操作： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465scala&gt; val map = scala.collection.mutable.Map[String, Any]()map: scala.collection.mutable.Map[String,Any] = Map()// 增加元素scala&gt; map("k1")=100// 增加元素scala&gt; map += "k2"-&gt;"v2"res13: map.type = Map(k2 -&gt; v2, k1 -&gt; 100)//判断元素值scala&gt; map("k2")=="v2"res14: Boolean = truescala&gt; map.get("k2")==Some("v2")res15: Boolean = truescala&gt; map.get("k3")==Noneres16: Boolean = truescala&gt; val mm = collection.mutable.Map(1-&gt;100,2-&gt;200,3-&gt;300)mm: scala.collection.mutable.Map[Int,Int] = Map(2 -&gt; 200, 1 -&gt; 100, 3 -&gt; 300)//有则取之，无则加之scala&gt; mm getOrElseUpdate (3,-1)res17: Int = 300scala&gt; mm getOrElseUpdate (4,-1)res18: Int = -1//删除元素scala&gt; mm -= 1res19: mm.type = Map(2 -&gt; 200, 4 -&gt; -1, 3 -&gt; 300)//删除元素scala&gt; mm -= (2,3)res20: mm.type = Map(4 -&gt; -1)//添加一个Mapscala&gt; mm += (1-&gt;100,2-&gt;200,3-&gt;300)res21: mm.type = Map(2 -&gt; 200, 4 -&gt; -1, 1 -&gt; 100, 3 -&gt; 300)//删除元素scala&gt; mm --= List(1,2)res22: mm.type = Map(4 -&gt; -1, 3 -&gt; 300)//删除元素scala&gt; mm remove 1res23: Option[Int] = Nonescala&gt; mm += (1-&gt;100,2-&gt;200,3-&gt;300)res24: mm.type = Map(2 -&gt; 200, 4 -&gt; -1, 1 -&gt; 100, 3 -&gt; 300)scala&gt; mm.retain((x,y) =&gt; x&gt;1)res25: mm.type = Map(2 -&gt; 200, 4 -&gt; -1, 3 -&gt; 300)//转换操作scala&gt; mm transform ((x,y)=&gt; 0)res26: mm.type = Map(2 -&gt; 0, 4 -&gt; 0, 3 -&gt; 0)scala&gt; mm transform ((x,y)=&gt; x*10)res27: mm.type = Map(2 -&gt; 20, 4 -&gt; 40, 3 -&gt; 30)scala&gt; mm transform ((x,y)=&gt; y+3)res28: mm.type = Map(2 -&gt; 23, 4 -&gt; 43, 3 -&gt; 33) ListMapListMap被用来表示一个保存键-值映射的链表。一般情况下，ListMap操作都需要遍历整个列表，所以操作的运行时间也同列表长度成线性关系。实际上ListMap在Scala中很少使用，因为标准的不可变映射通常速度会更快。唯一的例外是，在构造映射时由于某种原因，链表中靠前的元素被访问的频率大大高于其他的元素。 12345scala&gt; val map = scala.collection.immutable.ListMap(1-&gt;"one", 2-&gt;"two")map: scala.collection.immutable.ListMap[Int,java.lang.String] = Map(1 -&gt; one, 2 -&gt; two)scala&gt; map(2)res30: String = "two" 不可变Seq实体类List列表List是一种有限的不可变序列式。 列表定义： 12val list:List[Int] = List(1,3,4,5,6) // 或者 List(1 to 6:_*)val list1 = List("a","b","c","d") // 或者 List('a' to 'd':_*) map (_.toString) 合并： 123456val list2 = "a"::"b"::"c"::Nil // Nil是必须的val list3 = "begin" :: list2 // list2不变，只能加在头，不能加在尾//多个List合并用++，也可以用:::(不如++)val list4 = list2 ++ "end" ++ Nilval list4 = list2 ::: "end" :: Nil // 相当于 list2 ::: List("end") 建议定义方式： 1234567val head::body = List(4,"a","b","c","d")// head: Any = 4// body: List[Any] = List(a, b, c, d)val a::b::c = List(1,2,3)// a: Int = 1// b: Int = 2// c: List[Int] = List(3) ListBuffer是可变的： 12345val lb = collection.mutable.ListBuffer[Int]()lb += (1,3,5,7)lb ++= List(9,11) // ListBuffer(1, 3, 5, 7, 9, 11)lb.toList // List(1, 3, 5, 7, 9, 11)lb.clear // ListBuffer() Stream流Stream与List很相似，只不过其中的每一个元素都经过了一些简单的计算处理。也正是因为如此，stream结构可以无限长。只有那些被要求的元素才会经过计算处理，除此以外stream结构的性能特性与List基本相同。 鉴于List通常使用::运算符来进行构造，stream使用外观上很相像的#::。这里用一个包含整数1，2和3的stream来做一个简单的例子： 12scala&gt; val str = 1 #:: 2 #:: 3 #:: Stream.empty //同List的构造，最后一个必须为空str: scala.collection.immutable.Stream[Int] = Stream(1, ?) 该stream的头结点是1，尾是2和3，尾部并没有被打印出来，因为还没有被计算。stream被特别定义为懒惰计算，并且stream的toString方法很谨慎的设计为不去做任何额外的计算。 下面给出一个稍复杂些的例子。这里讲一个以两个给定的数字为起始的斐波那契数列转换成stream。斐波那契数列的定义是，序列中的每个元素等于序列中在它之前的两个元素之和。 12scala&gt; def fibFrom(a: Int, b: Int): Stream[Int] = a #:: fibFrom(b, a + b)fibFrom: (a: Int,b: Int)Stream[Int] 这个函数看起来比较简单。序列中的第一个元素显然是a，其余部分是以b和位于其后的a+b为开始斐波那契数列。这段程序最大的亮点是在对序列进行计算的时候避免了无限递归。如果函数中使用::来替换#::，那么之后的每次调用都会产生另一次新的调用，从而导致无限递归。在此例中，由于使用了#::，等式右值中的调用在需要求值之前都不会被展开。这里尝试着打印出以1，1开头的斐波那契数列的前几个元素： 1234scala&gt; val fibs = fibFrom(1, 1).take(7)fibs: scala.collection.immutable.Stream[Int] = Stream(1, ?)scala&gt; fibs.toListres9: List[Int] = List(1, 1, 2, 3, 5, 8, 13) Stream相当于lazy List，避免在中间过程中生成不必要的集合。 例子1： 12Range(1,50000000).filter (_ % 13==0)(1) // 26, 但很慢，需要大量内存Stream.range(1,50000000).filter(_%13==0)(1) // 26，很快，只计算最终结果需要的内容 注意：第一个版本在filter后生成一个中间集合，大小为50000000/13；而后者不生成此中间集合，只计算到26即可。 例子2： 12(1 to 100).map(i=&gt; i*3+7).filter(i=&gt; (i%10)==0).sum // map和filter生成两个中间collection(1 to 100).toStream.map(i=&gt; i*3+7).filter(i=&gt; (i%10)==0).sum Vector向量Vector是用来解决列表不能高效的随机访问的一种结构。Vector结构能够在“更高效”的固定时间内访问到列表中的任意元素。虽然这个时间会比访问头结点或者访问某数组元素所需的时间长一些，但至少这个时间也是个常量。因此，使用Vector的算法不必仅是小心的处理数据结构的头结点。由于可以快速修改和访问任意位置的元素，所以对Vector结构做写操作很方便。 Seq的缺省实现是List： 12scala&gt; Seq(1,2,3)res84: Seq[Int] = List(1, 2, 3) IndexSeq的缺省实现是Vector: 12scala&gt; IndexedSeq(1,2,3)res85: IndexedSeq[Int] = Vector(1, 2, 3) Vector类型的构建和修改与其他的序列结构基本一样。 12345678scala&gt; val vec = scala.collection.immutable.Vector.emptyvec: scala.collection.immutable.Vector[Nothing] = Vector()scala&gt; val vec2 = vec :+ 1 :+ 2vec2: scala.collection.immutable.Vector[Int] = Vector(1, 2)scala&gt; val vec3 = 100 +: vec2vec3: scala.collection.immutable.Vector[Int] = Vector(100, 1, 2)scala&gt; vec3(0)res1: Int = 100 Vector结构通常被表示成具有高分支因子的树（树或者图的分支因子是指数据结构中每个节点的子节点数目）。每一个树节点包含最多32个vector元素或者至多32个子树节点。包含最多32个元素的vector可以表示为一个单一节点，而一个间接引用则可以用来表示一个包含至多32*32=1024个元素的vector。从树的根节点经过两跳到达叶节点足够存下有2的15次方个元素的vector结构，经过3跳可以存2的20次方个，4跳2的25次方个，5跳2的30次方个。所以对于一般大小的vector数据结构，一般经过至多5次数组访问就可以访问到指定的元素。这也就是我们之前所提及的随机数据访问时“运行时间的相对高效”。 由于Vectors结构是不可变的，所以您不能通过修改vector中元素的方法来返回一个新的vector。尽管如此，您仍可以通过update方法从一个单独的元素中创建出区别于给定数据结构的新vector结构： 123456scala&gt; val vec = Vector(1, 2, 3)vec: scala.collection.immutable.Vector[Int] = Vector(1, 2, 3)scala&gt; vec updated (2, 4)res0: scala.collection.immutable.Vector[Int] = Vector(1, 2, 4)scala&gt; vecres1: scala.collection.immutable.Vector[Int] = Vector(1, 2, 3) 从上面例子的最后一行我们可以看出，update方法的调用并不会改变vec的原始值。与元素访问类似，vector的update方法的运行时间也是“相对高效的固定时间”。对vector中的某一元素进行update操作可以通过从树的根节点开始拷贝该节点以及每一个指向该节点的节点中的元素来实现。这就意味着一次update操作能够创建1到5个包含至多32个元素或者子树的树节点。当然，这样做会比就地更新一个可变数组败家很多，但比起拷贝整个vector结构还是绿色环保了不少。 由于vector在快速随机选择和快速随机更新的性能方面做到很好的平衡，所以它目前正被用作不可变索引序列的默认实现方式。 12scala&gt; collection.immutable.IndexedSeq(1, 2, 3)res2: scala.collection.immutable.IndexedSeq[Int] = Vector(1, 2, 3) Stack如果您想要实现一个后入先出的序列，那您可以使用Stack。您可以使用push向栈中压入一个元素，用pop从栈中弹出一个元素，用top查看栈顶元素而不用删除它。所有的这些操作都仅仅耗费固定的运行时间。 这里提供几个简单的stack操作的例子： 12345678910scala&gt; val stack = scala.collection.immutable.Stack.emptystack: scala.collection.immutable.Stack[Nothing] = Stack()scala&gt; val hasOne = stack.push(1)hasOne: scala.collection.immutable.Stack[Int] = Stack(1)scala&gt; stackstack: scala.collection.immutable.Stack[Nothing] = Stack()scala&gt; hasOne.topres20: Int = 1scala&gt; hasOne.popres21: scala.collection.immutable.Stack[Int] = Stack() 不可变stack一般很少用在Scala编程中，因为List结构已经能够覆盖到它的功能：push操作同List中的::基本相同，pop则对应着tail。 QueueQueue是一种与stack很相似的数据结构，除了与stack的后入先出不同，Queue结构的是先入先出的。 RangeRange表示的是一个有序的等差整数数列。 创建 Range： 12345678910scala&gt; Range(0, 5)res58: scala.collection.immutable.Range = Range(0, 1, 2, 3, 4)//等同于：scala&gt; 0 until 5res59: scala.collection.immutable.Range = Range(0, 1, 2, 3, 4)//等同于：scala&gt; 0 to 4res60: scala.collection.immutable.Range.Inclusive = Range(0, 1, 2, 3, 4) 两个Range相加： 12scala&gt; ('0' to '9') ++ ('A' to 'Z')res61: scala.collection.immutable.IndexedSeq[Char] = Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z) Range和List、Vector转换： 1234567891011scala&gt; 1 to 5 toListwarning: there were 1 feature warning(s); re-run with -feature for detailsres62: List[Int] = List(1, 2, 3, 4, 5)//相当与：scala&gt; List(1 to 5:_*)res63: List[Int] = List(1, 2, 3, 4, 5)//或者：scala&gt; Vector(1 to 5: _*)res64: scala.collection.immutable.Vector[Int] = Vector(1, 2, 3, 4, 5) Array数组定义： 123456789101112val list1 = new Array[String](0) // Array()val list2 = new Array[String](3) // Array(null, null, null)val list3:Array[String] = new Array(3) // // Array(null, null, null)val list1 = Array("a","b","c","d") // 相当于Array.apply("a","b","c","d")//定义一个类型为Any的Array：val aa = Array[Any](1, 2)val aa: Array[Any] = Array(1, 2)val aa: Array[_] = Array(1, 2) Array (1,3,5,7,9,11)Array[Int](1 to 11 by 2:_*) 与Array对应的可变ArrayBuffer： 12345val ab = collection.mutable.ArrayBuffer[Int]()ab += (1,3,5,7)ab ++= List(9,11) // ArrayBuffer(1, 3, 5, 7, 9, 11)ab toArray // Array (1, 3, 5, 7, 9, 11)ab clear // ArrayBuffer() Tuple定义方式： 123val t1 = ("a","b","c")var t2 = ("a", 123, 3.14, new Date())val (a,b,c) = (2,4,6) 最简单的Tuple： 11-&gt;"hello world" 和下面的写法是等价的： 1(1, "hello world") 参考资料 Scala 课堂 Scala 2.8+ Handbook CSDN CODE翻译的Scala容器库(Scala’s Collections Library)]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala基本语法和概念]]></title>
    <url>%2F2015%2F04%2F20%2Fbasic-of-scala%2F</url>
    <content type="text"><![CDATA[本文主要包括Scala的安装过程并理解Scala的基本语法和概念，包括表达式、变量、基本类型、函数、流程控制等相关内容。 1. 安装从All Versions Scala下载所需版本Scala安装包，解压到指定目录之后，配置环境变量并使其生效。 如果你使用Mac，则可以使用brew安装： 1⇒ brew install scala 在终端键入scala查看Scala的版本，并进入Scala的解释器： 123456⇒ scalaWelcome to Scala version 2.11.6 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_60).Type in expressions to have them evaluated.Type :help for more information.scala&gt; 2. 表达式12scala&gt; 1 + 1res0: Int = 2 res0是解释器自动创建的变量名称，用来指代表达式的计算结果。它是Int类型，值为2。 resX识别符还将用在后续的代码行中。例如,既然res0已在之前设为3，res0 * 3就是 9: 12scala&gt; res0 * 3res1: Int = 9 打印 “Hello, world!’’ : 12scala&gt; println("Hello, world!")Hello, world! println函数在标准输出上打印传给它的字串，就跟Java里的System.out.println一样。 3. 变量Scala 有两种变量：val 和 var。val 类似于 Java 里的 final 变量。一旦初始化了，val 就不能再赋值了。与之对应的，var 如同 Java 里面的非 final 变量。var 可以在它生命周期 中被多次赋值。下面是一个 val 的定义: 12scala&gt; val msg = "Hello, world!"msg: String = Hello, world! 这个语句引入了msg当作字串”Hello, world!”的名字。类型是 java.lang.String，因为 Scala 的字串是由 Java 的 String 类实现的。 这里定义的msg变量并没有指定类型，Scala解释器会自动推断出其类型，当然你也可以显示的标注类型： 12scala&gt; val msg2: java.lang.String = "Hello again, world!"msg2: String = Hello again, world! 因为在Scala程序里java.lang类型的简化名也是可见的，所以可以简化为: 12scala&gt; val msg3: String = "Hello yet again, world!"msg3: String = Hello yet again, world! 如果你想定义一个变量并修改它的值，你可以选择使用var。 12scala&gt; var greeting = "Hello, world!"greeting: String = Hello, world! 由于 greeting 是 var 而不是 val,你可以在之后对它重新赋值。 12scala&gt; greeting = "Leave me alone, world!"greeting: String = Leave me alone, world! 要输入一些能跨越多行的东西,只要一行行输进去就行。如果输到行尾还没结束,解释器 将在下一行回应一个竖线。 123scala&gt; val multiLine = | "This is the next line."multiLine: java.lang.String = This is the next line. 如果你意识到你输入了一些错误的东西,而解释器仍在等着你更多的输入,你可以通过按 两次回车取消掉: 12345scala&gt; val oops = | |You typed two blank lines. Starting a new command.scala&gt; var 变量可重新赋值，如果赋值为_，则表示使用缺省值(0、false、null)，例如： 1234var d:Double = _ // d = 0.0var i:Int = _ // i = 0var s:String = _ // s = nullvar t:T = _ // 泛型T对应的默认值 Scala还能像Python一样方便的赋值： 1234567891011121314151617181920212223242526// 给多个变量赋同一初始值scala&gt; val x,y=0x: Int = 0y: Int = 0// 同时定义多个变量，注意：val x,y=10,"hello" 是错误的scala&gt; val (x,y) = (10, "hello")x: Int = 10y: String = hello// x = 1, y = List(2,3,4)scala&gt; val x::y = List(1,2,3,4)x: Int = 1y: List[Int] = List(2, 3, 4)// a = 1, b = 2, c = 3scala&gt; val List(a,b,c) = List(1,2,3)a: Int = 1b: Int = 2c: Int = 3// 也可以用List，Seqscala&gt; val Array(a, b, _, _, c @ _*) = Array(1, 2, 3, 4, 5, 6, 7)a: Int = 1b: Int = 2c: Seq[Int] = Vector(5, 6, 7) // Array(5, 6, 7), _*匹配0个到多个 使用正则表达式赋值： 1234567scala&gt; val regex = "(\\d+)/(\\d+)/(\\d+)".rregex: scala.util.matching.Regex = (\d+)/(\d+)/(\d+)scala&gt; val regex(year, month, day) = "2015/04/20"year: String = 2015month: String = 04day: String = 20 除了val，你还可以使用lazy： val：定义时就一次求值完成，保持不变 lazy：定义时不求值，第一次使用时完成求值，保持不变 总结： Scala是严格意义上的静态类型语言，由于其采用了先进的类型推断技术，程序员不需要在写程序时显式指定类型，编译器会根据上下文推断出类型信息。 Scala程序语句结尾没有分号，这也是 Scala中约定俗成的编程习惯。大多数情况下分号都是可省的，如果你需要将两条语句写在同一行，则需要用分号分开它们。 val用于定义不能修改的变量，var定义的变量可以修改和赋值，赋值为_表示使用默认值。 4. 基本类型和操作一些基本类型Scala 中的一些基本类型和其实例值域范围如下： 值 类型 Byte 8 位有符号补码整数(-27~27-1) Short 16 位有符号补码整数(-215~215-1) Int 32 位有符号补码整数(-231~231-1) Long 64 位有符号补码整数(-263~263-1) Char 16 位无符号Unicode字符(0~216-1) String 字符序列 Float 32 位 IEEE754 单精度浮点数 Double 64 位 IEEE754 单精度浮点数 Boolean true 或 false 除了String归于java.lang包之外，其余所有的基本类型都是包scala的成员。如Int的全名是scala.Int。然而，由于包scala和java.lang的所有成员都被每个Scala源文件自动引用，你可以在任何地方只用简化名。 Scala的基本类型与Java的对应类型范围完全一样，这让Scala编译器能直接把Scala的值类型在它产生的 字节码里转译成 Java 原始类型。 Scala用Any统一了原生类型和引用类型。 123456789101112131415Any AnyRef java String 其他Java类型 ScalaObject AnyVal scala Double scala Float scala Long scala Int scala Short scala Unit scala Boolean scala Char scala Byte 对于基本类型，可以用asInstanseOf[T]方法来强制转换类型： 12345scala&gt; def i = 10.asInstanceOf[Double]i: Doublescala&gt; List('A','B','C').map(c=&gt;(c+32).asInstanceOf[Char])res1: List[Char] = List(a, b, c) 用isInstanceOf[T]方法来判断类型： 12scala&gt; val b = 10.isInstanceOf[Int]b: Boolean = true 而在match ... case中可以直接判断而不用此方法。 操作符和方法Scala为它的基本类型提供了丰富的操作符集。例如，1 + 2与(1).+(2)其实是一回事。换句话说，就是 Int 类包含了叫做+的方法，它带一个 Int 参数并返回一个 Int 结果。这个+方法在两 个 Int 相加时被调用： 12scala&gt; val sum = 1 + 2 // Scala调用了(1).+(2) sum: Int = 3 想要证实这点，可以把表达式显式地写成方法调用： 12scala&gt; val sumMore = (1).+(2)sumMore: Int = 3 而真正的事实是，Int包含了许多带不同的参数类型的重载的+方法。 符号+是操作符——更明确地说，是中缀操作符。操作符标注不仅限于像+这种其他语言里 看上去像操作符一样的东西。你可以把任何方法都当作操作符来标注。例如，类 String 有一个方法 indexOf 带一个 Char 参数。indexOf 方法搜索 String 里第一次出现的指定字符，并返回它的索引或 -1 如果没有找到。你可以把 indexOf 当作中缀操作符使用，就像这样： 1234scala&gt; val s = "Hello, world!"s: String = Hello, world!scala&gt; s indexOf 'o' // Scala调用了s.indexOf(’o’) res30: Int = 4 String 提供一个重载的 indexOf 方法，带两个参数，分别是要搜索的字符和从哪个索引开始搜索。尽管 这个 indexOf 方法带两个参数,你仍然可以用操作符标注的方式使用它。 12scala&gt; s indexOf ('o', 5) // Scala调用了s.indexOf(’o’, 5) res31: Int = 8 任何方法都可以是操作符。 Scala 还有另外两种操作符标注：前缀和后缀。前缀标注中，方法名被放在调用的对象之前，如，-7 里的-。后缀标注在方法放在对象之后，如7 toLong里的toLong。 与中缀操作符——-操作符带后两个操作数，一个在左一个在右，相反，前缀和后缀操作符都是一元 unary 的：它们仅带一个操作数。前缀方式中，操作数在操作符的右边。前缀操作符的例子有 -2.0、!found和~0xFF。与中缀操作符一致，这些前缀操作符是在值类型对象上调用方法的简写方式。然而这种情况下，方法名在操作符字符上前缀了unary_。 例如，Scala 会把表达式 -2.0 转换成方法调用(2.0).unary_-。你可以输入通过操作符和显式方法名两种方式对方法的调用来演示这一点： 123scala&gt; -2.0 // Scala调用了(2.0).unary_- res2: Double = -2.0scala&gt; (2.0).unary_-res32: Double = -2.0 可以当作前缀操作符用的标识符只有+,-,!和~ 后缀操作符是不用点或括号调用的不带任何参数的方法。 1234scala&gt; val s = "Hello, world!"s: String = Hello, world!scala&gt; s.toLowerCaseres33: String = hello, world! 后面的这个例子里，方法没带参数，或者还可以去掉点，采用后缀操作符标注方式： 12scala&gt; s toLowerCaseres34: String = hello, world! 数学运算Int 无++、--操作，但可以+=、-=, 如下： 12345var i = 0i++ // 报错，无此操作i+=1 // 1i-- // 报错，无此操作i-=1 // 0 对象相等性如果你想比较一下看看两个对象是否相等，可以使用==，或它的反义!=。 123456789101112131415161718192021222324252627282930313233//比较基本类型scala&gt; 1 == 2res36: Boolean = falsescala&gt; 1 != 2res37: Boolean = truescala&gt; 2 == 2res38: Boolean = true//比较对象scala&gt; List(1, 2, 3) == List(1, 2, 3)res39: Boolean = truescala&gt; List(1, 2, 3) == List(4, 5, 6)res40: Boolean = false//比较不同类型scala&gt; 1 == 1.0res41: Boolean = truescala&gt; List(1, 2, 3) == "hello"res42: Boolean = false//和null进行比较，不会有任何异常抛出scala&gt; List(1, 2, 3) == nullres43: Boolean = falsescala&gt; null == List(1, 2, 3)res44: Boolean = falsescala&gt; null == List(1, 2, 3)res45: Boolean = false Scala的==很智能，他知道对于数值类型要调用Java中的==，引用类型要调用Java的equals()： 12scala&gt; "hello"=="Hello".toLowerCase()res46: Boolean = truescala 在java中为false，在scala中为true。 Scala的==总是内容对比，eq才是引用对比，例如： 123456val s1,s2 = "hello"val s3 = new String("hello")s1==s2 // trues1 eq s2 // trues1==s3 // true 值相同s1 eq s3 // false 不是同一个引用 富包装器Scala的每一个基本类型都有一个富包装类： Byte：scala.runtime.RichByte Short：scala.runtime.RichShort Int：scala.runtime.RichInt Long：scala.runtime.RichLong Char：scala.runtime.RichChar String：scala.runtime.RichString Float：scala.runtime.RichFloat Double：scala.runtime.RichDouble Boolean：scala.runtime.RichBoolean 一些富操作的例子如下： 1234567890 max 50 min 5-2.7 abs-2.7 round1.5 isInfinity(1.0 / 0) isInfinity4 to 6"bob" capitalize"robert" drop 2 5. 函数函数的地位和一般的变量是同等的，可以作为函数的参数，可以作为返回值。传入函数的任何输入是只读的，比如一个字符串，不会被改变，只会返回一个新的字符串。 Java里面的一个问题就是很多只用到一次的private方法，没有和使用它的方法紧密结合；Scala可以在函数里面定义函数，很好地解决了这个问题。 函数定义函数和方法一般用def定义。 1234scala&gt; def max(x: Int, y: Int): Int = &#123; | if (x &gt; y) x | else y &#125;max: (x: Int, y: Int)Int 函数的基本结构如下： 有时候Scala编译器会需要你定义函数的结果类型。比方说，如果函数是递归的，你就必须显式地定义函数结果类型。然而在max的例子里，你可以不用写结果类型，编译器也能够推断它。同样，如果函数仅由一个句子组成，你可以可选地不写大括号。这样，你就可以把max函数写成这样: 1scala&gt; def max2(x: Int, y: Int) = if (x &gt; y) x else y 一旦你定义了函数，你就可以用它的名字调用它，如： 12scala&gt; max(3, 5)res25: Int = 5 函数不带参数，调用时括号可以省略： 12345678scala&gt; def three() = 1 + 2three: ()Intscala&gt; three()res26: Int = 3scala&gt; threeres27: Int = 3 还有既不带参数也不返回有用结果的函数定义： 12scala&gt; def greet() = println("Hello, world!")greet: ()Unit 当你定义了greet()函数，解释器会回应一个greet: ()Unit。空白的括号说明函数不带参数。Unit 是 greet 的结果类型，Unit 的结果类型指的是函数没有返回有用的值。Scala 的 Unit 类型比较接近 Java 的 void 类型，而且实际上 Java 里 每一个返回 void 的方法都被映射为 Scala 里返回 Unit 的方法。 总结： 函数体没有像Java那样放在{}里，Scala 中的一条语句其实是一个表达式 如果函数体只包含一条表达式，则可以省略{} 函数体没有显示的return语句，最后一条表达式的值会自动返回给函数的调用者 没有参数的函数调用时，括号可以省略 映射式定义一种特殊的定义：映射式定义（直接相当于数学中的映射关系）；其实也可以看成是没有参数的函数，返回一个匿名函数；调用的时候是调用这个返回的匿名函数。 12345678910111213141516171819def f:Int=&gt;Double = &#123; case 1 =&gt; 0.1 case 2 =&gt; 0.2 case _ =&gt; 0.0&#125;f(1) // 0.1f(3) // 0.0def m:Option[User]=&gt;User = &#123; case Some(x) =&gt; x case None =&gt; null&#125;m(o).getOrElse("none...")def m:(Int,Int)=&gt;Int = _+_m(2,3) // 5def m:Int=&gt;Int = 30+ // 相当于30+_,如果唯一的"_"在最后,可以省略m(5) // 35 特殊函数名 + - * /方法名可以是+、-、*、/： 123def *(x:Int, y:Int) = &#123; x*y &#125;*(10,20) // = 2001+2 //相当于1.+(2) 定义一元操作符（置前）可用unary_：一元的，单一元素的，单一构成的。 1234-2 //相当于：(2).unary_- +2 //相当于：(2).unary_+ !true //相当于：(true).unary_! ~0 //相当于 (0).unary_~ 函数的调用正常调用，不传参数时候可以省略括号： 123def f(s: String = "default") = &#123; s &#125;f // "hello world"f() // "hello world" 对象的无参数方法的调用，可以省略.和()： 1"hello world" toUpperCase // "HELLO WORLD" 对象的1个参数方法的调用，可以省略.和()： 1234"hello world" indexOf w // 6"hello world" substring 5 // "world"Console print 10 // 但不能写 print 10，只能print(10)，省略Console.1 + 2 // 相当于 (1).+(2) 对象的多个参数方法的调用,也可省略.但不能省略()： 1"hello world" substring (0, 5) // "hello" 注意： 不在class或者object中的函数不能如此调用： 12def m(i:Int) = i*im 10 // 错误 但在class或者object中可以使用this调用： 1234567object method &#123; def m(i:Int) = i*i def main(args: Array[String]) = &#123; val ii = this m 15 // 等同于 m(15), this 不能省略 println(ii) &#125;&#125; 匿名函数形式：((命名参数列表)=&gt;函数实现)(参数列表) 特殊地： 无参数： (()=&gt;函数实现)() 有一个参数且在最后： (函数实现)(参数) 无返回值： ((命名参数列表)=&gt;Unit)(参数列表) 使用=&gt;创建匿名函数： 123456789101112scala&gt; (x: Int)=&gt; x+1 res23: Int =&gt; Int = &lt;function1&gt;scala&gt; x:Int =&gt; x+1 // 没有大括号时，()是必须的&lt;console&gt;:1: error: ';' expected but '=&gt;' found. x:Int =&gt; x+1scala&gt; &#123;(x: Int)=&gt; x+1 &#125; res24: Int =&gt; Int = &lt;function1&gt;scala&gt; &#123;x:Int =&gt; x+1&#125; // 有大括号时，()可以去掉res25: Int =&gt; Int = &lt;function1&gt; 函数值是对象，所以如果你愿意可以把它们存入变量。它们也是函数，所以你可以使用通常的括号函数调用写法调用它们： 12345678scala&gt; val m1 = (x:Int)=&gt; x+1m1: Int =&gt; Int = &lt;function1&gt;scala&gt; val m2 = &#123;x:Int=&gt; x+1&#125; // 不用(), 用&#123;&#125;m2: Int =&gt; Int = &lt;function1&gt;scala&gt; m1(10)res26: Int = 11 有参数的匿名函数的调用： 12345scala&gt; ((i:Int)=&gt; i*i)(3)res25: Int = 9scala&gt; ((i:Int, j:Int) =&gt; i+j)(3, 4)res26: Int = 7 有一个参数且在最后的匿名函数的调用： 12345678scala&gt; (10*)(2) // 20, 相当于 ((x:Int)=&gt;10*x)(2)res27: Int = 20scala&gt; (10+)(2) // 12, 相当于 ((x:Int)=&gt;10+x)(2)res28: Int = 12scala&gt; (List("a","b","c") mkString)("=") // a=b=cres29: String = a=b=c 无参数的匿名函数的调用： 12scala&gt; (()=&gt; 10)() // 10res30: Int = 10 无参数无返回值： 1234567891011scala&gt; (() =&gt; Unit)res31: () =&gt; Unit.type = &lt;function0&gt;scala&gt; ( ()=&gt; &#123;println("hello"); 20*10&#125; )()hellores32: Int = 200//相当于调用一段方法scala&gt; &#123; println("hello"); 20*10 &#125;hellores33: Int = 200 匿名函数的两个例子： 例子1：直接使用匿名函数。 123456scala&gt; List(1,2,3,4).map( (i:Int)=&gt; i*i)res34: List[Int] = List(1, 4, 9, 16)//这里对变量 i 使用了类型推断scala&gt; List(1,2,3,4).map( i=&gt; i*i)res35: List[Int] = List(1, 4, 9, 16) 例子2：无参数的匿名函数 123456789//定义一个普通的函数，参数为函数，返回值为空scala&gt; def times3(m:()=&gt; Unit) = &#123; m();m();m() &#125;times3: (m: () =&gt; Unit)Unit//传入一个无参数的匿名函数scala&gt; times3 ( ()=&gt; println("hello world") )hello worldhello worldhello world 由于是无参数的匿名函数，可进一步简化： 1234567scala&gt; def times3(m: =&gt;Unit) = &#123; m;m;m &#125; // 参见“lazy参数”times3: (m: =&gt; Unit)Unitscala&gt; times3 ( println("hello world") )hello worldhello worldhello world 偏应用函数（Partial application）用下划线代替一个或多个参数的函数叫偏应用函数（partially applied function），例如： 12scala&gt; def sum(a: Int, b: Int, c: Int) = a + b + csum: (Int,Int,Int)Int 你就可以把函数 sum 应用到参数 1、2 和 3 上，如下: 12scala&gt; sum(1, 2, 3)res1: Int = 6 偏应用函数是一种表达式，你不需要提供函数需要的所有参数。代之以仅提供部分，或不提供所需参数。比如，要创建不提供任何三个所需参数的调用 sum 的偏应用表达式，只要在“sum”之后放一个下划线即可，然后可以把得到的函数存入变量。举例如下: 12scala&gt; val a = sum _a: (Int, Int, Int) =&gt; Int = &lt;function&gt; 有了这个代码，Scala 编译器以偏应用函数表达式sum _，实例化一个带三个缺失整数参数的函数值，并把这个新的函数值的索引赋给变量 a。当你把这个新函数值应用于三个参数之上时，它就转回头调用 sum，并传入这三个参数: 12scala&gt; a(1, 2, 3)res2: Int = 6 实际发生的事情是这样的：名为a的变量指向一个函数值对象。这个函数值是由Scala编译器依照偏应用函数表达式sum _，自动产生的类的一个实例。编译器产生的类有一个apply方法带三个参数。之所以带三个参数是因为sum _表达式缺少的参数数量为三。Scala编译器把表达式a(1,2,3)翻译成对函数值的apply方法的调用，传入三个参数 1、2、3。因此 a(1,2,3)是下列代码的短格式： 12scala&gt; a.apply(1, 2, 3)res3: Int = 6 也可针对部分参数使用： 1234scala&gt; val b = sum(1, _: Int, 3)b: (Int) =&gt; Int = &lt;function&gt;scala&gt; b(2)res4: Int = 6 _ 在Scala中的使用场景，见 Scala中下划线的用途。 如果_在最后，则可以省略： 121 to 5 foreach println1 to 5 map (10*) 柯里化函数有时会有这样的需求：允许别人一会在你的函数上应用一些参数，然后又应用另外的一些参数。 例如一个乘法函数，在一个场景需要选择乘数，而另一个场景需要选择被乘数。 12scala&gt; def multiply(m: Int)(n: Int): Int = m * nmultiply: (m: Int)(n: Int)Int 你可以直接传入两个参数。 12scala&gt; multiply(2)(3)res0: Int = 6 你可以填上第一个参数并且部分应用第二个参数。 12345scala&gt; val timesTwo = multiply(2) _timesTwo: (Int) =&gt; Int = &lt;function1&gt;scala&gt; timesTwo(3)res1: Int = 6 你可以对任何多参数函数执行柯里化。 12scala&gt; (multiply _).curriedres1: (Int) =&gt; (Int) =&gt; Int = &lt;function1&gt; 可变长度参数这是一个特殊的语法，可以向方法传入任意多个同类型的参数。例如要在多个字符串上执行String的capitalize函数，可以这样写： 12345678def capitalizeAll(args: String*) = &#123; args.map &#123; arg =&gt; arg.capitalize &#125;&#125;scala&gt; capitalizeAll("rarity", "applejack")res1: Seq[String] = ArrayBuffer(Rarity, Applejack) 函数内部，重复参数的类型是声明参数类型的数组。因此，capitalizeAll函数里被声明为类型“String*” 的args的类型实际上是 Array[String]。然而，如果你有一个合适类型的数组，并尝试把它当作重复参数传入，你会得到一个编译器错误: 1234567scala&gt; val arr = Array("what's", "up", "doc?")scala&gt; capitalizeAll(arr)&lt;console&gt;:31: error: type mismatch; found : Array[String] required: String capitalizeAll(arr) ^ 要实现这个做法，你需要在数组参数后添加一个冒号和一个_*符号，像这样: 12scala&gt; capitalizeAll(arr: _*)res2: Seq[String] = ArrayBuffer(What's, Up, Doc?) 这个标注告诉编译器把arr的每个元素当作参数，而不是当作单一的参数传给capitalizeAll。 lazy参数就是调用时用到函数的该参数，每次都重新计算。 lazy参数是变量或值： 123456789101112131415//一般参数def f1(x: Long) = &#123; val (a,b) = (x,x) println("a="+a+",b="+b)&#125;f1(System.nanoTime)//a=1429501936753731000,b=1429501936753731000//lazy参数def f2(x: =&gt;Long) = &#123; val (a,b) = (x,x) println("a="+a+",b="+b)&#125;f2(System.nanoTime)//a=1429502007512014000,b=1429502007512015000 lazy参数是函数时： 123456789101112131415//一般参数，打印一次scala&gt; def times1(m: Unit) = &#123; m;m;m &#125;times1: (m: Unit)Unitscala&gt; times1 ( println("hello world") )hello world//lazy参数，打印三次scala&gt; def times2(m: =&gt;Unit) = &#123; m;m;m &#125;times2: (m: =&gt; Unit)Unitscala&gt; times2 ( println("hello world") )hello worldhello worldhello world 6. 流程控制if..else使用 if else 表达式: 1if (x&gt;y) 100 else -1 while使用 while 为列表求和: 123456789def sum(xs: List[Int]) = &#123; var total = 0 var index = 0 while (index &lt; xs.size) &#123; total += xs(index) index += 1 &#125; total &#125; Scala 也有 do-while 循环。除了把状态测试从前面移到后面之外,与 while 循环没有区别。 12345var line = ""do &#123; line = readLine() println("Read: " + line)&#125; while (line != null) 循环操作for： 12345678910111213141516171819202122232425262728//循环中的变量不用定义，如：scala&gt; for(i&lt;-1 to 3; j=i*i) println(j) // 包含3149scala&gt; for (i &lt;- 1 until 3) println(i) // 不包含312//如果for条件是多行，不能用()，要用&#123;&#125;scala&gt; for&#123;i&lt;-0 to 5 | j&lt;-0 to 2&#125; yield i+jres3: scala.collection.immutable.IndexedSeq[Int] = Vector(0, 1, 2, 1, 2, 3, 2, 3, 4, 3, 4, 5, 4, 5, 6, 5, 6, 7)//带有过滤条件，可以有多个过滤条件，逗号分隔scala&gt; for (i &lt;- 1 until 10 if i%2 == 0) println(i)2468//for中的嵌套循环，包括多个 &lt;-scala&gt; for &#123;i &lt;- 1 until 3; j &lt;- 1 until 3&#125; println(i*j)1224 for yield：把每次循环的结果移进一个集合（类型和循环内的一致），格式：for {子句} yield {循环体} 123for (e&lt;-List(1,2,3)) yield (e*e) // List(1,4,9)for &#123;e&lt;-List(1,2,3)&#125; yield &#123; e*e &#125; // List(1,4,9)for &#123;e&lt;-List(1,2,3)&#125; yield e*e // List(1,4,9) foreach: 123456789101112131415161718192021222324252627282930313233343536373839scala&gt; List(1,2,3).foreach(println)123scala&gt; (1 to 3).foreach(println)123scala&gt; (1 until 4) foreach println123scala&gt; Range(1,4) foreach println123//可以写步长scala&gt; 1 to (11,2)res9: scala.collection.immutable.Range.Inclusive = Range(1, 3, 5, 7, 9, 11)scala&gt; 1 to 11 by 2res10: scala.collection.immutable.Range = Range(1, 3, 5, 7, 9, 11)scala&gt; 1 until (11,2)res11: scala.collection.immutable.Range = Range(1, 3, 5, 7, 9)scala&gt; 1 until 11 by 2res12: scala.collection.immutable.Range = Range(1, 3, 5, 7, 9)scala&gt; val r = (1 to 10 by 4)r: scala.collection.immutable.Range = Range(1, 5, 9)//也可以是BigIntscala&gt; (1:BigInt) to 3res13: scala.collection.immutable.NumericRange.Inclusive[BigInt] = NumericRange(1, 2, 3) forall：判断是否所有都符合。 123456789101112131415161718192021scala&gt; (1 to 3) forall (0&lt;)res0: Boolean = truescala&gt; (-1 to 3) forall (0&lt;)res1: Boolean = falsescala&gt; def isPrime(n:Int) = 2 until n forall (n%_!=0)isPrime: (n: Int)Booleanscala&gt; for (i&lt;-1 to 10 if isPrime(i)) println(i)12357scala&gt; (2 to 20) partition (isPrime _)res4: (scala.collection.immutable.IndexedSeq[Int], scala.collection.immutable.IndexedSeq[Int]) = (Vector(2, 3, 5, 7, 11, 13, 17, 19),Vector(4, 6, 8, 9, 10, 12, 14, 15, 16, 18, 20))//也可直接调用BigInt的内部方法scala&gt; (2 to 20) partition (BigInt(_) isProbablePrime(10))res5: (scala.collection.immutable.IndexedSeq[Int], scala.collection.immutable.IndexedSeq[Int]) = (Vector(2, 3, 5, 7, 11, 13, 17, 19),Vector(4, 6, 8, 9, 10, 12, 14, 15, 16, 18, 20)) reduceLeft方法首先应用于前两个元素，然后再应用于第一次应用的结果和接下去的一个元素，等等，直至整个列表。 1234567891011121314151617//计算阶乘scala&gt; def fac(n: Int) = 1 to n reduceLeft(_*_)fac: (n: Int)Intscala&gt; fac(5) // 5*4*3*2 = 120res6: Int = 120//求和scala&gt; List(2,4,6).reduceLeft(_+_)res7: Int = 12//取max：scala&gt; List(1,4,9,6,7).reduceLeft( (x,y)=&gt; if (x&gt;y) x else y )res8: Int = 9//或者简化为： scala&gt; List(1,4,9,6,7).reduceLeft(_ max _)res9: Int = 9 foldLeft： 12345678910111213141516171819202122//累加scala&gt; def sum(L: Seq[Int]) = L.foldLeft(0)((a, b) =&gt; a + b)sum: (L: Seq[Int])Intscala&gt; def sum(L: Seq[Int]) = L.foldLeft(0)(_ + _)sum: (L: Seq[Int])Intscala&gt; def sum(L: List[Int]) = (0/:L)&#123;_ + _&#125;sum: (L: List[Int])Intscala&gt; sum(List(1,3,5,7))res10: Int = 16//乘法：scala&gt; def multiply(L: Seq[Int]) = L.foldLeft(1)(_ * _)multiply: (L: Seq[Int])Intscala&gt; multiply(Seq(1,2,3,4,5))res11: Int = 120scala&gt; multiply(1 until 5+1)res12: Int = 120 scanLeft: 12345678910scala&gt; List(1,2,3,4,5).scanLeft(0)(_+_)res13: List[Int] = List(0, 1, 3, 6, 10, 15)//相当于 (0,(0+1),(0+1+2),(0+1+2+3),(0+1+2+3+4),(0+1+2+3+4+5))scala&gt; List(1,2,3,4,5).scanLeft(1)(_*_)res14: List[Int] = List(1, 1, 2, 6, 24, 120)//相当于 (1, 1*1, 1*1*2, 1*1*2*3, 1*1*2*3*4, 1*1*2*3*4*5)scala&gt; List(1,2,3,4,5).scanLeft(2)(_*_)res16: List[Int] = List(2, 2, 4, 12, 48, 240) take、drop、splitAt: 1231 to 10 by 2 take 3 // Range(1, 3, 5)1 to 10 by 2 drop 3 // Range(7, 9)1 to 10 by 2 splitAt 2 // (Range(1, 3),Range(5, 7, 9)) takeWhile、dropWhile、span： 123456789101112131 to 10 takeWhile (_&lt;5) // (1,2,3,4)1 to 10 takeWhile (_&gt;5) // ()10 to (1,-1) takeWhile(_&gt;6) // (10,9,8,7)1 to 10 takeWhile ( n=&gt; n*n&lt;25) // (1, 2, 3, 4)1 to 10 dropWhile (_&lt;5) // (5,6,7,8,9,10)1 to 10 dropWhile (n=&gt;n*n&lt;25) // (5,6,7,8,9,10) 1 to 10 span (_&lt;5) // ((1,2,3,4),(5,6,7,8)List(1,0,1,0) span (_&gt;0) // ((1), (0,1,0))// 注意，partition是和span完全不同的操作List(1,0,1,0) partition (_&gt;0) // ((1,1),(0,0)) match 表达式Scala 的匹配表达式允许你在许多可选项中做选择，就好象其它语言中的 switch 语句。通常说来 match 表达式可以让你使用任意的模式。 匹配值： 12345678val times = 1times match &#123; case -1|0 =&gt; "zore" case 1 =&gt; "one" case 2 =&gt; "two" case _ =&gt; "some other number"&#125; 使用守卫进行匹配: 12345times match &#123; case i if i == 1 =&gt; "one" case i if i == 2 =&gt; "two" case _ =&gt; "some other number"&#125; 匹配类型: 123456789def bigger(o: Any): Any = &#123; o match &#123; case i: Int if i &lt; 0 =&gt; i - 1 case i: Int =&gt; i + 1 case d: Double if d &lt; 0.0 =&gt; d - 0.1 case d: Double =&gt; d + 0.1 case text: String =&gt; text + &quot;s&quot; &#125;&#125; 匹配 Option 类型： 12345scala&gt; map.get(1) match &#123; | case Some(i) =&gt; println("Got something") | case None =&gt; println("Got nothing") | &#125;Got something 匹配类成员: 123456def calcType(calc: Calculator) = calc match &#123; case _ if calc.brand == "hp" &amp;&amp; calc.model == "20B" =&gt; "financial" case _ if calc.brand == "hp" &amp;&amp; calc.model == "48G" =&gt; "scientific" case _ if calc.brand == "hp" &amp;&amp; calc.model == "30B" =&gt; "business" case _ =&gt; "unknown"&#125; 与 Java 的 switch 语句比，匹配表达式还有一些重要的差别： case 后面可以是任意类型。 每个可选项的最后并没有 break，break 是隐含的。 match 表达式也能产生值。 在最后一行指令中的_是一个通配符，它保证了我们可以处理所有的情况。 参考 Effective Scala 对[什么时候使用模式匹配](http://twitter.github.com/effectivescala/#Functional programming-Pattern matching)和 [模式匹配格式化](http://twitter.github.com/effectivescala/#Formatting-Pattern matching)的建议。A Tour of Scala 也描述了模式匹配。 case if 表达式写法1： 123456(1 to 20) foreach &#123; case x if (x % 15 == 0) =&gt; printf("%2d:15n\n",x) case x if (x % 3 == 0) =&gt; printf("%2d:3n\n",x) case x if (x % 5 == 0) =&gt; printf("%2d:5n\n",x) case x =&gt; printf("%2d\n",x) &#125; 写法2： 123456(1 to 20) map (x=&gt; (x%3,x%5) match &#123; case (0,0) =&gt; printf("%2d:15n\n",x) case (0,_) =&gt; printf("%2d:3n\n",x) case (_,0) =&gt; printf("%2d:5n\n",x) case (_,_) =&gt; printf("%2d\n",x)&#125;) break、continueScala中没有break和continue语法，需要break得加辅助boolean变量，或者用库（continue没有）。 例子1：打印’a’到’z’的前10个 123var i=0; val rt = for(e&lt;-('a' to 'z') if &#123;i=i+1;i&lt;=10&#125;) printf("%d:%s\n",i,e)('a' to 'z').slice(0,10).foreach(println) 例子2：1 到 100 和小于1000的数 12var (n,sum)=(0,0); for(i&lt;-0 to 100 if (sum+i&lt;1000)) &#123; n=i; sum+=i &#125;// n = 44, sum = 990 例子3：使用库来实现break 12import scala.util.control.Breaks._for(e&lt;-1 to 10) &#123; val e2 = e*e; if (e2&gt;10) break; println(e) &#125; try catch finally123456789var f = openFile()try &#123; f = new FileReader("input.txt")&#125; catch &#123; case ex: FileNotFoundException =&gt; // Handle missing file case ex: IOException =&gt; // Handle other I/O error&#125; finally &#123; f.close()&#125; 捕获异常使用的是模式匹配。 7. 其他Null, None, Nil, Nothing Null： Trait，其唯一实例为null，是AnyRef的子类，不是 AnyVal的子类 Nothing： Trait，所有类型（包括AnyRef和AnyVal）的子类，没有实例 None： Option的两个子类之一，另一个是Some，用于安全的函数返回值 Unit： 无返回值的函数的类型，和java的void对应 Nil： 长度为0的List 区分&lt;-,=&gt;,-&gt;&lt;-用于for循环，符号∈的象形: 1for (i &lt;- 0 until 100) =&gt;用于匿名函数，也可用在import中定义别名：import javax.swing.{JFrame=&gt;jf} 12List(1,2,3).map(x=&gt; x*x)((i:Int)=&gt;i*i)(5) // 25 -&gt;用于Map初始化 1Map(1-&gt;"a",2-&gt;"b") // (1:"a",2:"b") 注意：在scala中任何对象都能调用-&gt;方法（隐式转换），返回包含键值对的二元组! 8. 参考资料 http://www.scala-lang.org/ Scala 课堂 Scala 2.8+ Handbook 面向 Java 开发人员的 Scala 指南系列]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark MLlib中的协同过滤]]></title>
    <url>%2F2015%2F04%2F17%2Fspark-mllib-collaborative-filtering%2F</url>
    <content type="text"><![CDATA[本文主要通过Spark官方的例子理解ALS协同过滤算法的原理和编码过程，然后通过对电影进行推荐来熟悉一个完整的推荐过程。 协同过滤协同过滤常被应用于推荐系统，旨在补充用户-商品关联矩阵中所缺失的部分。MLlib当前支持基于模型的协同过滤，其中用户和商品通过一小组隐语义因子进行表达，并且这些因子也用于预测缺失的元素。Spark MLlib实现了交替最小二乘法(ALS) 来学习这些隐性语义因子。 在 MLlib 中的实现类为org.apache.spark.mllib.recommendation.ALS.scala，其有如下的参数: numUserBlocks：是用于并行化计算的分块个数 (设置为-1，为自动配置)。 numProductBlocks：是用于并行化计算的分块个数 (设置为-1，为自动配置)。 rank：是模型中隐语义因子的个数。 iterations：是迭代的次数，推荐值：10-20。 lambda：惩罚函数的因数，是ALS的正则化参数，推荐值：0.01。 implicitPrefs：决定了是用显性反馈ALS的版本还是用适用隐性反馈数据集的版本。 alpha：是一个针对于隐性反馈 ALS 版本的参数，这个参数决定了偏好行为强度的基准。 seed：随机种子 可以调整这些参数，不断优化结果，使均方差变小。比如：iterations越多，lambda较小，均方差会较小，推荐结果较优。 提供以下方法： 1234567891011def run(ratings: RDD[Rating]): MatrixFactorizationModeldef train(ratings: RDD[Rating],rank: Int,iterations: Int,lambda: Double,blocks: Int,seed: Long): MatrixFactorizationModeldef train(ratings: RDD[Rating],rank: Int,iterations: Int,lambda: Double,blocks: Int): MatrixFactorizationModeldef train(ratings: RDD[Rating], rank: Int, iterations: Int, lambda: Double): MatrixFactorizationModeldef train(ratings: RDD[Rating], rank: Int, iterations: Int): MatrixFactorizationModeldef trainImplicit(ratings: RDD[Rating],rank: Int,iterations: Int,lambda: Double,blocks: Int,alpha: Double,seed: Long): MatrixFactorizationModeldef trainImplicit(ratings: RDD[Rating],rank: Int,iterations: Int,lambda: Double,blocks: Int,alpha: Double): MatrixFactorizationModeldef trainImplicit(ratings: RDD[Rating], rank: Int, iterations: Int, lambda: Double, alpha: Double): MatrixFactorizationModeldef trainImplicit(ratings: RDD[Rating], rank: Int, iterations: Int): MatrixFactorizationModel 以上所有方法需要一个参数Rating，其为一个包括三个元素的 case class： 1case class Rating(user: Int, product: Int, rating: Double) 另外，以上方法均返回MatrixFactorizationModel类型的对象，提供以下方法： 123456789101112131415/** Predict the rating of one user for one product. */def predict(user: Int, product: Int): Double/**Predict the rating of many users for many products.*/def predict(usersProducts: RDD[(Int, Int)]): RDD[Rating]// Recommends products to a user.def recommendProducts(user: Int, num: Int): Array[Rating]//Recommends users to a product.def recommendUsers(product: Int, num: Int): Array[Rating]def save(sc: SparkContext, path: String): Unitdef load(sc: SparkContext, path: String): MatrixFactorizationModel = 隐性反馈 vs 显性反馈基于矩阵分解的协同过滤的标准方法一般将用户商品矩阵中的元素作为用户对商品的显性偏好。在许多的现实生活中的很多场景中，我们常常只能接触到隐性的反馈（例如游览，点击，购买，喜欢，分享等等）在 MLlib 中所用到的处理这种数据的方法来源于文献： Collaborative Filtering for Implicit Feedback Datasets。 本质上，这个方法将数据作为二元偏好值和偏好强度的一个结合，而不是对评分矩阵直接进行建模。因此，评价就不是与用户对商品的显性评分而是和所观察到的用户偏好强度关联了起来。然后，这个模型将尝试找到隐语义因子来预估一个用户对一个商品的偏好。 代码示例下面例子来自http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html，并做了稍许修改。 Scala 示例为了测试简单，使用Spark本地运行模式进行测试。下面代码可以在spark-shell中运行： 1234567891011121314151617181920212223242526272829303132333435import org.apache.spark.mllib.recommendation.ALSimport org.apache.spark.mllib.recommendation.MatrixFactorizationModelimport org.apache.spark.mllib.recommendation.Rating// Load and parse the dataval data = sc.textFile("data/mllib/als/test.data")val ratings = data.map(_.split(',') match &#123; case Array(user, item, rate) =&gt; Rating(user.toInt, item.toInt, rate.toDouble) &#125;)// Build the recommendation model using ALSval rank = 10val numIterations = 20val model = ALS.train(ratings, rank, numIterations, 0.01)// Evaluate the model on rating dataval usersProducts = ratings.map &#123; case Rating(user, product, rate) =&gt; (user, product)&#125;val predictions = model.predict(usersProducts).map &#123; case Rating(user, product, rate) =&gt; ((user, product), rate) &#125;val ratesAndPreds = ratings.map &#123; case Rating(user, product, rate) =&gt; ((user, product), rate)&#125;.join(predictions)val MSE = ratesAndPreds.map &#123; case ((user, product), (r1, r2)) =&gt; val err = (r1 - r2) err * err&#125;.mean()println("Mean Squared Error = " + MSE)// Save and load modelmodel.save(sc, "myModelPath")val sameModel = MatrixFactorizationModel.load(sc, "myModelPath") Java示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import scala.Tuple2;import org.apache.spark.api.java.*;import org.apache.spark.api.java.function.Function;import org.apache.spark.mllib.recommendation.ALS;import org.apache.spark.mllib.recommendation.MatrixFactorizationModel;import org.apache.spark.mllib.recommendation.Rating;import org.apache.spark.SparkConf;public class JavaALS &#123; public static void main(String[] args) &#123; SparkConf conf = new SparkConf().setAppName("Collaborative Filtering Example"); JavaSparkContext sc = new JavaSparkContext(conf); // Load and parse the data String path = "data/mllib/als/test.data"; JavaRDD&lt;String&gt; data = sc.textFile(path); JavaRDD&lt;Rating&gt; ratings = data.map( new Function&lt;String, Rating&gt;() &#123; public Rating call(String s) &#123; String[] sarray = s.split(","); return new Rating(Integer.parseInt(sarray[0]), Integer.parseInt(sarray[1]), Double.parseDouble(sarray[2])); &#125; &#125; ); // Build the recommendation model using ALS int rank = 10; int numIterations = 20; float lambda = 0.01; MatrixFactorizationModel model = ALS.train(JavaRDD.toRDD(ratings), rank, numIterations, lambda); // Evaluate the model on rating data JavaRDD&lt;Tuple2&lt;Object, Object&gt;&gt; userProducts = ratings.map( new Function&lt;Rating, Tuple2&lt;Object, Object&gt;&gt;() &#123; public Tuple2&lt;Object, Object&gt; call(Rating r) &#123; return new Tuple2&lt;Object, Object&gt;(r.user(), r.product()); &#125; &#125; ); JavaPairRDD&lt;Tuple2&lt;Integer, Integer&gt;, Double&gt; predictions = JavaPairRDD.fromJavaRDD( model.predict(JavaRDD.toRDD(userProducts)).toJavaRDD().map( new Function&lt;Rating, Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Double&gt;&gt;() &#123; public Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Double&gt; call(Rating r)&#123; return new Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Double&gt;( new Tuple2&lt;Integer, Integer&gt;(r.user(), r.product()), r.rating()); &#125; &#125; )); JavaRDD&lt;Tuple2&lt;Double, Double&gt;&gt; ratesAndPreds = JavaPairRDD.fromJavaRDD(ratings.map( new Function&lt;Rating, Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Double&gt;&gt;() &#123; public Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Double&gt; call(Rating r)&#123; return new Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Double&gt;( new Tuple2&lt;Integer, Integer&gt;(r.user(), r.product()), r.rating()); &#125; &#125; )).join(predictions).values(); double MSE = JavaDoubleRDD.fromRDD(ratesAndPreds.map( new Function&lt;Tuple2&lt;Double, Double&gt;, Object&gt;() &#123; public Object call(Tuple2&lt;Double, Double&gt; pair) &#123; Double err = pair._1() - pair._2(); return err * err; &#125; &#125; ).rdd()).mean(); System.out.println("Mean Squared Error = " + MSE); &#125;&#125; Python示例下面代码可以在 pyspark 中运行下面代码： 123456789101112131415161718from pyspark.mllib.recommendation import ALSfrom numpy import array# Load and parse the datadata = sc.textFile("data/mllib/als/test.data")ratings = data.map(lambda line: array([float(x) for x in line.split(',')]))# Build the recommendation model using Alternating Least Squaresrank = 10numIterations = 20model = ALS.train(ratings, rank, numIterations)# Evaluate the model on training datatestdata = ratings.map(lambda p: (int(p[0]), int(p[1])))predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).reduce(lambda x, y: x + y)/ratesAndPreds.count()print("Mean Squared Error = " + str(MSE)) 总结使用Spark MLlib的ALS算法进行协同过滤，首先需要了解推荐的过程，然后需要根据测试不断修改训练测试，建立合理的模型，最后再给用户进行推荐商品，保存推荐结果。 另外，在网上找到一些Spark做推荐的项目： 提供Restfull接口的实时推荐：https://github.com/OndraFiedler/spark-recommender spark-elasticsearch-mllib：https://github.com/ebiznext/spark-elasticsearch-mllib Beyond Piwik Web Analytics：https://github.com/skrusche63/spark-piwik Serves predictions via a REST API：https://github.com/SeldonIO/seldon-server https://github.com/zhuixun/learningspark Spark-Movie-Recommendation：https://github.com/yuriy-voderatskiy/Spark-Movie-Recommendation 更多关于推荐相关的资源可以参考Reading List 2015-03。 参考文章 Movie Recommendation with MLlib Spark MLlib系列(二):基于协同过滤的电影推荐系统 ALSBenchmark.scala]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>recommendation</tag>
        <tag>mllib</tag>
        <tag>als</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark SQL中的数据源]]></title>
    <url>%2F2015%2F04%2F03%2Fspark-sql-datasource%2F</url>
    <content type="text"><![CDATA[Spark 支持通过 DataFrame 来操作大量的数据源，包括外部文件（如 json、avro、parquet、sequencefile 等等）、hive、关系数据库、cassandra 等等。 本文测试环境为 Spark 1.3。 加载和保存文件最简单的方式是调用 load 方法加载文件，默认的格式为 parquet，你可以修改 spark.sql.sources.default 指定默认的格式： 12scala&gt; val df = sqlContext.load("people.parquet")scala&gt; df.select("name", "age").save("namesAndAges.parquet") 你也可以收到指定数据源，使用全路径名称，如：org.apache.spark.sql.parquet，对于内置的数据源，你也可以使用简称，如：json、parquet、jdbc。 12scala&gt; val df = sqlContext.load("people.json", "json")scala&gt; df.select("name", "age").save("namesAndAges.parquet", "parquet") 保存操作还可以指定保存模式，用于处理文件已经存在的情况下如何操作。 Scala/Java Python 含义 SaveMode.ErrorIfExists (default) “error” (default) 如果存在，则报错 SaveMode.Append “append” 追加模式 SaveMode.Overwrite “overwrite” 覆盖模式 SaveMode.Ignore “ignore” 忽略，类似 SQL 中的 CREATE TABLE IF NOT EXISTS Parquet 数据源加载数据Spark SQL 支持读写 Parquet文件。 Scala: 1234567891011121314151617// sqlContext from the previous example is used in this example.// This is used to implicitly convert an RDD to a DataFrame.import sqlContext.implicits._val people: RDD[Person] = ... // An RDD of case class objects, from the previous example.// The RDD is implicitly converted to a DataFrame by implicits, allowing it to be stored using Parquet.people.saveAsParquetFile("people.parquet")// Read in the parquet file created above. Parquet files are self-describing so the schema is preserved.// The result of loading a Parquet file is also a DataFrame.val parquetFile = sqlContext.parquetFile("people.parquet")//Parquet files can also be registered as tables and then used in SQL statements.parquetFile.registerTempTable("parquetFile")val teenagers = sqlContext.sql("SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 19")teenagers.map(t =&gt; "Name: " + t(0)).collect().foreach(println) Java: 12345678910111213141516171819// sqlContext from the previous example is used in this example.DataFrame schemaPeople = ... // The DataFrame from the previous example.// DataFrames can be saved as Parquet files, maintaining the schema information.schemaPeople.saveAsParquetFile("people.parquet");// Read in the Parquet file created above. Parquet files are self-describing so the schema is preserved.// The result of loading a parquet file is also a DataFrame.DataFrame parquetFile = sqlContext.parquetFile("people.parquet");//Parquet files can also be registered as tables and then used in SQL statements.parquetFile.registerTempTable("parquetFile");DataFrame teenagers = sqlContext.sql("SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 19");List&lt;String&gt; teenagerNames = teenagers.map(new Function&lt;Row, String&gt;() &#123; public String call(Row row) &#123; return "Name: " + row.getString(0); &#125;&#125;).collect(); Python: 1234567891011121314151617# sqlContext from the previous example is used in this example.schemaPeople # The DataFrame from the previous example.# DataFrames can be saved as Parquet files, maintaining the schema information.schemaPeople.saveAsParquetFile("people.parquet")# Read in the Parquet file created above. Parquet files are self-describing so the schema is preserved.# The result of loading a parquet file is also a DataFrame.parquetFile = sqlContext.parquetFile("people.parquet")# Parquet files can also be registered as tables and then used in SQL statements.parquetFile.registerTempTable("parquetFile");teenagers = sqlContext.sql("SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 19")teenNames = teenagers.map(lambda p: "Name: " + p.name)for teenName in teenNames.collect(): print teenName SQL: 1234567CREATE TEMPORARY TABLE parquetTableUSING org.apache.spark.sql.parquetOPTIONS ( path "examples/src/main/resources/people.parquet")SELECT * FROM parquetTable 自动发现分区Parquet 数据源可以自动识别分区目录以及分区列的类型，目前支持数据类型和字符串类型。 例如，对于这样一个目录结构，有两个分区字段：gender、country。 12345678910111213141516171819path└── to └── table ├── gender=male │ ├── ... │ │ │ ├── country=US │ │ └── data.parquet │ ├── country=CN │ │ └── data.parquet │ └── ... └── gender=female ├── ... │ ├── country=US │ └── data.parquet ├── country=CN │ └── data.parquet └── ... 将 path/to/table 路径传递给 SQLContext.parquetFile 或 SQLContext.load 时，Spark SQL 将会字段获取分区信息，并返回 DataFrame 的 schema 如下： 12345root|-- name: string (nullable = true)|-- age: long (nullable = true)|-- gender: string (nullable = true)|-- country: string (nullable = true) schema 自动扩展Parquet 还支持 schema 自动扩展。 Scala: 123456789101112131415161718192021222324// sqlContext from the previous example is used in this example.// This is used to implicitly convert an RDD to a DataFrame.import sqlContext.implicits._// Create a simple DataFrame, stored into a partition directoryval df1 = sparkContext.makeRDD(1 to 5).map(i =&gt; (i, i * 2)).toDF("single", "double")df1.saveAsParquetFile("data/test_table/key=1")// Create another DataFrame in a new partition directory,// adding a new column and dropping an existing columnval df2 = sparkContext.makeRDD(6 to 10).map(i =&gt; (i, i * 3)).toDF("single", "triple")df2.saveAsParquetFile("data/test_table/key=2")// Read the partitioned tableval df3 = sqlContext.parquetFile("data/test_table")df3.printSchema()// The final schema consists of all 3 columns in the Parquet files together// with the partiioning column appeared in the partition directory paths.// root// |-- single: int (nullable = true)// |-- double: int (nullable = true)// |-- triple: int (nullable = true)// |-- key : int (nullable = true) Python: 123456789101112131415161718192021222324# sqlContext from the previous example is used in this example.# Create a simple DataFrame, stored into a partition directorydf1 = sqlContext.createDataFrame(sc.parallelize(range(1, 6))\ .map(lambda i: Row(single=i, double=i * 2)))df1.save("data/test_table/key=1", "parquet")# Create another DataFrame in a new partition directory,# adding a new column and dropping an existing columndf2 = sqlContext.createDataFrame(sc.parallelize(range(6, 11)) .map(lambda i: Row(single=i, triple=i * 3)))df2.save("data/test_table/key=2", "parquet")# Read the partitioned tabledf3 = sqlContext.parquetFile("data/test_table")df3.printSchema()# The final schema consists of all 3 columns in the Parquet files together# with the partiioning column appeared in the partition directory paths.# root# |-- single: int (nullable = true)# |-- double: int (nullable = true)# |-- triple: int (nullable = true)# |-- key : int (nullable = true) 配置参数 spark.sql.parquet.binaryAsString：默认为 false，是否将 binary 当做字符串处理 spark.sql.parquet.int96AsTimestamp：默认为 true spark.sql.parquet.cacheMetadata ：默认为 true，是否缓存元数据 spark.sql.parquet.compression.codec：默认为 gzip，支持的值：uncompressed, snappy, gzip, lzo spark.sql.parquet.filterPushdown：默认为 false spark.sql.hive.convertMetastoreParquet：默认为 false JSON 数据源Spark SQL 能够自动识别 JSON 数据的 schema ，SQLContext 中有两个方法处理 JSON： jsonFile：从一个 JSON 目录中加载数据，JSON 文件中每一行为一个 JSON 对象。 jsonRDD：从一个 RDD 中加载数据，RDD 的每一个元素为一个 JSON 对象的字符串。 一个 Scala 的例子如下： 1234567891011121314151617181920212223242526// sc is an existing SparkContext.val sqlContext = new org.apache.spark.sql.SQLContext(sc)// A JSON dataset is pointed to by path.// The path can be either a single text file or a directory storing text files.val path = "people.json"// Create a DataFrame from the file(s) pointed to by pathval people = sqlContext.jsonFile(path)// The inferred schema can be visualized using the printSchema() method.people.printSchema()// root// |-- age: integer (nullable = true)// |-- name: string (nullable = true)// Register this DataFrame as a table.people.registerTempTable("people")// SQL statements can be run by using the sql methods provided by sqlContext.val teenagers = sqlContext.sql("SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19")// Alternatively, a DataFrame can be created for a JSON dataset represented by// an RDD[String] storing one JSON object per string.val anotherPeopleRDD = sc.parallelize( """&#123;"name":"Yin","address":&#123;"city":"Columbus","state":"Ohio"&#125;&#125;""" :: Nil)val anotherPeople = sqlContext.jsonRDD(anotherPeopleRDD) Hive 数据源Spark SQL 支持读和写 Hive 中的数据。Spark 源码本身不包括 Hive，故编译时候需要添加 -Phive 和 -Phive-thriftserver 开启对 Hive 的支持。另外，Hive assembly jar 需要存在于每一个 worker 节点上，因为他们需要 SerDes 去访问存在于 Hive 中的数据。 Scala: 12345678// sc is an existing SparkContext.val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)sqlContext.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")sqlContext.sql("LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src")// Queries are expressed in HiveQLsqlContext.sql("FROM src SELECT key, value").collect().foreach(println) Java: 12345678// sc is an existing JavaSparkContext.HiveContext sqlContext = new org.apache.spark.sql.hive.HiveContext(sc);sqlContext.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)");sqlContext.sql("LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src");// Queries are expressed in HiveQL.Row[] results = sqlContext.sql("FROM src SELECT key, value").collect(); Python: 123456789# sc is an existing SparkContext.from pyspark.sql import HiveContextsqlContext = HiveContext(sc)sqlContext.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")sqlContext.sql("LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src")# Queries can be expressed in HiveQL.results = sqlContext.sql("FROM src SELECT key, value").collect() JDBC 数据源Spark SQL 支持通过 JDBC 访问关系数据库，这需要用到 JdbcRDD。为了访问某一个关系数据库，需要将其驱动添加到 classpath，例如： 1SPARK_CLASSPATH=postgresql-9.3-1102-jdbc41.jar bin/spark-shell 访问 jdbc 数据源需要提供以下参数： url dbtable driver partitionColumn, lowerBound, upperBound, numPartitions Scala 示例： 123val jdbcDF = sqlContext.load("jdbc", Map( "url" -&gt; "jdbc:postgresql:dbserver", "dbtable" -&gt; "schema.tablename")) Java: 12345Map&lt;String, String&gt; options = new HashMap&lt;String, String&gt;();options.put("url", "jdbc:postgresql:dbserver");options.put("dbtable", "schema.tablename");DataFrame jdbcDF = sqlContext.load("jdbc", options) Python: 1df = sqlContext.load("jdbc", url="jdbc:postgresql:dbserver", dbtable="schema.tablename") SQL: 123456CREATE TEMPORARY TABLE jdbcTableUSING org.apache.spark.sql.jdbcOPTIONS ( url "jdbc:postgresql:dbserver", dbtable "schema.tablename") 访问 Avro这不是 Spark 内置的数据源，要想访问 Avro 数据源 ，需要做些处理。这部分内容可以参考 如何将Avro数据加载到Spark 和 Spark with Avro。 访问 CassandraTODO 测试Spark 和 Parquet参考上面的例子，将 people.txt 文件加载到 Spark： 123456scala&gt; import sqlContext.implicits._scala&gt; case class People(name: String, age: Int)scala&gt; val people = sc.textFile("people.txt").map(_.split(",")).map(p =&gt; People(p(0), p(1).trim.toInt)).toDF()scala&gt; people.registerTempTable("people")scala&gt; val teenagers = sqlContext.sql("SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19")scala&gt; teenagers.map(t =&gt; "Name: " + t(0)).collect().foreach(println) 然后，将 people 这个 DataFrame 转换为 parquet 格式： 12scala&gt; people.saveAsParquetFile("people.parquet")scala&gt; val parquetFile = sqlContext.parquetFile("people.parquet") 另外，也可以从 hive 中加载 parquet 格式的文件。 12hive&gt; create table people_parquet like people stored as parquet;hive&gt; insert overwrite table people_parquet select * from people; 使用 HiveContext 来从 hive 中加载 parquet 文件，这里不再需要定义一个 case class ，因为 parquet 中已经包含了文件的 schema。 123456scala&gt; val hc = new org.apache.spark.sql.hive.HiveContext(sc)scala&gt; import hc.implicits._scala&gt;val peopleRDD = hc.parquetFile("people.parquet")scala&gt; peopleRDD.registerAsTempTable("pp")scala&gt;val teenagers = hc.sql("SELECT name FROM pp WHERE age &gt;= 13 AND age &lt;= 19")scala&gt;teenagers.collect.foreach(println) 注意到 impala 中处理 parquet 文件时，会将字符串保存为 Binary，为了修正这个问题，可以添加下面一行代码： 1scala&gt; sqlContext.setConf("spark.sql.parquet.binaryAsString","true") SparkSql Join下面是两个表左外连接的例子： 12345678910111213141516scala&gt;import sqlContext.implicits._scala&gt;import org.apache.spark.sql.catalyst.plans._scala&gt; case class Dept(dept_id:String,dept_name:String)scala&gt; val dept = sc.parallelize(List( ("DEPT01","Information Technology"), ("DEPT02","WHITE HOUSE"),("DEPT03","EX-PRESIDENTS OFFICE"),("DEPT04","SALES"))).map( d =&gt; Dept(d._1,d._2)).toDF.as( "dept" )scala&gt; case class Emp(first_name:String,last_name:String,dept_id:String)scala&gt; val emp = sc.parallelize(List( ("Rishi","Yadav","DEPT01"),("Barack","Obama","DEPT02"),("Bill","Clinton","DEPT04"))).map( e =&gt; Emp(e._1,e._2,e._3)).toDF.as("emp")scala&gt; val alldepts = dept.join(emp,dept("dept_id") === emp("dept_id"), "left_outer").select("dept.dept_id","dept_name","first_name","last_name")scala&gt; alldepts.foreach(println)[DEPT01,Information Technology,Rishi,Yadav][DEPT02,WHITE HOUSE,Barack,Obama][DEPT04,SALES,Bill,Clinton][DEPT03,EX-PRESIDENTS OFFICE,null,null] 支持的连接类型有：inner、outer、left_outer、right_outer、semijoin。 参考文章 Spark SQL and DataFrame Guide Spark 编程指南简体中文版-Spark SQL spark-cookbook]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>avro</tag>
        <tag>parquet</tag>
        <tag>spark-sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark本地模式运行]]></title>
    <url>%2F2015%2F03%2F30%2Fspark-test-in-local-mode%2F</url>
    <content type="text"><![CDATA[Spark的安装分为几种模式，其中一种是本地运行模式，只需要在单节点上解压即可运行，这种模式不需要依赖Hadoop 环境。在本地运行模式中，master和worker都运行在一个jvm进程中，通过该模式，可以快速的测试Spark的功能。 下载 Spark下载地址为http://spark.apache.org/downloads.html，根据页面提示选择一个合适的版本下载，这里我下载的是 spark-1.3.0-bin-cdh4.tgz。下载之后解压： 1234cd ~wget http://mirror.bit.edu.cn/apache/spark/spark-1.3.0/spark-1.3.0-bin-cdh4.tgztar -xf spark-1.3.0-bin-cdh4.tgzcd spark-1.3.0-bin-cdh4 下载之后的目录为： 123456789101112131415⇒ tree -L 1.├── CHANGES.txt├── LICENSE├── NOTICE├── README.md├── RELEASE├── bin├── conf├── data├── ec2├── examples├── lib├── python└── sbin 运行 spark-shell本地模式运行spark-shell非常简单，只要运行以下命令即可，假设当前目录是$SPARK_HOME 12$ MASTER=local $ bin/spark-shell MASTER=local就是表明当前运行在单机模式。如果一切顺利，将看到下面的提示信息： 12Created spark context..Spark context available as sc. 这表明spark-shell中已经内置了Spark context的变量，名称为sc，我们可以直接使用该变量进行后续的操作。 spark-shell 后面设置 master 参数，可以支持更多的模式，请参考 http://spark.apache.org/docs/latest/submitting-applications.html#master-urls。 我们在sparkshell中运行一下最简单的例子，统计在README.md中含有Spark的行数有多少，在spark-shell中输入如下代码： 1scala&gt;sc.textFile("README.md").filter(_.contains("Spark")).count 如果你觉得输出的日志太多，你可以从模板文件创建 conf/log4j.properties ： 1$ mv conf/log4j.properties.template conf/log4j.properties 然后修改日志输出级别为WARN： 1log4j.rootCategory=WARN, console 如果你设置的 log4j 日志等级为 INFO，则你可以看到这样的一行日志 INFO SparkUI: Started SparkUI at http://10.9.4.165:4040，意思是 Spark 启动了一个 web 服务器，你可以通过浏览器访问http://10.9.4.165:4040来查看 Spark 的任务运行状态等信息。 pyspark运行 bin/pyspark 的输出为： 123456789101112131415161718$ bin/pysparkPython 2.7.6 (default, Sep 9 2014, 15:04:36)[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.Spark assembly has been built with Hive, including Datanucleus jars on classpathPicked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-815/03/30 15:19:07 WARN Utils: Your hostname, june-mac resolves to a loopback address: 127.0.0.1; using 10.9.4.165 instead (on interface utun0)15/03/30 15:19:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address15/03/30 15:19:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableWelcome to ____ __ / __/__ ___ _____/ /__ _\ \/ _ \/ _ / __/ _/ /__ / .__/\_,_/_/ /_/\_\ version 1.3.0 /_/Using Python version 2.7.6 (default, Sep 9 2014 15:04:36)SparkContext available as sc, HiveContext available as sqlCtx. 你也可以使用 IPython 来运行 Spark： 1IPYTHON=1 ./bin/pyspark 如果要使用 IPython NoteBook，则运行： 1IPYTHON_OPTS=&quot;notebook&quot; ./bin/pyspark 从日志可以看到，不管是 bin/pyspark 还是 bin/spark-shell，他们都有两个内置的变量：sc 和 sqlCtx。 1SparkContext available as sc, HiveContext available as sqlCtx sc 代表着 Spark 的上下文，通过该变量可以执行 Spark 的一些操作，而 sqlCtx 代表着 HiveContext 的上下文。 spark-submit在Spark1.0之后提供了一个统一的脚本spark-submit来提交任务。 对于 python 程序，我们可以直接使用 spark-submit： 1234$ mkdir -p /usr/lib/spark/examples/python$ tar zxvf /usr/lib/spark/lib/python.tar.gz -C /usr/lib/spark/examples/python$ ./bin/spark-submit examples/python/pi.py 10 对于 Java 程序，我们需要先编译代码然后打包运行： 1$ spark-submit --class "SimpleApp" --master local[4] simple-project-1.0.jar 测试 RDD在 Spark 中，我们操作的集合被称为 RDD，他们被并行拷贝到集群各个节点上。我们可以通过 sc 来创建 RDD 。 创建 RDD 有两种方式： sc.parallelize() sc.textFile() 使用 Scala 对 RDD 的一些操作： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384val rdd1=sc.parallelize(List(1,2,3,3))val rdd2=sc.parallelize(List(3,4,5))//转换操作rdd1.map(2*).collect //等同于：rdd1.map(t=&gt;2*t).collect//Array[Int] = Array(2, 4, 6, 6)rdd1.filter(_&gt;2).collect//Array[Int] = Array(3, 3)rdd1.flatMap(_ to 4).collect//Array[Int] = Array(1, 2, 3, 4, 2, 3, 4, 3, 4, 3, 4)rdd1.sample(false, 0.3, 4).collect//Array[Int] = Array(3, 3)rdd1.sample(true, 0.3, 4).collect//Array[Int] = Array(3)rdd1.union(rdd2).collect//Array[Int] = Array(1, 2, 3, 3, 3, 4, 5)rdd1.distinct().collect//Array[Int] = Array(1, 2, 3)rdd1.map(i=&gt;(i,i)).groupByKey.collect//Array[(Int, Iterable[Int])] = Array((1,CompactBuffer(1)), (2,CompactBuffer(2)), (3,CompactBuffer(3, 3)))rdd1.map(i=&gt;(i,i)).reduceByKey(_ + _).collect//Array[(Int, Int)] = Array((1,1), (2,2), (3,6))rdd1.map(i=&gt;(i,i)).sortByKey(false).collect//Array[(Int, Int)] = Array((3,3), (3,3), (2,2), (1,1))rdd1.map(i=&gt;(i,i)).join(rdd2.map(i=&gt;(i,i))).collect//Array[(Int, (Int, Int))] = Array((3,(3,3)), (3,(3,3)))rdd1.map(i=&gt;(i,i)).cogroup(rdd2.map(i=&gt;(i,i))).collect//Array[(Int, (Iterable[Int], Iterable[Int]))] = Array((4,(CompactBuffer(),CompactBuffer(4))), (1,(CompactBuffer(1),CompactBuffer())), (5,(CompactBuffer(),CompactBuffer(5))), (2,(CompactBuffer(2),CompactBuffer())), (3,(CompactBuffer(3, 3),CompactBuffer(3))))rdd1.cartesian(rdd2).collect()//Array[(Int, Int)] = Array((1,3), (1,4), (1,5), (2,3), (2,4), (2,5), (3,3), (3,4), (3,5), (3,3), (3,4), (3,5))rdd1.pipe("head -n 1").collect//Array[String] = Array(1, 2, 3, 3)//动作操作rdd1.reduce(_ + _)//Int = 9rdd1.collect//Array[Int] = Array(1, 2, 3, 3)rdd1.first()//Int = 1rdd1.take(2)//Array[Int] = Array(1, 2)rdd1.top(2)//Array[Int] = Array(3, 3)rdd1.takeOrdered(2)//Array[Int] = Array(1, 2)rdd1.map(i=&gt;(i,i)).countByKey()//scala.collection.Map[Int,Long] = Map(1 -&gt; 1, 2 -&gt; 1, 3 -&gt; 2)rdd1.countByValue()//scala.collection.Map[Int,Long] = Map(1 -&gt; 1, 2 -&gt; 1, 3 -&gt; 2)rdd1.intersection(rdd2).collect()//Array[Int] = Array(3)rdd1.subtract(rdd2).collect()//Array[Int] = Array(1, 2)rdd1.foreach(println)//3//2//3//1rdd1.foreachPartition(x =&gt; println(x.reduce(_ + _))) 更多例子，参考http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html。]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reading List 2015-03]]></title>
    <url>%2F2015%2F03%2F30%2Freading-list-2015-03%2F</url>
    <content type="text"><![CDATA[这个月主要在关注流式处理和推荐系统方面的技术。如何从零构建一个推荐系统？网上能找到的有指导意义的资料太少，只能一点点摸索？ Spark LeanCloud 离线数据分析功能介绍 Spark在腾讯数据仓库TDW的应用 http://www.biaodianfu.com/spark-tdw.html Spark on Yarn：小火花照亮大数据 http://rdc.taobao.org/?p=512 Spark on Yarn：性能调优 http://rdc.taobao.org/?p=533 Spark 教程 Spark Shell Examples https://altiscale.zendesk.com/hc/en-us/articles/202627136-Spark-Shell-Examples http://www.javacodegeeks.com//?s=spark Spark SQL join 的例子：https://gist.github.com/ceteri/11381941 Spark Cook Book：http://www.infoobjects.com/spark-cookbook/ Spark做推荐系统 spark机器学习 http://blog.selfup.cn/category/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0 Spark MLlib系列(一)：入门介绍 http://blog.csdn.net/shifenglov/article/details/43762705 Spark MLlib系列(二)：基于协同过滤的电影推荐系统 http://blog.csdn.net/shifenglov/article/details/43795597 Spark机器学习库mllib之协同过滤 http://blog.csdn.net/oopsoom/article/details/34462329 快刀初试：Spark GraphX在淘宝的实践 http://www.csdn.net/article/2014-08-07/2821097 “Spark上流式机器学习算法实现”终期检查报告 http://blog.csdn.net/zhangyuming010/article/details/38364867 Spark 0.9.1 MLLib 机器学习库简介 http://rdc.taobao.org/?p=2163 Spark MLlib 概念 6：ALS（Alternating Least Squares） or (ALS-WR) http://www.cnblogs.com/zwCHAN/p/4269027.html MLlib实践经验(1) http://yanbohappy.sinaapp.com/?p=498 研究机器学习之MLlib实践经验 http://www.hengha.info/blog/home/view/id/31003 协同过滤算法在MapReduce与Spark上实现对比 http://data.qq.com/article?id=823 ItemBased with Spark http://weikey.me/articles/187.html Collaborative Filtering with Spark http://www.slideshare.net/MrChrisJohnson/collaborative-filtering-with-spark Movie Recommendations and More With Spark http://mlnick.github.io/blog/2013/04/01/movie-recommendations-and-more-with-spark/ Movie Recommendation with Mllib https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html 基于ALS算法的简易在线推荐系统 http://ju.outofmemory.cn/entry/110756 ALS 在 Spark MLlib 中的实现 http://dataunion.org/16856.html 阿基米德项目ALS矩阵分解算法应用案例 https://github.com/ceys/jdml/wiki/ALS ALS矩阵分解推荐模型 http://www.wfuyu.com/server/22823.html 协同过滤之ALS-WR算法 http://www.fuqingchuan.com/2015/03/812.html Spark上矩阵运算库 http://blog.csdn.net/u014252240/article/category/2384017 用MongoDB和Spark实现电影推荐 http://www.infoq.com/cn/news/2014/12/mongdb-spark-movie-recommend 基于Spark构建推荐引擎之一：基于物品的协同过滤推荐 http://blog.csdn.net/sunbow0/article/details/42737541 2015的 spark-summit ，使用 Spark 实时推荐系统：http://spark-summit.org/wp-content/uploads/2015/03/SSE15-18-Neumann-Alla.pdf 基于PredictionIO的推荐引擎打造，及大规模多标签分类探索 http://www.uml.org.cn/yunjisuan/2015041410.asp PDF：MLlib: Scalable Machine Learning on Spark http://stanford.edu/~rezab/sparkworkshop/slides/xiangrui.pdf 使用Spark的MLlib、Hbase作为模型、Hive作数据清洗的核心推荐引擎,在Spark on Yarn测试通过 https://github.com/wbj0110/spark_resrecomend 推荐系统 推荐算法总结Recommendation http://blog.csdn.net/oopsoom/article/details/33740799 Collaborative Filtering and Recommender Systems By Navisro Analytics，里面有推荐系统的步骤 http://www.slideshare.net/navisro/recommender-system-navisroanalytics?related=1 Item Based Collaborative Filtering Recommendation Algorithms http://www.slideshare.net/nextlib/item-based-collaborative-filtering-recommendation-algorithms?related=2 协同过滤CF推荐介绍 http://blog.sina.com.cn/s/blog_6e0035ad0102v26h.html Python 实现的机器学习库 scikit-learn：http://scikit-learn.org/stable/index.html @爱可可-爱生活 免费好书！《Practical Machine Learning: Innovations in Recommendation》机器学习&amp;推荐系统：简单构建有效的推荐系统；借搜索技术创新应用部署大规模推荐系统；从实时数据中提取信息改进推荐系统的方法和技巧。超赞&amp;推荐！ https://www.mapr.com/practical-machine-learning 另:讨论推荐系统设计模式的文章: https://www.mapr.com/blog/design-patterns-recommendation-systems-%E2%80%93-everyone-wants-pony @InfoQ推荐系统中最核心的数据之一是 user profile 数据。我们需要从大量历史用户行为中分析和挖掘各种维度的特征，来刻画用户的兴趣偏好。在QCon北京2015 @今日头条 架构师丁海峰，将分享中会介绍今日头条 user profile 系统的现状，面临的问题，系统演进，以及技术架构中的关键问题。http://www.qconbeijing.com/track/2509 @陈志武zwchen 电商网站用户，可分为两类：有购买欲望及明确购买目标，有购买欲望但无明确购买目标。前者为主动用户，决策较独立；后者为被动用户，需要被引导和刺激，协助其明确购买目标，如亚马逊强大的推荐系统，听说贡献了30%以上销售额。针对主动用户和被动用户，网站该如何设计呢？http://zwchen.iteye.com/blog/1439259 【重磅！大数据工程师 @飞林沙 的年终总结&amp;算法数据的思考】一个优秀的推荐算法，一个优秀的推荐系统的确可以为企业创造很多价值，曾经和某知名电商网站的数据总监交流，他们的推荐系统实实在在地把销售额增加了15%，但是过于神话迷恋推荐算法和过于看扁推荐算法都是一种偏激的行为 http://www.36dsj.com/archives/18821 @AixinSG 今天读了两篇关于微博推荐的文章。对推荐系统了解不深，感觉微博推荐应该是个非常困难的问题。推荐的不是普通的item而是”人”, 一个人有多重身份，一般需要一个较长的熟悉过程，还有线上线下两个不同的交际圈子，增加了信息不对等。关注一个人也有累加的时间成本，得到的信息是否能抵消成本也是一个问题。 也谈谈新浪微博可能感兴趣的人 @朝花夕拾录：最近看了几个推荐系统的文章，http://bigdata.memect.com/?tag=recommendationsystems 有入门级的教程，讲解推荐系统的经典解决方案，还有进阶体验，介绍如何在大数据平台（Hadoop，spark, mogodb)上形成实时推荐。还有两个搞笑的文案调侃推荐系统的用户体验。 使用Oryx和CDH进行个性化推荐 http://weikey.me/articles/222.html Myrrix 分布式推荐 http://weikey.me/articles/197.html 使用Mahout Kmeans算法进行中文聚类 http://weikey.me/articles/133.html 漫谈“推荐系统” http://youngfor.me/post/recsys/man-tan-tui-jian-xi-tong 推荐系统经典论文文献及业界应用 自己动手写一个推荐系统 http://www.cnblogs.com/flclain/archive/2013/03/03/2941397.html 什么是好的推荐系统 http://guoze.me/2015/01/29/good-recommendation/ 58同城的大数据环境下实现一个O2O通用推荐引擎的实践 http://www.tuicool.com/articles/3MFBfq 58同城推荐系统架构设计与实现 http://chuansong.me/n/949426 构建一个基于del.icio.us的链接推荐系统 http://lazynight.me/2740.html 基于协同过滤构建简单推荐系统 http://blog.csdn.net/database_zbye/article/details/8664516 使用Python简易推荐系统的构建 http://linux.readthedocs.org/zh_CN/latest/docsource/publication/alpha2/#id7 微博推荐算法简述 http://www.wbrecom.com/?p=80 使用 Azure、Hadoop 和 Mahout 构建一个推荐系统 http://www.oschina.net/translate/building-a-recommendation-engine-machine-learning Netflix的推荐和个性化系统架构 http://blog.sina.com.cn/s/blog_7ebae53b0101bnuy.html 探索推荐引擎内部的秘密 http://blog.sae.sina.com.cn/archives/2706 美团推荐算法实践 http://tech.meituan.com/mt-recommend-practice.html 百分点推荐引擎——从需求到架构 http://www.infoq.com/cn/articles/baifendian-recommendation-engine QConShanghai2013-杨浩-360推荐系统实践.pdf http://vdisk.weibo.com/s/A0GI9rYhL4I2 大规模电商推荐系统应用经验分享 http://vdisk.weibo.com/s/udVGwtAAA4xz4 打造最适合产品的推荐系统 http://vdisk.weibo.com/s/BPySloeWUaiZ4 爱奇艺推荐系统的整体技术架构 http://edu.51cto.com/lesson/id-56393.html 推荐系统架构小结 http://blog.csdn.net/idonot/article/details/7996733 淘宝推荐系统的学习 http://www.biaodianfu.com/taobao-recommendation-system.html 协同过滤算法：在线推荐系统如何工作？ http://www.csdn.net/article/2013-01-30/2814014-Collaborative-Filtering 一个完整推荐系统的设计实现-以百度关键词搜索推荐为例 活用您的 Big Data，實現線上服務行銷的精準推薦 http://www.slideshare.net/etusolution/big-data-13084872 推荐系统规划 http://www.slideshare.net/2005000613/ss-16169751?qid=4f0d8f9f-8b65-4d60-9685-5464a4b7b731 Github 上大数据/数据挖掘/推荐系统/机器学习相关资源 https://github.com/Flowerowl/Big-Data-Resources 基于项目流行度的协同过滤TopN推荐算法 计算准确率、召回率、覆盖率 &lt;http://wuchong.me/blog/2014/04/19/recsys-cf-study/ http://my.oschina.net/zhangjiawen/blog/185625&gt; 个性化推荐 @爱可可-爱生活 [文章]《Personalized Recommendations at Etsy》&lt;https://codeascraft.com/2014/11/17/personalized-recommendations-at-etsy/ &gt;介绍Etsy采用的个性化推荐算法，包括矩阵分解、交替最小二乘、随机SVD和局部敏感哈希等 @爱可可-爱生活 [文章]《Pinnability: Machine learning in the home feed》http://engineering.pinterest.com/post/114138410669/pinnability-machine-learning-in-the-home-feed 介绍Pinterest的Pinnability，基于机器学习提供个性化内容(推荐)列表 @付聪_BenFrank：主流商品往往代表了绝大多数用户的需求，而长尾商品往往代表了一小部分用户推荐系统的个性化需求。因此，如果要通过发掘长尾提高销售额，就必须充分研究用户的兴趣，而这正是个性化推荐系统主要解决的问题。 @6弗恩er：今日头条是一款基于数据化挖掘的个性化信息推荐引擎。根据微博行为、阅读行为、地理位置、职业年龄等挖掘出兴趣。用户每次动作后，10秒内更新用户模型。对每条信息提取几十个到几百个高维特征进行降维、相似计算、聚类等去重；通过大数据的处理进行个性化推荐，使用户无需设置，即可享受高质量信息 大数据系列文章第2篇——大数据之“用户行为分析”：http://36kr.com/p/205901.html 个性化推荐技术的十大挑战：http://www.programmer.com.cn/13824/ 个性化推荐系统的简单实现：http://www.slideshare.net/ssusera62527/ss-36914732 用Kiji构建实时、个性化推荐系统：http://www.infoq.com/cn/articles/kiji 一种基于LBS的移动个性化推荐系统：http://wenku.baidu.com/view/7f6bb028482fb4daa58d4b39.html 基于大规模隐式反馈的个性化推荐 http://www.jos.org.cn/html/2014/9/4648.htm 流式处理 江南白衣Calvin 写的《Storm笔记》，非常详细：http://calvin1978.blogcn.com/articles/stormnotes.html How Edmunds.com Used Spark Streaming to Build a Near Real-Time Dashboard Storm常见模式 Hadoop Streaming程序基础 http://blog.pureisle.net/archives/1760.html Hadoop Streaming 实战： 输出文件分割 http://blog.csdn.net/yfkiss/article/details/6406432 Hadoop Streaming原理及实践 http://shiyanjun.cn/archives/336.html Hadoop-Streaming实战经验及问题解决方法总结 http://www.crazyant.net/1122.html 总结：作为一个程序员，最重要的能力是自我学习、归纳、总结，知识在于总结而不是分享。如何把大量看到的、听到的信息、知识、笔记等转化为自己的经验值，是需要认真考虑的一件事情。]]></content>
      <categories>
        <category>work</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>mahout</tag>
        <tag>recommendation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark SQL中的DataFrame]]></title>
    <url>%2F2015%2F03%2F26%2Fspark-sql-dataframe%2F</url>
    <content type="text"><![CDATA[在2014年7月1日的 Spark Summit 上，Databricks 宣布终止对 Shark 的开发，将重点放到 Spark SQL 上。在会议上，Databricks 表示，Shark 更多是对 Hive 的改造，替换了 Hive 的物理执行引擎，因此会有一个很快的速度。然而，不容忽视的是，Shark 继承了大量的 Hive 代码，因此给优化和维护带来了大量的麻烦。随着性能优化和先进分析整合的进一步加深，基于 MapReduce 设计的部分无疑成为了整个项目的瓶颈。 详细内容请参看 Shark, Spark SQL, Hive on Spark, and the future of SQL on Spark。 Spark SQL 允许 Spark 执行用 SQL, HiveQL 或者 Scala 表示的关系查询。在 Spark 1.3 之前，这个模块的核心是一个新类型的 RDD-SchemaRDD。 SchemaRDDs 由行对象组成，行对象拥有一个模式（scheme） 来描述行中每一列的数据类型。SchemaRDD 与关系型数据库中的表很相似，可以通过存在的 RDD、一个 Parquet 文件、结构化的文件、外部数据库、或者对存储在 Apache Hive 中的数据执行 HiveSQL 查询中创建。 当前 Spark SQL 还处于 alpha 阶段，一些 API 在将将来的版本中可能会有所改变。例如，Apache Spark 1.3发布，新增Data Frames API，改进Spark SQL和MLlib。在 Spark 1.3 中，SchemaRDD 改为叫做 DataFrame。 本文是基于 Spark 1.3 写成，特此说明。 创建 SQLContextSpark SQL 中所有相关功能的入口点是 SQLContext 类或者它的子类， 创建一个 SQLContext 的所有需要仅仅是一个 SparkContext。 使用 Scala 创建方式如下： 12345val sc: SparkContext // An existing SparkContext.val sqlContext = new org.apache.spark.sql.SQLContext(sc)// this is used to implicitly convert an RDD to a DataFrame.import sqlContext.implicits._ 使用 Java 创建方式如下： 12JavaSparkContext sc = ...; // An existing JavaSparkContext.SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc); 使用 Python 创建方式如下： 12from pyspark.sql import SQLContextsqlContext = SQLContext(sc) 除了一个基本的 SQLContext，你也能够创建一个 HiveContext，它支持基本 SQLContext 所支持功能的一个超集。它的额外的功能包括用更完整的 HiveQL 分析器写查询去访问 HiveUDFs 的能力、 从 Hive 表读取数据的能力。用 HiveContext 你不需要一个已经存在的 Hive 开启，SQLContext 可用的数据源对 HiveContext 也可用。HiveContext 分开打包是为了避免在 Spark 构建时包含了所有 的 Hive 依赖。如果对你的应用程序来说，这些依赖不存在问题，Spark 1.3 推荐使用 HiveContext。以后的稳定版本将专注于为 SQLContext 提供与 HiveContext 等价的功能。 用来解析查询语句的特定 SQL 变种语言可以通过 spark.sql.dialect 选项来选择。这个参数可以通过两种方式改变，一种方式是通过 setConf 方法设定，另一种方式是在 SQL 命令中通过 SET key=value 来设定。对于 SQLContext，唯一可用的方言是 “sql”，它是 Spark SQL 提供的一个简单的 SQL 解析器。在 HiveContext 中，虽然也支持”sql”，但默认的方言是 “hiveql”，这是因为 HiveQL 解析器更完整。 创建 DataFrame使用 SQLContext，应用可以从一个存在的 RDD、Hive 表或者数据源中创建 DataFrame。 下载测试数据 people.json，并将其上传到 HDFS 上： 12$ wget https://raw.githubusercontent.com/apache/spark/master/examples/src/main/resources/people.json$ hadoop fs -put people.json 下面是使用 Scala 创建方式： 123456val sqlContext = new org.apache.spark.sql.SQLContext(sc)val df = sqlContext.jsonFile("people.json")// Displays the content of the DataFrame to stdoutdf.show() 下面是使用 Java 创建方式： 1234567JavaSparkContext sc = ...; // An existing JavaSparkContext.SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc);DataFrame df = sqlContext.jsonFile("people.json");// Displays the content of the DataFrame to stdoutdf.show(); 下面是使用 Python 创建方式： 1234567from pyspark.sql import SQLContextsqlContext = SQLContext(sc)df = sqlContext.jsonFile("people.json")# Displays the content of the DataFrame to stdoutdf.show() DataFrame API 请参考 Scala、Java 以及 Python。 DataFrame 操作运行 spark-shell 执行下面代码进行测试，运行的代码和输出结果如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879$ spark-shellSpark context available as sc.SQL context available as sqlContext.// Create the DataFramescala&gt; val df = sqlContext.jsonFile("people.json")scala&gt; df.count()res1: Long = 3scala&gt; df.first()res2: org.apache.spark.sql.Row = [null,Michael]scala&gt; df.head()res3: org.apache.spark.sql.Row = [null,Michael]scala&gt; df.collect()res4: Array[org.apache.spark.sql.Row] = Array([null,Michael], [30,Andy], [19,Justin])scala&gt; df.collectAsList()res5: java.util.List[org.apache.spark.sql.Row] = [[null,Michael], [30,Andy], [19,Justin]]// Show the content of the DataFramescala&gt; df.show()age namenull Michael30 Andy19 Justinscala&gt; df.take(2)res6: Array[org.apache.spark.sql.Row] = Array([null,Michael], [30,Andy])scala&gt; df.columnsres7: Array[String] = Array(age, name)scala&gt; df.dtypesres8: Array[(String, String)] = Array((age,LongType), (name,StringType))// Print the schema in a tree formatscala&gt; df.printSchema()root |-- age: long (nullable = true) |-- name: string (nullable = true)scala&gt; df.explain()== Physical Plan ==PhysicalRDD [age#0L,name#1], MapPartitionsRDD[96] at map at JsonRDD.scala:41// age columnscala&gt; val ageCol = df("age") // The following creates a new column that increases everybody's age by 10.scala&gt; df("age") + 10 // Select only the "name" columnscala&gt; df.select("name").show()nameMichaelAndyJustin// Select everybody, but increment the age by 1scala&gt; df.select(df("name"), df("age")+1).show()name (age + 1)Michael nullAndy 31Justin 20// Select people older than 21scala&gt; df.filter(df("age") &gt; 21).show()age name30 Andy// Count people by agescala&gt; df.groupBy("age").count().show()age countnull 119 130 1 运行 SQL 查询SQLContext 有一个 sql 方法，可以运行 SQL 查询。 1sqlContext.sql("SELECT * FROM table") Spark SQL 支持两种方法将存在的 RDD 转换为 DataFrame 。第一种方法使用反射来推断包含特定对象类型的 RDD 的模式。在你写 spark 程序的同时，当你已经知道了模式，这种基于反射的方法可以使代码更简洁并且程序工作得更好。 第二种方法是通过一个编程接口来实现，这个接口允许你构造一个模式，然后在存在的 RDD 上使用它。虽然这种方法更冗长，但是它允许你在运行期之前不知道列以及列的类型的情况下构造 DataFrame。 SQLContext 的 API 见 SQLContext 。 利用反射推断模式Spark SQL的 Scala 接口支持将包含样本类的 RDD 自动转换为 DataFrame。这个样本类定义了表的模式。样本类的参数名字通过反射来读取，然后作为列的名字。样本类可以嵌套或者包含复杂的类型如序列或者数组。这个 RDD 可以隐式转化为一个 DataFrame，然后注册为一个表，表可以在后续的 sql 语句中使用。 以 people.txt 作为测试数据，使用 Scala 语言来创建 DataFrame： 1234567891011121314151617181920// sc is an existing SparkContext.val sqlContext = new org.apache.spark.sql.SQLContext(sc)// this is used to implicitly convert an RDD to a DataFrame.import sqlContext.implicits._// Define the schema using a case class.// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,// you can use custom classes that implement the Product interface.case class People(name: String, age: Int)// Create an RDD of Person objects and register it as a table.val people = sc.textFile("people.txt").map(_.split(",")).map(p =&gt; People(p(0), p(1).trim.toInt)).toDF()people.registerTempTable("people")// SQL statements can be run by using the sql methods provided by sqlContext.val teenagers = sqlContext.sql("SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19")// The results of SQL queries are DataFrames and support all the normal RDD operations.// The columns of a row in the result can be accessed by ordinal.teenagers.map(t =&gt; "Name: " + t(0)).collect().foreach(println) 对于 Java 语言，需要创建一个 JavaBean，然后在将数据映射到它上面： 1234567891011121314151617181920public static class People implements Serializable &#123; private String name; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; 然后，使用 sqlContext 的 createDataFrame 方法，从 JavaBean 和数据上创建一个 DataFrame 并注册一个表，下面是一个比较完整的例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.Function;import org.apache.spark.sql.DataFrame;import org.apache.spark.sql.Row;import org.apache.spark.sql.SQLContext;import java.io.Serializable;import java.util.Arrays;import java.util.List;public class JavaSparkSQLByReflection &#123; public static void main(String[] args) throws Exception &#123; SparkConf sparkConf = new SparkConf().setAppName("JavaSparkSQLByReflection"); JavaSparkContext ctx = new JavaSparkContext(sparkConf); SQLContext sqlCtx = new SQLContext(ctx); System.out.println("=== Data source: RDD ==="); // Load a text file and convert each line to a Java Bean. JavaRDD&lt;People&gt; people = ctx.textFile("people.txt").map( new Function&lt;String, People&gt;() &#123; @Override public People call(String line) &#123; String[] parts = line.split(","); People people = new People(); people.setName(parts[0]); people.setAge(Integer.parseInt(parts[1].trim())); return people; &#125; &#125;); // Apply a schema to an RDD of Java Beans and register it as a table. DataFrame schemaPeople = sqlCtx.createDataFrame(people, People.class); schemaPeople.registerTempTable("people"); // SQL can be run over RDDs that have been registered as tables. DataFrame teenagers = sqlCtx.sql("SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19"); // The results of SQL queries are DataFrames and support all the normal RDD operations. // The columns of a row in the result can be accessed by ordinal. List&lt;String&gt; teenagerNames = teenagers.toJavaRDD().map(new Function&lt;Row, String&gt;() &#123; @Override public String call(Row row) &#123; return "Name: " + row.getString(0); &#125; &#125;).collect(); for (String name : teenagerNames) &#123; System.out.println(name); &#125; System.out.println("=== Data source: Parquet File ==="); // DataFrames can be saved as parquet files, maintaining the schema information. schemaPeople.saveAsParquetFile("people.parquet"); // Read in the parquet file created above. // Parquet files are self-describing so the schema is preserved. // The result of loading a parquet file is also a DataFrame. DataFrame parquetFile = sqlCtx.parquetFile("people.parquet"); //Parquet files can also be registered as tables and then used in SQL statements. parquetFile.registerTempTable("parquetFile"); DataFrame teenagers2 = sqlCtx.sql("SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 19"); teenagerNames = teenagers2.toJavaRDD().map(new Function&lt;Row, String&gt;() &#123; @Override public String call(Row row) &#123; return "Name: " + row.getString(0); &#125; &#125;).collect(); for (String name : teenagerNames) &#123; System.out.println(name); &#125; System.out.println("=== Data source: JSON Dataset ==="); // A JSON dataset is pointed by path. // The path can be either a single text file or a directory storing text files. String path = "people.json"; // Create a DataFrame from the file(s) pointed by path DataFrame peopleFromJsonFile = sqlCtx.jsonFile(path); // Because the schema of a JSON dataset is automatically inferred, to write queries, // it is better to take a look at what is the schema. peopleFromJsonFile.printSchema(); // The schema of people is ... // root // |-- age: IntegerType // |-- name: StringType // Register this DataFrame as a table. peopleFromJsonFile.registerTempTable("people"); // SQL statements can be run by using the sql methods provided by sqlCtx. DataFrame teenagers3 = sqlCtx.sql("SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19"); // The results of SQL queries are DataFrame and support all the normal RDD operations. // The columns of a row in the result can be accessed by ordinal. teenagerNames = teenagers3.toJavaRDD().map(new Function&lt;Row, String&gt;() &#123; @Override public String call(Row row) &#123; return "Name: " + row.getString(0); &#125; &#125;).collect(); for (String name : teenagerNames) &#123; System.out.println(name); &#125; // Alternatively, a DataFrame can be created for a JSON dataset represented by // a RDD[String] storing one JSON object per string. List&lt;String&gt; jsonData = Arrays.asList( "&#123;\"name\":\"Yin\",\"address\":&#123;\"city\":\"Columbus\",\"state\":\"Ohio\"&#125;&#125;"); JavaRDD&lt;String&gt; anotherPeopleRDD = ctx.parallelize(jsonData); DataFrame peopleFromJsonRDD = sqlCtx.jsonRDD(anotherPeopleRDD.rdd()); // Take a look at the schema of this new DataFrame. peopleFromJsonRDD.printSchema(); // The schema of anotherPeople is ... // root // |-- address: StructType // | |-- city: StringType // | |-- state: StringType // |-- name: StringType peopleFromJsonRDD.registerTempTable("people2"); DataFrame peopleWithCity = sqlCtx.sql("SELECT name, address.city FROM people2"); List&lt;String&gt; nameAndCity = peopleWithCity.toJavaRDD().map(new Function&lt;Row, String&gt;() &#123; @Override public String call(Row row) &#123; return "Name: " + row.getString(0) + ", City: " + row.getString(1); &#125; &#125;).collect(); for (String name : nameAndCity) &#123; System.out.println(name); &#125; ctx.stop(); &#125;&#125; 使用 Python 语言则需要用到 sqlContext 的 inferSchema 方法： 1234567891011121314151617181920# sc is an existing SparkContext.from pyspark.sql import SQLContext, RowsqlContext = SQLContext(sc)# Load a text file and convert each line to a Row.lines = sc.textFile("people.txt")parts = lines.map(lambda l: l.split(","))people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))# Infer the schema, and register the DataFrame as a table.schemaPeople = sqlContext.inferSchema(people)schemaPeople.registerTempTable("people")# SQL can be run over DataFrames that have been registered as a table.teenagers = sqlContext.sql("SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19")# The results of SQL queries are RDDs and support all the normal RDD operations.teenNames = teenagers.map(lambda p: "Name: " + p.name)for teenName in teenNames.collect(): print teenName 编程指定模式当样本类不能提前确定（例如，记录的结构是经过编码的字符串，或者一个文本集合将会被解析，不同的字段投影给不同的用户），一个 DataFrame 可以通过三步来创建。 从原来的 RDD 创建一个行的 RDD 创建由一个 StructType 表示的模式与第一步创建的 RDD 的行结构相匹配 在行 RDD 上通过 applySchema 方法应用模式 直接贴出代码，Scala 语言创建方式： 1234567891011121314151617181920212223242526272829303132val sc = new SparkContext(new SparkConf().setAppName("ScalaSparkSQL"))val sqlContext = new org.apache.spark.sql.SQLContext(sc)// Create an RDDval people = sc.textFile("people.txt")// The schema is encoded in a stringval schemaString = "name age"// Import Spark SQL data types and Row.import org.apache.spark.sql._// Generate the schema based on the string of schemaval schema = StructType( schemaString.split(" ").map(fieldName =&gt; StructField(fieldName, StringType, true)))// Convert records of the RDD (people) to Rows.val rowRDD = people.map(_.split(",")).map(p =&gt; Row(p(0), p(1).trim))// Apply the schema to the RDD.val peopleDataFrame = sqlContext.createDataFrame(rowRDD, schema)// Register the DataFrames as a table.peopleDataFrame.registerTempTable("people")// SQL statements can be run by using the sql methods provided by sqlContext.val results = sqlContext.sql("SELECT name FROM people")// The results of SQL queries are DataFrames and support all the normal RDD operations.// The columns of a row in the result can be accessed by ordinal.results.map(t =&gt; "Name: " + t(0)).collect().foreach(println) Java 创建的方式或许对一个 Java 程序员来说，更容易理解： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.Function;import org.apache.spark.sql.DataFrame;import org.apache.spark.sql.Row;import org.apache.spark.sql.SQLContext;import java.util.List;public class JavaSparkSQLBySchema &#123; public static void main(String[] args) throws Exception &#123; SparkConf sparkConf = new SparkConf().setAppName("JavaSparkSQLBySchema"); JavaSparkContext ctx = new JavaSparkContext(sparkConf); SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc); // Load a text file and convert each line to a JavaBean. JavaRDD&lt;String&gt; people = sc.textFile("people.txt"); // The schema is encoded in a string String schemaString = "name age"; // Generate the schema based on the string of schema List&lt;StructField&gt; fields = new ArrayList&lt;StructField&gt;(); for (String fieldName : schemaString.split(" ")) &#123; fields.add(DataType.createStructField(fieldName, DataType.StringType, true)); &#125; StructType schema = DataType.createStructType(fields); // Convert records of the RDD (people) to Rows. JavaRDD&lt;Row&gt; rowRDD = people.map( new Function&lt;String, Row&gt;() &#123; public Row call(String record) throws Exception &#123; String[] fields = record.split(","); return Row.create(fields[0], fields[1].trim()); &#125; &#125;); // Apply the schema to the RDD. DataFrame peopleDataFrame = sqlContext.createDataFrame(rowRDD, schema); // Register the DataFrame as a table. peopleDataFrame.registerTempTable("people"); // SQL can be run over RDDs that have been registered as tables. DataFrame results = sqlContext.sql("SELECT name FROM people"); // The results of SQL queries are DataFrames and support all the normal RDD operations. // The columns of a row in the result can be accessed by ordinal. List&lt;String&gt; names = results.map(new Function&lt;Row, String&gt;() &#123; public String call(Row row) &#123; return "Name: " + row.getString(0); &#125; &#125;).collect(); &#125;&#125; Python 语言的例子： 123456789101112131415161718192021222324252627282930# Import SQLContext and data typesfrom pyspark.sql import *# sc is an existing SparkContext.sqlContext = SQLContext(sc)# Load a text file and convert each line to a tuple.lines = sc.textFile("people.txt")parts = lines.map(lambda l: l.split(","))people = parts.map(lambda p: (p[0], p[1].strip()))# The schema is encoded in a string.schemaString = "name age"fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]schema = StructType(fields)# Apply the schema to the RDD.schemaPeople = sqlContext.createDataFrame(people, schema)# Register the DataFrame as a table.schemaPeople.registerTempTable("people")# SQL can be run over DataFrames that have been registered as a table.results = sqlContext.sql("SELECT name FROM people")# The results of SQL queries are RDDs and support all the normal RDD operations.names = results.map(lambda p: "Name: " + p.name)for name in names.collect(): print name 总结本文主要介绍了 DataFrame 是什么以及两种从 RDD 创建 DataFrame 的方法，完整的代码见 Github。 参考文章 Spark SQL and DataFrame Guide Spark 编程指南简体中文版-Spark SQL]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>spark-sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[将Avro数据转换为Parquet格式]]></title>
    <url>%2F2015%2F03%2F25%2Fconverting-avro-data-to-parquet-format%2F</url>
    <content type="text"><![CDATA[本文主要测试将Avro数据转换为Parquet格式的过程并查看 Parquet 文件的 schema 和元数据。 准备将文本数据转换为 Parquet 格式并读取内容，可以参考 Cloudera 的 MapReduce 例子：https://github.com/cloudera/parquet-examples。 准备文本数据 a.txt 为 CSV 格式： 1231,23,44,5 准备 Avro 测试数据，可以参考 将Avro数据加载到Spark 一文。 本文测试环境为：CDH 5.2，并且 Avro、Parquet 组件已经通过 YUM 源安装。 将 CSV 转换为 Parquet在 Hive 中创建一个表并导入数据： 123456create table mycsvtable (x int, y int)row format delimitedFIELDS TERMINATED BY ','STORED AS TEXTFILE;LOAD DATA LOCAL INPATH 'a.txt' OVERWRITE INTO TABLE mycsvtable; 创建 Parquet 表并转换数据： 12345create table myparquettable (a INT, b INT)STORED AS PARQUETLOCATION '/tmp/data';insert overwrite table myparquettable select * from mycsvtable; 查看 hdfs 上生成的 myparquettable 表的数据： 123$ hadoop fs -ls /tmp/dataFound 1 items-rwxrwxrwx 3 hive hadoop 331 2015-03-25 15:50 /tmp/data/000000_0 在 hive 中查看 myparquettable 表的数据： 1234567hive (default)&gt; select * from myparquettable;OKmyparquettable.a myparquettable.b1 23 44 5Time taken: 0.149 seconds, Fetched: 3 row(s) 查看 /tmp/data/000000_0 文件的 schema ： 12345$ hadoop parquet.tools.Main schema /tmp/data/000000_0message hive_schema &#123; optional int32 a; optional int32 b;&#125; 查看 /tmp/data/000000_0 文件的元数据： 123456789101112$ hadoop parquet.tools.Main meta /tmp/data/000000_0creator: parquet-mr version 1.5.0-cdh5.2.0 (build 8e266e052e423af5 [more]...file schema: hive_schema--------------------------------------------------------------------------------a: OPTIONAL INT32 R:0 D:1b: OPTIONAL INT32 R:0 D:1row group 1: RC:3 TS:102--------------------------------------------------------------------------------a: INT32 UNCOMPRESSED DO:0 FPO:4 SZ:51/51/1.00 VC:3 ENC:BIT [more]...b: INT32 UNCOMPRESSED DO:0 FPO:55 SZ:51/51/1.00 VC:3 ENC:BI [more]... 将 Avro 转换为 Parquet使用 将Avro数据加载到Spark 中的 schema 和 json 数据，从 json 数据生成 avro 数据： 1$ java -jar /usr/lib/avro/avro-tools.jar fromjson --schema-file twitter.avsc twitter.json &gt; twitter.avro 将 twitter.avsc 和 twitter.avro 上传到 hdfs： 12$ hadoop fs -put twitter.avsc$ hadoop fs -put twitter.avro 使用 https://github.com/laserson/avro2parquet 将 avro 转换为 parquet 格式： 1$ hadoop jar avro2parquet.jar twitter.avsc twitter.avro /tmp/out 然后，在 hive 中创建表并导入数据： 1234create table tweets_parquet (username string, tweet string, timestamp bigint) STORED AS PARQUET;load data inpath '/tmp/out/part-m-00000.snappy.parquet' overwrite into table tweets_parquet; 接下来，可以查询数据并查看 parquet 文件的 schema 和元数据，方法同上文。 参考文章 Converting Avro data to Parquet format in Hadoop]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>avro</tag>
        <tag>parquet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何将Avro数据加载到Spark]]></title>
    <url>%2F2015%2F03%2F24%2Fhow-to-load-some-avro-data-into-spark%2F</url>
    <content type="text"><![CDATA[这是一篇翻译，原文来自：How to load some Avro data into Spark。 首先，为什么使用 Avro ？最基本的格式是 CSV ，其廉价并且不需要顶一个一个 schema 和数据关联。 随后流行起来的一个通用的格式是 XML，其有一个 schema 和 数据关联，XML 广泛的使用于 Web Services 和 SOA 架构中。不幸的是，其非常冗长，并且解析 XML 需要消耗内存。 另外一种格式是 JSON，其非常流行易于使用因为它非常方便易于理解。 这些格式在 Big Data 环境中都是不可拆分的，这使得他们难于使用。在他们之上使用一个压缩机制（Snappy，Gzip）并不能解决这个问题。 因此不同的数据格式出现了。Avro 作为一种序列化平台被广泛使用，因为它能跨语言，提供了一个小巧紧凑的快速的二进制格式，支持动态 schema 发现（通过它的泛型）和 schema 演变，并且是可压缩和拆分的。它还提供了复杂的数据结构，例如嵌套类型。 例子让我们来看一个例子，创建一个 Avro schema 并生成一些数据。在一个真实案例的例子中，组织机构通常有一些更加普通的格式，例如 XML，的数据，并且他们需要通过一些工具例如 JAXB 将他们的数据转换成 Avro。我们来使用这个例子，其中 twitter.avsc 如下： 12345678910111213141516171819&#123; "type" : "record", "name" : "twitter_schema", "namespace" : "com.miguno.avro", "fields" : [ &#123; "name" : "username", "type" : "string", "doc" : "Name of the user account on Twitter.com" &#125;, &#123; "name" : "tweet", "type" : "string", "doc" : "The content of the user's Twitter message" &#125;, &#123; "name" : "timestamp", "type" : "long", "doc" : "Unix epoch time in seconds" &#125; ], "doc:" : "A basic schema for storing Twitter messages" &#125; twitter.json 中有一些数据： 12&#123;"username":"miguno","tweet":"Rock: Nerf paper, scissors is fine.","timestamp": 1366150681 &#125; &#123;"username":"BlizzardCS","tweet":"Works as intended. Terran is IMBA.","timestamp": 1366154481 &#125; 我们将这些数据转换成二进制的 Avro 格式： 1$ java -jar ~/avro-tools-1.7.7.jar fromjson --schema-file twitter.avsc twitter.json &gt; twitter.avro 然后，我们将 Avro 数据转换为 Java： 1$ java -jar /app/avro/avro-tools-1.7.7.jar compile schema /app/avro/data/twitter.avsc /app/avro/data/ 现在，我们编译这些类并将其打包： 123$ CLASSPATH=/app/avro/avro-1.7.7-javadoc.jar:/app/avro/avro-mapred-1.7.7-hadoop1.jar:/app/avro/avro-tools-1.7.7.jar$ javac -classpath $CLASSPATH /app/avro/data/com/miguno/avro/twitter_schema.java$ jar cvf Twitter.jar com/miguno/avro/*.class 我们启动 Spark，并将上面创建的 Jar 和一些需要的库（Hadoop 和 Avro）传递给 Spark 程序： 1$ ./bin/spark-shell --jars /app/avro/avro-mapred-1.7.7-hadoop1.jar,/avro/avro-1.7.7.jar,/app/avro/data/Twitter.jar 在 REPL 中，我们获取数据并创建一个 RDD： 1234567891011121314151617181920scala&gt;import com.miguno.avro.twitter_schemaimport org.apache.avro.file.DataFileReader;import org.apache.avro.file.DataFileWriter;import org.apache.avro.io.DatumReader;import org.apache.avro.io.DatumWriter;import org.apache.avro.specific.SpecificDatumReader;import org.apache.avro.mapreduce.AvroKeyInputFormatimport org.apache.avro.mapred.AvroKeyimport org.apache.hadoop.io.NullWritableimport org.apache.avro.mapred.AvroInputFormatimport org.apache.avro.mapred.AvroWrapperimport org.apache.avro.generic.GenericRecordimport org.apache.avro.mapred.&#123;AvroInputFormat, AvroWrapper&#125;import org.apache.hadoop.io.NullWritableval path = &quot;/app/avro/data/twitter.avro&quot;val avroRDD = sc.hadoopFile[AvroWrapper[GenericRecord], NullWritable, AvroInputFormat[GenericRecord]](path)avroRDD.map(l =&gt; new String(l._1.datum.get(&quot;username&quot;).toString() ) ).first 返回结果： 1res2: String = miguno 一些注意事项： 我们在使用 MR1 的类，但是 MR2的类同样能够运行。 我们使用GenericRecord 而不是 Specific ，因为我们生成了 Avro schema（并且导入了它）。更多内容参见 http://avro.apache.org/docs/current/gettingstartedjava.html 注意到即使 Avro 类是用 Java 编译的，你还是可以在 Spark 中导入他们，因为 Scala 也是运行在 JVM 之上。 Avro 允许你定义一个可选的方式去定义 schema 中每个节点的反序列化类型，即通过 key/value 的键值对，这是方式非常方便。参考 http://stackoverflow.com/questions/27827649/trying-to-deserialize-avro-in-spark-with-specific-type/27859980?noredirect=1%23comment44240726_27859980 。 还有大量的其他方式来实现这个功能，一种是使用 Kryo，另一种是使用 Spark SQL。然而，这需要你创建一个 Spark SQL 的上下文（见 https://github.com/databricks/spark-avro ），而不是一个纯粹的 Spark/Scala 方式。然而，也许这在将来会是一种最佳方式？ 翻译结束。 接下来，我将上述过程在 CDH 5.3 集群中测试一遍。 验证首先，在集群一个节点创建 twitter.avsc 和 twitter.json 两个文件。 然后，使用 avro-tools 将这些数据转换成二进制的 Avro 格式： 1$ java -jar /usr/lib/avro/avro-tools.jar fromjson --schema-file twitter.avsc twitter.json &gt; twitter.avro 这时候会生成 avro 文件： 12345$ ll总用量 12-rw-r--r-- 1 root root 543 3月 25 15:13 twitter.avro-rw-r--r-- 1 root root 590 3月 25 15:12 twitter.avsc-rw-r--r-- 1 root root 191 3月 25 15:12 twitter.json 将 Avro 数据转换为 Java： 1$ java -jar /usr/lib/avro/avro-tools.jar compile schema twitter.avsc . 这时候会生成 twitter_schema.java 文件： 123456789$ tree.├── com│ └── miguno│ └── avro│ └── twitter_schema.java├── twitter.avro├── twitter.avsc└── twitter.json 这时候会生成一个 Twitter.jar 的 jar 包。 编译这些类并将其打包： 123$ CLASSPATH=/usr/lib/avro/avro-mapred-hadoop2.jar:/usr/lib/avro/avro-tools.jar$ javac -classpath $CLASSPATH com/miguno/avro/twitter_schema.java$ jar cvf Twitter.jar com/miguno/avro/*.class 在当前目录，运行 spark-shell: 1spark-shell --jars /usr/lib/avro/avro-mapred-hadoop2.jar,/usr/lib/avro/avro.jar,Twitter.jar 将 twitter.avro 上传到 hdfs: 1hadoop fs -put twitter.avro 在 REPL 中，我们创建一个 RDD 并查看结果是否和上面一致： 1234567891011121314151617181920scala&gt;import com.miguno.avro.twitter_schema;import org.apache.avro.file.DataFileReader;import org.apache.avro.file.DataFileWriter;import org.apache.avro.io.DatumReader;import org.apache.avro.io.DatumWriter;import org.apache.avro.specific.SpecificDatumReader;import org.apache.avro.mapreduce.AvroKeyInputFormatimport org.apache.avro.mapred.AvroKeyimport org.apache.hadoop.io.NullWritableimport org.apache.avro.mapred.AvroInputFormatimport org.apache.avro.mapred.AvroWrapperimport org.apache.avro.generic.GenericRecordimport org.apache.avro.mapred.&#123;AvroInputFormat, AvroWrapper&#125;import org.apache.hadoop.io.NullWritableval path = &quot;twitter.avro&quot;val avroRDD = sc.hadoopFile[AvroWrapper[GenericRecord], NullWritable, AvroInputFormat[GenericRecord]](path)avroRDD.map(l =&gt; new String(l._1.datum.get(&quot;username&quot;).toString() ) ).first 更多的 Avro Tools 用法，可以参考 Avro 介绍。]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>avro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Avro介绍]]></title>
    <url>%2F2015%2F03%2F20%2Fabout-avro%2F</url>
    <content type="text"><![CDATA[1. 介绍Avro 是 Hadoop 中的一个子项目，也是 Apache 中一个独立的项目，Avro 是一个基于二进制数据传输高性能的中间件。在 Hadoop 的其他项目中，例如 HBase 和 Hive 的 Client 端与服务端的数据传输也采用了这个工具。Avro 是一个数据序列化的系统，它可以提供： 1、丰富的数据结构类型 2、快速可压缩的二进制数据形式 3、存储持久数据的文件容器 4、远程过程调用 RPC 5、简单的动态语言结合功能，Avro 和动态语言结合后，读写数据文件和使用 RPC 协议都不需要生成代码，而代码生成作为一种可选的优化只值得在静态类型语言中实现。 Avro 支持跨编程语言实现（C, C++, C#，Java, Python, Ruby, PHP），Avro 提供着与诸如 Thrift 和 Protocol Buffers 等系统相似的功能，但是在一些基础方面还是有区别的，主要是： 1、动态类型：Avro 并不需要生成代码，模式和数据存放在一起，而模式使得整个数据的处理过程并不生成代码、静态数据类型等等。这方便了数据处理系统和语言的构造。 2、未标记的数据：由于读取数据的时候模式是已知的，那么需要和数据一起编码的类型信息就很少了，这样序列化的规模也就小了。 3、不需要用户指定字段号：即使模式改变，处理数据时新旧模式都是已知的，所以通过使用字段名称可以解决差异问题。 Avro 和动态语言结合后，读/写数据文件和使用 RPC 协议都不需要生成代码，而代码生成作为一种可选的优化只需要在静态类型语言中实现。 当在 RPC 中使用 Avro 时，服务器和客户端可以在握手连接时交换模式。服务器和客户端有着彼此全部的模式，因此相同命名字段、缺失字段和多余字段等信息之间通信中需要解决的一致性问题就可以容易解决。 还有，Avro 模式是用 JSON（一种轻量级的数据交换模式）定义的，这样对于已经拥有 JSON 库的语言可以容易实现。 2. SchemaSchema 通过 JSON 对象表示。Schema 定义了简单数据类型和复杂数据类型，其中复杂数据类型包含不同属性。通过各种数据类型用户可以自定义丰富的数据结构。 基本类型有： 类型 说明 null no value boolean a binary value int 32-bit signed integer long 64-bit signed integer float single precision (32-bit) IEEE 754 floating-point number double double precision (64-bit) IEEE 754 floating-point number bytes sequence of 8-bit unsigned bytes string unicode character sequence Avro定义了六种复杂数据类型： Record：record 类型，任意类型的一个命名字段集合，JSON对象表示。支持以下属性： name：名称，必须 namespace doc aliases fields：一个 JSON 数组，必须 name doc type default order aliases Enum：enum 类型，支持以下属性： name：名称，必须 namespace doc aliases symbols：枚举值，必须 Array：array 类型，未排序的对象集合，对象的模式必须相同。支持以下属性： items Map：map 类型，未排序的对象键/值对。键必须是字符串，值可以是任何类型，但必须模式相同。支持以下属性： values Fixed：fixed 类型，一组固定数量的8位无符号字节。支持以下属性： name：名称，必须 namespace size：每个值的 byte 长度 aliases Union：union 类型，模式的并集，可以用JSON数组表示，每个元素为一个模式。 每一种复杂数据类型都含有各自的一些属性，其中部分属性是必需的，部分是可选的。 举例，一个 linked-list of 64-bit 的值： 123456789&#123; "type": "record", "name": "LongList", "aliases": ["LinkedLongs"], // old name for this "fields" : [ &#123;"name": "value", "type": "long"&#125;, // each element has a long &#123;"name": "next", "type": ["null", "LongList"]&#125; // optional next element ]&#125; 一个 enum 类型的： 1234&#123; "type": "enum", "name": "Suit", "symbols" : ["SPADES", "HEARTS", "DIAMONDS", "CLUBS"]&#125; array 类型： 1&#123;"type": "array", "items": "string"&#125; map 类型： 1&#123;"type": "map", "values": "long"&#125; fixed 类型： 1&#123;"type": "fixed", "size": 16, "name": "md5"&#125; 这里需要说明Record类型中field属性的默认值，当Record Schema实例数据中某个field属性没有提供实例数据时，则由默认值提供，具体值见下表。Union的field默认值由Union定义中的第一个Schema决定。 avro type json type example null null null boolean boolean true int,long integer 1 float,double number 1.1 bytes string “\u00FF” string string “foo” record object {“a”: 1} enum string “FOO” array array [1] map object {“a”: 1} fixed string “\u00ff” 3. 序列化/反序列化Avro 指定两种数据序列化编码方式：binary encoding 和 Json encoding。使用二进制编码会高效序列化，并且序列化后得到的结果会比较小；而 JSON 一般用于调试系统或是基于 WEB 的应用。 TODO 4. Avro ToolsAvro Tools 不加参数时: 123456789101112131415161718192021222324252627282930313233343536$ java -jar /usr/lib/avro/avro-tools.jarVersion 1.7.6-cdh5.2.0 of Apache AvroCopyright 2010 The Apache Software FoundationThis product includes software developed atThe Apache Software Foundation (http://www.apache.org/).C JSON parsing provided by Jansson andwritten by Petri Lehtinen. The original software isavailable from http://www.digip.org/jansson/.----------------Available tools: cat extracts samples from files compile Generates Java code for the given schema. concat Concatenates avro files without re-compressing. fragtojson Renders a binary-encoded Avro datum as JSON. fromjson Reads JSON records and writes an Avro data file. fromtext Imports a text file into an avro data file. getmeta Prints out the metadata of an Avro data file. getschema Prints out schema of an Avro data file. idl Generates a JSON schema from an Avro IDL file idl2schemata Extract JSON schemata of the types from an Avro IDL file induce Induce schema/protocol from Java class/interface via reflection. jsontofrag Renders a JSON-encoded Avro datum as binary. random Creates a file with randomly generated instances of a schema. recodec Alters the codec of a data file. rpcprotocol Output the protocol of a RPC service rpcreceive Opens an RPC Server and listens for one message. rpcsend Sends a single RPC message. tether Run a tethered mapreduce job. tojson Dumps an Avro data file as JSON, record per line or pretty. totext Converts an Avro data file to a text file. totrevni Converts an Avro data file to a Trevni file. trevni_meta Dumps a Trevni file&apos;s metadata as JSON.trevni_random Create a Trevni file filled with random instances of a schema.trevni_tojson Dumps a Trevni file as JSON. fromjson 命令语法如下： 123456789$ java -jar /usr/lib/avro/avro-tools.jar fromjsonExpected 1 arg: input_fileOption Description------ -------------codec Compression codec (default: null)--level &lt;Integer&gt; Compression level (only applies to deflate and xz) (default: -1)--schema Schema--schema-file Schema File 以 将Avro数据加载到Spark 为例，将 json 数据转换为 avro 数据： 1$ java -jar /usr/lib/avro/avro-tools.jar fromjson --schema-file twitter.avsc twitter.json &gt; twitter.avro 设置压缩格式： 1$ java -jar /usr/lib/avro/avro-tools.jar fromjson --codec snappy --schema-file twitter.avsc twitter.json &gt; twitter.snappy.avro 将 avro 转换为 json： 12$ java -jar /usr/lib/avro/avro-tools.jar tojson twitter.avro &gt; twitter.json$ java -jar /usr/lib/avro/avro-tools.jar tojson twitter.snappy.avro &gt; twitter.json 获取 avro 文件的 schema： 12$ java -jar /usr/lib/avro/avro-tools.jar getschema twitter.avro &gt; twitter.avsc$ java -jar /usr/lib/avro/avro-tools.jar getschema twitter.snappy.avro &gt; twitter.avsc 将 Avro 数据编译为 Java： 1$ java -jar /usr/lib/avro/avro-tools.jar compile schema twitter.avsc . 5. 文件结构TODO 6. 参考文章 Avro简介 Reading and Writing Avro Files From the Command Line]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>avro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装和测试Kafka]]></title>
    <url>%2F2015%2F03%2F17%2Finstall-and-test-kafka%2F</url>
    <content type="text"><![CDATA[本文主要介绍如何在单节点上安装 Kafka 并测试 broker、producer 和 consumer 功能。 下载进入下载页面：http://kafka.apache.org/downloads.html ，选择 Binary downloads下载 （Source download需要编译才能使用），这里我下载 kafka_2.11-0.8.2.1，其对应的 Scala 版本为 2.11： 1$ wget http://apache.fayea.com/kafka/0.8.2.1/kafka_2.11-0.8.2.1.tgz 解压并进入目录： 12$ tar -xzvf kafka_2.11-0.8.2.1.tgz$ cd kafka_2.11-0.8.2.1 查看目录结构： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253tree -L 2.├── bin│ ├── kafka-console-consumer.sh│ ├── kafka-console-producer.sh│ ├── kafka-consumer-offset-checker.sh│ ├── kafka-consumer-perf-test.sh│ ├── kafka-mirror-maker.sh│ ├── kafka-preferred-replica-election.sh│ ├── kafka-producer-perf-test.sh│ ├── kafka-reassign-partitions.sh│ ├── kafka-replay-log-producer.sh│ ├── kafka-replica-verification.sh│ ├── kafka-run-class.sh│ ├── kafka-server-start.sh│ ├── kafka-server-stop.sh│ ├── kafka-simple-consumer-shell.sh│ ├── kafka-topics.sh│ ├── windows│ ├── zookeeper-server-start.sh│ ├── zookeeper-server-stop.sh│ └── zookeeper-shell.sh├── config│ ├── consumer.properties│ ├── log4j.properties│ ├── producer.properties│ ├── server.properties│ ├── test-log4j.properties│ ├── tools-log4j.properties│ └── zookeeper.properties├── libs│ ├── jopt-simple-3.2.jar│ ├── kafka_2.11-0.8.2.1.jar│ ├── kafka_2.11-0.8.2.1-javadoc.jar│ ├── kafka_2.11-0.8.2.1-scaladoc.jar│ ├── kafka_2.11-0.8.2.1-sources.jar│ ├── kafka_2.11-0.8.2.1-test.jar│ ├── kafka-clients-0.8.2.1.jar│ ├── log4j-1.2.16.jar│ ├── lz4-1.2.0.jar│ ├── metrics-core-2.2.0.jar│ ├── scala-library-2.11.5.jar│ ├── scala-parser-combinators_2.11-1.0.2.jar│ ├── scala-xml_2.11-1.0.2.jar│ ├── slf4j-api-1.7.6.jar│ ├── slf4j-log4j12-1.6.1.jar│ ├── snappy-java-1.1.1.6.jar│ ├── zkclient-0.3.jar│ └── zookeeper-3.4.6.jar├── LICENSE└── NOTICE4 directories, 45 files 启动和停止运行 kafka ，需要依赖 zookeeper，你可以使用已有的 zookeeper 集群或者利用 kafka 提供的脚本启动一个 zookeeper 实例： 1$ bin/zookeeper-server-start.sh config/zookeeper.properties &amp; 默认的，zookeeper 会监听在 *:2181/tcp。 停止刚才启动的 zookeeper 实例： 1$ bin/zookeeper-server-stop.sh 启动Kafka server: 1$ bin/kafka-server-start.sh config/server.properties &amp; config/server.properties 中有一些默认的配置参数，这里仅仅列出参数，不做解释： 12345678910111213141516171819202122232425262728293031323334353637383940414243broker.id=0port=9092#host.name=localhost#advertised.host.name=&lt;hostname routable by clients&gt;#advertised.port=&lt;port accessible by clients&gt;num.network.threads=3num.io.threads=8socket.send.buffer.bytes=102400socket.receive.buffer.bytes=102400socket.request.max.bytes=104857600log.dirs=/tmp/kafka-logsnum.partitions=1num.recovery.threads.per.data.dir=1#log.flush.interval.messages=10000#log.flush.interval.ms=1000log.retention.hours=168#log.retention.bytes=1073741824log.segment.bytes=1073741824log.retention.check.interval.ms=300000log.cleaner.enable=falsezookeeper.connect=localhost:2181zookeeper.connection.timeout.ms=6000 如果你像我一样是在虚拟机中测试 kafka，那么你需要修改 kafka 启动参数中 JVM 内存大小。查看 kafka-server-start.sh 脚本，修改 KAFKA_HEAP_OPTS 处 -Xmx 和 -Xms 的值。 启动成功之后，会看到如下日志： 12345678[2015-03-17 11:19:30,528] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)[2015-03-17 11:19:30,604] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)[2015-03-17 11:19:30,605] INFO [Socket Server on Broker 0], Started (kafka.network.SocketServer)[2015-03-17 11:19:30,687] INFO Will not load MX4J, mx4j-tools.jar is not in the classpath (kafka.utils.Mx4jLoader$)[2015-03-17 11:19:30,756] INFO 0 successfully elected as leader (kafka.server.ZookeeperLeaderElector)[2015-03-17 11:19:30,887] INFO Registered broker 0 at path /brokers/ids/0 with address cdh1:9092. (kafka.utils.ZkUtils$)[2015-03-17 11:19:30,928] INFO [Kafka Server 0], started (kafka.server.KafkaServer)[2015-03-17 11:19:31,048] INFO New leader is 0 (kafka.server.ZookeeperLeaderElector$LeaderChangeListener) 从日志可以看到： log flusher 有一个默认的周期值 kafka server 监听在9092端口 在 cdh1:9092 上注册了一个 broker 0 ，路径为 /brokers/ids/0 停止 Kafka server : 1$ bin/kafka-server-stop.sh 单 broker 测试在启动 kafka-server 之后启动，运行producer： 1$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 在另一个终端运行 consumer： 1$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning 在 producer 端输入字符串并回车，查看 consumer 端是否显示。 多 broker 测试配置和启动 Kafka broker接下来参考 Running a Multi-Broker Apache Kafka 0.8 Cluster on a Single Node 这篇文章，基于 config/server.properties 配置文件创建多个 broker 的 kafka 集群。 创建第一个 broker： 1$ cp config/server.properties config/server1.properties 编写 config/server1.properties 并修改下面配置： 123broker.id=1port=9092log.dir=/tmp/kafka-logs-1 创建第二个 broker： 1$ cp config/server.properties config/server2.properties 编写 config/server2.properties 并修改下面配置： 123broker.id=2port=9093log.dir=/tmp/kafka-logs-2 创建第三个 broker： 1$ cp config/server.properties config/server3.properties 编写 config/server3.properties 并修改下面配置： 123broker.id=3port=9094log.dir=/tmp/kafka-logs-3 接下来分别启动这三个 broker： 123$ JMX_PORT=9999 ; nohup bin/kafka-server-start.sh config/server1.properties &amp;$ JMX_PORT=10000 ; nohup bin/kafka-server-start.sh config/server2.properties &amp;$ JMX_PORT=10001 ; nohup bin/kafka-server-start.sh config/server3.properties &amp; 下面是三个 broker 监听的网络接口和端口列表： 1234 Broker 1 Broker 2 Broker 3----------------------------------------------Kafka *:9092/tcp *:9093/tcp *:9094/tcpJMX *:9999/tcp *:10000/tcp *:10001/tcp 创建 Kafka topic在 Kafka 0.8 中有两种方式创建一个新的 topic： 在 broker 上开启 auto.create.topics.enable 参数，当 broker 接收到一个新的 topic 上的消息时候，会通过 num.partitions 和 default.replication.factor 两个参数自动创建 topic。 使用 bin/kafka-topics.sh 命令 创建一个名称为 zerg.hydra 的 topic： 1$ bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic zerg.hydra --partitions 3 --replication-factor 2 使用下面查看创建的 topic: 123$ bin/kafka-topics.sh --zookeeper localhost:2181 --listtestzerg.hydra 还可以查看更详细的信息： 12345$ bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic zerg.hydraTopic:zerg.hydra PartitionCount:3 ReplicationFactor:2 Configs: Topic: zerg.hydra Partition: 0 Leader: 2 Replicas: 2,3 Isr: 2,3 Topic: zerg.hydra Partition: 1 Leader: 3 Replicas: 3,0 Isr: 3,0 Topic: zerg.hydra Partition: 2 Leader: 0 Replicas: 0,2 Isr: 0,2 默认的，Kafka 持久化 topic 到 log.dir 参数定义的目录。 1234567891011121314151617181920212223242526272829$ tree /tmp/kafka-logs-&#123;1,2,3&#125;/tmp/kafka-logs-1 # first broker (broker.id = 1)├── zerg.hydra-0 # replica of partition 0 of topic "zerg.hydra" (this broker is leader)│ ├── 00000000000000000000.index│ └── 00000000000000000000.log├── zerg.hydra-2 # replica of partition 2 of topic "zerg.hydra"│ ├── 00000000000000000000.index│ └── 00000000000000000000.log└── replication-offset-checkpoint/tmp/kafka-logs-2 # second broker (broker.id = 2)├── zerg.hydra-0 # replica of partition 0 of topic "zerg.hydra"│ ├── 00000000000000000000.index│ └── 00000000000000000000.log├── zerg.hydra-1 # replica of partition 1 of topic "zerg.hydra" (this broker is leader)│ ├── 00000000000000000000.index│ └── 00000000000000000000.log└── replication-offset-checkpoint/tmp/kafka-logs-3 # third broker (broker.id = 3)├── zerg.hydra-1 # replica of partition 1 of topic "zerg.hydra"│ ├── 00000000000000000000.index│ └── 00000000000000000000.log├── zerg.hydra-2 # replica of partition 2 of topic "zerg.hydra" (this broker is leader)│ ├── 00000000000000000000.index│ └── 00000000000000000000.log└── replication-offset-checkpoint6 directories, 15 files 启动一个 producer以 sync 模式启动一个 producer： 1$ bin/kafka-console-producer.sh --broker-list localhost:9092,localhost:9093,localhost:9094 --sync --topic zerg.hydra 然后，输入以下内容： 12Hello, world!Rock: Nerf Paper. Scissors is fine. 启动一个 consumer在另一个终端运行： 1$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic zerg.hydra --from-beginning 注意，生产环境通常不会添加 --from-beginning 参数。 观察输出，你会看到下面内容： 12Hello, world!Rock: Nerf Paper. Scissors is fine. 把 consumer 停掉再启动，你还会看到相同的输出结果。 将日志推送到 kafka例如，将 apache 或者 nginx 或者 tomcat 等产生的日志 push 到 kafka，只需要执行下面代码即可： 1$ tail -n 0 -f /var/log/nginx/access.log | bin/kafka-console-producer.sh --broker-list localhost:9092,localhost:9093,localhost:9094 --sync --topic zerg.hydra 参考文章 Running a Multi-Broker Apache Kafka 0.8 Cluster on a Single Node spark读取 kafka nginx网站日志消息 并写入HDFS中]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot之一：如何运行项目]]></title>
    <url>%2F2015%2F03%2F13%2Fhow-to-run-spring-boot-application%2F</url>
    <content type="text"><![CDATA[介绍Spring Boot 是 Spring 产品中一个新的子项目，致力于简便快捷地搭建基于 Spring 的独立可运行的应用。大多数的 Spring Boot 应用只需要非常少的 Spring 配置。 你能够使用 Spring Boot 创建 Java 应用并通过 java -jar 来运行或者创建传统的通过 war 来部署的应用。Spring Boot 也提供了一个命令行工具来运行 spring 脚本。 Spring Boot 的目标是： 快速开发基于 Spring 的应用 开箱即用的微服务 提供一些大型项目常用的非功能性特性，例如：嵌入式服务、安全、监控、健康检查、外部配置 不用生成代码，没有 xml 配置 系统要求Spring Boot 的最新发布版本为 1.3.2.RELEASE，其需要 Java 7 和 Spring 4.1.3 或以上版本才能运行。当然，你也可以添加一些配置使其能够使用 Java 6。虽然，你可以使用 Java 6 或7 来运行 Spring Boot，但是，在可能的情况下，更建议你使用 Java 8。编译 Spring Boot 需要 Maven 3.2+ 或者 Gradle 1.12+。 Servlet 容易要求： Name Servlet Version Java Version Tomcat 8 3.1 Java 7+ Tomcat 7 3.0 Java 6+ Jetty 9 3.1 Java 7+ Jetty 8 3.0 Java 6+ Undertow 1.1 3.1 Java 7+ 你也可以在兼容 Servlet 3.0+ 的容器中部署 Spring Boot 应用。 安装首先，需要安装 Java 并确认版本是否满足要求。 Maven安装Spring Boot 可以使用 Maven3.2 以上版本进行编译。使用压缩包解压安装 Maven 或者通过命令安装，例如，在 Mac 上安装： 1$ brew install maven 要创建一个 Spring Boot 的项目，你需要创建一个 Maven 的 POM 文件并继承 spring-boot-starter-parent 项目，然后添加一些 Spring Boot 子项目的依赖，你也可以使用 spring-boot-maven-plugin 插件来创建可执行的 jar 包。 使用maven创建一个工程： 1$ mvn archetype:generate -DgroupId=com.javachen -DartifactId=spring-boot-hello -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveModel=false 删除掉创建的App类和单元测试。 然后修改 pom.xml 为如下： 12345678910111213141516171819202122232425262728293031323334&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.javachen&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-hello&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;!-- Inherit defaults from Spring Boot --&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.20.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;!-- Add typical dependencies for a web application --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- Package as an executable jar --&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 如果你不想继承 spring-boot-starter-parent 父项目，你也可以使用依赖管理来引入对 Spring Boot 的依赖。 123456789101112&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;!-- Import dependency management from Spring Boot --&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;1.5.20.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 还可以通过下面方式修改 Maven 使用的 Java 版本： 123&lt;properties&gt; &lt;java.version&gt;1.7&lt;/java.version&gt;&lt;/properties&gt; 编译代码： 1$ mvn compile Gradle 安装Spring Boot兼容Gradle 1.12或更高版本。如果没有安装Gradle，你可以参考 [www.gradle.org] 上的指南。 在上面通过maven创建的目录spring-boot-example中，新建一个 build.gradle 文件如下： 1234567891011121314151617181920212223242526272829buildscript &#123; repositories &#123; jcenter() maven &#123; url "http://repo.spring.io/snapshot" &#125; maven &#123; url "http://repo.spring.io/milestone" &#125; &#125; dependencies &#123; classpath("org.springframework.boot:spring-boot-gradle-plugin:1.3.2.RELEASE") &#125;&#125;apply plugin: 'java'apply plugin: 'spring-boot'jar &#123; baseName = 'spring-boot-example' version = '0.0.1-SNAPSHOT'&#125;repositories &#123; jcenter() maven &#123; url "http://repo.spring.io/snapshot" &#125; maven &#123; url "http://repo.spring.io/milestone" &#125;&#125;dependencies &#123; compile("org.springframework.boot:spring-boot-starter-web") testCompile("org.springframework.boot:spring-boot-starter-test")&#125; 编译代码： 1$ gradle build Spring Boot CLI 使用方法安装 Spring Boot CLI你可以手动安装：[https://github.com/spring-projects/spring-boot/releases] 或者使用 GVM 安装： 123$ gvm install springboot$ spring --versionSpring Boot v1.5.20.RELEASE 如果你使用 Mac，则可以通过 Homebrew 或者 MacPorts 安装： 12$ brew tap pivotal/tap$ brew install springboot 1$ sudo port install spring-boot-cli spring 命令用法： 12345678910$ springusage: spring [--help] [--version] &lt;command&gt; [&lt;args&gt;]Available commands are: run [options] &lt;files&gt; [--] [args] Run a spring groovy script ... more command help is shown here 使用 你可以很快创建一个简单的 web 应用来测试是否安装成功。创建 hello.groovy 文件，内容如下： 12345678@RestControllerclass ThisWillActuallyRun &#123; @RequestMapping("/") String home() &#123; "Hello World!" &#125;&#125; 然后，运行下面命令： 1$ spring run hello.groovy 使用浏览器打开 http://localhost:8080/ 你会看到下面的输出： 1Hello World! 你还可以传递一些命令行的参数，使用 -- 分隔命令行参数，下面命令修改服务启动端口为 9000： 1$ spring run hello.groovy -- --server.port=9000 设置 JVM 参数之后，在运行： 12$ JAVA_OPTS=-Xmx1024m $ spring run hello.groovy 部署 Spring Boot 应用Maven 项目如果你使用 Maven 编译项目，则你可以通过下面命令查看项目依赖： 1$ mvn dependency:tree 在src/main/java/com/javachen/下面添加HelloWorld的Controller： 1234567891011121314151617181920package com.javachen.controller;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;import java.util.HashMap;import java.util.Map;@Controllerpublic class HelloController &#123; @ResponseBody @RequestMapping("/hello") public Map&lt;String, Object&gt; showHelloWork() &#123; Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); map.put("msg", "Hello World ！"); return map; &#125;&#125; 在src/main/java/com/javachen下面添加启动类： 1234567891011121314package com.javachen;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;/** * spring boot启动类 */@SpringBootApplicationpublic class App &#123; public static void main( String[] args ) &#123; SpringApplication.run(App.class, args); &#125;&#125; 注意： 启动器可以和controller位于同一个包下，或者位于controller的上一级包中，但是不能放到controller的平级以及子包下。如果你的 pom.xml 使用了 spring-boot-starter-parent，则我们可以运行 mvn spring-boot:run 命令启动应用： 1$ mvn spring-boot:run 然后，使用浏览器打开 http://localhost:8080/ 你会看到下面的输出： 1&#123;"msg":"Hello World ！"&#125; 点击ctrl-c，可以关闭应用程序。 添加-Ddebug参数可以查看加载了哪些配置类： 123456789101112131415$ mvn spring-boot:run -Ddebug…=========================AUTO-CONFIGURATION REPORT=========================Positive matches:----------------- DispatcherServletAutoConfiguration - @ConditionalOnClass classes found: org.springframework.web.servlet.DispatcherServlet (OnClassCondition) - found web application StandardServletEnvironment (OnWebApplicationCondition)... 加载了太多的配置类，会影响程序性能，故可以自定义导入需要的配置类： 123456789101112131415161718@Configuration@Import(&#123; DispatcherServletAutoConfiguration.class, EmbeddedServletContainerAutoConfiguration.class, ErrorMvcAutoConfiguration.class, HttpEncodingAutoConfiguration.class, HttpMessageConvertersAutoConfiguration.class, JacksonAutoConfiguration.class, JmxAutoConfiguration.class, MultipartAutoConfiguration.class, ServerPropertiesAutoConfiguration.class, PropertyPlaceholderAutoConfiguration.class, ThymeleafAutoConfiguration.class, WebMvcAutoConfiguration.class, WebSocketAutoConfiguration.class,&#125;)public class App &#123;&#125; 如果你的 pom.xml 中添加了 spring-boot-maven-plugin 插件，你可以运行 mvn package 命令在 target 目录生成一个可执行的 jar 文件： 1234567891011121314151617181920212223242526272829303132333435➜ mvn package[INFO] Scanning for projects...[INFO][INFO] ------------------------------------------------------------------------[INFO] Building spring-boot-hello 1.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO][INFO] --- maven-resources-plugin:2.7:resources (default-resources) @ spring-boot-hello ---[INFO] Using 'UTF-8' encoding to copy filtered resources.[INFO] skip non existing resourceDirectory /Users/june/workspace/project/spring-boot-hello/src/main/resources[INFO] skip non existing resourceDirectory /Users/june/workspace/project/spring-boot-hello/src/main/resources[INFO][INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ spring-boot-hello ---[INFO] Nothing to compile - all classes are up to date[INFO][INFO] --- maven-resources-plugin:2.7:testResources (default-testResources) @ spring-boot-hello ---[INFO] Using 'UTF-8' encoding to copy filtered resources.[INFO] skip non existing resourceDirectory /Users/june/workspace/project/spring-boot-hello/src/test/resources[INFO][INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ spring-boot-hello ---[INFO] No sources to compile[INFO][INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ spring-boot-hello ---[INFO][INFO] --- maven-jar-plugin:2.6:jar (default-jar) @ spring-boot-hello ---[INFO] Building jar: /Users/june/workspace/project/spring-boot-hello/target/spring-boot-hello-1.0-SNAPSHOT.jar[INFO][INFO] --- spring-boot-maven-plugin:1.5.20.RELEASE:repackage (default) @ spring-boot-hello ---[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 3.396s[INFO] Finished at: Sat Apr 06 23:45:53 CST 2019[INFO] Final Memory: 16M/226M[INFO] ------------------------------------------------------------------------ 然后，你可以运行项目命令执行生成的 jar 文件： 1$ java -jar target/spring-boot-hello-1.0-SNAPSHOT.jar 如果 Maven 运行过程出现内存溢出，则可以添加下面参数： 1$ export JAVA_OPTS=-Xmx1024m -XX:MaxPermSize=128M -Djava.security.egd=file:/dev/./urandom Gradle 项目如果你使用 Gradle 编译项目，则可以在项目根路径直接运行下面命令来运行应用： 123$ export JAVA_OPTS=-Xmx1024m -XX:MaxPermSize=128M -Djava.security.egd=file:/dev/./urandom$ gradle bootRun 然后，通过浏览器访问 http://localhost:8080/ 。 也可以先 build 生成一个 jar 文件，然后执行该 jar 文件： 1$ gradle build &amp;&amp; java -jar build/libs/spring-boot-hello-1.0-SNAPSHOT.jar 你也可以启动远程调试： 1234$ gradle build $ java -Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=8000,suspend=n \ -jar build/libs/spring-boot-hello-1.0-SNAPSHOT.jar 参考文章 Spring Boot Reference Guide]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot之二：特性]]></title>
    <url>%2F2015%2F03%2F13%2Fsome-spring-boot-features%2F</url>
    <content type="text"><![CDATA[1. SpringApplicationSpringApplication 类是启动 Spring Boot 应用的入口类，你可以创建一个包含 main() 方法的类，来运行 SpringApplication.run 这个静态方法： 123public static void main(String[] args) &#123; SpringApplication.run(MySpringConfiguration.class, args);&#125; 运行该类会有如下输出： 1234567 . ____ _ __ _ _ /\\ / ___&apos;_ __ _ _(_)_ __ __ _ \ \ \ \( ( )\___ | &apos;_ | &apos;_| | &apos;_ \/ _` | \ \ \ \ \\/ ___)| |_)| | | | | || (_| | ) ) ) ) &apos; |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: v1.5.20.RELEASE 1.1 自定义Banner通过在classpath下添加一个banner.txt或设置banner.location来指定相应的文件可以改变启动过程中打印的banner。如果这个文件有特殊的编码，你可以使用banner.encoding设置它（默认为UTF-8）。 在banner.txt中可以使用如下的变量： ${application.version}：MANIFEST.MF 文件中的应用版本号 ${application.formatted-version} ${spring-boot.version}：你正在使用的 Spring Boot 版本号 ${spring-boot.formatted-version} 上面这些变量也可以通过 application.properties 来设置，后面再作介绍。 注：如果想以编程的方式产生一个banner，可以使用SpringBootApplication.setBanner(…)方法。使用org.springframework.boot.Banner接口，实现你自己的printBanner()方法。 1.2 自定义SpringApplication如果默认的SpringApplication不符合你的口味，你可以创建一个本地的实例并自定义它。例如，关闭banner你可以这样写： 12345public static void main(String[] args) &#123; SpringApplication app = new SpringApplication(MySpringConfiguration.class); app.setShowBanner(false); app.run(args);&#125; 1.3 流畅的构建API如果你需要创建一个分层的ApplicationContext（多个具有父子关系的上下文），或你只是喜欢使用流畅的构建API，你可以使用SpringApplicationBuilder。SpringApplicationBuilder允许你以链式方式调用多个方法，包括可以创建层次结构的parent和child方法。 12345new SpringApplicationBuilder() .showBanner(false) .sources(Parent.class) .child(Application.class) .run(args); 1.4 Application事件和监听器SpringApplication 启动过程会触发一些事件，你可以针对这些事件通过 SpringApplication.addListeners(…​) 添加一些监听器: ApplicationStartedEvent ApplicationEnvironmentPreparedEvent ApplicationPreparedEvent ApplicationFailedEvent SpringApplication 会注册一个 shutdown hook 以便在应用退出的时候能够保证 ApplicationContext 优雅地关闭，这样能够保证所有 Spring lifecycle 的回调都会被执行，包括 DisposableBean 接口的实现类以及 @PreDestroy 注解。 另外，你也可以实现 org.springframework.boot.ExitCodeGenerator 接口来定义你自己的退出时候的逻辑。 1.5 Web环境一个SpringApplication将尝试为你创建正确类型的ApplicationContext。在默认情况下，使用AnnotationConfigApplicationContext或AnnotationConfigEmbeddedWebApplicationContext取决于你正在开发的是否是web应用。 用于确定一个web环境的算法相当简单（基于是否存在某些类）。如果需要覆盖默认行为，你可以使用setWebEnvironment(boolean webEnvironment)。通过调用setApplicationContextClass(…)，你可以完全控制ApplicationContext的类型。 注：当JUnit测试里使用SpringApplication时，调用setWebEnvironment(false)是可取的。 1.6 获取应用参数如果你想获取应用程序传递给SpringApplication.run(…​)的参数，你可以注入一个org.springframework.boot.ApplicationArgumentsbean，ApplicationArguments这个接口提供了方法获取可选的和非可选的String[]类型的参数。 12345678910111213import org.springframework.boot.*import org.springframework.beans.factory.annotation.*import org.springframework.stereotype.*@Componentpublic class MyBean &#123; @Autowired public MyBean(ApplicationArguments args) &#123; boolean debug = args.containsOption("debug"); List&lt;String&gt; files = args.getNonOptionArgs(); // if run with "--debug logfile.txt" debug=true, files=["logfile.txt"] &#125;&#125; Spring Boot也会在Environment中注入一个CommandLinePropertySource，这允许你使用@Value注解注入一个应用参数。 1.7 使用ApplicationRunner或者CommandLineRunner1234567891011import org.springframework.boot.*import org.springframework.stereotype.*@Componentpublic class MyBean implements CommandLineRunner &#123; public void run(String... args) &#123; // Do something... &#125;&#125; 如果一些CommandLineRunner或者ApplicationRunner beans被定义必须以特定的次序调用，你可以额外实现org.springframework.core.Ordered接口或使用@Order注解。 1.8 程序退出SpringApplication会在JVM上注册一个关闭的hook已确认ApplicationContext是否优雅的关闭。所有的标准的Spring生命周期回调（例如，DisposableBean接口，或者@PreDestroy注解）都可以使用。 另外，beans可以实现org.springframework.boot.ExitCodeGenerator接口在应用程序结束的时候返回一个错误码。 1.9 管理员特性通过spring.application.admin.enabled开启。 2. 外化配置Spring Boot允许你针对不同的环境配置不同的配置参数，你可以使用 properties文件、YAML 文件、环境变量或者命令行参数来修改应用的配置。你可以在代码中使用@Value注解来获取配置参数的值。 Spring Boot使用一个特别的PropertySource来按顺序加载配置，加载顺序如下： 命令行参数 来自SPRING_APPLICATION_JSON的属性 java:comp/env 中的 JNDI 属性 Java系统环境变量 操作系统环境变量 RandomValuePropertySource，随机值，使用 random.* 来定义 jar 包外的 Profile 配置文件，如 application-{profile}.properties 和 YAML 文件 jar 包内的 Profile 配置文件，如 application-{profile}.properties 和 YAML 文件 jar 包外的 Application 配置，如 application.properties 和 application.yml 文件 jar 包内的 Application 配置，如 application.properties 和 application.yml 文件 在标有 @Configuration 注解的类标有@PropertySource注解的 默认值，使用 SpringApplication.setDefaultProperties 设置的 示例代码： 1234567891011import org.springframework.stereotype.*import org.springframework.beans.factory.annotation.*@Componentpublic class MyBean &#123; @Value("$&#123;name&#125;") private String name; // ...&#125; 你可以在 application.properties 中定义一个 name 变量，或者在运行该 jar 时候，指定一个命令行参数（以 -- 标识），例如：java -jar app.jar --name=&quot;Spring&quot; 也可以使用SPRING_APPLICATION_JSON属性： 12$ SPRING_APPLICATION_JSON='&#123;"foo":&#123;"bar":"spam"&#125;&#125;' $ java -jar myapp.jar 在这个例子中，你可以在Spring的Environment中通过foo.bar来引用变量。你可以在系统变量中定义pring.application.json： 1$ java -Dspring.application.json='&#123;"foo":"bar"&#125;' -jar myapp.jar 或者使用命令行参数： 1$ java -jar myapp.jar --spring.application.json='&#123;"foo":"bar"&#125;' 或者使用JNDI变量： 1java:comp/env/spring.application.json 2.1 随机变量RandomValuePropertySource 类型变量的示例如下： 12345my.secret=$&#123;random.value&#125;my.number=$&#123;random.int&#125;my.bignumber=$&#123;random.long&#125;my.number.less.than.ten=$&#123;random.int(10)&#125;my.number.in.range=$&#123;random.int[1024,65536]&#125; 2.3 应用属性文件SpringApplication 会在以下路径查找 application.properties 并加载该文件： /config 目录下 当前目录 classpath 中 /config 包下 classpath 根路径下 另外，你也可以通过 spring.config.location 来指定 application.properties 文件的存放路径，或者通过 spring.config.name 指定该文件的名称，例如： 1$ java -jar myproject.jar --spring.config.location=classpath:/default.properties,classpath:/override.properties 或者： 1$ java -jar myproject.jar --spring.config.name=myproject 2.4 指定Profile配置文件即application-{profile}.properties配置文件。 2.5 占位符在application.properties文件中可以引用Environment中已经存在的变量。 12app.name=MyAppapp.description=$&#123;app.name&#125; is a Spring Boot application 3. Profiles你可以使用 @Profile 注解来标注应用使用的环境 1234567@Configuration@Profile("production")public class ProductionConfiguration &#123; // ...&#125; 可以使用 spring.profiles.active 变量来定义应用激活的 profile： 1spring.profiles.active=dev,hsqldb 还可以通过 SpringApplication 来设置，调用 SpringApplication.setAdditionalProfiles(…​) 代码即可。 4. 日志Spring Boot 使用 Commons Logging 作为内部记录日志，你也可以使用 Java Util Logging, Log4J, Log4J2 和 Logback 来记录日志。 默认情况下，如果你使用了 Starter POMs ，则会使用 Logback 来记录日志。 默认情况，是输出 INFO 类型的日志，你可以通过设置命令行参数--debug来设置： 1$ java -jar myapp.jar --debug 如果你的终端支持 ANSI ，则日志支持彩色输出，这个可以通过 spring.output.ansi.enabled 设置，可配置的值有：ALWAYS、DETECT、NEVER。 1%clr(%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;)&#123;yellow&#125; 可选的颜色有： blue cyan faint green magenta red yellow 可以通过 logging.file 和 logging.path 设置日志输出文件名称和路径。 日志级别使用 logging.level.*=LEVEL 来定义，例如： 12logging.level.org.springframework.web: DEBUGlogging.level.org.hibernate: ERROR Spring Boot 通过 logging.config 来定义日志的配置文件存放路径，对于不同的日志系统，配置文件的名称不同： Logging System Customization Logback logback-spring.xml、logback-spring.groovy、logback.xml 、 logback.groovy Log4j log4j-spring.properties、log4j-spring.xml、log4j.properties 、log4j.xml Log4j2 log4j2-spring.xml、log4j2.xml JDK (Java Util Logging) logging.properties 对于logback-spring.xml这类的配置，建议使用-spring变量来加载配置文件。 Environment中可以自定义一些属性： Spring Environment System Property Comments logging.exception-conversion-word LOG_EXCEPTION_CONVERSION_WORD logging.file LOG_FILE logging.path LOG_PATH logging.pattern.console CONSOLE_LOG_PATTERN logging.pattern.file FILE_LOG_PATTERN logging.pattern.level LOG_LEVEL_PATTERN PID PID 5. 开发Web应用5.1 Spring Web MVC框架一个标准的@RestController例子返回JSON数据： 1234567891011121314151617181920@RestController@RequestMapping(value="/users")public class MyRestController &#123; @RequestMapping(value="/&#123;user&#125;", method=RequestMethod.GET) public User getUser(@PathVariable Long user) &#123; // ... &#125; @RequestMapping(value="/&#123;user&#125;/customers", method=RequestMethod.GET) List&lt;Customer&gt; getUserCustomers(@PathVariable Long user) &#123; // ... &#125; @RequestMapping(value="/&#123;user&#125;", method=RequestMethod.DELETE) public User deleteUser(@PathVariable Long user) &#123; // ... &#125;&#125; 5.1.1 Spring MVC自动配置Spring Boot为Spring MVC提供适用于多数应用的自动配置功能。在Spring默认基础上，自动配置添加了以下特性： 引入ContentNegotiatingViewResolver``和BeanNameViewResolver beans。 对静态资源的支持，包括对WebJars的支持。 自动注册Converter，GenericConverter，Formatter beans。 对HttpMessageConverters的支持。 自动注册MessageCodeResolver。 对静态index.html的支持。 对自定义Favicon的支持。 字段使用 ConfigurableWebBindingInitializer bean 如果想全面控制Spring MVC，你可以添加自己的@Configuration，并使用@EnableWebMvc对其注解。如果想保留Spring Boot MVC的特性，并只是添加其他的MVC配置(拦截器，formatters，视图控制器等)，你可以添加自己的WebMvcConfigurerAdapter类型的@Bean（不使用@EnableWebMvc注解）。 5.1.2 HttpMessageConvertersSpring MVC使用HttpMessageConverter接口转换HTTP请求和响应。合理的缺省值被包含的恰到好处（out of the box），例如对象可以自动转换为JSON（使用Jackson库）或XML（如果Jackson XML扩展可用则使用它，否则使用JAXB）。字符串默认使用UTF-8编码。 如果需要添加或自定义转换器，你可以使用Spring Boot的HttpMessageConverters类： 1234567891011121314import org.springframework.boot.autoconfigure.web.HttpMessageConverters;import org.springframework.context.annotation.*;import org.springframework.http.converter.*;@Configurationpublic class MyConfiguration &#123; @Bean public HttpMessageConverters customConverters() &#123; HttpMessageConverter&lt;?&gt; additional = ... HttpMessageConverter&lt;?&gt; another = ... return new HttpMessageConverters(additional, another); &#125;&#125; 任何在上下文中出现的HttpMessageConverter bean将会添加到converters列表，你可以通过这种方式覆盖默认的转换器（converters）。 5.1.3 MessageCodesResolverSpring MVC有一个策略，用于从绑定的errors产生用来渲染错误信息的错误码：MessageCodesResolver。如果设置spring.mvc.message-codes-resolver.format属性为PREFIX_ERROR_CODE或POSTFIX_ERROR_CODE（具体查看DefaultMessageCodesResolver.Format枚举值），Spring Boot会为你创建一个MessageCodesResolver。 5.1.4 静态内容默认情况下，Spring Boot从classpath下一个叫/static（/public，/resources或/META-INF/resources）的文件夹或从ServletContext根目录提供静态内容。这使用了Spring MVC的ResourceHttpRequestHandler，所以你可以通过添加自己的WebMvcConfigurerAdapter并覆写addResourceHandlers方法来改变这个行为（加载静态文件）。 12345678910@Configurationclass ClientResourcesConfig extends WebMvcConfigurerAdapter &#123; @Override public void addResourceHandlers(ResourceHandlerRegistry registry) &#123; registry.addResourceHandler("/**") .addResourceLocations("/WEB-INF/resources/") .setCachePeriod(0); &#125;&#125; 在一个单独的web应用中，容器默认的servlet是开启的，如果Spring决定不处理某些请求，默认的servlet作为一个回退（降级）将从ServletContext根目录加载内容。大多数时候，这不会发生（除非你修改默认的MVC配置），因为Spring总能够通过DispatcherServlet处理请求。 此外，上述标准的静态资源位置有个例外情况是Webjars内容。任何在/webjars/**路径下的资源都将从jar文件中提供，只要它们以Webjars的格式打包。 注：如果你的应用将被打包成jar，那就不要使用src/main/webapp文件夹。尽管该文件夹是一个共同的标准，但它仅在打包成war的情况下起作用，并且如果产生一个jar，多数构建工具都会静悄悄的忽略它。 如果你想刷新静态资源的缓存，你可以定义一个使用HASH结尾的URL，例如：&lt;link href=&quot;/css/spring-2a2d595e6ed9a0b24f027f2b63b134d6.css&quot;/&gt;。 为此，需要使用以下配置： 12spring.resources.chain.strategy.content.enabled=truespring.resources.chain.strategy.content.paths=/** 这里使用了ResourceUrlEncodingFilter过滤器，对于Thymeleaf和Velocity，该过滤器已经自动配置。其他的模板引擎，可以通过ResourceUrlProvider来定义。 当资源文件自动加载的时候，javascript模块加载器会重命名静态文件。还有一种“固定”的策略来修改文件名称。 12345spring.resources.chain.strategy.content.enabled=truespring.resources.chain.strategy.content.paths=/**spring.resources.chain.strategy.fixed.enabled=truespring.resources.chain.strategy.fixed.paths=/js/lib/spring.resources.chain.strategy.fixed.version=v12 使用了上面的配置之后，当javascript加载&quot;/js/lib/&quot;目录下的文件时，将会使用一个固定的版本&quot;/v12/js/lib/mymodule.js&quot;，而其他的静态资源仍然使用&lt;link href=&quot;/css/spring-2a2d595e6ed9a0b24f027f2b63b134d6.css&quot;/&gt;。 更多说明，参考ResourceProperties，或者阅读该偏文章。 5.1.5 ConfigurableWebBindingInitializerSpring MVC使用WebBindingInitializer来为一个特定的请求初始化WebDataBinder。如果你自带一个了一个ConfigurableWebBindingInitializer @Bean，Spring Boot会自动配置Spring MVC来使用它。 5.1.6 模板引擎正如REST web服务，你也可以使用Spring MVC提供动态HTML内容。Spring MVC支持各种各样的模板技术，包括Velocity,FreeMarker和JSPs。很多其他的模板引擎也提供它们自己的Spring MVC集成。 Spring Boot为以下的模板引擎提供自动配置支持： FreeMarker Groovy Thymeleaf Velocity Mustache 注：如果可能的话，应该忽略JSPs，因为在内嵌的servlet容器使用它们时存在一些已知的限制。 当你使用这些引擎的任何一种，并采用默认的配置，你的模板将会从src/main/resources/templates目录下自动加载。 注：IntelliJ IDEA根据你运行应用的方式会对classpath进行不同的整理。在IDE里通过main方法运行你的应用跟从Maven或Gradle或打包好的jar中运行相比会导致不同的顺序。这可能导致Spring Boot不能从classpath下成功地找到模板。如果遇到这个问题，你可以在IDE里重新对classpath进行排序，将模块的类和资源放到第一位。或者，你可以配置模块的前缀为classpath*:/templates/，这样会查找classpath下的所有模板目录。 5.1.7 错误处理Spring Boot默认提供一个/error映射用来以合适的方式处理所有的错误，并且它在servlet容器中注册了一个全局的 错误页面。对于机器客户端（相对于浏览器而言，浏览器偏重于人的行为），它会产生一个具有详细错误，HTTP状态，异常信息的JSON响应。对于浏览器客户端，它会产生一个白色标签样式（whitelabel）的错误视图，该视图将以HTML格式显示同样的数据（可以添加一个解析为erro的View来自定义它）。为了完全替换默认的行为，你可以实现ErrorController，并注册一个该类型的bean定义，或简单地添加一个ErrorAttributes类型的bean以使用现存的机制，只是替换显示的内容。 如果在某些条件下需要比较多的错误页面，内嵌的servlet容器提供了一个统一的Java DSL（领域特定语言）来自定义错误处理。 示例： 123456789101112131415@Beanpublic EmbeddedServletContainerCustomizer containerCustomizer()&#123; return new MyCustomizer();&#125;// ...private static class MyCustomizer implements EmbeddedServletContainerCustomizer &#123; @Override public void customize(ConfigurableEmbeddedServletContainer container) &#123; container.addErrorPages(new ErrorPage(HttpStatus.BAD_REQUEST, "/400")); container.addErrorPages(new ErrorPage(HttpStatus.NOT_FOUND, "/404")); container.addErrorPages(new ErrorPage(HttpStatus.INTERNAL_SERVER_ERROR, "/500")); &#125; &#125;&#125; 你也可以使用常规的Spring MVC特性来处理错误，比如@ExceptionHandler方法和@ControllerAdvice。ErrorController将会捡起任何没有处理的异常。 N.B. 如果你为一个路径注册一个ErrorPage，最终被一个过滤器（Filter）处理（对于一些非Spring web框架，像Jersey和Wicket这很常见），然后过滤器需要显式注册为一个ERROR分发器（dispatcher）。 12345678@Beanpublic FilterRegistrationBean myFilter() &#123; FilterRegistrationBean registration = new FilterRegistrationBean(); registration.setFilter(new MyFilter()); ... registration.setDispatcherTypes(EnumSet.allOf(DispatcherType.class)); return registration;&#125; 注：默认的FilterRegistrationBean没有包含ERROR分发器类型。 5.1.8 Spring HATEOAS如果你正在开发一个使用超媒体的RESTful API，Spring Boot将为Spring HATEOAS提供自动配置，这在多数应用中都工作良好。自动配置替换了对使用@EnableHypermediaSupport的需求，并注册一定数量的beans来简化构建基于超媒体的应用，这些beans包括一个LinkDiscoverer和配置好的用于将响应正确编排为想要的表示的ObjectMapper。ObjectMapper可以根据spring.jackson.*属性或一个存在的Jackson2ObjectMapperBuilder bean进行自定义。 通过使用@EnableHypermediaSupport，你可以控制Spring HATEOAS的配置。注意这会禁用上述的对ObjectMapper的自定义。 5.1.9 CORS支持你可以在方法上使用@CrossOrigin注解，或者配置一个全局的设置： 12345678910111213@Configurationpublic class MyConfiguration &#123; @Bean public WebMvcConfigurer corsConfigurer() &#123; return new WebMvcConfigurerAdapter() &#123; @Override public void addCorsMappings(CorsRegistry registry) &#123; registry.addMapping("/api/**"); &#125; &#125;; &#125;&#125; 5.2 JAX-RS和Jersey如果喜欢JAX-RS为REST端点提供的编程模型，你可以使用可用的实现替代Spring MVC。如果在你的应用上下文中将Jersey 1.x和Apache Celtix的Servlet或Filter注册为一个@Bean，那它们工作的相当好。Jersey 2.x有一些原生的Spring支持，所以我们会在Spring Boot为它提供自动配置支持，连同一个启动器（starter）。 想要开始使用Jersey 2.x只需要加入spring-boot-starter-jersey依赖，然后你需要一个ResourceConfig类型的@Bean，用于注册所有的端点（endpoints）。 123456@Componentpublic class JerseyConfig extends ResourceConfig &#123; public JerseyConfig() &#123; register(Endpoint.class); &#125;&#125; 所有注册的端点都应该被@Components和HTTP资源annotations（比如@GET）注解。 12345678@Component@Path("/hello")public class Endpoint &#123; @GET public String message() &#123; return "Hello"; &#125;&#125; 由于Endpoint是一个Spring组件（@Component），所以它的生命周期受Spring管理，并且你可以使用@Autowired添加依赖及使用@Value注入外部配置。Jersey servlet将被注册，并默认映射到/*。你可以将@ApplicationPath添加到ResourceConfig来改变该映射。 默认情况下，Jersey将在一个ServletRegistrationBean类型的@Bean中被设置成名称为jerseyServletRegistration的Servlet。通过创建自己的相同名称的bean，你可以禁止或覆盖这个bean。你也可以通过设置spring.jersey.type=filter来使用一个Filter代替Servlet（在这种情况下，被覆盖或替换的@Bean是jerseyFilterRegistration）。该servlet有@Order属性，你可以通过spring.jersey.filter.order进行设置。不管是Servlet还是Filter注册都可以使用spring.jersey.init.*定义一个属性集合作为初始化参数传递过去。 这里有一个Jersey示例，你可以查看如何设置相关事项。 5.3 内嵌的容器支持5.3.1 Servlets和Filters当使用内嵌的servlet容器时，你可以直接将servlet和filter注册为Spring的beans。在配置期间，如果你想引用来自application.properties的值，这是非常方便的。默认情况下，如果上下文只包含单一的Servlet，那它将被映射到根路径（/）。在多Servlet beans的情况下，bean的名称将被用作路径的前缀。过滤器会被映射到/*。 如果基于约定（convention-based）的映射不够灵活，你可以使用ServletRegistrationBean和FilterRegistrationBean类实现完全的控制。如果你的bean实现了ServletContextInitializer接口，也可以直接注册它们。 EmbeddedWebApplicationContextSpring Boot底层使用了一个新的ApplicationContext类型，用于对内嵌servlet容器的支持。EmbeddedWebApplicationContext是一个特殊类型的WebApplicationContext，它通过搜索一个单一的EmbeddedServletContainerFactory bean来启动自己。通常，TomcatEmbeddedServletContainerFactory，JettyEmbeddedServletContainerFactory或UndertowEmbeddedServletContainerFactory将被自动配置。 注：你通常不需要知道这些实现类。大多数应用将被自动配置，并根据你的行为创建合适的ApplicationContext和EmbeddedServletContainerFactory。 自定义内嵌servlet容器常见的Servlet容器设置可以通过Spring Environment属性进行配置。通常，你会把这些属性定义到application.properties文件中。 常见的服务器设置包括： server.port - 进来的HTTP请求的监听端口号 server.address - 绑定的接口地址 server.sessionTimeout - session超时时间 具体参考ServerProperties。 编程方式的自定义如果需要以编程的方式配置内嵌的servlet容器，你可以注册一个实现EmbeddedServletContainerCustomizer接口的Spring bean。EmbeddedServletContainerCustomizer提供对ConfigurableEmbeddedServletContainer的访问，ConfigurableEmbeddedServletContainer包含很多自定义的setter方法。 12345678910import org.springframework.boot.context.embedded.*;import org.springframework.stereotype.Component;@Componentpublic class CustomizationBean implements EmbeddedServletContainerCustomizer &#123; @Override public void customize(ConfigurableEmbeddedServletContainer container) &#123; container.setPort(9000); &#125;&#125; 直接自定义ConfigurableEmbeddedServletContainer如果上面的自定义手法过于受限，你可以自己注册TomcatEmbeddedServletContainerFactory，JettyEmbeddedServletContainerFactory或UndertowEmbeddedServletContainerFactory。 12345678@Beanpublic EmbeddedServletContainerFactory servletContainer() &#123; TomcatEmbeddedServletContainerFactory factory = new TomcatEmbeddedServletContainerFactory(); factory.setPort(9000); factory.setSessionTimeout(10, TimeUnit.MINUTES); factory.addErrorPages(new ErrorPage(HttpStatus.NOT_FOUND, "/notfound.html"); return factory;&#125; 很多可选的配置都提供了setter方法，也提供了一些受保护的钩子方法以满足你的某些特殊需求。具体参考相关文档。 JSP的限制在内嵌的servlet容器中运行一个Spring Boot应用时（并打包成一个可执行的存档archive），容器对JSP的支持有一些限制。 tomcat只支持war的打包方式，不支持可执行的jar。 内嵌的Jetty目前不支持JSPs。 Undertow不支持JSPs。 这里有个JSP示例，你可以查看如何设置相关事项。 参考文章 Spring Boot Reference Guide]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring AOP Example Tutorial]]></title>
    <url>%2F2015%2F03%2F11%2Fspring-aop-example-tutorial-aspect-advice-pointcut-joinpoint-annotations-xml-configuration%2F</url>
    <content type="text"><![CDATA[这是一篇翻译，原文：Spring AOP Example Tutorial – Aspect, Advice, Pointcut, JoinPoint, Annotations, XML Configuration Spring 框架发展出了两个核心概念：依赖注入 和面向切面编程（AOP）。我们已经了解了 Spring 的依赖注入 是如何实现的，今天我们来看看面向切面编程的核心概念以及 Spring 框架是如何实现它的。 AOP 概要大多数的企业应用都会有一些共同的对横切的关注，横切是否适用于不同的对象或者模型。一些共同关注的横切有日志、事务管理以及数据校验等等。在面向对象的编程中，应用模块是有类来实现的，然而面向切面的编程的应用模块是由切面（Aspect）来获取的，他们被配置用于切不同的类。 AOP 任务将横切任务的直接依赖从类中抽离出来，因为我们不能直接从面向对象编程的模型中获取这些依赖。例如，我们可以有一个单独的类用于记录日志，但是相同功能的类将不得不调用这些方法去获取应用的中的日志。 AOP 核心概念在我们深入了解 Spring 框架中 AOP 的实现方式之前，我们需要了解 AOP 中的一些核心概念。 Aspect：切面，实现企业应用中多个类的共同关注点，例如事务管理。切面可以是使用 Spring XML 配置的一个普通类或者是我们使用 Spring AspectJ 定义的一个标有 @Aspect 注解的类。 Join Point：连接点，一个连接点是应用程序中的一个特定的点，例如方法执行、异常处理、改变对象变量的值等等。在 Spring AOP 中，一个连接点永远是指一个方法的执行。 Advice：通知，通知是指在一个特别的连接点上发生的动作。在编程中，他们是当一个连接点匹配到一个切入点时执行的方法。你可以把通知想成 Strust2 中的拦截器或者是 Servlet 中的过滤器。 Pointcut：切入点，切入点是一些表达式，当匹配到连接点时决定通知是否需要执行。切入点使用不同种类型的表达式来匹配连接点，Spring 框架使用 AspectJ 表达式语法。 Target Object：通知执行的目标对象。Spring AOP 是使用运行时的代理来实现的，所以该对象永远是一个代理对象。这意味着一个子类在运行期间会被创建，该类的目标方法会被覆盖并且通知会基于他们的配置被引入。 AOP proxy：Spring AOP 实现使用 JDK 的动态代理来创建包含有目标类和通知调用的代理类，这些类被称为 AOP 代理类。我们也可以使用 CGLIB 代理来作为 Spring AOP 项目的依赖实现。 Weaving：织入，将切面和其他用于创建被通知的代理对象的类联系起来的过程。这可以是在运行时完成，也可以是在代码加载过程中或者运行时。Spring AOP 是在运行时完成织入的过程。 AOP 通知类型基于通知的执行策略，这里有以下几种通知类型： Before Advice：在连接点方法执行之前运行。我们可以使用 @Before 注解来标记一个通知类型为 Before Advice。 After (finally) Advice：在连接点方法执行完成之后运行。我们可以使用 @After 来创建一个 After (finally) Advice。 After Returning Advice：在方法返回之后运行，通过 @AfterReturning 注解创建。 After Throwing Advice：在方法抛出异常之后运行，通过 @AfterThrowing 注解创建。 Around Advice：这是最重要和最强的通知。这个通知在连接点方法前后运行并且我们可以决定该通知是否运行，通过 @Around 注解创建。 上面提到的知识点可能会使我们困惑，但是当我们看到 Spring AOP 的实现之后，就会豁然开朗了。下面我们来创建一个 Spring AOP 的项目。Spring 支持使用 AspectJ 的注解来创建切面，为了简单，我们将直接使用这些注解。上面提到的所有 AOP 的注解都定义在 org.aspectj.lang.annotation 包中。 Spring Tool Suite 提供了对 AspectJ 的支持，所以建议你使用它来创建项目。如果你对 STS 不熟悉，可以参考我的 Spring MVC 教程 来熟悉如何使用它。 创建一个简单的 Spring Maven 项目，通过 pom.xml 引入 Spring 的核心库。在项目创建成功之后，我们可以看到下面的目录结构： Spring AOP AspectJ 依赖Spring 框架默认提供了对 AOP 的支持，既然我们需要使用 AspectJ 的注解，则需要在 pom.xml 中引入相关的依赖： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.springframework.samples&lt;/groupId&gt; &lt;artifactId&gt;SpringAOPExample&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;!-- Generic properties --&gt; &lt;java.version&gt;1.6&lt;/java.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;!-- Spring --&gt; &lt;spring-framework.version&gt;4.0.2.RELEASE&lt;/spring-framework.version&gt; &lt;!-- Logging --&gt; &lt;logback.version&gt;1.0.13&lt;/logback.version&gt; &lt;slf4j.version&gt;1.7.5&lt;/slf4j.version&gt; &lt;!-- Test --&gt; &lt;junit.version&gt;4.11&lt;/junit.version&gt; &lt;!-- AspectJ --&gt; &lt;aspectj.version&gt;1.7.4&lt;/aspectj.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- Spring and Transactions --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-framework.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-framework.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Logging with SLF4J &amp; LogBack --&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;$&#123;logback.version&#125;&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- AspectJ dependencies --&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjrt&lt;/artifactId&gt; &lt;version&gt;$&#123;aspectj.version&#125;&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjtools&lt;/artifactId&gt; &lt;version&gt;$&#123;aspectj.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 需要注意的是，我在项目中添加了对 aspectjrt 和 aspectjtools （版本为 1.7.4）的依赖。并且我也把 Spring 的版本更新到了 4.0.2.RELEASE。 模型类下面我们来创建一个简单的 java bean： Employee.java 12345678910111213141516171819202122package com.journaldev.spring.model; import com.journaldev.spring.aspect.Loggable; public class Employee &#123; private String name; public String getName() &#123; return name; &#125; @Loggable public void setName(String nm) &#123; this.name=nm; &#125; public void throwException()&#123; throw new RuntimeException("Dummy Exception"); &#125; &#125; 你有注意到 setName() 方法上定义了一个 Loggable 注解吗？它是一个我们项目中创建的自定义注解。我们将在后面介绍它的用法。 服务类下面，我们来创建一个服务类来处理 Employee 对象： EmployeeService.java 12345678910111213141516package com.journaldev.spring.service; import com.journaldev.spring.model.Employee; public class EmployeeService &#123; private Employee employee; public Employee getEmployee()&#123; return this.employee; &#125; public void setEmployee(Employee e)&#123; this.employee=e; &#125;&#125; 我本来可以使用 Spring 注解来将其配置为一个 Spring 的组件，但是在该项目中我们将会使用 XML 来配置。EmployeeService 是一个非常标准的类，并提供了一个访问 Employee 的点。 AOP 配置我项目中的配置 spring.xml 如下： 1234567891011121314151617181920212223242526272829&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:aop="http://www.springframework.org/schema/aop" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.0.xsd"&gt; &lt;!-- Enable AspectJ style of Spring AOP --&gt;&lt;aop:aspectj-autoproxy /&gt; &lt;!-- Configure Employee Bean and initialize it --&gt;&lt;bean name="employee" class="com.journaldev.spring.model.Employee"&gt; &lt;property name="name" value="Dummy Name"&gt;&lt;/property&gt;&lt;/bean&gt; &lt;!-- Configure EmployeeService bean --&gt;&lt;bean name="employeeService" class="com.journaldev.spring.service.EmployeeService"&gt; &lt;property name="employee" ref="employee"&gt;&lt;/property&gt;&lt;/bean&gt; &lt;!-- Configure Aspect Beans, without this Aspects advices wont execute --&gt;&lt;bean name="employeeAspect" class="com.journaldev.spring.aspect.EmployeeAspect" /&gt;&lt;bean name="employeeAspectPointcut" class="com.journaldev.spring.aspect.EmployeeAspectPointcut" /&gt;&lt;bean name="employeeAspectJoinPoint" class="com.journaldev.spring.aspect.EmployeeAspectJoinPoint" /&gt;&lt;bean name="employeeAfterAspect" class="com.journaldev.spring.aspect.EmployeeAfterAspect" /&gt;&lt;bean name="employeeAroundAspect" class="com.journaldev.spring.aspect.EmployeeAroundAspect" /&gt;&lt;bean name="employeeAnnotationAspect" class="com.journaldev.spring.aspect.EmployeeAnnotationAspect" /&gt; &lt;/beans&gt; 在 Spring beans 中使用 AOP，我们需要添加： 申明 AOP 命名空间，如：xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot;。 添加 aop:aspectj-autoproxy 节点开启 Spring AspectJ 在运行时自动代理的支持。 配置 Aspect 类。 Before Aspect 例子EmployeeAspect.java： 123456789101112131415161718package com.journaldev.spring.aspect; import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before; @Aspectpublic class EmployeeAspect &#123; @Before("execution(public String getName())") public void getNameAdvice()&#123; System.out.println("Executing Advice on getName()"); &#125; @Before("execution(* com.journaldev.spring.service.*.get*())") public void getAllAdvice()&#123; System.out.println("Service method getter called"); &#125;&#125; 上面例子中重要的地方说明如下： Aspect 类需要添加 @Aspect 注解。 @Before注解用于创建 Before advice。 @Before 注解中的字符串参数是 Pointcut 表达式。 getNameAdvice() 通知将会在任何带有 `public String getName()`` 方法签名的 Spring Bean 方法执行时执行。这点是非常重要的，如果我们使用 new 操作符来创建一个 Employee bean，该通知并不会执行，其只会在 ApplicationContext 获取该 bean 时执行。 切点方法和重用有时候，我们需要在多个地方上使用相同的切点表达式，我们可以使用一个空方法的 @Pointcut 注解，然后在通知中将它作为表达式来使用。 EmployeeAspectPointcut.java 1234567891011121314151617181920212223242526272829303132package com.journaldev.spring.aspect; import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.aspectj.lang.annotation.Pointcut; @Aspectpublic class EmployeeAspectPointcut &#123; @Before("getNamePointcut()") public void loggingAdvice()&#123; System.out.println("Executing loggingAdvice on getName()"); &#125; @Before("getNamePointcut()") public void secondAdvice()&#123; System.out.println("Executing secondAdvice on getName()"); &#125; @Pointcut("execution(public String getName())") public void getNamePointcut()&#123;&#125; @Before("allMethodsPointcut()") public void allServiceMethodsAdvice()&#123; System.out.println("Before executing service method"); &#125; //Pointcut to execute on all the methods of classes in a package @Pointcut("within(com.journaldev.spring.service.*)") public void allMethodsPointcut()&#123;&#125; &#125; 上面的例子非常清晰，相对于表达式，我们在使用方法名称作为注解的参数。 连接点和通知参数我们可以使用 JoinPoint 作为通知方法的参数并且使用他获取方法签名或者目标对象。 我们可以在连接点中使用 args() 表达式来匹配任何方法的任何参数。如果我们使用它，则我们需要在通知方法中使用同参数相同的名称。我们也可以在通知参数中使用泛型。 EmployeeAspectJoinPoint.java 1234567891011121314151617181920212223242526package com.journaldev.spring.aspect; import java.util.Arrays; import org.aspectj.lang.JoinPoint;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before; @Aspectpublic class EmployeeAspectJoinPoint &#123; @Before("execution(public void com.journaldev.spring.model..set*(*))") public void loggingAdvice(JoinPoint joinPoint)&#123; System.out.println("Before running loggingAdvice on method="+joinPoint.toString()); System.out.println("Agruments Passed=" + Arrays.toString(joinPoint.getArgs())); &#125; //Advice arguments, will be applied to bean methods with single String argument @Before("args(name)") public void logStringArguments(String name)&#123; System.out.println("String argument passed="+name); &#125;&#125; After Advice 例子EmployeeAfterAspect.java 如下： 123456789101112131415161718192021222324252627package com.journaldev.spring.aspect; import org.aspectj.lang.JoinPoint;import org.aspectj.lang.annotation.After;import org.aspectj.lang.annotation.AfterReturning;import org.aspectj.lang.annotation.AfterThrowing;import org.aspectj.lang.annotation.Aspect; @Aspectpublic class EmployeeAfterAspect &#123; @After("args(name)") public void logStringArguments(String name)&#123; System.out.println("Running After Advice. String argument passed="+name); &#125; @AfterThrowing("within(com.journaldev.spring.model.Employee)") public void logExceptions(JoinPoint joinPoint)&#123; System.out.println("Exception thrown in Employee Method="+joinPoint.toString()); &#125; @AfterReturning(pointcut="execution(* getName())", returning="returnString") public void getNameReturningAdvice(String returnString)&#123; System.out.println("getNameReturningAdvice executed. Returned String="+returnString); &#125; &#125; 我们可以在切点表达式中使用 within 来申明该通知会在一个类的所有方法上执行。 Around Aspect 例子正如前面提到的，我们可以使用 Around aspect 来定义在方法前后进行执行指定的代码。 EmployeeAroundAspect.java 12345678910111213141516171819202122package com.journaldev.spring.aspect; import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect; @Aspectpublic class EmployeeAroundAspect &#123; @Around("execution(* com.journaldev.spring.model.Employee.getName())") public Object employeeAroundAdvice(ProceedingJoinPoint proceedingJoinPoint)&#123; System.out.println("Before invoking getName() method"); Object value = null; try &#123; value = proceedingJoinPoint.proceed(); &#125; catch (Throwable e) &#123; e.printStackTrace(); &#125; System.out.println("After invoking getName() method. Return value="+value); return value; &#125;&#125; 自定义的注解切点前面提到了 @Loggable 注解，其定义如下： 12345package com.journaldev.spring.aspect; public @interface Loggable &#123; &#125; 我们可以创建一个切面来使用该切点，EmployeeAnnotationAspect.java 如下： 12345678910111213package com.journaldev.spring.aspect; import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before; @Aspectpublic class EmployeeAnnotationAspect &#123; @Before("@annotation(com.journaldev.spring.aspect.Loggable)") public void myAdvice()&#123; System.out.println("Executing myAdvice!!"); &#125;&#125; myAdvice() 方法仅仅会在 setName() 方法执行前执行。 Spring AOP XML Configuration如果我们使用 Sping 的配置文件来定义切面，则定义方式如下。 EmployeeXMLConfigAspect.java 123456789101112131415161718package com.journaldev.spring.aspect; import org.aspectj.lang.ProceedingJoinPoint; public class EmployeeXMLConfigAspect &#123; public Object employeeAroundAdvice(ProceedingJoinPoint proceedingJoinPoint)&#123; System.out.println("EmployeeXMLConfigAspect:: Before invoking getName() method"); Object value = null; try &#123; value = proceedingJoinPoint.proceed(); &#125; catch (Throwable e) &#123; e.printStackTrace(); &#125; System.out.println("EmployeeXMLConfigAspect:: After invoking getName() method. Return value="+value); return value; &#125;&#125; 在 配置文件中定义如下： 12345678&lt;bean name="employeeXMLConfigAspect" class="com.journaldev.spring.aspect.EmployeeXMLConfigAspect" /&gt; &lt;!-- Spring AOP XML Configuration --&gt;&lt;aop:config&gt;&lt;aop:aspect ref="employeeXMLConfigAspect" id="employeeXMLConfigAspectID" order="1"&gt; &lt;aop:pointcut expression="execution(* com.journaldev.spring.model.Employee.getName())" id="getNamePointcut"/&gt; &lt;aop:around method="employeeAroundAdvice" pointcut-ref="getNamePointcut" arg-names="proceedingJoinPoint"/&gt;&lt;/aop:asp&gt; 最后，来看看一个简单的程序来说明切面如何作用在 bean 的方法上。 12345678910111213141516171819202122package com.journaldev.spring.main; import org.springframework.context.support.ClassPathXmlApplicationContext; import com.journaldev.spring.service.EmployeeService; public class SpringMain &#123; public static void main(String[] args) &#123; ClassPathXmlApplicationContext ctx = new ClassPathXmlApplicationContext("spring.xml"); EmployeeService employeeService = ctx.getBean("employeeService", EmployeeService.class); System.out.println(employeeService.getEmployee().getName()); employeeService.getEmployee().setName("Pankaj"); employeeService.getEmployee().throwException(); ctx.close(); &#125; &#125; 我们将看到如下输出： 1234567891011121314151617181920212223242526272829303132333435363738Mar 20, 2014 8:50:09 PM org.springframework.context.support.ClassPathXmlApplicationContext prepareRefreshINFO: Refreshing org.springframework.context.support.ClassPathXmlApplicationContext@4b9af9a9: startup date [Thu Mar 20 20:50:09 PDT 2014]; root of context hierarchyMar 20, 2014 8:50:09 PM org.springframework.beans.factory.xml.XmlBeanDefinitionReader loadBeanDefinitionsINFO: Loading XML bean definitions from class path resource [spring.xml]Service method getter calledBefore executing service methodEmployeeXMLConfigAspect:: Before invoking getName() methodExecuting Advice on getName()Executing loggingAdvice on getName()Executing secondAdvice on getName()Before invoking getName() methodAfter invoking getName() method. Return value=Dummy NamegetNameReturningAdvice executed. Returned String=Dummy NameEmployeeXMLConfigAspect:: After invoking getName() method. Return value=Dummy NameDummy NameService method getter calledBefore executing service methodString argument passed=PankajBefore running loggingAdvice on method=execution(void com.journaldev.spring.model.Employee.setName(String))Agruments Passed=[Pankaj]Executing myAdvice!!Running After Advice. String argument passed=PankajService method getter calledBefore executing service methodException thrown in Employee Method=execution(void com.journaldev.spring.model.Employee.throwException())Exception in thread &quot;main&quot; java.lang.RuntimeException: Dummy Exception at com.journaldev.spring.model.Employee.throwException(Employee.java:19) at com.journaldev.spring.model.Employee$$FastClassBySpringCGLIB$$da2dc051.invoke(&lt;generated&gt;) at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:711) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) at org.springframework.aop.aspectj.AspectJAfterThrowingAdvice.invoke(AspectJAfterThrowingAdvice.java:58) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179) at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:92) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179) at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:644) at com.journaldev.spring.model.Employee$$EnhancerBySpringCGLIB$$3f881964.throwException(&lt;generated&gt;) at com.journaldev.spring.main.SpringMain.main(SpringMain.java:17) 你将会看到通知将会基于切点配置一个个的执行。你应该一个个的配置他们，以免出现混乱。 上面是 Spring AOP 教程的所有内容，我希望你理解了 Spring AOP 的基本概念并能从例子中学习到更多。你可以从下面链接下载本文中的项目代码。 Download Spring AOP Project]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>aop</tag>
        <tag>aspect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速了解RESTEasy]]></title>
    <url>%2F2015%2F03%2F10%2Fquick-start-of-resteasy%2F</url>
    <content type="text"><![CDATA[什么是 RESTEasyRESTEasy 是 JBoss 的一个开源项目，提供各种框架帮助你构建 RESTful Web Services 和 RESTful Java 应用程序。它是 JAX-RS 规范的一个完整实现并通过 JCP 认证。作为一个 JBOSS 的项目，它当然能和 JBOSS 应用服务器很好地集成在一起。 但是，它也能在任何运行 JDK5 或以上版本的 Servlet 容器中运行。RESTEasy 还提供一个 RESTEasy JAX-RS 客户端调用框架，能够很方便与 EJB、Seam、Guice、Spring 和 Spring MVC 集成使用，支持在客户端与服务器端自动实现 GZIP 解压缩。 官方网站：http://resteasy.jboss.org/ 特性直接抄自官网说明： Fully certified JAX-RS implementation Portable to any app-server/Tomcat that runs on JDK 6 or higher Embeddedable server implementation for junit testing Client framework that leverages JAX-RS annotations so that you can write HTTP clients easily (JAX-RS only defines server bindings) Client “Browser” cache. Supports HTTP 1.1 caching semantics including cache revalidation Server in-memory cache. Local response cache. Automatically handles ETag generation and cache revalidation Rich set of providers for: XML, JSON, YAML, Fastinfoset, Multipart, XOP, Atom, etc. JAXB marshalling into XML, JSON, Jackson, Fastinfoset, and Atom as well as wrappers for maps, arrays, lists, and sets of JAXB Objects. GZIP content-encoding. Automatic GZIP compression/decompression suppport in client and server frameworks Asynchronous HTTP (Comet) abstractions for JBoss Web, Tomcat 6, and Servlet 3.0 Asynchronous Job Service. Rich interceptor model. OAuth2 and Distributed SSO with JBoss AS7 Digital Signature and encryption support with S/MIME and DOSETA EJB, Seam, Guice, Spring, and Spring MVC integration 安装和配置如果你在 Servlet3.0 容器中使用 Resteasy，则需要添加如下依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.jboss.resteasy&lt;/groupId&gt; &lt;artifactId&gt;resteasy-servlet-initializer&lt;/artifactId&gt; &lt;version&gt;3.0.9.Final&lt;/version&gt;&lt;/dependency&gt; 否则，如果你在 Servlet3.0 之前的容器中使用 Resteasy，则 WEB-INF/web.xml 中需要包括如下内容： 123456789101112131415&lt;servlet&gt; &lt;servlet-name&gt;Resteasy&lt;/servlet-name&gt; &lt;servlet-class&gt; org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher &lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;javax.ws.rs.Application&lt;/param-name&gt; &lt;param-value&gt;com.restfully.shop.services.ShoppingApplication&lt;/param-value&gt; &lt;/init-param&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;Resteasy&lt;/servlet-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 另外，还可以在 &lt;context-param&gt; 节点配置如下参数： resteasy.servlet.mapping.prefix resteasy.scan resteasy.scan.providers resteasy.scan.resources resteasy.providers resteasy.use.builtin.providers resteasy.resources resteasy.jndi.resources javax.ws.rs.Application resteasy.media.type.mappings resteasy.language.mappings resteasy.document.expand.entity.references resteasy.document.secure.processing.feature resteasy.document.secure.disableDTDs resteasy.wider.request.matching resteasy.use.container.form.params 以上参数在需要使用的时候查阅官方文档的说明即可。 在 Servlet3.0 之前，你可以将 RESTEasy 配置为 ServletContextListener： 12345&lt;listener&gt; &lt;listener-class&gt; org.jboss.resteasy.plugins.server.servlet.ResteasyBootstrap &lt;/listener-class&gt;&lt;/listener&gt; 同样，在 Servlet3.0 之前，你可以将 RESTEasy 配置为 Servlet Filter： 123456789101112131415&lt;filter&gt; &lt;filter-name&gt;Resteasy&lt;/filter-name&gt; &lt;filter-class&gt; org.jboss.resteasy.plugins.server.servlet.FilterDispatcher &lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;javax.ws.rs.Application&lt;/param-name&gt; &lt;param-value&gt;com.restfully.shop.services.ShoppingApplication&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;Resteasy&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 常见的注解@Path and @GET, @POST一个示例代码： 12345678910111213141516171819202122@Path("/library")public class Library &#123; @GET @Path("/books") public String getBooks() &#123;...&#125; @GET @Path("/book/&#123;isbn&#125;") public String getBook(@PathParam("isbn") String id) &#123; // search my database and get a string representation and return it &#125; @PUT @Path("/book/&#123;isbn&#125;") public void addBook(@PathParam("isbn") String id, @QueryParam("name") String name) &#123;...&#125; @DELETE @Path("/book/&#123;id&#125;") public void removeBook(@PathParam("id") String id &#123;...&#125; &#125; 说明： 类或方法是存在 @Path 注解或者 HTTP 方法的注解 如果方法上没有 HTTP 方法的注解，则称为 JAXRSResourceLocators @Path 注解支持正则表达式映射 例如： 1234567@Path("/resources")public class MyResource &#123; @GET @Path("&#123;var:.*&#125;/stuff") public String get() &#123;...&#125;&#125; 下面的 GETs 请求会映射到 get() 方法： 123GET /resources/stuffGET /resources/foo/stuffGET /resources/on/and/on/stuff 表达式的格式是： 1&quot;&#123;&quot; variable-name [ &quot;:&quot; regular-expression ] &quot;&#125;&quot; 当正则表达式不存在时，类似于： 1&quot;([]*)&quot; 例如， `@Path(“/resources/{var}/stuff”)`` 将会匹配下面请求： 12GET /resources/foo/stuffGET /resources/bar/stuff 而不会匹配： 1GET /resources/a/bunch/of/stuff @PathParam@PathParam 是一个参数注解，可以将一个 URL 上的参数映射到方法的参数上，它可以映射到方法参数的类型有基本类型、字符串、或者任何有一个字符串作为构造方法参数的 Java 对象、或者一个有字符串作为参数的静态方法 valueOf 的 Java 对象。 例如： 12345678@GET@Path("/book/&#123;isbn&#125;")public String getBook(@PathParam("isbn") ISBN id) &#123;...&#125;public class ISBN &#123; public ISBN(String str) &#123;...&#125;&#125; 或者： 123public class ISBN &#123; public static ISBN valueOf(String isbn) &#123;...&#125;&#125; @Path 注解中可以使用 @PathParam 注解对应的参数，例如： 123@GET@Path("/aaa&#123;param:b+&#125;/&#123;many:.*&#125;/stuff")public String getIt(@PathParam("param") String bs, @PathParam("many") String many) &#123;...&#125; 对于下面的请求，对应的 param 和 many 变量如下： Request param many GET /aaabb/some/stuff bb some GET/aaab/a/lot/of/stuff b a/lot/of 另外，@PathParam 注解也可以将 URL 后面的多个参数映射到内置的 javax.ws.rs.core.PathSegment 对象，该对象定义如下： 123456789101112131415public interface PathSegment &#123; /** * Get the path segment. * &lt;p&gt; * @return the path segment */ String getPath(); /** * Get a map of the matrix parameters associated with the path segment * @return the map of matrix parameters */ MultivaluedMap&lt;String, String&gt; getMatrixParameters(); &#125; 使用 PathSegment 作为参数类型： 123@GET@Path("/book/&#123;id&#125;")public String getBook(@PathParam("id") PathSegment id) &#123;...&#125; 则下面请求会映射到 getBook 方法： 1GET http://host.com/library/book;name=EJB 3.0;author=Bill Burke @QueryParam对于下面的请求： 1GET /books?num=5 可以使用 @QueryParam 注解进行映射： 1234@GETpublic String getBooks(@QueryParam("num") int num) &#123;...&#125; @HeaderParam@HeaderParam 注解用于将 HTTP header 中参数映射到方法的调用上，例如从 http header 中获取 From 变量的值映射到 from 参数上： 1234@GETpublic String getBooks(@HeaderParam("From") String from) &#123;...&#125; 同 PathParam 注解一样，方法的参数类型可以是基本类型、字符串、或者任何有一个字符串作为构造方法参数的 Java 对象、或者一个有字符串作为参数的静态方法 valueOf 的 Java 对象，例如，MediaType 对象有个 valueOf() 方法： 12@PUTpublic void put(@HeaderParam("Content-Type") MediaType contentType, ...) @MatrixParam对于 URL 中的多参数，也可以使用 @MatrixParam 注解，例如对下面的请求， 1GET http://host.com/library/book;name=EJB 3.0;author=Bill Burke 可以使用下面代码来处理： 12@GETpublic String getBook(@MatrixParam("name") String name, @MatrixParam("author") String author) &#123;...&#125; @CookieParam获取 Cookie 参数： 1234567@GETpublic String getBooks(@CookieParam("sessionid") int id) &#123;...&#125;@GETpubli cString getBooks(@CookieParam("sessionid") javax.ws.rs.core.Cookie id) &#123;...&#125; 同 PathParam 注解一样，方法的参数类型可以是基本类型、字符串、或者任何有一个字符串作为构造方法参数的 Java 对象、或者一个有字符串作为参数的静态方法 valueOf 的 Java 对象。 @FormParam将表单中的字段映射到方法调用上，例如，对于下面的表单： 1234567&lt;form method="POST" action="/resources/service"&gt;First name: &lt;input type="text" name="firstname"&gt;&lt;br&gt;Last name: &lt;input type="text" name="lastname"&gt;&lt;/form&gt; 通过 post 方法提交，处理该请求的方法为： 1234567@Path("/")public class NameRegistry &#123; @Path("/resources/service") @POST public void addName(@FormParam("firstname") String first, @FormParam("lastname") String last) &#123;...&#125;&#125; 你也可以添加 application/x-www-form-urlencoded 来反序列化 URL 中的多参数： 12345678@Path("/")public class NameRegistry &#123; @Path("/resources/service") @POST @Consumes("application/x-www-form-urlencoded") public void addName(@FormParam("firstname") String first, MultivaluedMap&lt;String, String&gt; form) &#123;...&#125;&#125; @Form@FormParam 只是将表单字段绑定到方法的参数上，而 @Form 可以将表单绑定到一个对象上。 例如： 1234567891011121314151617181920212223242526272829303132333435public static class Person&#123; @FormParam("name") private String name; @Form(prefix = "invoice") private Address invoice; @Form(prefix = "shipping") private Address shipping;&#125;public static class Address&#123; @FormParam("street") private String street;&#125;@Path("person")public static class MyResource&#123; @POST @Produces(MediaType.TEXT_PLAIN) @Consumes(MediaType.APPLICATION_FORM_URLENCODED) public String post(@Form Person p)&#123; return p.toString(); &#125; &#125;~~~ 客户端可以提交下面的参数：~~~ name=billinvoice.street=xxxshipping.street=yyy 另外，也可以设置 prefix 参数，映射到 map 和 list： 12345678910111213141516171819202122public static class Person &#123; @Form(prefix="telephoneNumbers") List&lt;TelephoneNumber&gt; telephoneNumbers; @Form(prefix="address") Map&lt;String, Address&gt; addresses;&#125;public static class TelephoneNumber &#123; @FormParam("countryCode") private String countryCode; @FormParam("number") private String number;&#125;public static class Address &#123; @FormParam("street") private String street; @FormParam("houseNumber") private String houseNumber;&#125;@Path("person")public static class MyResource &#123; @POST @Consumes(MediaType.APPLICATION_FORM_URLENCODED) public void post (@Form Person p) &#123;&#125;&#125; 然后，提交下面的参数： 12345678request.addFormHeader("telephoneNumbers[0].countryCode", "31");request.addFormHeader("telephoneNumbers[0].number", "0612345678");request.addFormHeader("telephoneNumbers[1].countryCode", "91");request.addFormHeader("telephoneNumbers[1].number", "9717738723");request.addFormHeader("address[INVOICE].street", "Main Street");request.addFormHeader("address[INVOICE].houseNumber", "2");request.addFormHeader("address[SHIPPING].street", "Square One");request.addFormHeader("address[SHIPPING].houseNumber", "13"); @DefaultValue用于设置默认值。 12@GETpublic String getBooks(@QueryParam("num") @DefaultValue("10") int num) &#123;...&#125; @Encoded 和 @Encoding对 @*Params 注解的参数进行编解码。 @Context该注解允许你将以下对象注入到一个实例： javax.ws.rs.core.HttpHeaders, javax.ws.rs.core.UriInfo javax.ws.rs.core.Request javax.servlet.HttpServletRequest javax.servlet.HttpServletResponse javax.servlet.ServletConfig javax.servlet.ServletContext javax.ws.rs.core.SecurityContext @Produces 和 @Consumes@Consumes 注解定义对应的方法处理的 content-type 请求类型。 1234567891011@Consumes("text/*")@Path("/library")public class Library &#123; @POST public String stringBook(String book) &#123;...&#125; @Consumes("text/xml") @POST public String jaxbBook(Book book) &#123;...&#125;&#125; 当客户端发送下面请求时，stringBook() 方法会调用： 1234POST /librarycontent-type: text/plainthsi sis anice book 当客户端发送下面请求时，jaxbBook() 方法会调用： 1234POST /librarycontent-type: text/xml&lt;book name=&quot;EJB 3.0&quot; author=&quot;Bill Burke&quot;/&gt; @Produces 用于映射客户端的请求并匹配客户端请求的 Accept header。 例如，对下面的代码： 1234567891011@Produces("text/*")@Path("/library")public class Library &#123; @GET @Produces("application/json") public String getJSON() &#123;...&#125; @GET public String get() &#123;...&#125;&#125; 则，客户端发送下面请求时，getJSON() 会被调用。 12GET /libraryAccept: application/json 可以修改 web.xml 中的配置，对 Accept 和 Accept-Language 做一些映射。例如： 12345678910111213141516171819202122&lt;web-app&gt; &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;context-param&gt; &lt;param-name&gt;resteasy.media.type.mappings&lt;/param-name&gt; &lt;param-value&gt;html : text/html, json : application/json, xml : application/xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;context-param&gt; &lt;param-name&gt;resteasy.language.mappings&lt;/param-name&gt; &lt;param-value&gt;en : en-US, es : es, fr : fr&lt;/param-value&gt; &lt;/context-param&gt; &lt;servlet&gt; &lt;servlet-name&gt;Resteasy&lt;/servlet-name&gt; &lt;servlet-class&gt;org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;Resteasy&lt;/servlet-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; 如果你调用 /foo/bar.xml.en 的 GET 请求，则等同于发送下面的请求： 123GET /foo/barAccept: application/xmlAccept-Language: en-US 另外，你也可以设置参数映射，通过参数指定 content-type。修改 web.xml： 1234567891011121314151617&lt;web-app&gt; &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;context-param&gt; &lt;param-name&gt;resteasy.media.type.param.mapping&lt;/param-name&gt; &lt;param-value&gt;someName&lt;/param-value&gt; &lt;/context-param&gt; &lt;servlet&gt; &lt;servlet-name&gt;Resteasy&lt;/servlet-name&gt; &lt;servlet-class&gt;org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;Resteasy&lt;/servlet-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; 然后，可以通过调用 http://service.foo.com/resouce?someName=application/xml 来得到一个 application/xml 的返回结果。 @GZIP配置请求输出内容为 Gzip 压缩，例如： 1234567@Path("/")public interface MyProxy &#123; @Consumes("application/xml") @PUT public void put(@GZIP Order order);&#125; JAX-RS Resource Locators and Sub Resources前面提到当方法上只有 @Path 注解没有 HTTP 方法的注解时，则该方法为资源定位器，该方法可以返回一个子资源，然后由资源来定义映射路径和对应的 HTTP 方法。 例如： 12345678910111213141516171819202122232425@Path("/")public class ShoppingStore &#123; @Path("/customers/&#123;id&#125;") public Customer getCustomer(@PathParam("id") int id) &#123; Customer cust = ...; // Find a customer object return cust; &#125;&#125;public class Customer &#123; @GET public String get() &#123;...&#125; @Path("/address") public String getAddress() &#123;...&#125;&#125;public class CorporateCustomer extends Customer &#123; @Path("/businessAddress") public String getAddress() &#123;...&#125;&#125; 访问下面的请求时，会调用 ShoppingStore 类的 getCustomer 方法，然后调用 Customer 类的 get() 方法。 1GET /customer/123 类似地，下面的请求会返回 Customer 类的 getAddress() 方法的值。 1GET /customer/123/address CORS在 Application 类中，注册一个单例的提供者类： 12CorsFilter filter = new CorsFilter();filter.getAllowedOrigins().add("http://localhost"); Content-Range Support12345678910111213141516@Path("/")public class Resource &#123; @GET @Path("file") @Produces("text/plain") public File getFile() &#123; return file; &#125;&#125;Response response = client.target(generateURL("/file")).request() .header("Range", "1-4").get();Assert.assertEquals(response.getStatus(), 206);Assert.assertEquals(4, response.getLength());System.out.println("Content-Range: " + response.getHeaderString("Content-Range"));]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java笔记：异常]]></title>
    <url>%2F2015%2F03%2F04%2Fnote-about-java-exception%2F</url>
    <content type="text"><![CDATA[定义在《java编程思想》中这样定义异常：阻止当前方法或作用域继续执行的问题。异常是Java程序设计中不可分割的一部分，如果不了解如何使用它们，那么我们只能完成很有限的工作。 分类异常分为3种： Error - 描述了Java运行系统中的内部错误以及资源耗尽的情况。应用程序不应该抛出这种类型的对象。如果这种内部错误出现，除了通知用户错误发生以及尽力安全的退出程序外，在其他方面是无能为力的。 编译时异常 Exception - 它指出了合理的应用程序想要捕获的条件。Exception又分为两类：IOException和RuntimeException。由编程导致的错误，会导致RuntimeException异常。而其他错误原因导致的异常（例如因为I/O错误导致曾经运行正确的程序出错），都不会导致RuntimeException异常。 运行时 RuntimeException - 表示运行时异常，不强制要求写出显示的捕获代码，但如果没有被捕获到，则线程会被强制中断 继承关系： 1234Throwable|--Error |--Exception |--RuntimeException 从 RuntimeException 衍生出来的异常包括下面的问题： 错误的类型转换 数组越界访问 试图访问空指针 从 IOException 衍生出来的异常包括： 试图从文件尾后面读取数据 试图打开一个错误可是的 URL 试图用一个字符串来构造一个 Class 对象，而与该字符串对应的类并不存在 异常的好处： 1、将问题进行封装 2、将正常流程代码和问题代码相分离，便于阅读。 处理异常机制在Java应用程序中，异常处理机制为：抛出异常，捕捉异常。 抛出异常：当一个方法出现错误引发异常时，方法创建异常对象并交付运行时系统，异常对象中包含了异常类型和异常出现时的程序状态等异常信息。运行时系统负责寻找处置异常的代码并执行。 捕获异常：在方法抛出异常之后，运行时系统将转为寻找合适的异常处理器（exception handler）。潜在的异常处理器是异常发生时依次存留在调用栈中的方法的集合。当异常处理器所能处理的异常类型与方法抛出的异常类型相符时，即为合适的异常处理器。运行时系统从发生异常的方法开始，依次回查调用栈中的方法，直至找到含有合适异常处理器的方法并执行。当运行时系统遍历调用栈而未找到合适的异常处理器，则运行时系统终止。同时，意味着Java程序的终止。 任何 Java 代码都可以抛出异常，如：自己编写的代码、来自 Java 开发环境包中代码，或者 Java 运行时系统。无论是谁，都可以通过 Java 的 throw 语句抛出异常。 从方法中抛出的任何异常都必须使用 throws 子句。 捕捉异常通过 try-catch 语句或者 try-catch-finally 语句实现。 原则编译时异常Exception,给了几条禁止的原则，他们是： 1）不要直接忽略异常； 2）不要用try-catch包住过多语句； 3）不要用异常处理来处理程序的正常控制流； 4）不要随便将异常迎函数栈向上传递，能处理尽量处理。 向上传播异常： 如果不能用上述恢复措施，就检查能不能向上传播，什么情况下可以向上传播呢？有多种说法，一种说法是当本方法恢复不了时，这个说法显然是错误，因为上层也不一定能恢复。另外还有两种说法是：1.当上层逻辑可以恢复程序时；2.当本方法除了打印之外不能做任何处理，而且不确定上层能否处理。这种两种说法都是正确的，但还不够，因为也有的情况，明确知道上层恢复不了也需要上层处理，所以我认为正确的做法是：当你认为本异常应该由上层处理时，才向上传播。 何时选用编译时异常： 1、如果调用者可以恢复此异常情况 2、如果调用者不能恢复，但能做出有意义的事，如转译等。如果你不确定调用者能否做出有意义的事，就别使编译时异常，免得被抱怨。 3、应尽最大可能使用编译时异常来代替错误码，这条也是编译时异常设计的目的。 另外，必须注意使用编译时异常的目的是为了恢复执行，所以设计异常类的时候，应提供尽量多的异常数据，以便于上层恢复，比如一个解析错误，可以在设计的异常类写几个变量来存储异常数据：解析出错的句子的内容，解析出错句子的行号，解析出错的字符在行中的位置。这些信息可能帮助调用恢复程序。 注意事项当使用多个 catch 语句块来捕获异常时，需要将父类的 catch 语句块放到子类型的 catch 块之后，这样才能保证后续的 catch 可能被执行，否则子类型的 catch 将永远无法到达，Java 编译器会报编译错误。 如果 try 语句块中存在 return 语句，那么首先会执行 finally 语句块中的代码，然后才返回。 如果 try 语句块中存在 System.exit(0) 语句，那么久不会执行 finally 语句块的代码了，因为 System.exit(0)会终止当前运行的 JVM。程序在 JVM 终止前结束执行。 常见面试题目 1、error和exception有什么区别 error 表示系统级的错误，是java运行环境内部错误或者硬件问题，不能指望程序来处理这样的问题，除了退出运行外别无选择，它是Java虚拟机抛出的。 exception 表示程序需要捕捉、需要处理的异常，是由与程序设计的不完善而出现的问题，程序必须处理的问题 2、运行时异常和一般异常有何不同 Java 提供了两类主要的异常：runtimeException 和 checkedException 一般异常（checkedException）主要是指 IO 异常、SQL 异常等。对于这种异常，JVM 要求我们必须对其进行 cathc 处理，所以，面对这种异常，不管我们是否愿意，都是要 写一大堆的 catch 块去处理可能出现的异常。 运行时异常（runtimeException）我们一般不处理，当出现这类异常的时候程序会由虚拟机接管。比如，我们从来没有去处理过 NullPointerException，而且这个异常还是最 常见的异常之一。 出现运行时异常的时候，程序会将异常一直向上抛，一直抛到遇到处理代码，如果没有 catch 块进行处理，到了最上层，如果是多线程就有 Thread.run()抛出，如果不是多线程 那么就由 main.run() 抛出。抛出之后，如果是线程，那么该线程也就终止了，如果是主程序，那么该程序也就终止了。 其实运行时异常的也是继承自 Exception，也可以用 catch 块对其处理，只是我们一般不处理罢了，也就是说，如果不对运行时异常进行 catch 处理，那么结果不是线程退出就是 主程序终止。 如果不想终止，那么我们就必须捕获所有可能出现的运行时异常。如果程序中出现了异常数据，但是它不影响下面的程序执行，那么我们就该在catch块里面将异常数据舍弃， 然后记录日志。如果，它影响到了下面的程序运行，那么还是程序退出比较好些。 3、Java 中异常处理机制的原理 Jav a通过面向对象的方式对异常进行处理，Java 把异常按照不同的类型进行分类，并提供了良好的接口。在 Java 中，每个异常都是一个对象，它都是 Throwable 或其子类的实例。当一个方法出现异常后就会抛出一个异常对象，该对象中包含有异常信息，调用这个对象的方法可以捕获到这个异常并对异常进行处理。Java 的异常处理是通过5个 关键词来实现的：try、catch、throw、throws、finally。 try：用来指定一块预防所有异常的程序 catch：紧跟在try后面，用来捕获异常 throw：用来明确的抛出一个异常 throws：用来标明一个成员函数可能抛出的各种异常 finally：确保一段代码无论发生什么异常都会被执行的一段代码。 4、你平时在项目中是怎样对异常进行处理的。 1）尽量避免出现 runtimeException。例如对于可能出现空指针的代码，带使用对象之前一定要判断一下该对象是否为空，必要的时候对 runtimeException也进行 try catch 处理。 2）进行 try catch 处理的时候要在 catch 代码块中对异常信息进行记录，通过调用异常类的相关方法获取到异常的相关信息。 5、final、finally、finalize的区别 （1）、final 用于声明变量、方法和类的，分别表示变量值不可变，方法不可覆盖，类不可以继承 （2）、finally 是异常处理中的一个关键字，表示 finally{} 里面的代码一定要执行 （3）、finalize 是 Object 类的一个方法，在垃圾回收的时候会调用被回收对象的此方法。 参考资料 JAVA异常笔记 Java异常学习笔记]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装和配置Hue]]></title>
    <url>%2F2015%2F02%2F28%2Finstall-and-config-hue%2F</url>
    <content type="text"><![CDATA[本文主要记录使用 yum 源安装 Hue 以及配置 Hue 集成 Hdfs、Hive、Impala、Yarn、Kerberos、LDAP、Sentry、Solr 等的过程。 安装环境： 操作系统：CentOs6.5 Hadoop：cdh5.2.0 Hue：3.6.0+cdh5.2.0 安装 Hue在 Hadoop 集群的一个节点上安装 Hue server，这里我是在我的测试集群中的 cdh1 节点上安装： 1$ yum install hue hue-server 配置 Hue配置hue server12345[desktop] http_host=cdh1 http_port=8888 secret_key=qpbdxoewsqlkhztybvfidtvwekftusgdlofbcfghaswuicmqp time_zone=Asia/Shanghai 如果想配置 SSL，则添加下面设置： 12ssl_certificate=/path/to/certificatessl_private_key=/path/to/key 并使用下面命令生成证书： 1234# Create a key $ openssl genrsa 1024 &gt; host.key # Create a self-signed certificate $ openssl req -new -x509 -nodes -sha1 -key host.key &gt; host.cert 配置 DB QueryDB Query 的相关配置在 hue.ini 中 databases 节点下面，目前共支持 sqlite, mysql, postgresql 和 oracle 四种数据库，默认使用的是 sqlite 数据库，你可以按自己的需要修改为其他的数据库。 123[[database]] engine=sqlite3 name=/var/lib/hue/desktop.db 配置 Hadoop 参数HDFS 集群配置在 hadoop.hdfs_clusters.default 节点下配置以下参数： fs_defaultfs： logical_name： NameNode 逻辑名称 webhdfs_url： security_enabled：是否开启 Kerberos hadoop_conf_dir： hadoop 配置文件路径 完整配置如下： 12345678910111213141516171819[hadoop] [[hdfs_clusters]] [[[default]]] # Enter the filesystem uri fs_defaultfs=hdfs://mycluster # NameNode logical name. logical_name=mycluster # Use WebHdfs/HttpFs as the communication mechanism. # Domain should be the NameNode or HttpFs host. # Default port is 14000 for HttpFs. ## webhdfs_url=http://localhost:50070/webhdfs/v1 webhdfs_url=http://cdh1:14000/webhdfs/v1 # Change this if your HDFS cluster is Kerberos-secured security_enabled=true hadoop_conf_dir=/etc/hadoop/conf 配置 WebHDFS 或者 HttpFSHue 可以通过下面两种方式访问 Hdfs 中的数据： WebHDFS：提供高速的数据传输，客户端直接和 DataNode 交互 HttpFS：一个代理服务，方便与集群外部的系统集成 两者都支持 HTTP REST API，但是 Hue 只能配置其中一种方式；对于 HDFS HA部署方式，只能使用 HttpFS。 1、对于 WebHDFS 方式，在每个节点上的 hdfs-site.xml 文件添加如下配置并重启服务： 1234&lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 2、 配置 Hue 为其他用户和组的代理用户。对于 WebHDFS 方式，在 core-site.xml 添加： 123456789&lt;!-- Hue WebHDFS proxy user setting --&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; 对于 HttpFS 方式，在 /etc/hadoop-httpfs/conf/httpfs-site.xml 中添加下面配置并重启 HttpFS 进程： 123456789&lt;!-- Hue HttpFS proxy user setting --&gt;&lt;property&gt; &lt;name&gt;httpfs.proxyuser.hue.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;httpfs.proxyuser.hue.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; 对于 HttpFS 方式，在 core-site.xml 中添加下面配置并重启 hadoop 服务： 12345678&lt;property&gt; &lt;name&gt;hadoop.proxyuser.httpfs.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.httpfs.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; 3、修改 /etc/hue/conf/hue.ini 中 hadoop.hdfs_clusters.default.webhdfs_url 属性。 对于 WebHDFS： 1webhdfs_url=http://cdh1:50070/webhdfs/v1/ 对于 HttpFS： 1webhdfs_url=http://cdh1:14000/webhdfs/v1/ YARN 集群配置在 hadoop.yarn_clusters.default 节点下配置： 12345678910[hadoop] [[yarn_clusters]] [[[default]]] resourcemanager_host=cdh1 resourcemanager_port=8032 submit_to=True security_enabled=true resourcemanager_api_url=http://cdh1:8088 proxy_api_url=http://cdh1:8088 history_server_api_url=http://cdh1:19888 集成 Hive在 beeswax 节点下配置： 1234[beeswax] hive_server_host=cdh1 hive_server_port=10000 hive_conf_dir=/etc/hive/conf 这里是配置为连接一个 Hive Server2 节点，如有需要可以配置负载均衡，连接一个负载节点。 集成 Impala在 impala 节点下配置 123456789101112[impala] # Host of the Impala Server (one of the Impalad) server_host=cdh1 # Port of the Impala Server server_port=21050 # Kerberos principal impala_principal=impala/cdh1@JAVACHEN.COM # Turn on/off impersonation mechanism when talking to Impala impersonation_enabled=True 这里是配置为连接一个 Impala Server 节点，如有需要可以配置负载均衡，连接一个负载节点。 参考 Configuring Per-User Access for Hue 和 Use the Impala App with Sentry for real security，在配置 impersonation_enabled 为 true 的情况下，还需要在 impalad 的启动参数中添加 authorized_proxy_user_config 参数，修改 /etc/default/impala中的 IMPALA_SERVER_ARGS 添加下面一行： 1-authorized_proxy_user_config=hue=* \ 另外，如果集群开启了 Kerberos，别忘了配置 impala_principal 参数。 集成 kerberos首先，需要在 kerberos server 节点上生成 hue 用户的凭证，并将其拷贝到 /etc/hue/conf 目录。： 1234$ kadmin: addprinc -randkey hue/cdh1@JAVACHEN.COM$ kadmin: xst -k hue.keytab hue/cdh1@JAVACHEN.COM$ cp hue.keytab /etc/hue/conf/ 然后，修改 hue.ini 中 kerberos 节点： 123456789[[kerberos]] # Path to Hue's Kerberos keytab file hue_keytab=/etc/hue/conf/hue.keytab # Kerberos principal name for Hue hue_principal=hue/cdh1@JAVACHEN.COM # Path to kinit kinit_path=/usr/bin/kinit 接下来，修改 /etc/hadoop/conf/core-site.xml，添加： 12345678910111213&lt;!--hue kerberos--&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hue.kerberos.principal.shortname&lt;/name&gt; &lt;value&gt;hue&lt;/value&gt;&lt;/property&gt; 最后，重启 hadoop 服务。 集成 LDAP开启 ldap 验证，使用 ldap 用户登录 hue server，修改 auth 节点： 123[desktop] [[auth]] backend=desktop.auth.backend.LdapBackend 另外修改 ldap 节点： 12345678910111213[desktop] [[ldap]] base_dn="dc=javachen,dc=com" ldap_url=ldap://cdh1 # ldap用户登陆时自动在hue创建用户 create_users_on_login = true # 开启direct bind mechanism search_bind_authentication=false # ldap登陆用户的模板，username运行时被替换 ldap_username_pattern="uid=&lt;username&gt;,ou=people,dc=javachen,dc=com" 注意：在开启ldap验证前，先普通方法创建一个ldap存在的用户，赋超级用户权限，否则无法管理hue用户。 集成 Sentry如果 hive 和 impala 中集成了 Sentry，则需要修改 hue.ini 中的 libsentry 节点： 123456789[libsentry] # Hostname or IP of server. hostname=cdh1 # Port the sentry service is running on. port=8038 # Sentry configuration directory, where sentry-site.xml is located. sentry_conf_dir=/etc/sentry/conf 另外，修改 /etc/sentry/conf/sentry-store-site.xml 确保 hue 用户可以连接 sentry： 1234&lt;property&gt; &lt;name&gt;sentry.service.allow.connect&lt;/name&gt; &lt;value&gt;impala,hive,solr,hue&lt;/value&gt;&lt;/property&gt; 集成 Sqoop2在 sqoop 节点配置 server_url 参数为 sqoop2 的地址即可。 集成 HBase在 hbase 节点配置下面参数： truncate_limit：Hard limit of rows or columns per row fetched before truncating. hbase_clusters：HBase Thrift 服务列表，例如：Cluster1|cdh1:9090,Cluster2|cdh2:9090，默认为：Cluster|localhost:9090 集成 Zookeeper在 zookeeper 节点配置下面两个参数： host_ports：zookeeper 节点列表，例如：localhost:2181,localhost:2182,localhost:2183 rest_url：zookeeper 的 REST 接口，默认值为 http://localhost:9998 集成 Oozie未使用，暂不记录。 管理 Hue如果配置了 kerberos，则先获取 hue 凭证： 1kinit -k -t /etc/hue/conf/hue.keytab hue/cdh1@JAVACHEN.COM 启动 hue server： 1$ service hue start 停止 hue server： 1$ service hue stop hue server 默认使用 8888 作为 web 访问端口，故需要在防火墙上开放该端口。 你可以在 /var/log/hue 目录查看 hue 的日志，或者通过 http://cdh1:8888/logs 查看。 测试在开启了 LDAP 后，使用 LDAP 中的管理员用户登录 hue，根据提示向导进行设置并将 LDAP 中的用户同步到 Hue Server，然后依次测试每一个功能是否运行正常。 关于 Hue 的使用向导，请参考 http://archive.cloudera.com/cdh5/cdh/5/hue/user-guide/index.html。 Enjoy it !]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>hue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Streaming 原理]]></title>
    <url>%2F2015%2F02%2F12%2Fhadoop-streaming%2F</url>
    <content type="text"><![CDATA[简介Hadoop Streaming 是 Hadoop 提供的一个 MapReduce 编程工具，它允许用户使用任何可执行文件、脚本语言或其他编程语言来实现 Mapper 和 Reducer，从而充分利用 Hadoop 并行计算框架的优势和能力，来处理大数据。 一个简单的示例，以 shell 脚本为例： 12345hadoop jar hadoop-streaming.jar \ -input myInputDirs \ -output myOutputDir \ -mapper /bin/cat \ -reducer /usr/bin/wc Streaming 方式是 基于 Unix 系统的标准输入输出 来进行 MapReduce Job 的运行，它区别与 Pipes 的地方主要是通信协议，Pipes 使用的是 Socket 通信，是对使用 C++ 语言来实现 MapReduce Job 并通过 Socket 通信来与 Hadopp 平台通信，完成 Job 的执行。 任何支持标准输入输出特性的编程语言都可以使用 Streaming 方式来实现 MapReduce Job，基本原理就是输入从 Unix 系统标准输入，输出使用 Unix 系统的标准输出。 Hadoop 是使用 Java 语言编写的，所以最直接的方式的就是使用 Java 语言来实现 Mapper 和 Reducer，然后配置 MapReduce Job，提交到集群计算环境来完成计算。但是很多开发者可能对 Java 并不熟悉，而是对一些具有脚本特性的语言，如 C++、Shell、Python、 Ruby、PHP、Perl 有实际开发经验，Hadoop Streaming 为这一类开发者提供了使用 Hadoop 集群来进行处理数据的工具，即工具包 hadoop-streaming.jar。 在标准的输入输出中，Key 和 Value 是以 Tab 作为分隔符，并且在 Reducer 的标准输入中，Hadoop 框架保证了输入的数据是经过了按 Key 排序的。 原理Hadoop Streaming 使用了 Unix 的标准输入输出作为 Hadoop 和其他编程语言的开发接口，因此在其他的编程语言所写的程序中，只需要将标准输入作为程序的输入，将标准输出作为程序的输出就可以了。 mapper 和 reducer 会从标准输入中读取用户数据，一行一行处理后发送给标准输出。Streaming 工具会创建 MapReduce 作业，发送给各个 tasktracker，同时监控整个作业的执行过程。 如果一个文件（可执行或者脚本）作为 mapper，mapper 初始化时，每一个 mapper 任务会把该文件作为一个单独进程启动，mapper 任务运行时，它把输入切分成行并把每一行提供给可执行文件进程的标准输入。 同时，mapper 收集可执行文件进程标准输出的内容，并把收到的每一行内容转化成 key/value 对，作为 mapper 的输出。 默认情况下，一行中第一个 tab 之前的部分作为 key，之后的（不包括tab）作为 value。如果没有 tab，整行作为 key 值，value 值为 null。 对于 reducer，类似。 以上是 Map/Reduce 框架和 streaming mapper/reducer 之间的基本通信协议。 用户可以定义 stream.non.zero.exit.is.failure 参数为 true 或者 false 以定义一个以非0状态退出的 streaming 的任务是失败还是成功。默认情况下，以非0状态退出的任务都任务是失败的。 用法命令如下： 1hadoop jar hadoop-streaming.jar [genericOptions] [streamingOptions] streaming 参数以 Hadoop 2.6.0 为例，可选的 streaming 参数如下： 参数 是否可选 描述 -input directoryname or filename Required mapper的输入路径 -output directoryname Required reducer输出路径 -mapper executable or JavaClassName Required Mapper可执行程序或 Java 类名 -reducer executable or JavaClassName Required Reducer 可执行程序或 Java 类名 -file filename Optional mapper, reducer 或 combiner 依赖的文件 -inputformat JavaClassName Optional key/value 输入格式，默认为 TextInputFormat -outputformat JavaClassName Optional key/value 输出格式，默认为 TextOutputformat -partitioner JavaClassName Optional Class that determines which reduce a key is sent to -combiner streamingCommand or JavaClassName Optional map 输出结果执行 Combiner 的命令或者类名 -cmdenv name=value Optional 环境变量 -inputreader Optional 向后兼容，定义输入的 Reader 类，用于取代输出格式 -verbose Optional 输出日志 -lazyOutput Optional 延时输出 -numReduceTasks Optional 定义 reduce 数量 -mapdebug Optional map 任务运行失败时候，执行的脚本 -reducedebug Optional reduce 任务运行失败时候，执行的脚本 定义 Java 类作为 mapper 和 reducer： 123456hadoop jar hadoop-streaming.jar \ -input myInputDirs \ -output myOutputDir \ -inputformat org.apache.hadoop.mapred.KeyValueTextInputFormat \ -mapper org.apache.hadoop.mapred.lib.IdentityMapper \ -reducer /usr/bin/wc 如果 mapper 和 reducer 的可执行文件在集群上不存在，则可以通过 -file 参数将其提交到集群上去： 123456hadoop jar hadoop-streaming.jar \ -input myInputDirs \ -output myOutputDir \ -mapper myPythonScript.py \ -reducer /usr/bin/wc \ -file myPythonScript.py 你也可以将 mapper 和 reducer 的可执行文件用到的文件和配置上传到集群上： 1234567hadoop jar hadoop-streaming.jar \ -input myInputDirs \ -output myOutputDir \ -mapper myPythonScript.py \ -reducer /usr/bin/wc \ -file myPythonScript.py \ -file myDictionary.txt 你也可以定义其他参数： 1234-inputformat JavaClassName-outputformat JavaClassName-partitioner JavaClassName-combiner streamingCommand or JavaClassName 定义一个环境变量： 1-cmdenv EXAMPLE_DIR=/home/example/dictionaries/ 通用参数 参数 是否可选 描述 -conf configuration_file Optional 定义应用的配置文件 -D property=value Optional 定义参数 -fs host:port or local Optional 定义 namenode 地址 -files Optional 定义需要拷贝到 Map/Reduce 集群的文件，多个文件以逗号分隔 -libjars Optional 定义需要引入到 classpath 的 jar 文件，多个文件以逗号分隔 -archives Optional 定义需要解压到计算节点的压缩文件，多个文件以逗号分隔 定义参数： 123-D mapred.local.dir=/tmp/local-D mapred.system.dir=/tmp/system-D mapred.temp.dir=/tmp/temp 定义 reduce 个数： 1-D mapreduce.job.reduces=0 你也可以使用 -D stream.reduce.output.field.separator=SEP 和 -D stream.num.reduce.output.fields=NUM 自定义 mapper 输出的分隔符为SEP，并且按 SEP 分隔之后的前 NUM 部分内容作为 key，如果分隔符少于 NUM，则整行作为 key。例如，下面的例子指定分隔符为 ....： 1234567hadoop jar hadoop-streaming.jar \ -D stream.map.output.field.separator=. \ -D stream.num.map.output.key.fields=4 \ -input myInputDirs \ -output myOutputDir \ -mapper /bin/cat \ -reducer /bin/cat hadoop 提供配置供用户自主设置分隔符： -D stream.map.output.field.separator ：设置 map 输出中 key 和 value 的分隔符-D stream.num.map.output.key.fields ：设置 map 程序分隔符的位置，该位置之前的部分作为 key，之后的部分作为 value-D map.output.key.field.separator : 设置 map 输出分区时 key 内部的分割符-D mapreduce.partition.keypartitioner.options : 指定分桶时，key 按照分隔符切割后，其中用于分桶 key 所占的列数（配合 -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner 使用）-D stream.reduce.output.field.separator：设置 reduce 输出中 key 和 value 的分隔符-D stream.num.reduce.output.key.fields：设置 reduce 程序分隔符的位置 定义解压文件： 1234567891011121314151617181920212223242526272829303132333435363738$ ls test_jar/cache.txt cache2.txt$ jar cvf cachedir.jar -C test_jar/ .added manifestadding: cache.txt(in = 30) (out= 29)(deflated 3%)adding: cache2.txt(in = 37) (out= 35)(deflated 5%)$ hdfs dfs -put cachedir.jar samples/cachefile$ hdfs dfs -cat /user/root/samples/cachefile/input.txtcachedir.jar/cache.txtcachedir.jar/cache2.txt$ cat test_jar/cache.txtThis is just the cache string$ cat test_jar/cache2.txtThis is just the second cache string$ hadoop jar hadoop-streaming.jar \ -archives 'hdfs://hadoop-nn1.example.com/user/root/samples/cachefile/cachedir.jar' \ -D mapreduce.job.maps=1 \ -D mapreduce.job.reduces=1 \ -D mapreduce.job.name="Experiment" \ -input "/user/root/samples/cachefile/input.txt" \ -output "/user/root/samples/cachefile/out" \ -mapper "xargs cat" \ -reducer "cat"$ hdfs dfs -ls /user/root/samples/cachefile/outFound 2 items-rw-r--r-- 1 root supergroup 0 2013-11-14 17:00 /user/root/samples/cachefile/out/_SUCCESS-rw-r--r-- 1 root supergroup 69 2013-11-14 17:00 /user/root/samples/cachefile/out/part-00000$ hdfs dfs -cat /user/root/samples/cachefile/out/part-00000This is just the cache stringThis is just the second cache string 复杂的例子Hadoop Partitioner ClassHadoop 中有一个类 KeyFieldBasedPartitioner，可以将 map 输出的内容按照分隔后的一定列，而不是整个 key 内容进行分区，例如： 1234567891011hadoop jar hadoop-streaming.jar \ -D stream.map.output.field.separator=. \ -D stream.num.map.output.key.fields=4 \ -D map.output.key.field.separator=. \ -D mapreduce.partition.keypartitioner.options=-k1,2 \ -D mapreduce.job.reduces=12 \ -input myInputDirs \ -output myOutputDir \ -mapper /bin/cat \ -reducer /bin/cat \ -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner 关键参数说明： map.output.key.field.separator=.：设置 map 输出分区时 key 内部的分割符为 . mapreduce.partition.keypartitioner.options=-k1,2：设置按前两个字段分区 mapreduce.job.reduces=12：reduce 数为12 假设 map 的输出为： 1234511.12.1.211.14.2.311.11.4.111.12.1.111.14.2.2 按照前两个字段进行分区，则会分为三个分区： 123456711.11.4.1-----------11.12.1.211.12.1.1-----------11.14.2.311.14.2.2 在每个分区内对整行内容排序后为： 123456711.11.4.1-----------11.12.1.111.12.1.2-----------11.14.2.211.14.2.3 Hadoop Comparator ClassHadoop 中有一个类 KeyFieldBasedComparator，提供了 Unix/GNU 中排序的一部分特性。 1234567891011hadoop jar hadoop-streaming.jar \ -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \ -D stream.map.output.field.separator=. \ -D stream.num.map.output.key.fields=4 \ -D mapreduce.map.output.key.field.separator=. \ -D mapreduce.partition.keycomparator.options=-k2,2nr \ -D mapreduce.job.reduces=1 \ -input myInputDirs \ -output myOutputDir \ -mapper /bin/cat \ -reducer /bin/cat 关键参数说明： mapreduce.partition.keycomparator.options=-k2,2nr：指定第二个字段为排序字段，-n 是指按自然顺序排序，-r 指倒叙排序。 假设 map 的输出为： 1234511.12.1.211.14.2.311.11.4.111.12.1.111.14.2.2 则 reduce 输出结果为： 1234511.14.2.311.14.2.211.12.1.211.12.1.111.11.4.1 Hadoop Aggregate PackageHadoop 中有一个类 Aggregate，Aggregate 提供了一个特定的 reduce 类和 combiner 类，以及一些对 reduce 输出的聚合函数，例如 sum、min、max 等等。 为了使用 Aggregate，只需要定义 -reducer aggregate： 123456hadoop jar hadoop-streaming.jar \ -input myInputDirs \ -output myOutputDir \ -mapper myAggregatorForKeyCount.py \ -reducer aggregate \ -file myAggregatorForKeyCount.py \ myAggregatorForKeyCount.py 文件大概内容如下： 12345678910111213141516171819#!/usr/bin/pythonimport sys;def generateLongCountToken(id): return "LongValueSum:" + id + "\t" + "1"def main(argv): line = sys.stdin.readline(); try: while line: line = line[:-1]; fields = line.split("\t"); print generateLongCountToken(fields[0]); line = sys.stdin.readline(); except "end of file": return Noneif __name__ == "__main__": main(sys.argv) Hadoop Field Selection ClassHadoop 中有一个类 FieldSelectionMapReduce，运行你像 unix 中的 cut 命令一样处理文本。 例子： 12345678910111213hadoop jar hadoop-streaming.jar \ -D mapreduce.map.output.key.field.separator=. \ -D mapreduce.partition.keypartitioner.options=-k1,2 \ -D mapreduce.fieldsel.data.field.separator=. \ -D mapreduce.fieldsel.map.output.key.value.fields.spec=6,5,1-3:0- \ -D mapreduce.fieldsel.reduce.output.key.value.fields.spec=0-2:5- \ -D mapreduce.map.output.key.class=org.apache.hadoop.io.Text \ -D mapreduce.job.reduces=12 \ -input myInputDirs \ -output myOutputDir \ -mapper org.apache.hadoop.mapred.lib.FieldSelectionMapReduce \ -reducer org.apache.hadoop.mapred.lib.FieldSelectionMapReduce \ -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner 关键参数说明： mapreduce.fieldsel.map.output.key.value.fields.spec=6,5,1-3:0-：意思是 map 的输出中 key 部分包括分隔后的第 6、5、1、2、3列，而 value 部分包括分隔后的所有的列 mapreduce.fieldsel.reduce.output.key.value.fields.spec=0-2:5-：意思是 map 的输出中 key 部分包括分隔后的第 0、1、2列，而 value 部分包括分隔后的从第5列开始的所有列 测试上面讲了 Hadoop Streaming 的原理和一些用法，现在来运行一些例子做测试。关于如何用 Python 来编写 Hadoop Streaming 程序，可以参考 Writing an Hadoop MapReduce Program in Python，中文翻译在 这里，其他非 Java 的语言，都可以参照这篇文章。 下面以 word count 为例做测试。 准备测试数据同 Writing an Hadoop MapReduce Program in Python，我们使用古腾堡项目中的三本电子书作为测试： The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson The Notebooks of Leonardo Da Vinci Ulysses by James Joyce 下载这些电子书的 txt格式，并将其上传到 hdfs： 12345678910111213$ mkdir /tmp/gutenberg/ &amp;&amp; cd /tmp/gutenberg/$ wget http://www.gutenberg.org/files/20417/20417.txt$ wget http://www.gutenberg.org/cache/epub/5000/pg5000.txt$ wget http://www.gutenberg.org/files/4300/4300.txt$ hadoop fs -copyFromLocal /tmp/gutenberg gutenberg$ hadoop fs -ls gutenbergFound 4 items-rw-r--r-- 3 hive hive 674762 2015-02-11 17:34 gutenberg/20417.txt-rw-r--r-- 3 hive hive 1573079 2015-02-11 17:34 gutenberg/4300.txt-rw-r--r-- 3 hive hive 1423803 2015-02-11 17:34 gutenberg/pg5000.txt 编写 Shell 版程序mapper.sh 如下： 12345678#! /bin/bashwhile read LINE; do for word in $LINE do echo "$word 1" donedone reducer.sh 程序如下： 1234567891011121314151617#! /bin/bashcount=0started=0word=""while read LINE;do newword=`echo $LINE | cut -d ' ' -f 1` if [ "$word" != "$newword" ];then [ $started -ne 0 ] &amp;&amp; echo -e "$word\t$count" word=$newword count=1 started=1 else count=$(( $count + 1 )) fidoneecho -e "$word\t$count" 在本机以脚本方式测试： 12345$ echo "foo foo quux labs foo bar quux" | sh mapper.sh |sort -k1,1| sh reducer.shbar 1foo 3labs 1quux 2 以 Hadoop Streaming 方式运行： 123456789101112131415161718192021222324$ hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \ -D mapred.reduce.tasks=6 \ -input gutenberg/* \ -output gutenberg-output \ -mapper mapper.sh\ -reducer reducer.sh\ -file mapper.sh \ -file reducer.sh15/02/11 17:50:59 INFO mapreduce.Job: map 0% reduce 0%15/02/11 17:51:18 INFO mapreduce.Job: map 17% reduce 0%15/02/11 17:51:52 INFO mapreduce.Job: map 17% reduce 6%15/02/11 17:51:53 INFO mapreduce.Job: map 33% reduce 6%15/02/11 17:51:55 INFO mapreduce.Job: map 60% reduce 17%15/02/11 17:51:56 INFO mapreduce.Job: map 100% reduce 17%15/02/11 17:51:59 INFO mapreduce.Job: map 100% reduce 67%15/02/11 17:53:11 INFO mapreduce.Job: map 100% reduce 68%15/02/11 17:54:49 INFO mapreduce.Job: map 100% reduce 69%15/02/11 17:57:12 INFO mapreduce.Job: map 100% reduce 70%15/02/11 17:58:45 INFO mapreduce.Job: map 100% reduce 71%15/02/11 17:58:55 INFO mapreduce.Job: map 100% reduce 81%15/02/11 17:59:05 INFO mapreduce.Job: map 100% reduce 100%15/02/11 17:59:08 INFO streaming.StreamJob: Job complete: job_1421752803837_573615/02/11 17:59:09 INFO streaming.StreamJob: Output: /user/root/gutenberg-output 编写 Python 版程序mapper.py 程序如下： 123456789101112131415161718192021222324#!/usr/bin/env python"""A more advanced Mapper, using Python iterators and generators."""import sysdef read_input(file): for line in file: # split the line into words yield line.split()def main(separator='\t'): # input comes from STDIN (standard input) data = read_input(sys.stdin) for words in data: # write the results to STDOUT (standard output); # what we output here will be the input for the # Reduce step, i.e. the input for reducer.py # # tab-delimited; the trivial word count is 1 for word in words: print '%s%s%d' % (word, separator, 1)if __name__ == "__main__": main() reducer.py 程序如下： 12345678910111213141516171819202122232425262728#!/usr/bin/env python"""A more advanced Reducer, using Python iterators and generators."""from itertools import groupbyfrom operator import itemgetterimport sysdef read_mapper_output(file, separator='\t'): for line in file: yield line.rstrip().split(separator, 1)def main(separator='\t'): # input comes from STDIN (standard input) data = read_mapper_output(sys.stdin, separator=separator) # groupby groups multiple word-count pairs by word, # and creates an iterator that returns consecutive keys and their group: # current_word - string containing a word (the key) # group - iterator yielding all ["&amp;lt;current_word&amp;gt;", "&amp;lt;count&amp;gt;"] items for current_word, group in groupby(data, itemgetter(0)): try: total_count = sum(int(count) for current_word, count in group) print "%s%s%d" % (current_word, separator, total_count) except ValueError: # count was not a number, so silently discard this item passif __name__ == "__main__": main() 关于 Java 的一些例子，这个需要单独创建一个 maven 工程，然后做一些测试。 注意事项mapper 中不能使用 shell 的别名，但可以使用变量1234567891011121314151617$ hdfs dfs -cat /user/me/samples/student_marksalice 50bruce 70charlie 80dan 75$ c2='cut -f2'; hadoop jar hadoop-streaming-2.6.0.jar \ -D mapreduce.job.name='Experiment' \ -input /user/me/samples/student_marks \ -output /user/me/samples/student_out \ -mapper "$c2" -reducer 'cat'$ hdfs dfs -cat /user/me/samples/student_out/part-0000050707580 mapper 中不能使用 unix 的管道-mapper 中使用 “cut -f1 | sed s/foo/bar/g”，会出现 java.io.IOException: Broken pipe 异常 指定 streaming 临时空间1-D stream.tmpdir=/export/bigspace/... 指定多个输入文件123hadoop jar hadoop-streaming-2.6.0.jar \ -input '/user/foo/dir1' -input '/user/foo/dir2' \ (rest of the command) 处理 XML123hadoop jar hadoop-streaming-2.6.0.jar \ -inputreader "StreamXmlRecord,begin=BEGIN_STRING,end=END_STRING" \ (rest of the command) BEGIN_STRING 和 END_STRING 之前的内容会被认为是 map 任务的一条记录。 参考文章 Hadoop Streaming Hadoop Streaming 编程 Writing an Hadoop MapReduce Program in Python]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>mapreduce</tag>
        <tag>streaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reading List 2015-02]]></title>
    <url>%2F2015%2F02%2F10%2Freading-list-2015-02%2F</url>
    <content type="text"><![CDATA[一直有个想法没有付诸实践，想做个分享知识的网站，类似 Leanote、开发者头条、GitHunt 等等的可检索的有思想的一个产品。作为尝试，在想法成型之前，先参考 http://www.dbthink.com/archives/910、http://yanjunyi.com/blog/category/discovery/ 的方式整理为知笔记中以前看到、搜集的一些文章、链接、工具等等的。 前端 http://githunt.io/ http://ohmycss.com/：简化版的bootstrap https://octicons.github.com/ ：css 图标，类似的有http://glyphicons.com/ https://github.com/allmarkedup/purl：一个 javascript 的 URL 解析库， select2：javascript 下拉框控件 http://momentjs.com/：一个 javascript 日期时间库 http://adambom.github.com/parallel.js/ Javascript 并行计算库 http://unknownworlds.com/blog/lua-ide-decoda-open-source/ Lua IDE decoda 开源了 http://baixing.github.io/Puerh 百姓网 UI 库 https://github.com/cloudfuji/kandan 开源免费的团队聊天工具 Kandan http://parsleyjs.org/非常强大的 jQuery 表单验证库 http://canvasjs.com/ HTML5 绘图工具 http://jsonselect.org JSON CSS-like query language http://jsonmate.com/ JSON格式化 http://justgage.com/ javascript 仪表盘控件 &lt;http://www.ez-css.org/ &gt;css 布局框架 http://blueprintcss.org/ css 框架 http://extralogical.net/projects/udon/ javascript函数编程 http://fogcreek.github.io/WebPutty/ WebPutty is a simple CSS editing and hosting service http://imakewebthings.com/deck.js/ Modern HTML Presentations http://blog.fogcreek.com/the-trello-tech-stack/ http://w2ui.com/web/home New JavaScript UI Library http://ace.c9.io/#nav=about http://www.javelinjs.com/ a frontend Javascript library developed at Facebook https://github.com/jtyjty99999/mobileTech 移动 Web App 开发的各种知识和经验的索引 后端 https://github.com/linkedin/databus Linkedin 开源的数据库变化监控工具 Databus &lt;http://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper28.pdf &gt; Impala: A Modern, Open-Source SQL Engine for Hadoop 微信 SDK https://github.com/node-webot/wechat 微信公共平台消息接口服务中间件 https://github.com/chanjarster/weixin-java-tools 微信公众号、企业号Java SDK https://github.com/belerweb/social-sdk 集成新浪微博开放平台、QQ互联、腾讯微博开发平台、微信公众平台等社交平台的接口的Java库。 https://github.com/liyiorg/weixin-popular 微信公众平台 Java SDK https://github.com/blahgeek/weixin-mp 微信公众平台，django https://github.com/wwj718/werobot_django 使用werobot作为微信后端,使werobot能存取django中的数据. https://github.com/wwj718/weixin_robot 使用WeRoBot搭建的微信后台 https://github.com/foxinmy/weixin4j (微信开发工具包)weixin sdk for java https://github.com/huyongliang/WeiXinSDK https://github.com/zengsn/zengsource-weixin 微信公众平台开发者模式Java SDK http://www.7keji.cn/813.html 10款微信公共账号开源软件 一些调度系统 https://github.com/spotify/luigi Python模块，支持依赖管理、流程管理、可视化和 Hadoop https://github.com/airbnb/chronos Mesos 的调度器 http://docs.cascading.org/ Cascading is the proven application development platform for building data applications on Hadoop https://github.com/thieman/dagobah Simple DAG-based job scheduler in Python https://github.com/azkaban/azkaban Azkaban workflow manager http://demo.gethue.com/ Cloudera 的 HUE https://github.com/alibaba/zeus 阿里宙斯作业平台 https://github.com/michael8335/zeus2 在阿里Zeus的基础上开发的支持Hadoop2 的调度平台 Dubbo 资料 基于ZooKeeper的Dubbo注册中心 Dubbo集群特性分析 Dubbo常用功能解析 初识分布式服务管理框架-Dubbo Dubbo框架设计简介 如何更好地学习dubbo源代码 Dubbo扩展点重构 Dubbox 样例代码分析 RPC框架系列–Dubbo A mini blog with NoSql,Dubbo and Spring 总结：作为一个程序员，最重要的能力是自我学习、归纳、总结，知识在于总结而不是分享。如何把大量看到的、听到的信息、知识、笔记等转化为自己的经验值，是需要认真考虑的一件事情。]]></content>
      <categories>
        <category>work</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Useful Hadoop Commands]]></title>
    <url>%2F2015%2F02%2F10%2Fuseful-commands-in-hadoop%2F</url>
    <content type="text"><![CDATA[hadoop解压 gz 文件到文本文件 1$ hadoop fs -text /hdfs_path/compressed_file.gz | hadoop fs -put - /tmp/uncompressed-file.txt 解压本地文件 gz 文件并上传到 hdfs 1$ gunzip -c filename.txt.gz | hadoop fs -put - /tmp/filename.txt 使用 awk 处理 csv 文件，参考 Using awk and friends with Hadoop: 1$ hadoop fs -cat people.txt | awk -F"," '&#123; print $1","$2","$3$4$5 &#125;' | hadoop fs -put - people-coalesed.txt 创建 lzo 文件、上传到 hdfs 并添加索引： 1234567$ lzop -Uf data.txt$ hadoop fs -moveFromLocal data.txt.lzo /tmp/# 1. 单机$ hadoop jar /usr/lib/hadoop/lib/hadoop-lzo.jar com.hadoop.compression.lzo.LzoIndexer /tmp/data.txt.lzo# 2. 运行 mr$ hadoop jar /usr/lib/hadoop/lib/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer /tmp/data.txt.lzo 如果 people.txt 通过 lzo 压缩了，则可以使用下面命令解压缩、处理数据、压缩文件： 1$ hadoop fs -cat people.txt.lzo | lzop -dc | awk -F"," '&#123; print $1","$2","$3$4$5 &#125;' | lzop -c | hadoop fs -put - people-coalesed.txt.lzo hive后台运行： 123$ nohup hive -f sample.hql &gt; output.out 2&gt;&amp;1 &amp; $ nohup hive --database "default" -e "select * from tablename;" &gt; output.out 2&gt;&amp;1 &amp; 替换分隔符： 1$ hive --database "default" -f query.hql 2&gt; err.txt | sed 's/[\t]/,/g' 1&gt; output.txt 打印表头： 123$ hive --database "default" -e "SHOW COLUMNS FROM table_name;" | tr '[:lower:]' '[:upper:]' | tr '\n' ',' 1&gt; headers.txt$ hive --database "default" -e "SET hive.cli.print.header=true; select * from table_name limit 0;" | tr '[:lower:]' '[:upper:]' | sed 's/[\t]/,/g' 1&gt; headers.txt 查看执行时间： 12$ hive -e "select * from tablename;" 2&gt; err.txt 1&gt; out.txt $ cat err.txt | grep "Time taken:" | awk '&#123;print $3,$6&#125;' hive 中如何避免用科学计数法表示浮点数？参考 http://www.zhihu.com/question/28887115 ： 1SELECT java_method("String", "format", "%f", my_column) FROM mytable LIMIT 1]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在CDH5上运行Spark应用]]></title>
    <url>%2F2015%2F02%2F04%2Fhow-to-run-a-simple-apache-spark-app-in-cdh-5%2F</url>
    <content type="text"><![CDATA[这篇文章参考 How-to: Run a Simple Apache Spark App in CDH 5 编写而成，没有完全参照原文翻译，而是重新进行了整理，例如：spark 版本改为 1.3.0，添加了 Python 版的程序。 创建 maven 工程使用下面命令创建一个普通的 maven 工程： 1$ mvn archetype:generate -DgroupId=com.cloudera.sparkwordcount -DartifactId=sparkwordcount -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false 在 sparkwordcount 目录下添加 scala 源文件目录和相应的包目录： 1$ mkdir -p sparkwordcount/src/main/scala/com/cloudera/sparkwordcount 修改 pom.xml 添加 scala 和 spark 依赖： 1234567891011121314151617&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;2.10.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.10&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 添加编译 scala 的插件： 123456789101112 &lt;plugin&gt; &lt;groupId&gt;org.scala-tools&lt;/groupId&gt; &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 添加 scala 编译插件需要的仓库： 1234567&lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;scala-tools.org&lt;/id&gt; &lt;name&gt;Scala-tools Maven2 Repository&lt;/name&gt; &lt;url&gt;http://scala-tools.org/repo-releases&lt;/url&gt; &lt;/pluginRepository&gt;&lt;/pluginRepositories&gt; 另外，添加 cdh hadoop 的仓库： 1234567891011121314151617&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;scala-tools.org&lt;/id&gt; &lt;name&gt;Scala-tools Maven2 Repository&lt;/name&gt; &lt;url&gt;http://scala-tools.org/repo-releases&lt;/url&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;maven-hadoop&lt;/id&gt; &lt;name&gt;Hadoop Releases&lt;/name&gt; &lt;url&gt;https://repository.cloudera.com/content/repositories/releases/&lt;/url&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;cloudera-repos&lt;/id&gt; &lt;name&gt;Cloudera Repos&lt;/name&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt; 运行下面命令检查工程是否能够成功编译： 1mvn package 编写示例代码以 WordCount 为例，该程序需要完成以下逻辑： 读一个输入文件 统计每个单词出现次数 过滤少于一定次数的单词 对剩下的单词统计每个字母出现次数 在 MapReduce 中，上面的逻辑需要两个 MapReduce 任务，而在 Spark 中，只需要一个简单的任务，并且代码量会少 90%。 编写Scala 程序如下： 123456789101112131415161718192021222324import org.apache.spark.SparkContextimport org.apache.spark.SparkContext._import org.apache.spark.SparkConf object SparkWordCount &#123; def main(args: Array[String]) &#123; val sc = new SparkContext(new SparkConf().setAppName("Spark Count")) val threshold = args(1).toInt // split each document into words val tokenized = sc.textFile(args(0)).flatMap(_.split(" ")) // count the occurrence of each word val wordCounts = tokenized.map((_, 1)).reduceByKey(_ + _) // filter out words with less than threshold occurrences val filtered = wordCounts.filter(_._2 &amp;gt;= threshold) // count characters val charCounts = filtered.flatMap(_._1.toCharArray).map((_, 1)).reduceByKey(_ + _) System.out.println(charCounts.collect().mkString(", ")) &#125;&#125; Spark 使用懒执行的策略，意味着只有当动作执行的时候，转换才会运行。上面例子中的动作操作是 collect 和 saveAsTextFile，前者是将数据推送给客户端，后者是将数据保存到 HDFS。 作为对比，Java 版的程序如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import java.util.ArrayList;import java.util.Arrays;import org.apache.spark.api.java.*;import org.apache.spark.api.java.function.*;import org.apache.spark.SparkConf;import scala.Tuple2; public class JavaWordCount &#123; public static void main(String[] args) &#123; JavaSparkContext sc = new JavaSparkContext(new SparkConf().setAppName("Spark Count")); final int threshold = Integer.parseInt(args[1]); // split each document into words JavaRDD tokenized = sc.textFile(args[0]).flatMap( new FlatMapFunction() &#123; public Iterable call(String s) &#123; return Arrays.asList(s.split(" ")); &#125; &#125; ); // count the occurrence of each word JavaPairRDD counts = tokenized.mapToPair( new PairFunction() &#123; public Tuple2 call(String s) &#123; return new Tuple2(s, 1); &#125; &#125; ).reduceByKey( new Function2() &#123; public Integer call(Integer i1, Integer i2) &#123; return i1 + i2; &#125; &#125; ); // filter out words with less than threshold occurrences JavaPairRDD filtered = counts.filter( new Function, Boolean&gt;() &#123; public Boolean call(Tuple2 tup) &#123; return tup._2 &gt;= threshold; &#125; &#125; ); // count characters JavaPairRDD charCounts = filtered.flatMap( new FlatMapFunction, Character&gt;() &#123; public Iterable call(Tuple2 s) &#123; ArrayList chars = new ArrayList(s._1.length()); for (char c : s._1.toCharArray()) &#123; chars.add(c); &#125; return chars; &#125; &#125; ).mapToPair( new PairFunction() &#123; public Tuple2 call(Character c) &#123; return new Tuple2(c, 1); &#125; &#125; ).reduceByKey( new Function2() &#123; public Integer call(Integer i1, Integer i2) &#123; return i1 + i2; &#125; &#125; ); System.out.println(charCounts.collect()); &#125;&#125; 另外，Python 版的程序如下： 1234567891011121314151617181920import sysfrom pyspark import SparkContextfile="inputfile.txt"count=2if __name__ == "__main__": sc = SparkContext(appName="PythonWordCount") lines = sc.textFile(file, 1) counts = lines.flatMap(lambda x: x.split(' ')) \ .map(lambda x: (x, 1)) \ .reduceByKey(lambda a, b: a + b) \ .filter(lambda (a, b) : b &gt;= count) \ .flatMap(lambda (a, b): list(a)) \ .map(lambda x: (x, 1)) \ .reduceByKey(lambda a, b: a + b) print ",".join(str(t) for t in counts.collect()) sc.stop() 编译运行下面命令生成 jar： 1$ mvn package 运行成功之后，会在 target 目录生成 sparkwordcount-0.0.1-SNAPSHOT.jar 文件。 运行首先，将测试文件 inputfile.txt 上传到 HDFS 上； 12$ wget https://github.com/sryza/simplesparkapp/blob/master/data/inputfile.txt$ hadoop fs -put inputfile.txt 其次，将 sparkwordcount-0.0.1-SNAPSHOT.jar 上传到集群中的一个节点；然后，使用 spark-submit 脚本运行 Scala 版的程序： 1$ spark-submit --class com.cloudera.sparkwordcount.SparkWordCount --master local sparkwordcount-0.0.1-SNAPSHOT.jar inputfile.txt 2 或者，运行 Java 版本的程序： 1$ spark-submit --class com.cloudera.sparkwordcount.JavaWordCount --master local sparkwordcount-0.0.1-SNAPSHOT.jar inputfile.txt 2 对于 Python 版的程序，运行脚本为： 123456$ spark-submit --master local PythonWordCount.py~~~ 如果，你的集群部署的是 standalone 模式，则你可以替换 master 参数的值为 `spark://&lt;master host&gt;:&lt;master port&gt;`，也可以以 Yarn 的模式运行。最后的 Python 版的程序运行输出结果如下： (u’a’, 4),(u’c’, 1),(u’b’, 1),(u’e’, 6),(u’f’, 1),(u’i’, 1),(u’h’, 1),(u’l’, 1),(u’o’, 2),(u’n’, 4),(u’p’, 2),(u’r’, 2),(u’u’, 1),(u’t’, 2),(u’v’, 1) 完整的代码在：&lt;https://github.com/sryza/simplesparkapp&gt;]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark编程指南笔记]]></title>
    <url>%2F2015%2F02%2F03%2Fspark-programming-guide%2F</url>
    <content type="text"><![CDATA[本文是参考Spark官方编程指南（Spark 版本为1.2）整理出来的学习笔记，主要是用于加深对 Spark 的理解，并记录一些知识点。 1. Spark介绍Spark是UC Berkeley AMP lab所开源的类Hadoop MapReduce 框架，都是基于map reduce算法所实现的分布式计算框架，拥有Hadoop MapReduce所具有的优点；不同于MapReduce的是Job中间输出和结果可以保存在内存中，而不需要读写HDFS，因此Spark能更好地适用于machine learning等需要迭代的map reduce算法。 产生原因 1、 MapReduce具有很多局限性： 仅支持Map和Reduce两种操作 迭代效率低 不适合交互式处理 不擅长流式处理 2、 现有的计算框架各自为战 批处理计算：MapReduce、Hive 流式计算：Storm 交互式计算：Impala ​ 设计目标在一个统一的框架下能够进行批处理、流式计算和交互式计算。 2. 一些概念每一个 Spark 的应用，都是由一个驱动程序构成，它运行用户的 main 函数，在一个集群上执行各种各样的并行操作。 Spark 提出的最主要抽象概念是弹性分布式数据集，它是一个有容错机制（划分到集群的各个节点上）并可以被并行操作的元素集合。 分布在集群中的对象集合 存储在磁盘或者内存中 通过并行“转换”操作构造 失效后自动重构 目前有两种类型的RDD： 并行集合：接收一个已经存在的 Scala 集合，然后进行各种并行计算。 外部数据集：外部存储系统，例如一个共享的文件系统，HDFS、HBase以及任何支持 Hadoop InputFormat 的数据源。 这两种类型的 RDD 都可以通过相同的方式进行操作。用户可以让 Spark 保留一个 RDD 在内存中，使其能在并行操作中被有效的重复使用，并且，RDD 能自动从节点故障中恢复。 Spark 的第二个抽象概念是共享变量，可以在并行操作中使用。在默认情况下，Spark 通过不同节点上的一系列任务来运行一个函数，它将每一个函数中用到的变量的拷贝传递到每一个任务中。有时候，一个变量需要在任务之间，或任务与驱动程序之间被共享。 Spark 支持两种类型的共享变量：广播变量，可以在内存的所有的结点上缓存变量；累加器：只能用于做加法的变量，例如计数或求和。 3. 如何编程初始化 Spark在一个Spark程序中要做的第一件事就是创建一个SparkContext对象来告诉Spark如何连接一个集群。为了创建SparkContext，你首先需要创建一个SparkConf对象，这个对象会包含你的应用的一些相关信息。这个通常是通过下面的构造器来实现的： 1new SparkContext(master, appName, [sparkHome], [jars]) 参数说明： master：用于指定所连接的 Spark 或者 Mesos 集群的 URL。 appName ：应用的名称，将会在集群的 Web 监控 UI 中显示。 sparkHome：可选，你的集群机器上 Spark 的安装路径（所有机器上路径必须一致）。 jars：可选，在本地机器上的 JAR 文件列表，其中包括你应用的代码以及任何的依赖，Spark 将会把他们部署到所有的集群结点上。 在 python 中初始化，示例代码如下： 123//conf = SparkContext("local", "Hello Spark")conf = SparkConf().setAppName("Hello Spark").setMaster("local")sc = SparkContext(conf=conf) 说明：如果部署到集群，在分布式模式下运行，最后两个参数是必须的，第一个参数可以是以下任一种形式： Master URL 含义 loca 默认值，使用一个 Worker 线程本地化运行(完全不并行) local[N] 使用 N 个 Worker 线程本地化运行，N 为 * 时，表示使用系统中所有核 local[N,M] 第一个代表的是用到的核个数；第二个参数代表的是容许该作业失败M次 spark://HOST:PORT 连接到指定的 Spark 单机版集群 master 进程所在的主机和端口，端口默认是7077 mesos://HOST:PORT 连接到指定的 Mesos 集群。host 参数是Moses master的hostname。端口默认是5050 如果你在一个集群上运行 spark-shell，则 master 参数默认为 local。在实际使用中，当你在集群中运行你的程序，你一般不会把 master 参数写死在代码中，而是通过用 spark-submit 运行程序来获得这个参数。但是，在本地测试以及单元测试时，你仍需要自行传入 local 来运行Spark程序。 运行代码运行代码有几种方式，一是通过 spark-shell 来运行 scala 代码，一是编写 java 代码并打成包以 spark on yarn 方式运行，还有一种是通过 PySpark 来运行 python 代码。 在 spark-shell 和 PySpark 命令行中，一个特殊的集成在解释器里的 SparkContext 变量已经建立好了，变量名叫做 sc，创建你自己的 SparkContext 不会起作用。 4. 弹性分布式数据集4.1 并行集合并行集合是通过调用 SparkContext 的 parallelize 方法，在一个已经存在的 Scala 集合上创建一个 Seq 对象。 parallelize 方法还可以接受一个参数 slices，表示数据集切分的份数。Spark 将会在集群上为每一份数据起一个任务。典型地，你可以在集群的每个 CPU 上分布 2-4个 slices。一般来说，Spark 会尝试根据集群的状况，来自动设定 slices 的数目，当然，你也可以手动设置。 Scala 示例程序： 12345678scala&gt; val data = Array(1, 2, 3, 4, 5)data: Array[Int] = Array(1, 2, 3, 4, 5)scala&gt; var distData = sc.parallelize(data)distData: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:14scala&gt; distData.reduce((a, b) =&gt; a + b)res4: Int = 15 Java 示例程序： 123List&lt;Integer&gt; data = Arrays.asList(1, 2, 3, 4, 5);JavaRDD&lt;Integer&gt; distData = sc.parallelize(data);Integer sum=distData.reduce((a, b) -&gt; a + b); Python 示例程序： 123data = [1, 2, 3, 4, 5]distData = sc.parallelize(data)distData.reduce(lambda a, b: a + b) 4.2 外部数据源Spark可以从存储在 HDFS，或者 Hadoop 支持的其它文件系统（包括本地文件，Amazon S3， Hypertable， HBase 等等）上的文件创建分布式数据集。Spark 支持 TextFile、SequenceFiles 以及其他任何 Hadoop InputFormat 格式的输入。 TextFile 的 RDD 可以通过下面方式创建，该方法接受一个文件的 URI 地址，该地址可以是本地路径，或者 hdfs://、s3n:// 等 URL 地址。 12345678// scala 语法val distFile = sc.textFile("data.txt")// java 语法JavaRDD&lt;String&gt; distFile = sc.textFile("data.txt");// python 语法distFile = sc.textFile("data.txt") 一些说明： 如果使用了本地文件路径时，要保证在worker节点上这个文件也能够通过这个路径访问。这点可以通过将这个文件拷贝到所有worker上或者使用网络挂载的共享文件系统来解决。 包括textFile在内的所有基于文件的Spark读入方法，都支持将文件夹、压缩文件、包含通配符的路径作为参数。比如，以下代码都是合法的： 123textFile("/my/directory")textFile("/my/directory/*.txt")textFile("/my/directory/*.gz") textFile方法也可以传入第二个可选参数来控制文件的分片数量。默认情况下，Spark会为文件的每一个块（在HDFS中块的大小默认是64MB）创建一个分片。但是你也可以通过传入一个更大的值来要求Spark建立更多的分片。注意，分片的数量绝不能小于文件块的数量。 除了文本文件之外，Spark 还支持其他格式的输入： SparkContext.wholeTextFiles 方法可以读取一个包含多个小文件的目录，并以 filename，content 键值对的方式返回结果。 对于 SequenceFiles，可以使用 SparkContext 的 `sequenceFile[K, V]`` 方法创建。像 IntWritable 和 Text 一样，它们必须是 Hadoop 的 Writable 接口的子类。另外，对于几种通用 Writable 类型，Spark 允许你指定原生类型来替代。例如：sequencFile[Int, String] 将会自动读取 IntWritable 和 Texts。 对于其他类型的 Hadoop 输入格式，你可以使用 SparkContext.hadoopRDD 方法，它可以接收任意类型的 JobConf 和输入格式类，键类型和值类型。按照像 Hadoop 作业一样的方法设置输入源就可以了。 RDD.saveAsObjectFile 和 SparkContext.objectFile 提供了以 Java 序列化的简单方式来保存 RDD。虽然这种方式没有 Avro 高效，但也是一种简单的方式来保存任意的 RDD。 4.3 RDD 操作RDD支持两类操作： 转化操作，用于从已有的数据集转化产生新的数据集； 启动操作，用于在计算结束后向驱动程序返回结果。 举个例子，map是一个转化操作，可以将数据集中每一个元素传给一个函数，同时将计算结果作为一个新的RDD返回。另一方面，reduce操作是一个启动操作，能够使用某些函数来聚集计算RDD中所有的元素，并且向驱动程序返回最终结果（同时还有一个并行的reduceByKey操作可以返回一个分布数据集）。 在Spark所有的转化操作都是惰性求值的，就是说它们并不会立刻真的计算出结果。相反，它们仅仅是记录下了转换操作的操作对象（比如：一个文件）。只有当一个启动操作被执行，要向驱动程序返回结果时，转化操作才会真的开始计算。这样的设计使得Spark运行更加高效——比如，我们会发觉由map操作产生的数据集将会在reduce操作中用到，之后仅仅是返回了reduce的最终的结果而不是map产生的庞大数据集。 在默认情况下，每一个由转化操作得到的RDD都会在每次执行启动操作时重新计算生成。但是，你也可以通过调用persist(或cache)方法来将RDD持久化到内存中，这样Spark就可以在下次使用这个数据集时快速获得。Spark同样提供了对将RDD持久化到硬盘上或在多个节点间复制的支持。 Scala 示例： 123val lines = sc.textFile("data.txt")val lineLengths = lines.map(s =&gt; s.length)val totalLength = lineLengths.reduce((a, b) =&gt; a + b) Java 示例： 123JavaRDD&lt;String&gt; lines = sc.textFile("data.txt");JavaRDD&lt;Integer&gt; lineLengths = lines.map(s -&gt; s.length());int totalLength = lineLengths.reduce((a, b) -&gt; a + b); Python 示例： 123lines = sc.textFile("data.txt")lineLengths = lines.map(lambda s: len(s))totalLength = lineLengths.reduce(lambda a, b: a + b) 第一行定义了一个由外部文件产生的基本RDD。这个数据集不是从内存中载入的也不是由其他操作产生的；lines仅仅是一个指向文件的指针。第二行将lineLengths定义为map操作的结果。再强调一次，由于惰性求值的缘故，lineLengths并不会被立即计算得到。最后，我们运行了reduce操作，这是一个启动操作。从这个操作开始，Spark将计算过程划分成许多任务并在多机上运行，每台机器运行自己部分的map操作和reduce操作，最终将自己部分的运算结果返回给驱动程序。 如果我们希望以后重复使用lineLengths，只需在reduce前加入下面这行代码： 1lineLengths.persist() 这条代码将使得 lineLengths 在第一次计算生成之后保存在内存中。 除了使用 lambda 表达式，也可以通过函数来运行转换或者动作，使用函数需要注意局部变量的作用域问题。 例如下面的 Python 代码中的 field 变量： 1234567class MyClass(object): def __init__(self): self.field = "Hello" def doStuff(self, rdd): field = self.field return rdd.map(lambda s: field + x) 如果使用 Java 语言，则需要用到匿名内部类： 12345678910class GetLength implements Function&lt;String, Integer&gt; &#123; public Integer call(String s) &#123; return s.length(); &#125;&#125;class Sum implements Function2&lt;Integer, Integer, Integer&gt; &#123; public Integer call(Integer a, Integer b) &#123; return a + b; &#125;&#125;JavaRDD&lt;String&gt; lines = sc.textFile("data.txt");JavaRDD&lt;Integer&gt; lineLengths = lines.map(new GetLength());int totalLength = lineLengths.reduce(new Sum()); Spark 也支持键值对的操作，这在分组和聚合操作时候用得到。定义一个键值对对象时，需要自定义该对象的 equals() 和 hashCode() 方法。 在 Scala 中有一个 Tuple2 对象表示键值对，这是一个内置的对象，通过 (a,b) 就可以创建一个 Tuple2 对象。在你的程序中，通过导入 org.apache.spark.SparkContext._ 就可以对 Tuple2 进行操作。对键值对的操作方法，可以查看 PairRDDFunctions 下面是一个用 scala 统计单词出现次数的例子： 123val lines = sc.textFile("data.txt")val pairs = lines.map(s =&gt; (s, 1))val counts = pairs.reduceByKey((a, b) =&gt; a + b) 接下来，你还可以执行 counts.sortByKey()、counts.collect() 等操作。 如果用 Java 统计，则代码如下： 123JavaRDD&lt;String&gt; lines = sc.textFile("data.txt");JavaPairRDD&lt;String, Integer&gt; pairs = lines.mapToPair(s -&gt; new Tuple2(s, 1));JavaPairRDD&lt;String, Integer&gt; counts = pairs.reduceByKey((a, b) -&gt; a + b); 用 Python 统计，代码如下： 123lines = sc.textFile("data.txt")pairs = lines.map(lambda s: (s, 1))counts = pairs.reduceByKey(lambda a, b: a + b) 测试现在来结合上面的例子实现一个完整的例子。下面，我们来 分析 Nginx 日志中状态码出现次数，并且将结果按照状态码从小到大排序。 先将测试数据上传到 hdfs: 1$ hadoop fs -put access.log 然后，编写一个 python 文件，保存为 SimpleApp.py： 12345678910111213141516171819from pyspark import SparkContextlogFile = "access.log"sc = SparkContext("local", "Simple App")rdd = sc.textFile(logFile).cache()counts = rdd.map(lambda line: line.split()[8]).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b).sortByKey(lambda x: x) # This is just a demo on how to bring all the sorted data back to a single node. # In reality, we wouldn't want to collect all the data to the driver node.output = counts.collect() for (word, count) in output: print "%s: %i" % (word, count) counts.saveAsTextFile("/data/result")sc.stop() 接下来，运行下面代码： 1$ spark-submit --master local[4] SimpleApp.py 运行成功之后，你会在终端看到以下输出： 1234567200: 6827206: 120301: 7304: 10403: 38404: 125416: 1 并且，在hdfs 上 /user/spark/spark_results/part-00000 内容如下： 1234567(u'200', 6827)(u'206', 120)(u'301', 7)(u'304', 10)(u'403', 38)(u'404', 125)(u'416', 1) 其实，这个例子和官方提供的例子很相像，具体请看 wordcount.py。 如果用 scala 来实现，代码如下： 1234rdd = sc.textFile("access.log").cache()rdd.flatMap(_.split(' ')).map( (_,1)).reduceByKey(_+_).sortByKey(true).saveAsTextFile("/data/result")sc.stop() 如果想要对结果按照次数进行排序，则代码如下： 1234rdd = sc.textFile("access.log").cache()rdd.flatMap(_.split(' ')).map( (_,1)).reduceByKey(_+_).map( x =&gt; (x._2,x._1)).sortByKey(false).map( x =&gt; (x._2,x._1) ).saveAsTextFile("/data/resultSorted")sc.stop() 常见的转换 转换 含义 map(func) 每一个输入元素经过func函数转换后输出一个元素 filter(func) 返回经过 func 函数计算后返回值为 true 的输入元素组成的一个新数据集 flatMap(func) 类似于 map，但是每一个输入元素可以被映射为0或多个输出元素，因此 func 应该返回一个序列 mapPartitions(func) 类似于 map，但独立地在 RDD 的每一个分块上运行，因此在类型为 T 的 RDD 上运行时，func 的函数类型必须是 Iterator[T] ⇒ Iterator[U] mapPartitionsWithSplit(func) 类似于 mapPartitions, 但 func 带有一个整数参数表示分块的索引值。因此在类型为 T的RDD上运行时，func 的函数类型必须是 (Int, Iterator[T]) ⇒ Iterator[U] sample(withReplacement,fraction, seed) 根据 fraction 指定的比例，对数据进行采样，可以选择是否用随机数进行替换，seed 用于指定随机数生成器种子 union(otherDataset) 返回一个新的数据集，新数据集是由源数据集和参数数据集联合而成 distinct([numTasks])) 返回一个包含源数据集中所有不重复元素的新数据集 groupByKey([numTasks]) 在一个键值对的数据集上调用，返回一个(K，Seq[V])对的数据集 。注意：默认情况下，只有8个并行任务来做操作，但是你可以传入一个可选的 numTasks 参数来改变它 reduceByKey(func, [numTasks]) 在一个键值对的数据集上调用时，返回一个键值对的数据集，使用指定的 reduce 函数，将相同 key 的值聚合到一起。类似 groupByKey，reduce 任务个数是可以通过第二个可选参数来配置的 sortByKey([ascending], [numTasks]) 在一个键值对的数据集上调用，K 必须实现 Ordered 接口，返回一个按照 Key 进行排序的键值对数据集。升序或降序由 ascending 布尔参数决定 join(otherDataset, [numTasks]) 在类型为（K,V)和（K,W) 类型的数据集上调用时，返回一个相同key对应的所有元素对在一起的 (K, (V, W)) 数据集 cogroup(otherDataset, [numTasks]) 在类型为（K,V)和（K,W) 的数据集上调用，返回一个 (K, Seq[V], Seq[W]) 元组的数据集。这个操作也可以称之为 groupwith cartesian(otherDataset) 笛卡尔积，在类型为 T 和 U 类型的数据集上调用时，返回一个 (T, U) 对数据集(两两的元素对) pipe(command, [envVars]) 对 RDD 进行管道操作 coalesce(numPartitions) 减少 RDD 的分区数到指定值。在过滤大量数据之后，可以执行此操作 repartition(numPartitions) 重新给 RDD 分区 repartitionAndSortWithinPartitions(partitioner) 重新给 RDD 分区，并且每个分区内以记录的 key 排序 常见的动作常见的动作列表： 动作 含义 reduce(func) 通过函数 func 聚集数据集中的所有元素。这个功能必须可交换且可关联的，从而可以正确的被并行执行。 collect() 在驱动程序中，以数组的形式，返回数据集的所有元素。这通常会在使用 filter 或者其它操作并返回一个足够小的数据子集后再使用会比较有用。 count() 返回数据集的元素的个数。 first() 返回数据集的第一个元素，类似于 take(1) take(n) 返回一个由数据集的前 n 个元素组成的数组。注意，这个操作目前并非并行执行，而是由驱动程序计算所有的元素 takeSample(withReplacement,num, seed) 返回一个数组，在数据集中随机采样 num 个元素组成，可以选择是否用随机数替换不足的部分，seed 用于指定的随机数生成器种子 takeOrdered(n, [ordering]) 返回自然顺序或者自定义顺序的前 n 个元素 saveAsTextFile(path) 将数据集的元素，以 textfile 的形式，保存到本地文件系统，HDFS或者任何其它 hadoop 支持的文件系统。对于每个元素，Spark 将会调用 toString 方法，将它转换为文件中的文本行 saveAsSequenceFile(path) (Java and Scala) 将数据集的元素，以 Hadoop sequencefile 的格式保存到指定的目录下 saveAsObjectFile(path) (Java and Scala) 将数据集的元素，以 Java 序列化的方式保存到指定的目录下 countByKey() 对(K,V)类型的 RDD 有效，返回一个 (K，Int) 对的 Map，表示每一个key对应的元素个数 foreach(func) 在数据集的每一个元素上，运行函数 func 进行更新。这通常用于边缘效果，例如更新一个累加器，或者和外部存储系统进行交互，例如 HBase 4.4 RDD持久化Spark 的一个重要功能就是在将数据集持久化（或缓存）到内存中以便在多个操作中重复使用。当我们持久化一个 RDD 是，每一个节点将这个RDD的每一个分片计算并保存到内存中以便在下次对这个数据集（或者这个数据集衍生的数据集）的计算中可以复用。这使得接下来的计算过程速度能够加快（经常能加快超过十倍的速度）。缓存是加快迭代算法和快速交互过程速度的关键工具。 你可以通过调用persist或cache方法来标记一个想要持久化的 RDD。在第一次被计算产生之后，它就会始终停留在节点的内存中。Spark 的缓存是具有容错性的——如果 RDD 的任意一个分片丢失了，Spark 就会依照这个 RDD 产生的转化过程自动重算一遍。 另外，每一个持久化的 RDD 都有一个可变的存储级别，这个级别使得用户可以改变 RDD 持久化的储存位置。比如，你可以将数据集持久化到硬盘上，也可以将它以序列化的 Java 对象形式（节省空间）持久化到内存中，还可以将这个数据集在节点之间复制，或者使用 Tachyon 将它储存到堆外。这些存储级别都是通过向 persist() 传递一个 StorageLevel 对象（Scala, Java, Python）来设置的。 注意：在Python中，储存的对象永远是通过 Pickle 库序列化过的，所以设不设置序列化级别不会产生影响。 Spark 还会在 shuffle 操作（比如 reduceByKey）中自动储存中间数据，即使用户没有调用 persist。这是为了防止在 shuffle 过程中某个节点出错而导致的全盘重算。不过如果用户打算复用某些结果 RDD，我们仍然建议用户对结果 RDD 手动调用 persist，而不是依赖自动持久化机制。 使用以下两种方法可以标记要缓存的 RDD： 12lineLengths.persist() lineLengths.cache() 取消缓存则用： 1lineLengths.unpersist() 每一个 RDD 都可以用不同的保存级别进行保存，通过将一个 org.apache.spark.storage.StorageLevel 对象传递给 persist(self, storageLevel) 可以控制 RDD 持久化到磁盘、内存或者是跨节点复制等等。cache() 方法是使用默认存储级别的快捷方法，也就是 StorageLevel.MEMORY_ONLY。 完整的可选存储级别如下： 存储级别 意义 MEMORY_ONLY 默认的级别， 将 RDD 作为反序列化的的对象存储在 JVM 中。如果不能被内存装下，一些分区将不会被缓存，并且在需要的时候被重新计算 MEMORY_AND_DISK 将 RDD 作为反序列化的的对象存储在 JVM 中。如果不能被与内存装下，超出的分区将被保存在硬盘上，并且在需要时被读取 MEMORY_ONLY_SER 将 RDD 作为序列化的的对象进行存储（每一分区占用一个字节数组）。通常来说，这比将对象反序列化的空间利用率更高，尤其当使用fast serializer,但在读取时会比较占用CPU MEMORY_AND_DISK_SER 与 MEMORY_ONLY_SER 相似，但是把超出内存的分区将存储在硬盘上而不是在每次需要的时候重新计算 DISK_ONLY 只将 RDD 分区存储在硬盘上 MEMORY_ONLY_2、MEMORY_AND_DISK_2等 与上述的存储级别一样，但是将每一个分区都复制到两个集群结点上 OFF_HEAP 开发中 Spark 的不同存储级别，旨在满足内存使用和 CPU 效率权衡上的不同需求。我们建议通过以下的步骤来进行选择： 如果你的 RDD 可以很好的与默认的存储级别契合，就不需要做任何修改了。这已经是 CPU 使用效率最高的选项，它使得 RDD的操作尽可能的快。 如果不行，试着使用 MEMORY_ONLY_SER 并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。 尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。 如果你想有快速故障恢复能力，使用复制存储级别。例如：用 Spark 来响应web应用的请求。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在 RDD 上持续的运行任务，而不需要等待丢失的分区被重新计算。 如果你想要定义你自己的存储级别，比如复制因子为3而不是2，可以使用 StorageLevel 单例对象的 apply()方法。 5. 共享变量通常情况下，当一个函数传递给一个在远程集群节点上运行的Spark操作（比如map和reduce）时，Spark会对涉及到的变量的所有副本执行这个函数。这些变量会被复制到每个机器上，而且这个过程不会被反馈给驱动程序。通常情况下，在任务之间读写共享变量是很低效的。但是，Spark仍然提供了有限的两种共享变量类型用于常见的使用场景：广播变量和累加器。 广播变量广播变量允许程序员在每台机器上保持一个只读变量的缓存而不是将一个变量的拷贝传递给各个任务。它们可以被使用，比如，给每一个节点传递一份大输入数据集的拷贝是很低效的。Spark 试图使用高效的广播算法来分布广播变量，以此来降低通信花销。可以通过 SparkContext.broadcast(v) 来从变量 v 创建一个广播变量。这个广播变量是 v 的一个包装，同时它的值可以功过调用 value 方法来获得。以下的代码展示了这一点： 12345broadcastVar = sc.broadcast([1, 2, 3])&lt;pyspark.broadcast.Broadcast object at 0x102789f10&gt;&gt;&gt;&gt; broadcastVar.value[1, 2, 3] 在广播变量被创建之后，在所有函数中都应当使用它来代替原来的变量v，这样就可以保证v在节点之间只被传递一次。另外，v变量在被广播之后不应该再被修改了，这样可以确保每一个节点上储存的广播变量的一致性（如果这个变量后来又被传输给一个新的节点）。 累加器累加器是在一个相关过程中只能被”累加”的变量，对这个变量的操作可以有效地被并行化。它们可以被用于实现计数器（就像在MapReduce过程中）或求和运算。Spark原生支持对数字类型的累加器，程序员也可以为其他新的类型添加支持。累加器被以一个名字创建之后，会在Spark的UI中显示出来。这有助于了解计算的累进过程（注意：目前Python中不支持这个特性）。 可以通过SparkContext.accumulator(v)来从变量v创建一个累加器。在集群中运行的任务随后可以使用add方法或+=操作符（在Scala和Python中）来向这个累加器中累加值。但是，他们不能读取累加器中的值。只有驱动程序可以读取累加器中的值，通过累加器的value方法。 以下的代码展示了向一个累加器中累加数组元素的过程： 123456789accum = sc.accumulator(0)Accumulator&lt;id=0, value=0&gt;&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))...10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 sscala&gt; accum.value10 这段代码利用了累加器对 int 类型的内建支持，程序员可以通过继承 AccumulatorParam 类来创建自己想要的类型支持。AccumulatorParam 的接口提供了两个方法：zero 用于为你的数据类型提供零值；addInPlace 用于计算两个值得和。比如，假设我们有一个 Vector类表示数学中的向量，我们可以这样写： 12345678910class VectorAccumulatorParam(AccumulatorParam): def zero(self, initialValue): return Vector.zeros(initialValue.size) def addInPlace(self, v1, v2): v1 += v2 return v1# Then, create an Accumulator of this type:vecAccum = sc.accumulator(Vector(...), VectorAccumulatorParam()) 累加器的更新操作只会被运行一次，Spark 提供了保证，每个任务中对累加器的更新操作都只会被运行一次。比如，重启一个任务不会再次更新累加器。在转化过程中，用户应该留意每个任务的更新操作在任务或作业重新运算时是否被执行了超过一次。 累加器不会该别 Spark 的惰性求值模型。如果累加器在对RDD的操作中被更新了，它们的值只会在启动操作中作为 RDD 计算过程中的一部分被更新。所以，在一个懒惰的转化操作中调用累加器的更新，并没法保证会被及时运行。下面的代码段展示了这一点： accum = sc.accumulator(0) data.map(lambda x =&gt; acc.add(x); f(x)) 6. 参考文章 http://spark.apache.org/docs/latest/programming-guide.html http://rdc.taobao.org/?p=2024 http://blog.csdn.net/u011391905/article/details/37929731 http://segmentfault.com/blog/whuwb/1190000000723037 [翻译]Spark编程指南(Python版)]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Yeoman构建AngularJS项目]]></title>
    <url>%2F2015%2F02%2F02%2Fbuild-angularjs-app-with-yeoman%2F</url>
    <content type="text"><![CDATA[这篇文章不是一篇翻译也不是一篇原创文章，类似于一篇学习笔记，主要是记录一些关键的过程，方便查阅加深理解和记忆。 Yeoman 介绍Yeoman 是 Google 的团队和外部贡献者团队合作开发的，他的目标是通过 Grunt（一个用于开发任务自动化的命令行工具）和 Bower（一个HTML、CSS、Javascript 和图片等前端资源的包管理器）的包装为开发者创建一个易用的工作流。 Yeoman 的目的不仅是要为新项目建立工作流，同时还是为了解决前端开发所面临的诸多严重问题，例如零散的依赖关系。 Yeoman 主要有三部分组成：yo（脚手架工具）、grunt（构建工具）、bower（包管理器）。这三个工具是分别独立开发的，但是需要配合使用，来实现我们高效的工作流模式。 Yo 搭建新应用的脚手架，编写你的 Grunt 配置并且安装你有可能在构建中需要的相关的 Grunt 任务。 Grunt 被用来构建，预览以及测试你的项目，感谢来自那些由 Yeoman 团队和 runt-contrib 所管理的任务的帮助。 Bower 被用来进行依赖管理，所以你不再需要手动的下载和管理你的脚本了。 下面这幅图很形象的表明了他们三者之间的协作关系。 Yeoman 特性： 快速创建骨架应用程序。使用可自定义的模板（例如：HTML5、Boilerplate、Twitter Bootstrap 等）、AMD（通过 RequireJS）以及其他工具轻松地创建新项目的骨架。 自动编译 CoffeeScrip 和 Compass。在做出变更的时候，Yeoman 的 LiveReload 监视进程会自动编译源文件，并刷新浏览器，而不需要你手动执行。 自动完善你的脚本。所有脚本都会自动针对 JSHint 运行，从而确保它们遵循语言的最佳实践。 内建的预览服务器。你不需要启动自己的 HTTP 服务器。内建的服务器用一条命令就可以启动。 非常棒的图像优化。使用 OptPNG 和 JPEGTran 对所有图像做了优化。 生成 AppCache 清单。Yeoman 会为你生成应用程序缓存的清单，你只需要构建项目就好。 “杀手级”的构建过程。你所做的工作不仅被精简到最少，让你更加专注，为你节省大量工作。 集成的包管理。Yeoman 让你可以通过命令行轻松地查找新的包，安装并保持更新，而不需要你打开浏览器。 对 ES6 模块语法的支持。你可以使用最新的 ECMAScript 6 模块语法来编写模块。这还是一种实验性的特性，它会被转换成 ES5，从而你可以在所有流行的浏览器中使用编写的代码。 PhantomJS 单元测试。你可以通过 PhantomJS 轻松地运行单元测试。当你创建新的应用程序的时候，它还会为你自动创建测试内容的骨架。 安装安装前提一份完整的新手上路指南在这里可以找到，但是对于那些希望快速上手操练的家伙，请确定你已经安装了 Node.js, Git。Ruby 和 Compass 是可选的(如果你想要使用Compass)。 Node.js 版本要求为 v0.10.x+，npm 版本要求为 v2.1.0+，运行下面命令 检查版本： 1$ node --version &amp;&amp; npm --version 也可以检查 Git 版本： 1$ git --version 安装 Yeoman 工具集确保 Node 安装之后，安装 Yeoman 工具集： 1$ npm install --global yo bower grunt-cli 运行下面命令检查是否安装成功： 12345$ yo --version &amp;&amp; bower --version &amp;&amp; grunt --version1.4.51.3.12grunt-cli v0.1.13 安装 AngularJS 的生成器Yeoman 生成器使用 npm 命令,现在可用的生成器数量已经超过了 1000+个生成器，这其中很多都是由开源社区编写的。 你可以安装 web 应用的生成器 1$ npm install -g generator-webapp 也可以安装 generator-angular 生成器： 1$ npm install --global generator-angular 创建项目创建一个目录用于作为工程目录： 1$ mkdir mytodo &amp;&amp; cd mytodo 你可以查看所有的生成器： 12345678910$ yo? 'Allo june! What would you like to do? Run a generator Angular Karma Webapp❯ Mocha ────────────── Update your generators(Move up and down to reveal more choices) 运行 Angular 生成器，会提示你是否使用 Sass 和引入 Bootstrap，以及加载哪些 Angular 模块： 1234567891011121314151617$ yo angular _-----_ | | .--------------------------. |--(o)--| | Welcome to Yeoman, | `---------´ | ladies and gentlemen! | ( _´U`_ ) '--------------------------' /___A___\ | ~ | __'.___.'__ ´ ` |° ´ Y `Out of the box I include Bootstrap and some AngularJS recommended modules.? Would you like to use Sass (with Compass)? No? Would you like to include Bootstrap? Yes? Which modules would you like to include? angular-animate.js, angular-cookies.js, angular-resource.js, angular-route.js, angular-sanitize.js, angular-touch.js 选择需要下载的模块，然后回车。过一段时间之后，生成的目录结构如下： 123456789101112131415161718192021222324mytodo├── Gruntfile.js├── app│ ├── 404.html│ ├── favicon.ico│ ├── images│ │ └── yeoman.png│ ├── index.html│ ├── robots.txt│ ├── scripts│ │ ├── app.js│ │ └── controllers│ ├── styles│ │ └── main.css│ └── views│ ├── about.html│ └── main.html├── bower.json├── bower_components├── package.json└── test ├── karma.conf.js └── spec └── controllers 示例中的所有 js 代码都使用了严格模式，有关严格模式的内容可以参考 http://www.waylau.com/javascript-use-strict-mode/ 运行下面命令启动服务： 1$ grunt serve 浏览器访问 localhost:9000，你会看到： 创建 AngularJS 应用创建新模板展现 Todo 列表打开 scripts/controllers/main.js ，代码修改为： 12345'use strict';angular.module('webApp').controller('MainCtrl', function ($scope) &#123; $scope.todos = ['Item 1', 'Item 2', 'Item 3']; &#125;); 修改 views/main.html，将 todos 中的项目以 input 标签形式输出： 123456&lt;div class="container"&gt; &lt;h2&gt;My todos&lt;/h2&gt; &lt;p class="form-group" ng-repeat="todo in todos"&gt; &lt;input type="text" ng-model="todo" class="form-control"&gt; &lt;/p&gt;&lt;/div&gt; 页面会显示如下： 添加一个 todo 项首先，添加一个输入框和添加按钮，将 views/main.html 修改为： 123456789101112131415161718192021&lt;div class="container"&gt; &lt;h2&gt;My todos&lt;/h2&gt; &lt;!-- Todos input --&gt; &lt;form role="form" ng-submit="addTodo()"&gt; &lt;div class="row"&gt; &lt;div class="input-group"&gt; &lt;input type="text" ng-model="todo" placeholder="What needs to be done?" class="form-control"&gt; &lt;span class="input-group-btn"&gt; &lt;input type="submit" class="btn btn-primary" value="Add"&gt; &lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;/form&gt; &lt;p&gt;&lt;/p&gt; &lt;!-- Todos list --&gt; &lt;p class="form-group" ng-repeat="todo in todos"&gt; &lt;input type="text" ng-model="todo" class="form-control"&gt; &lt;/p&gt;&lt;/div&gt; 这时候，页面内容如下： 修改 main.js 添加 addTodo() 事件： 12345678910'use strict';angular.module('webApp') .controller('MainCtrl', function ($scope) &#123; $scope.todos = ['Item 1', 'Item 2', 'Item 3']; $scope.addTodo = function () &#123; $scope.todos.push($scope.todo); $scope.todo = ''; &#125;; &#125;); 这样，就完成了添加 todo 项的功能。 移除一个 todo 项目先在列表中每一个 todo 项目的边上加上一个移除按钮，修改 views/main.html 中 Todos list 注释部分的代码为： 1234567&lt;!-- Todos list --&gt;&lt;p class="input-group" ng-repeat="todo in todos"&gt; &lt;input type="text" ng-model="todo" class="form-control"&gt; &lt;span class="input-group-btn"&gt; &lt;button class="btn btn-danger" ng-click="removeTodo($index)" aria-label="Remove"&gt;X&lt;/button&gt; &lt;/span&gt;&lt;/p&gt; 修改 main.js 添加 removeTodo($index) 事件： 1234567891011121314'use strict';angular.module('webApp') .controller('MainCtrl', function ($scope) &#123; $scope.todos = ['Item 1', 'Item 2', 'Item 3']; $scope.addTodo = function () &#123; $scope.todos.push($scope.todo); $scope.todo = ''; &#125;; $scope.removeTodo = function (index) &#123; $scope.todos.splice(index, 1); &#125;; &#125;); 现在，删除按钮能够响应删除事件了。虽然我们可以添加和移除 Todo 事项，但是这些记录都不能永久地保存。一旦页面被刷新了，更改的记录都会不见了，又恢复到 main.js 中设置的todo 数组的值。 另外，上面添加 Todo 项时，如果重复添加相同的记录，则后台会报错，这是因为脚本中没有做校验。 对 Todo 事项进行排序接下来，我们安装并引入 AngularUI Sortable 模块，使得 Todo 事项可以排序。这里，我们需要使用 bower 安装 angular-ui-sortable 和 jquery-ui： 1$ bower install --save angular-ui-sortable jquery-ui 添加参数 --save 可以更新 bower.json 文件中关于 angular-ui-sortable 和 jquery-ui 的依赖，这样你就不用手动去 bower.json 中更新依赖了。 再次启动 grunt server： 1$ grunt serve 为了使用Sortable模块，我们需要在 scripts/app.js 中更新Angular 模块，将 Sortable 可以加载到我们的应用中，更改前代码， 将 ui.sortable 添加进数组中,如下： 123456789angular.module('webApp', [ 'ngAnimate', 'ngCookies', 'ngResource', 'ngRoute', 'ngSanitize', 'ngTouch', 'ui.sortable' ]) 最后，在 views/main.html 中，我们需要将 ui-sortable 指令作为一个 div 将 ng-repeat 层包起来。 123&lt;!-- Todos list --&gt;&lt;div ui-sortable ng-model="todos"&gt; &lt;p class="input-group" ng-repeat="todo in todos"&gt; 添加一些内联的 CSS，将鼠标显示为 “可移动” 样式来告诉用户这些 todo 项是可以移动的： 1&lt;p class="input-group" ng-repeat="todo in todos" style="padding:5px 10px; cursor: move;"&gt; 完整代码如下： 123456789&lt;!-- Todos list --&gt;&lt;div ui-sortable ng-model="todos"&gt; &lt;p class="input-group" ng-repeat="todo in todos" style="padding:5px 10px; cursor: move;"&gt; &lt;input type="text" ng-model="todo" class="form-control"&gt; &lt;span class="input-group-btn"&gt; &lt;button class="btn btn-danger" ng-click="removeTodo($index)" aria-label="Remove"&gt;X&lt;/button&gt; &lt;/span&gt; &lt;/p&gt;&lt;/div&gt; 服务浏览器，我们就可以对 Todo 事项进行拖拽排序了。 持久化存储之前项目的数据，当浏览器刷新后就不会保存了。我们可以安装 Angular 模块 angular-local-storage，快速实现本地存储。 下载依赖： 1$ bower install --save angular-local-storage 编辑 scripts/app.js 添加 LocalStorageModule的 适配器: 1234567891011angular .module('webApp', [ 'ngAnimate', 'ngCookies', 'ngResource', 'ngRoute', 'ngSanitize', 'ngTouch', 'ui.sortable', 'LocalStorageModule']) 同时也要配置 localStorageServiceProvider，用 ls 作为 localStorage名称前缀： 123.config(['localStorageServiceProvider', function(localStorageServiceProvider)&#123; localStorageServiceProvider.setPrefix('ls');&#125;]) 完整的 scripts/app.js 文件： 12345678910111213141516171819202122232425262728'use strict';angular .module('webApp', [ 'ngAnimate', 'ngCookies', 'ngResource', 'ngRoute', 'ngSanitize', 'ngTouch', 'ui.sortable', 'LocalStorageModule' ]).config(['localStorageServiceProvider', function(localStorageServiceProvider)&#123; localStorageServiceProvider.setPrefix('ls'); &#125;]).config(function ($routeProvider) &#123; $routeProvider .when('/', &#123; templateUrl: 'views/main.html', controller: 'MainCtrl' &#125;) .when('/about', &#123; templateUrl: 'views/about.html', controller: 'AboutCtrl' &#125;) .otherwise(&#123; redirectTo: '/' &#125;); &#125;); 然后，需要修改 scripts/controllers/main.js ，改为从本地存储访问数据： 1234567891011121314151617181920212223242526272829303132333435363738394041'use strict';angular.module('webApp').controller('MainCtrl', function ($scope, localStorageService) &#123; // 初始化时为空 var todosInStore = localStorageService.get('todos'); $scope.todos = todosInStore &amp;&amp; todosInStore.split('\n') || []; // 监听变化 $scope.$watch('todos', function () &#123; localStorageService.add('todos', $scope.todos.join('\n')); &#125;, true); $scope.addTodo = function () &#123; $scope.todos.push($scope.todo); $scope.todo = ''; &#125;; $scope.removeTodo = function (index) &#123; $scope.todos.splice(index, 1); &#125;; &#125;);~~~ 在浏览器中查看应用，你会发现 todo 列表中没有任何东西。因为这个应用从本地存储中读取了 todo 数组，而本地存储中还没有任何 todo 项。在添加一些项目到列表后，我们再次刷新我们的浏览器的时候，这些项目都还在。![](/static/images/2015/yeoman-web2.jpg)# 测试[Karma](http://karma-runner.github.io/) 是一个 JS 测试框架。Angular 生成器本身已经包括了两个测试框架：[ngScenario](https://code.angularjs.org/1.2.16/docs/guide/e2e-testing) 和 [Jasmine](http://jasmine.github.io/)。当之前我们运行 `yo angular` 的时候，在 mytodo 文件夹下会生成了一个 test 目录，还有一个 karma.conf.js 文件，它会被放入在 Node 模块中以使用 Karma。我们将会编辑一个 Jasmine 脚本来完成我们的测试。现在先来看看要怎么进行测试。先安装依赖：~~~bash$ npm install -g phantomjs$ npm install grunt-karma --save-dev 更新 Karma 配置首先，修改 karma.conf.js，添加 123'bower_components/jquery/dist/jquery.js','bower_components/jquery-ui/ui/jquery-ui.js','bower_components/angular-ui-sortable/sortable.js', 最终的样子是： 12345678910111213141516files: [ 'bower_components/angular/angular.js', 'bower_components/angular-mocks/angular-mocks.js', 'bower_components/angular-animate/angular-animate.js', 'bower_components/angular-cookies/angular-cookies.js', 'bower_components/angular-resource/angular-resource.js', 'bower_components/angular-route/angular-route.js', 'bower_components/angular-sanitize/angular-sanitize.js', 'bower_components/angular-touch/angular-touch.js', 'bower_components/jquery/dist/jquery.js', 'bower_components/jquery-ui/ui/jquery-ui.js', 'bower_components/angular-ui-sortable/sortable.js', 'app/scripts/**/*.js', 'test/mock/**/*.js', 'test/spec/**/*.js'], 运行测试现在回到命令行结束 grunt server 的进程（使用 Ctrl+c）。在 Gruntfile.js 中已经有了用于运行测试的 grunt 任务，可以直接像下面这样运行： 1$ grunt test 添加更多测试修改 test/spec/controllers/main.js 如下： 1234567891011121314151617181920212223242526272829303132333435'use strict';describe('Controller: MainCtrl', function () &#123; // load the controller's module beforeEach(module('webApp')); var MainCtrl, scope; // Initialize the controller and a mock scope beforeEach(inject(function ($controller, $rootScope) &#123; scope = $rootScope.$new(); MainCtrl = $controller('MainCtrl', &#123; $scope: scope &#125;); &#125;)); it('should attach a list of awesomeThings to the scope', function () &#123; expect(scope.awesomeThings.length).toBe(3); &#125;); it('should add items to the list', function () &#123; scope.todo = 'Test 1'; scope.addTodo(); expect(scope.todos.length).toBe(4); &#125;); it('should add then remove an item from the list', function () &#123; scope.todo = 'Test 1'; scope.addTodo(); scope.removeTodo(0); expect(scope.todos.length).toBe(2); &#125;);&#125;); 更多有关单元测试的内容，请参考 Unit Testing Best Practices in AngularJS。 发布应用为了将应用发布为产品版本，还需要做很多工作： 校验我们的代码 运行我们的测试 合并和缩小脚本和样式来减少网络请求 优化任何使用到的图像 对所有输出进行编译处理，使程序瘦身 实现上述目标只需一句： 1$ grunt 这个命令将会完成 Grunt 的任务以及根据 Gruntfile.js 文件进行配置，创建一个可以运行的应用版本。只需等上一分钟，你就能得到一个完整的编译版本，和一份编译过程耗时的报告。 编译完成后的文件，放在了 dist 目录下，是一个可以拿去服务器上的部署的真正的产品。 你也可以运行下面命令自动编译项目，并且启动 web 服务器 1$ grunt serve:dist 总结Anugular 生成器也支持创建新的视图、指令和控制器。例如：可以通过运行 yo angular:route routeName 搭建一个新的控制器，同时在 app.js 中的路由也会被更新。 了解更多有关于 Angular 生成器的 Yeoman 命令，请查看 generator readme。 当然，Yeoman 还可以做更多的事情，它还支持其他框架的脚手架。 除了 Yeoman 之外，还有几个框架可以生产 Angular 项目，请参考 5 Angular JS Seeds &amp; Bootstrap Apps 参考文章 LET’S SCAFFOLD A WEB APP WITH YEOMAN 在Windows环境下用Yeoman构建AngularJS项目]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>angular.js</tag>
        <tag>yeoman</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Require.JS快速入门]]></title>
    <url>%2F2015%2F02%2F02%2Fquick-start-of-requirejs%2F</url>
    <content type="text"><![CDATA[Require.JS 介绍Require.JS 是一个基于 AMD 规范的 JavaScript 模块加载框架。实现 JavaScript 文件的异步加载，管理模块之间的依赖性，提升网页的加载速度。 AMD 是 Asynchronous Module Definition 的缩写，意思就是 异步模块定义。它采用异步方式加载模块，模块的加载不影响它后面语句的运行。所有依赖这个模块的语句，都定义在一个回调函数中，等到加载完成之后，这个回调函数才会运行。 官网地址：&lt;www.requirejs.org&gt; Require.JS 的诞生主要为了解决两个问题： 1）实现 JavaScript 文件的异步加载，避免网页失去响应； 2）管理模块之间的依赖性，便于代码的编写和维护。 加载 Require.JS去官方网站下载最新版本，然后创建一个工程，目录如下： 12345678910project-directory├── project.html└── scripts ├── lib │ ├── a.js │ ├── b.js │ ├── backbone.js │ └── underscore.js ├── main.js └── require.js 编写 project.html 内容，引入 Require.JS 文件： 123456789101112&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;My Sample Project&lt;/title&gt; &lt;!-- data-main attribute tells require.js to load scripts/main.js after require.js loads. --&gt; &lt;script data-main="scripts/main" src="scripts/require.js"&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;My Sample Project&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; 加载 scripts/require.js 这个文件，也可能造成网页失去响应。解决办法有两个，一个是把它放在网页底部加载，另一个是写成下面这样： 1&lt;script data-main="scripts/main" src="js/require.js" defer async="true" &gt;&lt;/script&gt; async 属性表明这个文件需要异步加载，避免网页失去响应。IE 不支持这个属性，只支持 defer，所以把 defer 也写上。 加载 Require.JS 以后，下一步就要加载我们自己的代码了，这里我们使用 data-main 属性来指定网页程序的主模块。在上例中，就是 scripts 目录下面的 main.js，这个文件会第一个被 Require.JS 加载。由于 Require.JS 默认的文件后缀名是 js，所以可以把 main.js 简写成 main。 在浏览器中打开 project.html 文件，查看浏览器是否提示有错误。 定义模块Require.JS 加载的模块，采用 AMD 规范。也就是说，模块必须按照 AMD 的规定来写。具体来说，就是模块必须采用特定的 define() 函数来定义。如果一个模块不依赖其他模块，那么可以直接定义在 define() 函数之中。 定义： 1define(id, dependencies, factory); 参数： id : 模块标示符[可省略] dependencies : 所依赖的模块[可省略] factory : 模块的实现，或者一个 JavaScript 对象。 Require.JS 定义模块的几种方式： 1、定义一个最简单的模块： 1234define(&#123; color: "black", size: "unisize"&#125;); 2、传入匿名函数定义一个模块： 123456define(function () &#123; return &#123; color: "black", size: "unisize" &#125; &#125;); 3、定义一个模块并依赖于 cart.js 123456789define(["./cart"], function(cart) &#123; return &#123; color: "blue", size: "large", addToCart: function() &#123; cart.add(this); &#125; &#125;&#125;); 4、定义一个名称为 hello 且依赖于 cart.js 的模块 123define("hello",["./cart"],function(cart) &#123; //do something&#125;); 5、兼容 commonJS 模块的写法(使用 require 获取依赖模块，使用 exports 导出 API) 123456define(function(require, exports, module) &#123; var base = require('base'); exports.show = function() &#123; // todo with module base &#125; &#125;); 举例，在 lib/a.js 中，我们可以定义模块： 12345678define(function ()&#123; var add = function (x,y)&#123; return x+y; &#125;; return &#123; add: add &#125;;&#125;); 然后，在 main.js 中使用： 123require(['lib/a'], function (a)&#123; alert(a.add(1,1));&#125;); 如果这个模块还依赖其他模块，那么 define() 函数的第一个参数，必须是一个数组，指明该模块的依赖性。 例如，lib/b.js 文件如下： 123456789define(['lib/a'], function(a)&#123; function fun1()&#123; alert(2); &#125; return &#123; fun1: fun1, fun2: function()&#123;alert(3)&#125; &#125;;&#125;); 定义的模块返回函数个数问题: define 的 return 只有一个函数，require 的回调函数可以直接用别名代替该函数名。 define 的 return 当有多个函数，require 的回调函数必须用别名调用所有的函数。 这时候修改 main.js 内容为： 123456require(['lib/a','lib/b'], function(a,b) &#123; alert(a.add(0,1)) b.fun1(); b.fun2();&#125;); 然后，在浏览器中打开 project.html 文件，查看输出内容。 全局配置如果你想改变 RequireJS 的默认配置来使用自己的配置，你可以使用 require.config 函数。require.config() 方法一般写在主模块(main.js)最开始。参数为一个对象，通过对象的键值对加载进行配置: baseUrl：用于加载模块的根路径。如果 baseUrl 没有指定，那么就会使用 data-main 中指定的路径。 paths：用于映射不存在根路径下面的模块路径。 map : 对于给定的相同的模块名，加载不同的模块，而不是加载相同的模块 packages : 配置从 CommonJS 包来加载模块 waitSeconds：是指定最多花多长等待时间来加载一个 JavaScript 文件，用户不指定的情况下默认为 7 秒。 shims：配置在脚本/模块外面并没有使用 RequireJS 的函数依赖并且初始化函数。 deps：加载依赖关系。 其他可配置的选项还包括 locale、context、callback 等，有兴趣的读者可以在 RequireJS 的官方网站查阅相关文档。 main.js 中修改如下： 123456789101112131415require.config(&#123; //By default load any module IDs from scripts/main baseUrl: 'scripts', paths: &#123; "jquery" : ["http://libs.baidu.com/jquery/2.0.3/jquery", "scripts/jquery"], "a":"lib/a" &#125;&#125;);//通过别名来引用模块require(["jquery","a"],function($)&#123; $(function()&#123; alert("load finished"); &#125;)&#125;) 在这个例子中，通过 baseUrl 把根路径设置为了 scripts，通过 paths 的配置会使我们的模块名字更精炼，paths 还有一个重要的功能，就是可以配置多个路径，如果远程加载没有成功，可以加载本地的库。 上面例子中，先从远程加载 jquery 模块，如果加载失败，则从本地 scripts/jquery 加载；a 模块名称为 lib/a 路径的简称，加载 a 模块时，实际上是加载的 script/lib/a.js 文件。 通过 require加载的模块一般都需要符合 AMD 规范即使用 define 来申明模块，但是部分时候需要加载非 AMD 规范的 JavaScript，这时候就需要用到另一个功能：shim。比如我要是用 underscore 类库，但是他并没有实现 AMD 规范，那我们可以这样配置： 1234567require.config(&#123; shim: &#123; "underscore" : &#123; exports : "_"; &#125; &#125;&#125;) 这样配置后，我们就可以在其他模块中引用 underscore 模块： 123require(["underscore"], function(_)&#123; _.each([1,2,3], alert);&#125;) 在新版本的 jquery 中，继承了 AMD 规范，所以可以直接使用 require[&quot;jquery&quot;]，但是我们经常用到的 jquery 插件基本都不符合AMD规范。 12345678910require.config(&#123; shim: &#123; "underscore" : &#123; exports : "_"; &#125;, "jquery.form" : &#123; deps : ["jquery"] &#125; &#125;&#125;) 也可以简写为： 12345678require.config(&#123; shim: &#123; "underscore" : &#123; exports : "_"; &#125;, "jquery.form" : ["jquery"] &#125;&#125;) 这样配置之后我们就可以使用加载插件后的 jquery 了： 12345require.config(["jquery", "jquery.form"], function($)&#123; $(function()&#123; $("#form").ajaxSubmit(&#123;...&#125;); &#125;)&#125;) Require.JS 插件Require.JS 还提供一系列插件，实现一些特定的功能。 domready 插件，可以让回调函数在页面 DOM 结构加载完成后再运行。 123require(['domready!'], function (doc)&#123; // called once the DOM is ready&#125;); tex t和 image 插件，则是允许 Require.JS 加载文本和图片文件。 123456789define([ 'text!review.txt', 'image!cat.jpg' ], function(review,cat)&#123; console.log(review); document.body.appendChild(cat); &#125;); 类似的插件还有 json 和 mdown，用于加载 json 文件和 markdown 文件。 插件清单：https://github.com/jrburke/requirejs/wiki/Plugins 其他问题 1、循环依赖 在一些情况中，我们可能需要模块 moduleA 和 moduleA 中的函数需要依赖一些应用。这就是循环依赖。 1234567891011// js/app/moduleA.jsdefine( [ "require", "app/app"], function( require, app ) &#123; return &#123; foo: function( title ) &#123; var app = require( "app/app" ); return app.something(); &#125; &#125; &#125;); 2、得到模块的地址 如果你需要得到模块的地址，你可以这么做…… 1var path = require.toUrl("./style.css"); 3、JSONP 我们可以这样处理 JSONP： 12345require( [ "http://someapi.com/foo?callback=define"], function (data) &#123; console.log(data);&#125;); 代码合并于压缩 请参考 优化 RequireJS 项目（合并与压缩） 参考文章 使用require.js进行JavaScript模块化编程]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>require.js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django中SQL查询]]></title>
    <url>%2F2015%2F01%2F30%2Fraw-sql-query-in-django%2F</url>
    <content type="text"><![CDATA[当 Django 中模型提供的查询 API 不能满足要求时，你可能需要使用原始的 sql 查询，这时候就需要用到 Manager.raw() 方法。 Manager 类提供下面的一个方法，可以用于执行 sql： 1Manager.raw(raw_query, params=None, translations=None) 使用方法为： 1234&gt;&gt;&gt; for p in Person.objects.raw('SELECT * FROM myapp_person'):... print(p)John SmithJane Jones raw() 可以通过名称自动将 sql 语句中的字段映射到模型，所以你不必考虑 sql 语句中的字段顺序： 1234&gt;&gt;&gt; Person.objects.raw('SELECT id, first_name, last_name, birth_date FROM myapp_person')...&gt;&gt;&gt; Person.objects.raw('SELECT last_name, birth_date, first_name, id FROM myapp_person')... 字段映射如果一个表中有和 person 相同的数据，只是字段名称不一致，你可以使用 sql 语句中 的 as 关键字给字段取别名： 12345&gt;&gt;&gt; Person.objects.raw('''SELECT first AS first_name,... last AS last_name,... bd AS birth_date,... pk AS id,... FROM some_other_table''') 另外，你还可以添加 translations 参数，手动设置映射关系： 12&gt;&gt;&gt; name_map = &#123;'first': 'first_name', 'last': 'last_name', 'bd': 'birth_date', 'pk': 'id'&#125;&gt;&gt;&gt; Person.objects.raw('SELECT * FROM some_other_table', translations=name_map) 延迟获取字段下面的查询： 1&gt;&gt;&gt; people = Person.objects.raw('SELECT id, first_name FROM myapp_person') sql 语句中只包含两个字段，如果你从 people 中获取不在 sql 语句中的字段的值的时候，其会从数据库再查询一次： 123456&gt;&gt;&gt; for p in Person.objects.raw('SELECT id, first_name FROM myapp_person'):... print(p.first_name, # This will be retrieved by the original query... p.last_name) # This will be retrieved on demand...John SmithJane Jones 但是，需要记住的是 sql 语句中一定要包括主键，因为只有主键才能唯一标识数据库中的一条记录。 传递参数如果你想给 sql 语句传递参数，你可以添加 raw() 的 params 参数。 12&gt;&gt;&gt; lname = 'Doe'&gt;&gt;&gt; Person.objects.raw('SELECT * FROM myapp_person WHERE last_name = %s', [lname]) 记住：上面的 sql 语句一定不要写成下面的方式，因为这样可能会有 sql 注入的问题！ 12&gt;'SELECT * FROM myapp_person WHERE last_name = %s' % lname&gt; 运行自定义的 sql Manager.raw() 只能执行 sql 查询，确不能运行 UPDATE、INSERT 或 DELETE 查询，在这种情况下，你需要使用 django.db.connection 类。 1234567891011 from django.db import connectiondef my_custom_sql(self): cursor = connection.cursor() cursor.execute("UPDATE bar SET foo = 1 WHERE baz = %s", [self.baz]) cursor.execute("SELECT foo FROM bar WHERE baz = %s", [self.baz]) row = cursor.fetchone() return row 在 Django 1.6 之后，修改数据之后会自动提交事务，而不用手动调用 transaction.commit_unless_managed() 了。 如果 sql 中你想使用 % 字符 ，则你需要写两次： 12cursor.execute("SELECT foo FROM bar WHERE baz = '30%'")cursor.execute("SELECT foo FROM bar WHERE baz = '30%%' AND id = %s", [self.id]) 如果有多个数据库： 123from django.db import connectionscursor = connections['my_db_alias'].cursor()# Your code here... 默认情况下，Python 的 DB API 返回的结果只包括数据不包括每个字段的名称，你可以牺牲一点性能代价，将查询结果转换为字典： 1234567def dictfetchall(cursor): "Returns all rows from a cursor as a dict" desc = cursor.description return [ dict(zip([col[0] for col in desc], row)) for row in cursor.fetchall() ] 在 Django 1.7 中，可以使用 cursor 作为一个上下文管理器： 12with connection.cursor() as c: c.execute(...) 其等价于： 12345c = connection.cursor()try: c.execute(...)finally: c.close() 本文参考：Performing raw SQL queries。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装和部署Presto]]></title>
    <url>%2F2015%2F01%2F26%2Finstall-and-deploy-presto%2F</url>
    <content type="text"><![CDATA[1. 安装环境 操作系统：CentOs6.5 Hadoop 集群：CDH5.3 JDK 版本：jdk1.8.0_31 为了测试简单，我是将 Presto 的 coordinator 和 worker 都部署在 cdh1 节点上，并且该节点上部署了 hive-metastore 服务。下面的安装和部署过程参考自 http://prestodb.io/docs/current/installation.html。 2. 安装 Presto下载 Presto 的压缩包，目前最新版本为 presto-server-0.90，然后解压为 presto-server-0.90 。 12wget https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.90/presto-server-0.90.tar.gztar zxvf presto-server-0.90.tar.gz 解压后的目录结构为： 1234567891011121314151617181920212223[$ presto-server-0.90]# tree -L 2.├── bin│ ├── launcher│ ├── launcher.properties│ ├── launcher.py│ └── procname├── lib├── NOTICE├── plugin│ ├── cassandra│ ├── example-http│ ├── hive-cdh4│ ├── hive-cdh5│ ├── hive-hadoop1│ ├── hive-hadoop2│ ├── kafka│ ├── ml│ ├── mysql│ ├── postgresql│ ├── raptor│ └── tpch└── README.txt 从 plugin 目录可以看到所有 Presto 支持的插件有哪些，这里我主要使用 hive-cdh5 插件，也成为连接器。 3. 配置 Presto在 presto-server-0.90 目录创建 etc 目录，并创建以下文件： node.properties：每个节点的环境配置 jvm.config：jvm 参数 config.properties：配置 Presto Server 参数 log.properties：配置日志等级 Catalog Properties：Catalog 的配置 etc/node.properties 示例配置如下： 123node.environment=productionnode.id=ffffffff-ffff-ffff-ffff-ffffffffffffnode.data-dir=/var/presto/data 参数说明： node.environment：环境名称。一个集群节点中的所有节点的名称应该保持一致。 node.id：节点唯一标识的名称。 node.data-dir：数据和日志存放路径。 etc/jvm.config 示例配置如下： 123456789-server-Xmx16G-XX:+UseConcMarkSweepGC-XX:+ExplicitGCInvokesConcurrent-XX:+CMSClassUnloadingEnabled-XX:+AggressiveOpts-XX:+HeapDumpOnOutOfMemoryError-XX:OnOutOfMemoryError=kill -9 %p-XX:ReservedCodeCacheSize=150M etc/config.properties 包含 Presto Server 相关的配置，每一个 Presto Server 可以通时作为 coordinator 和 worker 使用。你可以将他们配置在一个极点上，但是，在一个大的集群上建议分开配置以提高性能。 coordinator 的最小配置： 123456coordinator=truenode-scheduler.include-coordinator=falsehttp-server.http.port=8080task.max-memory=1GBdiscovery-server.enabled=truediscovery.uri=http://cdh1:8080 worker 的最小配置： 1234coordinator=falsehttp-server.http.port=8080task.max-memory=1GBdiscovery.uri=http://cdh1:8080 可选的，作为测试，你可以在一个节点上同时配置两者： 123456coordinator=truenode-scheduler.include-coordinator=truehttp-server.http.port=8080task.max-memory=1GBdiscovery-server.enabled=truediscovery.uri=http://cdh1:8080 参数说明： coordinator：Presto 实例是否以 coordinator 对外提供服务 node-scheduler.include-coordinator：是否允许在 coordinator 上进行调度任务 http-server.http.port：HTTP 服务的端口 task.max-memory=1GB：每一个任务（对应一个节点上的一个查询计划）所能使用的最大内存 discovery-server.enabled：是否使用 Discovery service 发现集群中的每一个节点。 discovery.uri：Discovery server 的 url etc/log.properties 可以设置某一个 java 包的日志等级： 1com.facebook.presto=INFO 关于 Catalog 的配置，首先需要创建 etc/catalog 目录，然后根据你想使用的连接器来创建对应的配置文件，比如，你想使用 jmx 连接器，则创建 jmx.properties： 1connector.name=jmx 如果你想使用 hive 的连接器，则创建 hive.properties： 123connector.name=hive-cdh5hive.metastore.uri=thrift://cdh1:9083 #修改为 hive-metastore 服务所在的主机名称，这里我是安装在 cdh1节点hive.config.resources=/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml 更多关于连接器的说明，请参考 Connectors 。 4. 运行 Presto你可以使用下面命令后台启动： 1bin/launcher start 也可以前台启动，观察输出日志： 1bin/launcher run 另外，你也可以通过下面命令停止： 1bin/launcher stop 更多命令，你可以通过 --help 参数来查看。 12345678910111213141516171819202122232425[root@cdh1 presto-server-0.90]# bin/launcher --helpUsage: launcher [options] commandCommands: run, start, stop, restart, kill, statusOptions: -h, --help show this help message and exit -v, --verbose Run verbosely --launcher-config=FILE Defaults to INSTALL_PATH/bin/launcher.properties --node-config=FILE Defaults to INSTALL_PATH/etc/node.properties --jvm-config=FILE Defaults to INSTALL_PATH/etc/jvm.config --config=FILE Defaults to INSTALL_PATH/etc/config.properties --log-levels-file=FILE Defaults to INSTALL_PATH/etc/log.properties --data-dir=DIR Defaults to INSTALL_PATH --pid-file=FILE Defaults to DATA_DIR/var/run/launcher.pid --launcher-log-file=FILE Defaults to DATA_DIR/var/log/launcher.log (only in daemon mode) --server-log-file=FILE Defaults to DATA_DIR/var/log/server.log (only in daemon mode) -D NAME=VALUE Set a Java system property~~~ 启动之后，你可以观察 /var/presto/data/ 目录： [root@cdh1 /var/presto/data/]# tree. ├── etc -&gt; /opt/presto-server-0.90/etc├── plugin -&gt; /opt/presto-server-0.90/plugin└── var ├── log │ ├── http-request.log │ ├── launcher.log │ └── server.log └── run └── launcher.pid 5 directories, 4 files 1234567891011121314151617181920212223在 /var/presto/data/var/log 目录可以查看日志：- `launcher.log`：启动日志- `server.log`：Presto Server 输出日志- `http-request.log`：HTTP 请求日志# 5. 测试 Presto CLI 下载 [presto-cli-0.90-executable.jar](https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.90/presto-cli-0.90-executable.jar) 并将其重命名为 presto-cli（你也可以重命名为 presto），然后添加执行权限。运行下面命令进行测试：~~~bash[root@cdh1 bin]# ./presto-cli --server localhost:8080 --catalog hive --schema defaultpresto:default&gt; show tables; Table-------(0 rows)Query 20150126_062137_00012_qgwvy, FINISHED, 1 nodeSplits: 2 total, 2 done (100.00%)0:00 [0 rows, 0B] [0 rows/s, 0B/s] 在 执行 show tables 命令之前，你可以查看 http://cdh1:8080/ 页面： 然后在执行该命令之后再观察页面变化。单击第一行记录，会跳转到明细页面： 可以运行 --help 命令查看更多参数，例如你可以在命令行直接运行下面命令： 1./presto-cli --server localhost:8080 --catalog hive --schema default --execute &quot;show tables;&quot; 默认情况下，Presto 的查询结果是使用 less 程序分页输出的，你可以通过修改环境变量 PRESTO_PAGER 的值将其改为其他命令，如 more，或者将其置为空以禁止分页输出。 6. 测试 jdbc使用 jdbc 连接 Presto，需要下载 jdbc 驱动 presto-jdbc-0.90 并将其加到你的应用程序的 classpath 中。 支持以下几种 JDBC URL 格式： 123jdbc:presto://host:portjdbc:presto://host:port/catalogjdbc:presto://host:port/catalog/schema 连接 hive 数据库中 sales 库，示例如下： 1jdbc:presto://cdh1:8080/hive/sales 7. 总结本文主要记录 Presto 的安装部署过程，并使用 hive-cdh5 连接器进行简单测试。下一步，需要基于一些生产数据做一些功能测试以及和 impala 做一些对比测试。]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>presto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH 5.2中Impala认证集成LDAP和Kerberos]]></title>
    <url>%2F2015%2F01%2F23%2Fnew-in-cdh-5-2-impala-authentication-with-ldap-and-kerberos%2F</url>
    <content type="text"><![CDATA[这是一篇翻译的文章，原文为 New in CDH 5.2: Impala Authentication with LDAP and Kerberos。由于翻译水平有限，难免会一些翻译不准确的地方，欢迎指正！ Impala 认证现在可以通过 LDAP 和 Kerberos 联合使用来解决。下文来解释为什么和怎样解决。 Impala，是基于 Apache Hadoop 的一个开源的分析数据库，使用 Kerberos 和 LDAP 来支持认证-作为一种角色来证明你是否是你所说的你是谁。Kerberos 在1.0版本中就已经被支持了，而 LDAP 是最近才被支持，在 CDH 5.2 中，你能够同时使用两者。 同时使用 LDAP 和 Kerberos 提供了显著的价值；Kerberos 保留了核心的认证协议并且总是用在 Impala 进程彼此之间以及和 Hadoop 集群连接的时候。然后，Kerberos 需要更多的维护支持。LDAP 是在整个企业中无处不在，并且通常由通过 ODBC 和 JDBC 驱动程序连接到 Impala 的客户端应用程序来使用。两者的组合使用是有一定道理的。 下表表明了各种组合和它们的使用场景： Impala 认证 使用场景 No Authentication 非安全的集群。我们确定既然你在阅读这篇文章，那你不会对这种场景感兴趣 Kerberos Only Hadoop 集群开启 Kerberos 认证，并且连接到 Impala 的每个用户或者客户端都有一个 Kerberos principal。（详细说明见下文。） LDAP Only Hadoop 集群 没有开启 Kerberos 认证，但是你想认证连接 Impala 的外包的客户端，如你使用 Active Directory 或者其他的 LDAP 服务。 Kerberos and LDAP Hadoop 集群开启 Kerberos 认证。在 Impala 外部的用户没有 Kerberos principal，但是有一个 LDAP 的凭证。 在这篇文章中，我将解释为何以及如何使用 LDAP 和 Kerberos 的组合来建立 Impala 认证。 1. KerberosKerberos 仍然是 Apache Hadoop 的主要认证机制。下面是 Kerberos 的一些术语将会帮助你理解这篇文章讨论的内容。 principal 是 Kerberos 主体，就要一个用户或者一个守护进程。对于我们来说，一个 principal 对于守护进程来说是 name/hostname@realm，或者对于用户来说仅仅是 name@realm。 name 字段可能是一个进程，例如 impala，或者是一个用户名，例如 myoder`。 hostname 可能是一个机器的全名称，或者是一个 Hadoop 定义的 _HOST 字符串，通常会机器全名称自动替换。 realm 类似于（但不必要和其一样）一个 DNS 域名。 Kerberos 主体能够证明他们是谁，要么通过提供一个密码（如果主体是人），或者通过提供“密钥表”的文件。 Impala 守护进程需要一个密钥表文件，该文件必须得到很好的保护：任何可以读取密钥表文件的人可以冒充 Impala 守护进程。 Basic support for Kerberos in impala for this process is straightforward: Supply the following arguments, and the daemons will use the given principal and the keys in the keytab file to take on the identity of the principal for all communication. 在 Impala 中对 Kerberos 基本的支持很简单：提供以下参数，守护程序将使用给定的主体和 key，在密钥表文件中验证所有通信的主体的身份。 12--principal=impala/hostname@realm--keytab_file=/full/path/to/keytab 还有另一外一种情况是 Impala 守护进程（impalad）运行在一个负载均衡器下。 当客户端通过负载平衡器（一个代理）运行查询时，客户端期望 impalad 有一个和负载平衡器的名称的主体。所以当 Impalad 对外部查询提供服务时，需要使用一个和代理相同名称的主体，但是当做后台程序之间的通讯时需要一个主体匹配实际的主机名称。 123--principal=impala/proxy-hostname@realm--be_principal=impala/actual-hostname@realm--keytab_file=/full/path/to/keytab 第一个参数 --principal 定义 Impalad 服务外部查询时使用哪一个主体，--be_principal 参数定义 Impalad 进程之间通信时使用哪一个主体。两个主体的 key 必须存在于相同的 keytab 文件中。 调试 KerberosKerberos是一个优雅的协议，但是当出现错误的时候实际的实现不是非常有帮助的。下面主要有两件事情需要检查，在出现认证失败的时候： Time。Kerberos 是依赖于同步时钟，因此在所有使用 Kerberos 的机器上安装和使用 NTP（网络时间协议）是一个最佳实践。 DNS。请确保您的主机名是全名称的并且正向（名称 - &gt; IP）和反向（IP-&gt;名称）DNS查找是否正确。 除此之外，也可以设置两个环境变量以输出 Kerberos 调试信息。输出可能是一小部分的内容，但通常是帮助你解决问题。 KRB5_TRACE=/full/path/to/trace/output.log：该环境变量指定调试日志输出路径。 JAVA_TOOL_OPTIONS=-Dsun.security.krb5.debug=true：该环境变量会传给 Impala 守护进程，并传递给内部的 java 组件。 Kerberos FlagsCloudera documentation for Kerberos and Impala 给出了详细的说明，这里只做简要介绍： 2. LDAPKerberos 是伟大的，但它确实需要最终用户有一个有效的 Kerberos 证书，这在许多环境中是不实际的，因为每个与 Impala 和Hadoop集群交互的用户都必须配置 Kerberos 主体。对于使用 Active Directory 来管理用户帐户的组织，可能需要在 MIT Kerberos 领域频繁的创建每个用户对应的用户帐户。取而代之的时许多企业环境使用 LDAP 协议，在客户使用自己的用户名和密码进行身份验证自己。 当配置为使用LDAP，把 impalad 看作为一个 LDAP 代理：客户端（ Impala shell，ODBC，JDBC，hue 等等）发送其用户名和密码到 impalad ，然后 impalad 将用户名和密码发送给 LDAP 服务器并尝试登录。在 LDAP 术语中，impalad 发出 LDAP “绑定” 操作。如果 LDAP 服务器为该次登陆尝试返回成功，则 impalad 接受连接。 LDAP只用于验证外部客户，如Impala shell，ODBC，JDBC，和 hue。所有其他后端认证由 Kerberos 的处理。 LDAP 配置LDAP 是复杂的（而且强大的），因为它非常灵活；有许多方式来配置 LDAP 实体和验证这些实体。在一般情况下，每个人都在 LDAP 具有专有名称或DN，可被认为是 LDAP 中的用户名或主体。 让我们来看看对于两个不同的LDAP服务器用户是如何设置的。第一个用户名为“Test1 Person”，并存在 Windows2008 Active Directory 中。 12345# Test1 Person, Users, ad.sec.cloudera.comdn: CN=Test1 Person,CN=Users,DC=ad,DC=sec,DC=cloudera,DC=comcn: Test1 PersonsAMAccountName: test1userPrincipalName: test1@ad.sec.cloudera.com 第二个用户是我：用户 myoder 的项，存在于 OpenLDAP 服务器中： 12345# myoder, People, cloudera.comdn: uid=myoder,ou=People,dc=cloudera,dc=comcn: Michael Yoderuid: myoderhomeDirectory: /home/myoder 为了简单，上面中的其他的许多项都被删除了。下面来看看这两个账户的相同点和不同点： DN：在注释后面的第一行就是 DN。这是一个 LDAP 账户最主要的标识串。 CN：通用名称。对 AD 来说，他和 DN 是一样的；对于 OpenLDAP 来说他是一个人的名称，他不是 DN 中的 uid。 sAMAccountName：只存在 AD 中的条目，是一个用户名称的过时的形式。尽管其过时了，但是被广泛使用中。 userPrincipalName：只存在 AD 中的条目，通过转换，应该映射到用户的邮箱名称。其通常像这样的格式：sAMAccountName@fully.qualified.domain.com。这是 Active Directory 更现代的名称并且被广泛使用。 这里还有一些额外有趣的关于 AD 的实现细节。通常，LDAP 认证是基于 DN。对于 AD，下面几项会被轮流尝试： 首先，使用 DN userPrincipalName sAMAccountName + “@” + the DNS domain name Netbios domain name + “&quot; + the sAMAccountName 其他的一些机制，请看上面的链接里的内容。 LDAP 和 Impalad考虑所有这些的不同点，幸运的是 Impala 进程提供了几个机制去标识 LDAP 的配置。首先，使用简单的配置： --enable_ldap_auth 必须设置为 true --ldap_uri=ldap://ldapserver.your.company.com 必须定义 仅仅设置这些，传递给 impalad 的用户名（通过 impala shell、jdbc、odbc 等等）直接传递到 LDAP 服务器。这个过程对于 AD 来说没什么问题，如果用户名称是全名称，就像 test1@ad.sec.cloudera.com - 其使用 userPrincipal 或者 sAMAccountName 加上 DNS 域名都会匹配上。 也可以设置 impalad 启动参数以便 这域名（在当前情况下是 ad.sec.cloudera.com ）可以自动添加到 用户名上，这需要通过设置参数 --ldap_domain=ad.sec.cloudera.com 添加到 impalad 的启动参数中。现在，当一个客户端用户访问时，例如 test1 用户，其会将域名名称追加到用户名称之后以便追加后的结果 test1@ad.sec.cloudera.com 传递给 AD 。这种方式对于你的用户来说是很方便的。 目前，对于 AD 来说都运行正常。但是对于其他的 LDAP 服务器，例如 OpenLDAP 呢？OpenLDAP 没有 sAMAccountName 或者 userPrincipalName 中的任何一个，取而代之的是我们不得不直接使用 DN 进行认证。用户将不会去关心他们的 LDAP DN！ 幸运地是，impalade 对于这种场景也提供了一些参数。--ldap_baseDN=X 参数用户将用户名称转换为 LDAP DN，以便这个 DN 结果看上去像 uid=username,X。例如，如果设置 --ldap_baseDN=ou=People,dc=cloudera,dc=com，传递的用户名是 myoder，传到到 LDAP 的查询将是 uid=myoder,ou=People,dc=cloudera,dc=com，这将会和 myoder 的 DN 想匹配。 为了获得最大的灵活性，也可以通过 --ldap_bind_pattern 参数将指定用户名任意映射到一个DN。这个想法是，该参数中必须有一个名称为 #UID 的占位符，并且 #UID 会被用户名替代。例如你可以通过指定 --ldap_bind_pattern=uid=#UID,ou=People,dc=cloudera,dc=com 来定义 --ldap_baseDN。当 myoder 这个用户进来时，其将会替换 #UID，并且我们将得到和上面一直的字符串。这个参数应该在需要对 DN 有更多控制的时候才使用。 LDAP 和 TLS当使用 LDAP 时，传递个 LDAP 服务器的用户名和密码都是明文的。这意味着没有任何的保护，任何人都可以看到传输中的密码。为了阻止这一点，你必须使用 TLS（Transport Layer Security，更为人熟知的是 SSL） 来保护连接。 这里有两种不同的连接需要保护：客户端和 impalad 进程之间以及 impalad 进程和 LDAP 服务器之间。 客户端和 impalad 之间TLS 连接的认证需要使用证书来完成，所以 impalad 进程（作为 TLS server）需要他自己的证书。Impalad 呈现该证书给客户端以证明他的确是 impalad 进程。为了提供该证书，impalad 进程需要设置下面两个参数： 12--ssl_server_certificate=/full/path/to/impalad-cert.pem --ssl_private_key=/full/path/to/impalad-key.pem 现在客户端必须使用 TLS 和 impalad 通信。在 impala-shell 中，你必须设置 --ssl 和 --ca_cert=/full/path/to/ca-certificate.pem 这两个参数。ca_cert 参数必须指定为上面 ssl_server_certificate 参数定义的值。对于 ODBC 连接来说，请参考 the Cloudera ODBC driver for Impala。其提供了一个全面的描述。关于设置证书、认证和 TLS 的必要性。 坦白地说，在 impala 客户端和 impalad 进程之间使用 TLS 是一个很好的想法，不管是否使用 LDAP。然而，你的查询，以及这些查询的结果都是通过明文传输的。 Impalad 和 LDAP 服务器之间这里有两种方法开启和 LDAP 服务器之间的 TLS ： 在 impalad 启动参数中添加 --ldap_tls。该连接会使用 LDAP 的端口，但是在第一次连接之后，会发送一个 STRATETLS 的请求，该请求会使用 TLS 升级该连接为一个安全的连接并且使用相同的端口。 URL 使用 ldaps:// 开头。这将会使用一个不同于 ldap:// 的端口。 最后，到 LDAP 服务器的连接需要他自己的认证；在这种情况下，你知道 impalad 是在和正确的 ldap 服务器通话并且你不会将你的密码发送给一个无赖的人为工具的请求。你需要添加 --ldap_ca_certificate 参数到 impalad 定义 LDAP 服务器的证书存放的位置。 LDAP FlagsCloudera documentation for LDAP and Impala 一文包括这部分的信息，并且建议你阅读 TLS between the Impala client and the Impala daemon 这篇文章。 3. 一起使用如果我们想同事使用 Kerberos 和 LDAP 认证，则需要如下配置： 12345678impalad --enable_ldap_auth \ --ldap_uri=ldap://ldapserver.your.company.com \ --ldap_tls \ --ldap_ca_certificate=/full/path/to/certs/ldap-ca-cert.pem \ --ssl_server_certificate=/full/path/to/certs/impala-cert.pem \ --ssl_private_key=/full/path/to/certs/impala-key.pem \ --principal=impala/_HOST@EXAMPLE.COM \ --keytab_file=/full/path/to/keytab 当使用 Kerberos 认证时，使用 impala shell 连接： 123impala-shell.sh --ssl \ --ca_cert=/full/path/to/cert/impala-ca-cert.pem \ -k 当使用 LDAP 认证时： 123impala-shell.sh --ssl \ --ca_cert=/full/path/to/cert/impala-ca-cert.pem \ -l -u myoder@cloudera.com 原文作者为 Michael Yoder，Cloudera 软件工程师。]]></content>
      <categories>
        <category>impala</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>impala</tag>
        <tag>kerberos</tag>
        <tag>ldap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Presto介绍]]></title>
    <url>%2F2015%2F01%2F23%2Fpresto-overview%2F</url>
    <content type="text"><![CDATA[1. 简介Presto 是一个运行在集群之上的分布式系统。一个完全的安装报考一个 coordinator 进程和多个 workers 进程。查询通过一个客户端例如 Presto CLI 提交到 coordinator 进程。这个 coordinator 进程解析、分析并且生成查询的执行计划，然后将执行过程分发到 workers 进程。 下面是一个架构图（图来自 http://www.dw4e.com/?p=141，此图将官网的架构图稍微修改了一下，增加了 Discovery 的服务，这样可能看起来会更清楚一些）： Presto 查询引擎是一个 Master-Slave 的架构，由一个 Coordinato r节点，一个 Discovery Server 节点，多个 Worker 节点组成，Discovery Server 通常内嵌于 Coordinator 节点中。Coordinator 负责解析 SQL 语句，生成执行计划，分发执行任务给 Worker 节点执行。Worker 节点负责实际执行查询任务。Worker 节点启动后向 Discovery Server 服务注册，Coordinator 从 Discovery Server 获得可以正常工作的 Worker 节点。如果配置了 Hive Connector，需要配置一个 Hive MetaStore 服务为 Presto 提供 Hive 元信息，Worker 节点与 HDFS 交互读取数据。 2. 要求Presto 有以下几个基本要求： Linux 或者 Mac OS X 系统 Java 8，64位 Python 2.4++ 2.1 连接器Presto 支持可插拔的连接器用于提供数据查询。不同连接器的要求不一样。 HADOOP/HIVEPresto 支持读以下版本的 hive 数据： Apache Hadoop 1.x，使用 hive-hadoop1 连接器 Apache Hadoop 2.x，使用 hive-hadoop2 连接器 Cloudera CDH 4，使用 hive-cdh4 连接器 Cloudera CDH 5，使用 hive-cdh5 连接器 支持以下格式：Text、SequenceFile、RCFile、ORC。 另外，还需要一个远程的 Hive metastore 服务。本地的或者嵌入式模式是不支持的。Presto 不使用 MapReduce 并且只需要 HDFS。 CASSANDRACassandra 2.x 是需要的。这个连接器是完全独立于 Hive 连接器的并且仅仅需要一个安装好的 Cassandra 集群。 TPC-HTPC-H 连接器动态地生成数据用于实验和测试 Presto。这个连接器没有额外的要求。 当然，Presto 还支持一些其他的连接器，包括： JMX Kafka MySQL PostgreSQL 3. 使用场景3.1 What Presto Is NotPresto 支持 SQL 并提供了一个标准数据库的语法特性，但其不是一个通常意义上的关系数据库，他不是关系数据库，如 MySQL、PostgreSQL 或者 Oracle 的替代品。Presto 不是设计用来解决在线事物处理（OLTP）。 3.2 What Presto IsPresto 是一个工具，被用来通过分布式查询来有效的查询大量的数据。Presto 是一个可选的工具，可以用来查询 HDFS，通过使用 MapReduce 的作业的流水线，例如 hive，pig，但是又不限于查询 HDFS 数据，它还能查询其他的不同数据源的数据，包括关系数据库以及其他的数据源，比如 cassandra。 Presto 被设计为处理数据仓库和分析：分析数据，聚合大量的数据并产生报表，这些场景通常被定义为 OLAP。 3.3 Who uses Presto?国外： Facebook，Presto 的开发者 国内： 腾讯，待考证 美团，Presto实现原理和美团的使用实践 窝窝团，#数据技术选型#即席查询Shib+Presto，集群任务调度HUE+Oozie 4. 资料以下是一些资料，希望对你了解 Presto 有所帮助： Presto官方文档：http://prestodb.io/ Shib：Shib is a web-client written in Node.js designed to query Presto and Hive. Facebook Presto团队介绍Presto的文章： https://www.facebook.com/notes/facebook-engineering/presto-interacting-with-petabytes-of-data-at-facebook/10151786197628920 SlideShare两个分享Presto 的PPT： http://www.slideshare.net/zhusx/presto-overview?from_search=1 和 http://www.slideshare.net/frsyuki/hadoop-source-code-reading-15-in-japan-presto Presto的单节点和多节点配置 Impala Presto wiki 主要介绍了 Presto 的架构、原理和工作流程，以及和 impala 的对比。 记录Presto数据查询引擎的配置过程]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>presto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven的一些技巧]]></title>
    <url>%2F2015%2F01%2F20%2Fmaven-skills%2F</url>
    <content type="text"><![CDATA[本文主要收集一些 Maven 的使用技巧，包括 Maven 常见命令、创建多模块项目、上传本地 jar 到插件以及常用的插件等等，本篇文章会保持不停的更新。 创建 maven 项目1$ mvn archetype:generate -DgroupId=com.javachen -DartifactId=spark-examples -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false Maven安装本地jar到本地仓库，举例： 1$ mvn install:install-file -DgroupId=com.gemstone.gemfire -DartifactId=gfsh -Dversion=6.6 -Dpackaging=jar -Dfile=/backup/gfsh-6.6.jar 解决m2e插件maven-dependency-plugin问题：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;build&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.eclipse.m2e&lt;/groupId&gt; &lt;artifactId&gt;lifecycle-mapping&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;configuration&gt; &lt;lifecycleMappingMetadata&gt; &lt;pluginExecutions&gt; &lt;pluginExecution&gt; &lt;pluginExecutionFilter&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;versionRange&gt;[2.0,)&lt;/versionRange&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;/pluginExecutionFilter&gt; &lt;action&gt; &lt;ignore /&gt; &lt;/action&gt; &lt;/pluginExecution&gt; &lt;/pluginExecutions&gt; &lt;/lifecycleMappingMetadata&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-dependencies&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/lib&lt;/outputDirectory&gt; &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt; &lt;stripVersion&gt;true&lt;/stripVersion&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 使用Maven构建多模块项目创建system-parent目录，然后在该目录下执行： mvn archetype:generate -DgroupId=com.javachen -DartifactId=system-parent -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false将src文件夹删除，然后修改pom.xml文件，将&lt;packaging&gt;jar&lt;/packaging&gt;修改为&lt;packaging&gt;pom&lt;/packaging&gt;，pom表示它是一个被继承的模块，修改后的内容如下：]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django中的ORM]]></title>
    <url>%2F2015%2F01%2F15%2Fdjango-orm%2F</url>
    <content type="text"><![CDATA[通过《如何创建一个Django网站》大概清楚了如何创建一个简单的 Django 网站，并了解了Django 中模板和模型使用方法。本篇文章主要在此基础上，了解 Django 中 ORM 相关的用法。 一个 blog 的应用中 mysite/blog/models.py 有以下实体： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970from django.db import modelsclass Blog(models.Model): name = models.CharField(max_length=100) tagline = models.TextField() def __str__(self): # __unicode__ on Python 2 return self.nameclass Author(models.Model): name = models.CharField(max_length=50) email = models.EmailField() def __str__(self): # __unicode__ on Python 2 return self.nameclass Entry(models.Model): blog = models.ForeignKey(Blog) headline = models.CharField(max_length=255) body_text = models.TextField() pub_date = models.DateField() mod_date = models.DateField() authors = models.ManyToManyField(Author) n_comments = models.IntegerField() n_pingbacks = models.IntegerField() rating = models.IntegerField() def __str__(self): # __unicode__ on Python 2 return self.headline~~~ ## 创建对象~~~python&gt;&gt;&gt; from blog.models import Blog&gt;&gt;&gt; b = Blog(name='Beatles Blog', tagline='All the latest Beatles news.')&gt;&gt;&gt; b.save()~~~ 你也可以修改实体，然后保存：~~~python&gt;&gt;&gt; b5.name = 'New name'&gt;&gt;&gt; b5.save() ~~~ ## 保存外键信息下面例子更新 entry 实例的 blog 属性：~~~python&gt;&gt;&gt; from blog.models import Entry&gt;&gt;&gt; entry = Entry.objects.get(pk=1)&gt;&gt;&gt; cheese_blog = Blog.objects.get(name="Cheddar Talk")&gt;&gt;&gt; entry.blog = cheese_blog&gt;&gt;&gt; entry.save()~~~ 下面是更新一个 ManyToManyField 字段：~~~python&gt;&gt;&gt; from blog.models import Author&gt;&gt;&gt; joe = Author.objects.create(name="Joe")&gt;&gt;&gt; entry.authors.add(joe)&gt;&gt;&gt; &gt;&gt;&gt; john = Author.objects.create(name="John")&gt;&gt;&gt; paul = Author.objects.create(name="Paul")&gt;&gt;&gt; george = Author.objects.create(name="George")&gt;&gt;&gt; ringo = Author.objects.create(name="Ringo")&gt;&gt;&gt; entry.authors.add(john, paul, george, ringo) 查询对象Django 中查询数据库需要 Manager 和 QuerySet 两个对象。从数据库里检索对象，可以通过模型的 Manage 来建立 QuerySet,一个 QuerySet 表现为一个数据库中对象的结合，他可以有0个一个或多个过滤条件，在 SQL里 QuerySet 相当于 select 语句用 where 或 limit 过滤。你通过模型的 Manage 来获取 QuerySet。 ManagerManager 对象附在模型类里，如果没有特指定，每个模型类都会有一个 objects 属性，它构成了这个模型在数据库所有基本查询。 Manager 的几个常用方法： all：返回一个包含模式里所有数据库记录的 QuerySet filter：返回一个包含符合指定条件的模型记录的 QuerySet exclude：和 filter 相反，查找不符合条件的那些记录 get：获取单个符合条件的记录（没有找到或者又超过一个结果都会抛出异常） order_by：改变 QuerySet 默认的排序 你可以通过模型的 Manager 对象获取 QuerySet 对象： 1234567&gt;&gt;&gt; Blog.objects&lt;django.db.models.manager.Manager object at ...&gt;&gt;&gt;&gt; b = Blog(name='Foo', tagline='Bar')&gt;&gt;&gt; b.objectsTraceback: ...AttributeError: "Manager isn't accessible via Blog instances." 获取所有的 blog 内容: 123456&gt;&gt;&gt; all_entries = Entry.objects.all()#正向排序Entry.objects.all().order_by("headline")#反向排序Entry.objects.all().order_by("-headline") 获取 headline 为 Python 开头的 blog : 1234Entry.objects.filter(headline__startswith="Python")#支持链式操作Entry.objects.filter(headline__startswith="Python").exclude(pub_date__gte=datetime.now()).filter(pub_date__gte=datetime(2014, 1, 1)) QuerySet 类QuerySet 接受动态的关键字参数，然后转换成合适的 SQL 语句在数据库上执行。 QuerySet 的几个常用方法： distinct values values_list select_related filter：返回一个包含符合指定条件的模型记录的 QuerySet extra：增加结果集以外的字段 延时查询每次你完成一个 QuerySet，你获得一个全新的结果集，不包括前面的。每次完成的结果集是可以贮存，使用或复用： 123&gt;&gt;&gt; q1 = Entry.objects.filter(headline__startswith="What")&gt;&gt;&gt; q2 = q1.exclude(pub_date__gte=datetime.date.today())&gt;&gt;&gt; q3 = q1.filter(pub_date__gte=datetime.date.today()) 三个 QuerySets 是分开的，第一个是 headline 以 “What” 单词开头的结果集，第二个是第一个的子集，即 pub_date 不大于现在的，第三个是第一个的子集 ，pub_date 大于现在的。 QuerySets 是延迟的，创建 QuerySets 不会触及到数据库操作，你可以多个过滤合并到一起，直到求值的时候 django才会开始查询。如： 1234&gt;&gt;&gt; q = Entry.objects.filter(headline__startswith="What")&gt;&gt;&gt; q = q.filter(pub_date__lte=datetime.date.today())&gt;&gt;&gt; q = q.exclude(body_text__icontains="food")&gt;&gt;&gt; print(q) 虽然看起来执行了三个过滤条件，实际上最后执行 print q 的时候，django 才开始查询执行 SQL 到数据库。 可以使用 python 的数组限制语法限定 QuerySet，如： 12345&gt;&gt;&gt; Entry.objects.all()[:5]&gt;&gt;&gt; Entry.objects.all()[5:10]&gt;&gt;&gt; Entry.objects.all().order_by("headline")[:4]&gt;&gt;&gt; Entry.objects.all().order_by("headline")[4:8] 一般的，限制 QuerySet 返回新的 QuerySet，不会立即求值查询，除非你使用了 “step” 参数 123&gt;&gt;&gt; Entry.objects.all()[:10:2]&gt;&gt;&gt; Entry.objects.order_by('headline')[0]&gt;&gt;&gt; Entry.objects.order_by('headline')[0:1].get() 字段过滤字段查找是指定 SQL 语句的 WHERE 条件从句，通过 QuerySet 的方法 filter(), exclude() 和 get() 指定查询关键字。 格式为：field__lookuptype=value。 lookuptype 有以下几种： gt ： 大于 gte : 大于等于 in : 包含 lt : 小于 lte : 小于等于 exact： iexact： contains：包含查询，区分大小写 icontains：不区分大小写 startswith：匹配开头 endswith：匹配结尾 istartswith：匹配开头，不区分大小写 iendswith：匹配结尾，不区分大小写 1&gt;&gt;&gt; Entry.objects.filter(pub_date__lte='2006-01-01') 等价于: 1SELECT * FROM blog_entry WHERE pub_date &lt;= '2006-01-01'; 当实体中存在 ForeignKey 时，其外键字段名称为模型名称加上 ‘_id’： 1&gt;&gt;&gt; Entry.objects.filter(blog_id=4) 下面是一些举例： a、exact 1&gt;&gt;&gt; Entry.objects.get(headline__exact="Man bites dog") 相当于： 1SELECT ... WHERE headline = 'Man bites dog'; 如果查询没有提供双下划线，那么会默认 __exact: 12345Entry.objects.get(id__exact=14) # Explicit formEntry.objects.get(id=14) # __exact is implied#主键查询Entry.objects.get(pk=14) # pk implies id__exact b、iexact——忽略大小写 1&gt;&gt;&gt; Blog.objects.get(name__iexact="beatles blog") 将要匹配 blog 名称为 “Beatles Blog”, “beatles blog”, 甚至是 “BeAtlES blOG”。 c、contains——包含查询，区分大小写 1Entry.objects.get(headline__contains='Lennon') 转化为 SQL: 1SELECT ... WHERE headline LIKE '%Lennon%'; 如果有百分号，则会进行转义： 1Entry.objects.filter(headline__contains='%') 转义为： 1SELECT ... WHERE headline LIKE '%\%%'; d、in 查询 12# Get blogs with id 1, 4 and 7Entry.objects.filter(pk__in=[1,4,7]) 跨关系查询跨关系查询是针对有主外键依赖关系的对象而言的，例如上面的 Author 和 Entry 对象是多对多的映射，可以通过 Entry 对象来过滤 Author的 name： 获取所有 blog 名称为 Beatles Blog 的 Entry 列表： 1&gt;&gt;&gt; Entry.objects.filter(blog__name='Beatles Blog') 也可以反向查询： 1&gt;&gt;&gt; Blog.objects.filter(entry__headline__contains='Lennon') 如果跨越多层关系查询，中间模型没有值，django会作为空对待不会发生异常。 1234Blog.objects.filter(entry__authors__name='Lennon')Blog.objects.filter(entry__authors__name__isnull=True)Blog.objects.filter(entry__authors__isnull=False, entry__authors__name__isnull=True) 也支持多条件跨关系查询： 12Blog.objects.filter(entry__headline__contains='Lennon', entry__pub_date__year=2008) 或者： 12Blog.objects.filter(entry__headline__contains='Lennon').filter( entry__pub_date__year=2008) 使用 Extra 调整 SQL用extra可以修复QuerySet生成的原始SQL的各个部分，它接受四个关键字参数。如下： select：修改select语句 where：提供额外的where子句 tables：提供额外的表 params：安全的替换动态参数 增加结果集以外的字段： 1queryset.extra(select=&#123;'成年':'age&gt;18'&#125;) 提供额外的 where 条件： 1queryset.extra(where=["first like '%小明%' "]) 提供额外的表： 1queryset.extra(tables=['myapp_person']) 安全的替换动态参数： 12##'%s' is not replaced with normal string matches = Author.objects.all().extra(where=["first = '%s' "], params= [unknown-input ( ) ]) F 关键字参数前面给的例子里，我们建立了过滤，比照模型字段值和一个固定的值，但是如果我们想比较同一个模型里的一个字段和另一个字段的值，django 提供 F()——专门取对象中某列值的操作。 12&gt;&gt;&gt; from django.db.models import F&gt;&gt;&gt; Entry.objects.filter(n_comments__gt=F('n_pingbacks')) 当然，还支持加减乘除和模计算： 1234&gt;&gt;&gt; Entry.objects.filter(n_comments__gt=F('n_pingbacks') * 2)&gt;&gt;&gt; Entry.objects.filter(rating__lt=F('n_comments') + F('n_pingbacks'))&gt;&gt;&gt; &gt;&gt;&gt; Entry.objects.filter(authors__name=F('blog__name')) 对于日期类型字段，可以使用 timedelta 方法： 12&gt;&gt;&gt; from datetime import timedelta&gt;&gt;&gt; Entry.objects.filter(mod_date__gt=F('pub_date') + timedelta(days=3)) 还支持位操作 .bitand() 和 .bitor()： 1&gt;&gt;&gt; F('somefield').bitand(16) 主键查找Django 支持使用 pk 代替主键： 123&gt;&gt;&gt; Blog.objects.get(id__exact=14) # Explicit form&gt;&gt;&gt; Blog.objects.get(id=14) # __exact is implied&gt;&gt;&gt; Blog.objects.get(pk=14) # pk implies id__exact pk 还可以用于其他的查找类型： 123456789# Get blogs entries with id 1, 4 and 7&gt;&gt;&gt; Blog.objects.filter(pk__in=[1,4,7])# Get all blog entries with id &gt; 14&gt;&gt;&gt; Blog.objects.filter(pk__gt=14)&gt;&gt;&gt; Entry.objects.filter(blog__id__exact=3) # Explicit form&gt;&gt;&gt; Entry.objects.filter(blog__id=3) # __exact is implied&gt;&gt;&gt; Entry.objects.filter(blog__pk=3) # __pk implies __id__exact Q 关键字参数QuerySet 可以通过一个叫 Q 的关键字参数封装类进一步参数化，允许使用更复杂的逻辑查询。其结果 Q对 象可以作为 filter 或 exclude 方法的关键字参数。 例子： 12from django.db.models import QQ(question__startswith='What') 支持 &amp; 和 | 操作符： 1Q(question__startswith='Who') | Q(question__startswith='What') 上面的查询翻译成 sql 语句： 1WHERE question LIKE 'Who%' OR question LIKE 'What%' 取反操作： 1Q(question__startswith='Who') | ~Q(pub_date__year=2005) 也可以用在 filter()、exclude()、get() 中： 1234Poll.objects.get( Q(question__startswith='Who'), Q(pub_date=date(2005, 5, 2)) | Q(pub_date=date(2005, 5, 6))) 翻译成 sql 语句为： 12SELECT * from polls WHERE question LIKE 'Who%' AND (pub_date = '2005-05-02' OR pub_date = '2005-05-06') 删除对象12345&gt;&gt;&gt;entry = Entry.objects.get(pk=1)&gt;&gt;&gt;entry.delete()&gt;&gt;&gt;Blog.objects.all().delete()&gt;&gt;&gt;Entry.objects.filter(pub_date__year=2005).delete() 关系对象当对象之间存在映射关系或者关联时，该如何查询呢？ 当你在模型里定义一个关系时，模型实例会有一个方便的 API 来访问关系对象。以下分几种映射关系分别描述。 One-to-many关系如果一个对象有ForeignKey，这个模型实例访问关系对象通过简单的属性: 12&gt;&gt;&gt; e = Entry.objects.get(id=2)&gt;&gt;&gt; e.blog # Returns the related Blog object. 你可以凭借外键属性获取和赋值，修改外键值知道执行 save() 方法才会保存到数据库: 123&gt;&gt;&gt; e = Entry.objects.get(id=2)&gt;&gt;&gt; e.blog = some_blog&gt;&gt;&gt; e.save() 如果关联的对象可以为空，则可以将关联对象职位 None，删除关联： 123&gt;&gt;&gt; e = Entry.objects.get(id=2)&gt;&gt;&gt; e.blog = None&gt;&gt;&gt; e.save() # "UPDATE blog_entry SET blog_id = NULL ...;" 子查询： 123&gt;&gt;&gt; e = Entry.objects.get(id=2)&gt;&gt;&gt; print(e.blog) # Hits the database to retrieve the associated Blog.&gt;&gt;&gt; print(e.blog) # Doesn't hit the database; uses cached version. 也可以使用 select_related() 方法，该方法会提前将关联对象查询出来： 123&gt;&gt;&gt; e = Entry.objects.select_related().get(id=2)&gt;&gt;&gt; print(e.blog) # Doesn't hit the database; uses cached version.&gt;&gt;&gt; print(e.blog) # Doesn't hit the database; uses cached version. 你也可以通过 模型_set 来访问关系对象的另一边，在 Blog 对象并没有维护 Entry 列表，但是你可以通过下面方式从 Blog 对象访问 Entry 列表： 123456&gt;&gt;&gt; b = Blog.objects.get(id=1)&gt;&gt;&gt; b.entry_set.all() # Returns all Entry objects related to Blog.# b.entry_set is a Manager that returns QuerySets.&gt;&gt;&gt; b.entry_set.filter(headline__contains='Lennon')&gt;&gt;&gt; b.entry_set.count() 模型_set 可以通过 related_name 属性来修改，例如将 Entry 模型中的定义修改为： 1blog = ForeignKey(Blog, related_name='entries') 上面的查询就会变成： 123456&gt;&gt;&gt; b = Blog.objects.get(id=1)&gt;&gt;&gt; b.entries.all() # Returns all Entry objects related to Blog.# b.entries is a Manager that returns QuerySets.&gt;&gt;&gt; b.entries.filter(headline__contains='Lennon')&gt;&gt;&gt; b.entries.count() Many-to-many关系1234567e = Entry.objects.get(id=3)e.authors.all() # Returns all Author objects for this Entry.e.authors.count()e.authors.filter(name__contains='John')a = Author.objects.get(id=5)a.entry_set.all() # Returns all Entry objects for this Author. One-to-one关系123456class EntryDetail(models.Model): entry = models.OneToOneField(Entry) details = models.TextField()ed = EntryDetail.objects.get(id=2)ed.entry # Returns the related Entry object. 当反向查询时： 12e = Entry.objects.get(id=2)e.entrydetail # returns the related EntryDetail object 这时候如果没有关联对象，则会抛出 DoesNotExist 异常。 并且还可以修改： 1e.entrydetail = ed 参考资料 Making queries Eclipse的django开发学习笔记（2）–模型（M） Django：模型的使用 django orm总结]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django中的模型]]></title>
    <url>%2F2015%2F01%2F14%2Fdjango-model%2F</url>
    <content type="text"><![CDATA[Django 中的模型主要用于定义数据的来源信息，其包括一些必要的字段和一些对存储的数据的操作。通常，一个模型对应着数据库中的一个表。 简单的概念： Django 中每一个 Model 都继承自 django.db.models.Model。 在 Model 当中每一个属性 attribute 都代表一个数据库字段。 通过 Django Model API 可以执行数据库的增删改查, 而不需要写一些数据库的查询语句。 1. 模型1.1 一个示例下面在 myapp 应用种定义了一个 Person 模型，包括两个字段：first_name 和 last_name。 12345from django.db import modelsclass Person(models.Model): first_name = models.CharField(max_length=30) last_name = models.CharField(max_length=30) first_name 和 last_name 是模型的字段，每一个字段对应着一个类的属性，并且每一个属性对应数据库表中的一个列。 上面的 Person 模型对应数据库中的表如下： 12345CREATE TABLE myapp_person ( "id" serial NOT NULL PRIMARY KEY, "first_name" varchar(30) NOT NULL, "last_name" varchar(30) NOT NULL); 说明： 表名 myapp_person 是由模型的元数据自动生成的，格式为 应用_模型，你可以设置元数据覆盖该值。 id 字段在模型中是自动添加的，同样该字段名称也可以通过元数据覆盖。 上面的 sql 语法是 PostgreSQL 中的语法，你可以通过设置数据库类型，生成不同数据库对应的 sql。 1.2 使用模型使用模型之前，你需要先创建应用，然后将其加入 INSTALLED_APPS，然后编写该应用中的 models.py 文件，最后运行 manage.py makemigrations 和 manage.py migrate 在数据库中创建该实体对应的表。 12345INSTALLED_APPS = ( #... 'myapp', #...) manage.py 参数列表 syncdb：创建所有应用所需的数据表 sql：显示CREATETABLE调用 sqlall 如同上面的sql一样，从sql文件中初始化数据载入语句 sqlindexs：显示对主键创建索引的调用 sqlclear：显示DROP TABLE的调用 sqlcustom：显示指定.sql文件里的自定义SQL语句 loaddata：载入初始数据 dumpdata：把原有的数据库里的数据输出伟JSON，XML等格式 sql、sqlall、sql、sqlindexs、sqlclear、sqlcustom 不更新数据库，只打印SQL语句以作检验之用。 1.3 模型属性每个模型有一个默认的属性 Manager，他是模型访问数据库的接口。如果没有自定义的 Manager，则其默认名称为 objects。 1.4 模型字段字段名称不能和 clean、save 或者 delete 冲突。一个示例： 123456789101112from django.db import modelsclass Musician(models.Model): first_name = models.CharField(max_length=50) last_name = models.CharField(max_length=50) instrument = models.CharField(max_length=100)class Album(models.Model): artist = models.ForeignKey(Musician) name = models.CharField(max_length=100) release_date = models.DateField() num_stars = models.IntegerField() 模型中的每一个字段都必须为 Field 类的一个实例，Django 使用该类型来决定数据库中对应的列的类型，并且每一个字段都有一些可选的参数。 模型的字段可能的类型及参数如下： 字段名 参数 意义 AutoField 一个能够根据可用ID自增的 IntegerField BooleanField 一个真/假字段 CharField (max_length) 适用于中小长度的字符串。对于长段的文字，请使用 TextField CommaSeparatedIntegerField (max_length) 一个用逗号分隔开的整数字段 DateField ([auto_now], [auto_now_add]) 日期字段 DateTimeField 时间日期字段,接受跟 DateField 一样的额外选项 EmailField 一个能检查值是否是有效的电子邮件地址的 CharField FileField (upload_to) 一个文件上传字段 FilePathField (path,[match],[recursive]) 一个拥有若干可选项的字段，选项被限定为文件系统中某个目录下的文件名 FloatField (max_digits,decimal_places) 一个浮点数，对应 Python 中的 float 实例 ImageField (upload_to, [height_field] ,[width_field]) 像 FileField 一样，只不过要验证上传的对象是一个有效的图片。 IntegerField 一个整数。 IPAddressField 一个IP地址，以字符串格式表示（例如： “24.124.1.30” ）。 NullBooleanField 就像一个 BooleanField ，但它支持 None /Null 。 PhoneNumberField 它是一个 CharField ，并且会检查值是否是一个合法的美式电话格式 PositiveIntegerField 和 IntegerField 类似，但必须是正值。 PositiveSmallIntegerField 与 PositiveIntegerField 类似，但只允许小于一定值的值,最大值取决于数据库 SlugField 嵌条 就是一段内容的简短标签，这段内容只能包含字母、数字、下划线或连字符。通常用于 URL 中 SmallIntegerField 和 IntegerField 类似，但是只允许在一个数据库相关的范围内的数值（通常是-32,768到 TextField 一个不限长度的文字字段 TimeField 时分秒的时间显示。它接受的可指定参数与 DateField 和 DateTimeField 相同。 URLField 用来存储 URL 的字段。 USStateField 美国州名称缩写，两个字母。 XMLField (schema_path) 它就是一个 TextField ，只不过要检查值是匹配指定schema的合法XML。 通用字段参数列表如下（所有的字段类型都可以使用下面的参数，所有的都是可选的。）： 参数名 意义 null 如果设置为 True 的话，Django将在数据库中存储空值为 NULL 。默认False 。 blank 如果是 True ，该字段允许留空，默认为 False 。 choices 一个包含双元素元组的可迭代的对象，用于给字段提供选项。 db_column 当前字段在数据库中对应的列的名字。 db_index 如果为 True ，Django会在创建表时对这一列创建数据库索引。 default 字段的默认值 editable 如果为 False ，这个字段在管理界面或表单里将不能编辑。默认为 True 。 help_text 在管理界面表单对象里显示在字段下面的额外帮助文本。 primary_key 如果为 True ，这个字段就会成为模型的主键。 radio_admin 如果 radio_admin 设置为 True 的话，Django 就会使用单选按钮界面。 unique 如果是 True ，这个字段的值在整个表中必须是唯一的。 unique_for_date 把它的值设成一个 DataField 或者 DateTimeField 的字段的名称，可以确保字段在这个日期内不会出现重复值。 unique_for_month 和 unique_for_date 类似，只是要求字段在指定字段的月份内唯一。 unique_for_year 和 unique_for_date 及 unique_for_month 类似，只是时间范围变成了一年。 verbose_name 除 ForeignKey 、 ManyToManyField 和 OneToOneField 之外的字段都接受一个详细名称作为第一个位置参数。 举例1，一个 choices 类型的例子如下： 12345678910from django.db import modelsclass Person(models.Model): SHIRT_SIZES = ( ('S', 'Small'), ('M', 'Medium'), ('L', 'Large'), ) name = models.CharField(max_length=60) shirt_size = models.CharField(max_length=1, choices=SHIRT_SIZES) 你可以通过 get_FOO_display 来访问 choices 字段显示的名称： 123456&gt;&gt;&gt; p = Person(name="Fred Flintstone", shirt_size="L")&gt;&gt;&gt; p.save()&gt;&gt;&gt; p.shirt_sizeu'L'&gt;&gt;&gt; p.get_shirt_size_display()u'Large' 举例2，自定义主键： 1234from django.db import modelsclass Fruit(models.Model): name = models.CharField(max_length=100, primary_key=True) 使用 primary_key 可以指定某一个字段为主键。 12345&gt;&gt;&gt; fruit = Fruit.objects.create(name='Apple')&gt;&gt;&gt; fruit.name = 'Pear'&gt;&gt;&gt; fruit.save()&gt;&gt;&gt; Fruit.objects.values_list('name', flat=True)['Apple', 'Pear'] 说明：values_list 函数 1.5 Meta 元数据类模型里定义的变量 fields 和关系 relationships 提供了数据库的布局以及稍后查询模型时要用的变量名–经常你还需要添加unicode 和 get_absolute_url 方法或是重写 内置的 save 和 delete方法。 然而，模型的定义还有第三个方面–告知Django关于这个模型的各种元数据信息的嵌套类 Meta，Meta 类处理的是模型的各种元数据的使用和显示： 比如在一个对象对多个对象是，它的名字应该怎么显示 查询数据表示默认的排序顺序是什么 数据表的名字是什么 多变量唯一性 （这种限制没有办法在每个单独的变量声明上定义） Meta类有以下属性： abstract：定义当前的模型类是不是一个抽象类。 app_label：这个选项只在一种情况下使用，就是你的模型类不在默认的应用程序包下的 models.py 文件中，这时候你需要指定你这个模型类是那个应用程序的 db_table：指定自定义数据库表名 db_tablespace：指定这个模型对应的数据库表放在哪个数据库表空间 get_latest_by：由于 Django 的管理方法中有个 lastest()方法，就是得到最近一行记录。如果你的数据模型中有 DateField 或 DateTimeField 类型的字段，你可以通过这个选项来指定 lastest() 是按照哪个字段进行选取的。 managed：由于 Django 会自动根据模型类生成映射的数据库表，如果你不希望 Django 这么做，可以把 managed 的值设置为 False。 order_with_respect_to：这个选项一般用于多对多的关系中，它指向一个关联对象。就是说关联对象找到这个对象后它是经过排序的。指定这个属性后你会得到一个 get_XXX_order() 和 set_XXX_order() 的方法,通过它们你可以设置或者返回排序的对象。 ordering：定义排序字段 permissions：为了在 Django Admin 管理模块下使用的，如果你设置了这个属性可以让指定的方法权限描述更清晰可读 proxy：为了实现代理模型使用的 unique_together：定义多个字段保证数据的唯一性 verbose_name：给你的模型类起一个更可读的名字 verbose_name_plural：这个选项是指定模型的复数形式是什么 1.6 模型方法Manager 提供的是表级别的方法，模型中还可以定义字段级别的方法。例如： 123456789101112131415161718192021from django.db import modelsclass Person(models.Model): first_name = models.CharField(max_length=50) last_name = models.CharField(max_length=50) birth_date = models.DateField() def baby_boomer_status(self): "Returns the person's baby-boomer status." import datetime if self.birth_date &lt; datetime.date(1945, 8, 1): return "Pre-boomer" elif self.birth_date &lt; datetime.date(1965, 1, 1): return "Baby boomer" else: return "Post-boomer" def _get_full_name(self): "Returns the person's full name." return '%s %s' % (self.first_name, self.last_name) full_name = property(_get_full_name) 最后一个方法是 property 的一个示例。 每一个模型有一些 Django 自动添加的方法，你也可以在模型的定义中覆盖这些方法： __str__() (Python 3) __unicode__() (Python 2) get_absolute_url() 你也可以覆盖模型中和数据库相关的方法，通常是 save() 和 delete() 两个方法。例如： 12345678910from django.db import modelsclass Blog(models.Model): name = models.CharField(max_length=100) tagline = models.TextField() def save(self, *args, **kwargs): do_something() super(Blog, self).save(*args, **kwargs) # Call the "real" save() method. do_something_else() 1.7 执行自定义的 sql这部分内容请参考：Django中SQL查询 2. 模型之间的关系Django 提供了三种模型之间的关联关系： many-to-one、many-to-many 和 one-to-one。 2.1 多对一定义多对一的关系，需要使用 django.db.models.ForeignKey 来引用被关联的模型。 例如，一本书有多个作者： 123456class Author(models.Model): name = models.CharField(max_length=100)class Book(models.Model): title = models.CharField(max_length=100) author = models.ForeignKey(Author) Django 的外键表现很直观，其主要参数就是它要引用的模型类；但是注意要把被引用的模型放在前面。不过，如果不想留意顺序，也可以用字符串代替。 12345678class Book(models.Model): title = models.CharField(max_length=100) author = models.ForeignKey("Author") #if Author class is defined in another file myapp/models.py #author = models.ForeignKey("myapp.Author")class Author(models.Model): name = models.CharField(max_length=100) 如果要引用自己为外键，可以设置 models.ForeignKey(&quot;self&quot;) ，这在定义层次结构等类似场景很常用，比如 Employee 类可以具有类似 supervisor 或是 hired_by 这样的属性。 外键 ForeignKey 只定义了关系的一端，但是另一端可以根据关系追溯回来，因为这是一种多对一的关系，多个子对象可以引用同一个父对象，而父对象可以访问到一组子对象。看下面的例子： 12345678#取一本书“Moby Dick”book = Book.objects.get(title="Moby Dick")#取作者名字author = Book.author#获取这个作者所有的书books = author.book_set.all() 这里从 Author 到 Book 的反向关系式通过 Author.book_set 属性来表示的（这是一个manager对象），是由 ORM 自动添加的，可以通过在 ForeignKey 里指定 related_name 参数来改变它的名字。比如： 12345class Book(models.Model): author = models.ForeignKey("Author", related_name = "books")#获取这个作者所有的书books = author.books.all() 对简单的对象层次来说， related_name 不是必需的，但是更复杂的关系里，比如当有多个 ForeignKey 的时候就一定要指定了。 2.2 多对多上面的例子假设的是一本书只有一个作者，一个作者有多本书，所以是多对一的关系；但是如果一本书也有多个作者呢？这就是多对多的关系；由于SQL没有定义这种关系，必须通过外键用它能理解的方式实现多对多 这里 Django 提供了第二种关系对象映射变量 ManyToManyField，语法上来讲， 这和 ForeignKey 是一模一样的，你在关系的一端定义，把要关联的类传递进来，ORM 会自动为另一端生成使用这个关系必要的方法和属性。 不过由于 ManyToManyField 的特性，在哪一端定义它通常都没有关系，因为这个关系是对称的。 123456789101112131415class Author(models.Model): name = models.CharField(max_length=100)class Book(models.Model): title = models.CharField(max_length=100) authors = models.ManyToManyField(Author)#获取一本书book = Book.objects.get(title="Python Web Dev Django")#获取该书所有的作者authors = Book.author_set.all()#获取第三个作者出版过的所有的书books = authors[2].book_set.all() ManyToManyField 的秘密在于它在背后创建了一张新的表来满足这类关系的查询的需要，而这张表用的则是 SQL 外键，其中每一行都代表了两个对象的一个关系，同时包含了两端的外键 这张查询表在 Django ORM 中一般是隐藏的，不可以单独查询，只能通过关系的某一端查询；不过可以在 ManyToManyField 上指定一个特殊的选项 through 来指向一个显式的中间模型类，更方便你的手动管理关系的两端 1234567891011class Author(models.Model): name = models.CharField(max_length=100)class Book(models.Model): title = models.CharField(max_length=100) authors = models.ManyToManyField(Author, through = "Authoring")class Authoring(models.Model): collaboration_type = models.CharField(max_length=100) book = model.ForeignKey(Book) author = model.ForeignKey(Author) 查询 Author 和 Book 的方法和之前完全一样，另外还能构造对 authoring 的查询： 1234chan_essay_compilations = Book.objects.filter( author__name__endswith = 'Chun' authoring__collaboration_type = 'essays') 2.3 一对一定义一个一对一的关系，需要使用 OneToOneField 类。 例如，Restaurant 和 Place 为一对一： 1234567891011121314151617181920212223from django.db import modelsclass Place(models.Model): name = models.CharField(max_length=50) address = models.CharField(max_length=80) def __str__(self): # __unicode__ on Python 2 return "%s the place" % self.nameclass Restaurant(models.Model): place = models.OneToOneField(Place, primary_key=True) serves_hot_dogs = models.BooleanField(default=False) serves_pizza = models.BooleanField(default=False) def __str__(self): # __unicode__ on Python 2 return "%s the restaurant" % self.place.nameclass Waiter(models.Model): restaurant = models.ForeignKey(Restaurant) name = models.CharField(max_length=50) def __str__(self): # __unicode__ on Python 2 return "%s the waiter at %s" % (self.name, self.restaurant) 一个 Restaurant 都会有一个 Place，实际上你也可以使用继承的方式来定义。 下面是一些操作的例子。 创建 Place： 1234&gt;&gt;&gt; p1 = Place(name='Demon Dogs', address='944 W. Fullerton')&gt;&gt;&gt; p1.save()&gt;&gt;&gt; p2 = Place(name='Ace Hardware', address='1013 N. Ashland')&gt;&gt;&gt; p2.save() 创建 Restaurant 并关联到 Place： 12&gt;&gt;&gt; r = Restaurant(place=p1, serves_hot_dogs=True, serves_pizza=False)&gt;&gt;&gt; r.save() 然后，可以这样访问： 1234&gt;&gt;&gt; r.place&lt;Place: Demon Dogs the place&gt;&gt;&gt;&gt; p1.restaurant&lt;Restaurant: Demon Dogs the restaurant&gt; 这时候，p2 没有与之关联的 Restaurant，如果通过 p2 访问 Restaurant 就会提示异常： 123456&gt;&gt;&gt; from django.core.exceptions import ObjectDoesNotExist&gt;&gt;&gt; try:&gt;&gt;&gt; p2.restaurant&gt;&gt;&gt; except ObjectDoesNotExist:&gt;&gt;&gt; print("There is no restaurant here.")There is no restaurant here. 你可以通过下面的方式来避免出现异常： 12&gt;&gt;&gt; hasattr(p2, 'restaurant')False 下面可以做一些赋值操作： 123456&gt;&gt;&gt; r.place = p2&gt;&gt;&gt; r.save()&gt;&gt;&gt; p2.restaurant&lt;Restaurant: Ace Hardware the restaurant&gt;&gt;&gt;&gt; r.place&lt;Place: Ace Hardware the place&gt; 也可以反向赋值： 123&gt;&gt;&gt; p1.restaurant = r&gt;&gt;&gt; p1.restaurant&lt;Restaurant: Demon Dogs the restaurant&gt; 查询所有的 Restaurant 和 Place： 1234&gt;&gt;&gt; Restaurant.objects.all()[&lt;Restaurant: Demon Dogs the restaurant&gt;, &lt;Restaurant: Ace Hardware the restaurant&gt;]&gt;&gt;&gt; Place.objects.order_by('name')[&lt;Place: Ace Hardware the place&gt;, &lt;Place: Demon Dogs the place&gt;] 当然，也可以使用跨关系查找： 12345678&gt;&gt;&gt; Restaurant.objects.get(place=p1)&lt;Restaurant: Demon Dogs the restaurant&gt;&gt;&gt;&gt; Restaurant.objects.get(place__pk=1)&lt;Restaurant: Demon Dogs the restaurant&gt;&gt;&gt;&gt; Restaurant.objects.filter(place__name__startswith="Demon")[&lt;Restaurant: Demon Dogs the restaurant&gt;]&gt;&gt;&gt; Restaurant.objects.exclude(place__address__contains="Ashland")[&lt;Restaurant: Demon Dogs the restaurant&gt;] 反向查找： 12345678&gt;&gt;&gt; Place.objects.get(pk=1)&lt;Place: Demon Dogs the place&gt;&gt;&gt;&gt; Place.objects.get(restaurant__place=p1)&lt;Place: Demon Dogs the place&gt;&gt;&gt;&gt; Place.objects.get(restaurant=r)&lt;Place: Demon Dogs the place&gt;&gt;&gt;&gt; Place.objects.get(restaurant__place__name__startswith="Demon")&lt;Place: Demon Dogs the place&gt; 创建一个 Waiter： 1234&gt;&gt;&gt; w = r.waiter_set.create(name='Joe')&gt;&gt;&gt; w.save()&gt;&gt;&gt; w&lt;Waiter: Joe the waiter at Demon Dogs the restaurant&gt; 然后，查询： 1234&gt;&gt;&gt; Waiter.objects.filter(restaurant__place=p1)[&lt;Waiter: Joe the waiter at Demon Dogs the restaurant&gt;]&gt;&gt;&gt; Waiter.objects.filter(restaurant__place__name__startswith="Demon")[&lt;Waiter: Joe the waiter at Demon Dogs the restaurant&gt;] 3. 模型继承Django目前支持几种不同的继承方式： 使用单个表。整个继承树共用一张表。使用唯一的表，包含所有基类和子类的字段。 每个具体类一张表，这种方式下，每张表都包含具体类和继承树上所有父类的字段。因为多个表中有重复字段，从整个继承树上来说，字段是冗余的。 每个类一张表，继承关系通过表的JOIN操作来表示。这种方式下，每个表只包含类中定义的字段，不存在字段冗余，但是要同时操作子类和所有父类所对应的表。 方式一：每个类一张表 1234567891011from django.db import models class Person(models.Model): name = models.CharField(max_length=20) sex = models.BooleanField(default=True) class teacher(Person): subject = models.CharField(max_length=20) class student(Person): course = models.CharField(max_length=20) 执行 python manage.py sqlall： 12345678910111213141516171819BEGIN;CREATE TABLE "blog_person" ( "id" integer NOT NULL PRIMARY KEY, "name" varchar(20) NOT NULL, "sex" bool NOT NULL);CREATE TABLE "blog_teacher" ( "person_ptr_id" integer NOT NULL PRIMARY KEY REFERENCES "blog_person" ("id"), "subject" varchar(20) NOT NULL);CREATE TABLE "blog_student" ( "person_ptr_id" integer NOT NULL PRIMARY KEY REFERENCES "blog_person" ("id"), "course" varchar(20) NOT NULL); COMMIT; 方式二：每个具体类一张表，父类不需要创建表 1234567891011121314from django.db import models class Person(models.Model): name = models.CharField(max_length=20) sex = models.BooleanField(default=True) class Meta: abstract = True class teacher(Person): subject = models.CharField(max_length=20) class student(Person): course = models.CharField(max_length=20) 执行 python manage.py sqlall： 1234567891011121314151617BEGIN;CREATE TABLE "blog_teacher" ( "id" integer NOT NULL PRIMARY KEY, "name" varchar(20) NOT NULL, "sex" bool NOT NULL, "subject" varchar(20) NOT NULL);CREATE TABLE "blog_student" ( "id" integer NOT NULL PRIMARY KEY, "name" varchar(20) NOT NULL, "sex" bool NOT NULL, "course" varchar(20) NOT NULL); COMMIT; 可以通过 Meta 嵌套类自定义每个子类的表名： 1234567891011121314151617181920from django.db import models class Person(models.Model): name = models.CharField(max_length=20) sex = models.BooleanField(default=True) class Meta: abstract = True class teacher(Person): subject = models.CharField(max_length=20) class Meta: db_table = "Teacher" class student(Person): course = models.CharField(max_length=20) class Meta: db_table = "Student" 方式三：代理模型，为子类增加方法，但不能增加属性 12345678from django.db import models class Person(User): class Meta: proxy = True def some_function(self): pass 这样的方式不会改变数据存储结构，但可以纵向的扩展子类Person的方法，并且基础User父类的所有属性和方法。 4. 参考文章 Django 数据模型的字段列表整理 跟我一起Django - 04 定义和使用模型 django的模型总结 django ORM数据模型的定义]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AngularJS PhoneCat代码分析]]></title>
    <url>%2F2015%2F01%2F09%2Fangular-phonecat-examples%2F</url>
    <content type="text"><![CDATA[AngularJS 官方网站提供了一个用于学习的示例项目：PhoneCat。这是一个Web应用，用户可以浏览一些Android手机，了解它们的详细信息，并进行搜索和排序操作。 本文主要分析 AngularJS 官方网站提供的一个用于学习的示例项目 PhoneCat 的构建、测试过程以及代码的运行原理。希望能够对 PhoneCat 项目有一个更加深入全面的认识。这其中包括以下内容： 该项目如何运行起来的 该项目如何进行前端单元测试 AngularJS 相关代码分析 以下内容如有理解不正确，欢迎指正！ 1. 环境搭建对于 PhoneCat 项目的开发环境和测试环境的搭建，官方网站上提供了详细的指导：http://docs.angularjs.org/tutorial，你可以找到一些中文的翻译。 PhoneCat 项目的源代码托管在 GitHub 上，可以通过下面命令下载源代码： 1$ git clone --depth=20 https://github.com/angular/angular-phonecat.git --depth=20 选项的意思为：仅下载最近20次的代码提交版本；这么做可以减少下载的文件大小，加快下载。 PhoneCat 是一个 Web 应用程序，因此最好在 Web 服务器中运行，以期获得最佳结果。官方推荐安装 Node.js。 PhoneCat 项目的运行与测试依赖一些别的工具，可以在安装 Node.js 后通过 npm 命令来安装这些依赖包。以下命令需在 angular-phonecat 项目路径下运行： 1$ npm install 运行该命令后，会在 angular-phonecat 项目路径下安装以下依赖包： Bower 包管理器 Http-Server 轻量级Web服务器 Karma 用于运行单元测试 Protractor 用于运行端到端测试 几乎所有的 AngularJS 学习教程，都会写到用这个命令来启动服务： 1$ node scripts/web-server.js 但实际上 PhoneCat 项目已经放弃使用 web-server 了，git 上取下来的的项目里没有 scripts/web-server.js 文件了。 我们可以用下面的方式来启动工程： 1$ npm start 然后通过 http://localhost:8000/app/index.html 访问。 2. 依赖包介绍在克隆项目之后，目录如下： 123456789101112131415161718192021222324➜ angular-phonecat git:(master) ✗ tree -L 2.├── LICENSE├── README.md├── app│ ├── bower_components│ ├── css│ ├── img│ ├── index.html│ ├── js│ ├── partials│ └── phones├── bower.json├── package.json├── scripts│ ├── private│ └── update-repo.sh└── test ├── e2e ├── karma.conf.js ├── protractor-conf.js └── unit20 directories, 8 files 这个目录下存在一个文件 package.json，该文件是做什么用的呢？ 在 NodeJS 项目中，用 package.json 文件来声明项目中使用的模块，这样在新的环境部署时，只要在 package.json 文件所在的目录执行 npm install 命令即可安装所需要的模块。 关于 package.json 中可配置的选项请参考 package.json字段全解 。 从该文件可以看出 PhoneCat 的依赖： 12345678910"devDependencies": &#123; "karma": "^0.12.16", "karma-chrome-launcher": "^0.1.4", "karma-jasmine": "^0.1.5", "protractor": "~1.0.0", "http-server": "^0.6.1", "tmp": "0.0.23", "bower": "^1.3.1", "shelljs": "^0.2.6"&#125; 以及一些脚本： 123456789101112131415161718"scripts": &#123; "postinstall": "bower install", "prestart": "npm install", "start": "http-server -a 0.0.0.0 -p 8000", "pretest": "npm install", "test": "node node_modules/karma/bin/karma start test/karma.conf.js", "test-single-run": "node node_modules/karma/bin/karma start test/karma.conf.js --single-run", "preupdate-webdriver": "npm install", "update-webdriver": "webdriver-manager update", "preprotractor": "npm run update-webdriver", "protractor": "protractor test/protractor-conf.js", "update-index-async": "node -e \"require('shelljs/global'); sed('-i', /\\/\\/@@NG_LOADER_START@@[\\s\\S]*\\/\\/@@NG_LOADER_END@@/, '//@@NG_LOADER_START@@\\n' + cat('bower_components/angular-loader/angular-loader.min.js') + '\\n//@@NG_LOADER_END@@', 'app/index-async.html');\"" &#125; 从上可以看出运行 npm start 之前会运行 npm install，然后运行 http-server -a 0.0.0.0 -p 8000 启动一个 web 服务器，最后是运行 bower install 安装 bower 管理的包。 bower 管理的包由 bower.json 文件定义： 1234567891011121314151617&#123; "name": "angular-phonecat", "description": "A starter project for AngularJS", "version": "0.0.0", "homepage": "https://github.com/angular/angular-phonecat", "license": "MIT", "private": true, "dependencies": &#123; "angular": "1.3.x", "angular-mocks": "1.3.x", "jquery": "~2.1.1", "bootstrap": "~3.1.1", "angular-route": "1.3.x", "angular-resource": "1.3.x", "angular-animate": "1.3.x" &#125;&#125; 当然，package.json 文件中还定义了一些测试相关的命令。 bower关于 bower 的介绍，参考博客内文章：bower介绍。 在本项目中，bower 下载的包保存在 angular-phonecat/app/bower_components 目录下，依赖如下： 12345678├── bower_components│ ├── angular│ ├── angular-animate│ ├── angular-mocks│ ├── angular-resource│ ├── angular-route│ ├── bootstrap│ └── jquery karmaKarma 是一个 Javascript 测试运行工具，可以帮助你关闭反馈循环。Karma 可以在特定的文件被修改时运行测试，它也可以在不同的浏览器上并行测试。不同的设备可以指向 Karma 服务器来覆盖实际场景。 关于 Karma 的使用，本文不做介绍。 http-serverhttp-server 是一个简单的零配置命令行 HTTP 服务器，基于 Node.js。 在命令行中使用方式是： 1$ node http-server 在package.json 中定义方式是： 123"scripts": &#123; "start": "http-server -a 0.0.0.0 -p 8000",&#125; 支持的参数： 12345678910111213141516171819 -p 端口号 (默认 8080)-a IP 地址 (默认 0.0.0.0)-d 显示目录列表 (默认 'True')-i 显示 autoIndex (默认 'True')-e or --ext 如果没有提供默认的文件扩展名(默认 'html')-s or --silent 禁止日志信息输出--cors 启用 CORS -o 在开始服务后打开浏览器-h or --help 打印列表并退出-c 为 cache-control max-age header 设置Cache time(秒) ，禁用 caching, 则值设为 -1 . ProtractorProtractor 是一个端对端的测试运行工具，模拟用户交互，帮助你验证你的 Angular 应用的运行状况。 Protractor 使用 Jasmine 测试框架来定义测试。Protractor 为不同的页面交互提供一套健壮的 API。 当然，也有其他的端对端工具，不过 Protractor 有着自己的优势，它知道怎么和 AngularJS 的代码一起运行，特别是面临 $digest 循环的时候。 关于 Protractor 的使用，本文不做介绍。 ShellJSShellJS 是 Node.js 扩展，用于实现 Unix shell 命令执行，支持 Windows。 一个示例代码： 12345678910111213141516171819202122232425require('shelljs/global');if (!which('git')) &#123; echo('Sorry, this script requires git'); exit(1);&#125;// Copy files to release dirmkdir('-p', 'out/Release');cp('-R', 'stuff/*', 'out/Release');// Replace macros in each .js filecd('lib');ls('*.js').forEach(function(file) &#123; sed('-i', 'BUILD_VERSION', 'v0.1.2', file); sed('-i', /.*REMOVE_THIS_LINE.*\n/, '', file); sed('-i', /.*REPLACE_LINE_WITH_MACRO.*\n/, cat('macro.js'), file);&#125;);cd('..');// Run external tool synchronouslyif (exec('git commit -am "Auto-commit"').code !== 0) &#123; echo('Error: Git commit failed'); exit(1);&#125; 在 PhoneCat 中，主要是用在下面： 1"update-index-async": "node -e \"require('shelljs/global'); sed('-i', /\\/\\/@@NG_LOADER_START@@[\\s\\S]*\\/\\/@@NG_LOADER_END@@/, '//@@NG_LOADER_START@@\\n' + cat('bower_components/angular-loader/angular-loader.min.js') + '\\n//@@NG_LOADER_END@@', 'app/index-async.html');\"" 3. 测试运行单元测试PhoneCat 项目中的单元测试是使用 Karma 来完成的，所有的单元测试用例都存放在 test/unit 目录下。可以通过执行以下命令来运行单元测试： 1$ npm test 值得一提的是，在运行单元测试前，计算机上必须安装 Google Chrome 浏览器，因为这里用到了 karma-chrome-launcher。 运行端到端测试PhoneCat 项目使用端到端测试来保证 Web 应用的可操作性，而这个端到端测试是通过使用 Protractor 来实现的，所有的端到端测试用例都存放在test/e2e 目录下。可以通过执行以下步骤来运行端到端测试： 1234//更新webdriver，此命令只需运行一次$ npm run update-webdriver//运行PhoneCat$ npm start 打开另一个命令行窗口，在其中运行： 1$ npm run protractor 4. 代码分析在介绍了 PhoneCat 的运行和测试环境后，来看看 PhoneCat 的页面和 js 是怎么组织起来的。 首先，从 index.html 内容可以看到 PhoneCat 页面使用 bootstrap 框架，并且引入了 jquery 以及 angular 的相关依赖，包括一些附加模块：路由、动画、资源。 angular 应用范围由 ng-app 定义在 html 节点上，即作用于整个页面，其名称为 phonecatApp。 通过 ng-view 指定加载子视图的位置，这里主要包括 partials/phone-list.html 和 partials/phone-detail.html 两个视图。 app.js 是应用的入口，并且依赖 animations.js、controllers.js、filters.js、services.js 等文件。从这里可以看出，一个 angular 应用的 js 大概包括哪几个部分的内容。 app.js 内容如下： 123456789101112131415161718192021222324252627282930//JavaScript语法支持严格模式:如果在语法检测时发现语法问题，则整个代码块失效，并导致一个语法异常；如果在运行期出现了违反严格模式的代码，则抛出执行异常。'use strict';/* App Module *///定义一个模块，模块名称和页面 ng-app 中内容一致var phonecatApp = angular.module('phonecatApp', [ 'ngRoute', 'phonecatAnimations', 'phonecatControllers', 'phonecatFilters', 'phonecatServices']);//定义路由phonecatApp.config(['$routeProvider', function($routeProvider) &#123; $routeProvider. when('/phones', &#123; templateUrl: 'partials/phone-list.html', controller: 'PhoneListCtrl' &#125;). when('/phones/:phoneId', &#123; templateUrl: 'partials/phone-detail.html', controller: 'PhoneDetailCtrl' &#125;). otherwise(&#123; redirectTo: '/phones' &#125;); &#125;]); phonecatApp 模块依赖其他几个模块：ngRoute、phonecatAnimations、phonecatControllers、phonecatFilters、phonecatServices。 ngRoute 是内置的路由模块，定义路由规则： 当访问 /phones，由 PhoneListCtrl 控制器处理，并且由 partials/phone-list.html 模板渲染显示内容。 当访问 /phones/:phoneId，由 PhoneDetailCtrl 控制器处理，并且由 partials/phone-detail.html 模板渲染显示内容。 如果不满足上面条件，则重定向到 /phones phonecatAnimations 模块是定义动画效果，没有真个模块不影响程序的主要功能的运行，故不分析这部分代码。 phonecatControllers 模块定义在 controllers.js 文件中： 123456789101112131415161718192021222324'use strict';/* Controllers */var phonecatControllers = angular.module('phonecatControllers', []);// 定义 PhoneListCtrl，并注入 Phone 对象phonecatControllers.controller('PhoneListCtrl', ['$scope', 'Phone', function($scope, Phone) &#123; $scope.phones = Phone.query(); $scope.orderProp = 'age'; &#125;]);// 定义 PhoneDetailCtrl，并注入 Phone 对象和 $routeParams，$routeParams 封装了路由参数。phonecatControllers.controller('PhoneDetailCtrl', ['$scope', '$routeParams', 'Phone', function($scope, $routeParams, Phone) &#123; $scope.phone = Phone.get(&#123;phoneId: $routeParams.phoneId&#125;, function(phone) &#123; //回调方法 $scope.mainImageUrl = phone.images[0]; &#125;); $scope.setImage = function(imageUrl) &#123; $scope.mainImageUrl = imageUrl; &#125; &#125;]); phonecatFilters 模块定义在 filter.js 文件中，主要是自定义了一个过滤器 checkmark：根据输入是否有内容判断返回 ✓ 还是 ✘。 phonecatServices 模块定义在 services.js 文件中： 123456789101112'use strict';/* Services */var phonecatServices = angular.module('phonecatServices', ['ngResource']);// 定义 Phone 服务，并提供了一个 query 方法，还包括一个内置的 get 方法。调用 get 方法实际上就是调用 query 方法，并且可以传递一个参数 phoneIdphonecatServices.factory('Phone', ['$resource', function($resource)&#123; return $resource('phones/:phoneId.json', &#123;&#125;, &#123; query: &#123;method:'GET', params:&#123;phoneId:'phones'&#125;, isArray:true&#125; &#125;); &#125;]); 5. 参考文章 AngularJS初探：搭建PhoneCat项目的开发与测试环境 Angular 实例项目 angular-phonecat 的一些问题]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>angular.js</tag>
        <tag>node.js</tag>
        <tag>bower</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gradle构建多模块项目]]></title>
    <url>%2F2015%2F01%2F07%2Fbuild-multi-module-project-with-gradle%2F</url>
    <content type="text"><![CDATA[废话不多说，直接进入主题。 1. 创建项目首先创建项目，名称为 test： 12mkdir test &amp;&amp; cd testgradle init 这时候的项目结构如下： 123456789101112➜ test tree.├── build.gradle├── gradle│ └── wrapper│ ├── gradle-wrapper.jar│ └── gradle-wrapper.properties├── gradlew├── gradlew.bat└── settings.gradle2 directories, 6 files 然后，创建多个模块，这里以 core 和 web 模块为例，先创建两个目录： 1234mkdir -p core/src/main/javamkdir -p core/src/test/javamkdir -p web/src/main/javamkdir -p web/src/test/java 这时候的项目结构如下： 123456789101112131415161718192021222324➜ test tree.├── build.gradle├── core│ └── src│ ├── main│ │ └── java│ └── test│ └── java├── gradle│ └── wrapper│ ├── gradle-wrapper.jar│ └── gradle-wrapper.properties├── gradlew├── gradlew.bat├── settings.gradle└── web └── src ├── main │ └── java └── test └── java14 directories, 6 files 2. 修改配置接下来修改根目录下的 settings.gradle 文件，引入子模块： 1include &apos;core&apos;,&apos;web&apos; 修改根目录下的 build.gradle： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273// 所有子项目的通用配置subprojects &#123; apply plugin: 'java' apply plugin: 'eclipse' apply plugin: 'idea' version = '1.0' // JVM 版本号要求 sourceCompatibility = 1.7 targetCompatibility = 1.7 // java编译的时候缺省状态下会因为中文字符而失败 [compileJava,compileTestJava,javadoc]*.options*.encoding = 'UTF-8' //定义版本号 ext &#123; springVersion = '3.2.11.RELEASE' hibernateVersion='4.3.1.Final' &#125; repositories &#123; mavenCentral() &#125; jar &#123; manifest &#123; attributes("Implementation-Title": "Gradle") &#125; &#125; configurations &#123; // 所有需要忽略的包定义在此 all*.exclude group: 'commons-httpclient' all*.exclude group: 'commons-logging' all*.exclude group: 'commons-beanutils', module: 'commons-beanutils' &#125; dependencies &#123; // 通用依赖 compile( "org.springframework:spring-context:$springVersion", "org.springframework:spring-orm:$springVersion", "org.springframework:spring-tx:$springVersion", "org.springframework.data:spring-data-jpa:1.5.2.RELEASE", "org.hibernate:hibernate-entitymanager:$hibernateVersion", "c3p0:c3p0:0.9.1.2", "mysql:mysql-connector-java:5.1.26", "org.slf4j:slf4j-nop:1.7.6", "commons-fileupload:commons-fileupload:1.3.1", "com.fasterxml.jackson.core:jackson-databind:2.3.1" ) // 依赖maven中不存在的jar ext.jarTree = fileTree(dir: 'libs', include: '**/*.jar') ext.rootProjectLibs = new File(rootProject.rootDir, 'libs').getAbsolutePath() ext.jarTree += fileTree(dir: rootProjectLibs, include: '**/*.jar') compile jarTree // 测试依赖 testCompile( "org.springframework:spring-test:$springVersion", "junit:junit:4.11" ) &#125; // 显示当前项目下所有用于 compile 的 jar. task listJars(description: 'Display all compile jars.') &lt;&lt; &#123; configurations.compile.each &#123; File file -&gt; println file.name &#125; &#125;&#125; 接下来可以修改 core/build.gradle 来定义 core 模块的依赖： 1234// jar包的名字archivesBaseName = 'core'// 还可以定义其他配置，这里直接继承父模块中的配置 web 模块需要依赖 core 模块，故定义 web/build.gradle 如下： 12345678910111213141516171819202122232425262728293031323334353637apply plugin:"war" dependencies&#123; // 依赖 core 模块 compile project(":core") compile( "org.springframework:spring-webmvc:$springVersion", "org.apache.taglibs:taglibs-standard-impl:1.2.1" ) //系统提供的依赖 providedCompile( "javax.servlet:javax.servlet-api:3.1.0", "javax.servlet.jsp:jsp-api:2.2.1-b03", "javax.servlet.jsp.jstl:javax.servlet.jsp.jstl-api:1.2.1" ) &#125; task jarWithoutResources(type: Jar) &#123; baseName project.name from("$buildDir/classes/main") &#125; war&#123; dependsOn jarWithoutResources from("$projectDir/src/main/resources") &#123; include "*.properties" into("WEB-INF/classes") &#125; classpath=classpath - sourceSets.main.output classpath fileTree(dir:libsDir, include:"$&#123;project.name&#125;-$&#123;version&#125;.jar") &#125; task('jarPath')&lt;&lt;&#123; configurations.runtime.resolve().each &#123; print it.toString()+";" &#125; println(); &#125; 3. 编译项目查看所有 jar： 1$ gradle listJars 查看各个模块的依赖： 12$ gradle :core:dependencies$ gradle :web:dependencies 编译所有模块： 1$ gradle build 对比一下，这时候的目录如下： 1234567891011121314151617181920212223242526272829303132333435363738394041➜ test tree.├── build.gradle├── core│ ├── build│ │ ├── libs│ │ │ └── core-1.0.jar│ │ └── tmp│ │ └── jar│ │ └── MANIFEST.MF│ ├── build.gradle│ └── src│ ├── main│ │ └── java│ └── test│ └── java├── gradle│ └── wrapper│ ├── gradle-wrapper.jar│ └── gradle-wrapper.properties├── gradlew├── gradlew.bat├── settings.gradle└── web ├── build │ ├── libs │ │ ├── web-1.0.jar │ │ └── web-1.0.war │ └── tmp │ ├── jarWithoutResources │ │ └── MANIFEST.MF │ └── war │ └── MANIFEST.MF ├── build.gradle └── src ├── main │ └── java └── test └── java23 directories, 14 files 这样，core和web模块都是gradle项目了，你也可以单独编译某一个模块，例如，编译core模块： 12345678910111213141516171819$ cd core$ rm -rf build$ gradle build$ tree.├── build│ ├── libs│ │ └── core-1.0.jar│ └── tmp│ └── jar│ └── MANIFEST.MF├── build.gradle└── src ├── main │ └── java └── test └── java9 directories, 3 files 4. 一些小技巧1. 善用 gradle dependencies1gradle dependencies &gt; depend.log 2. java 编译时候报编码错误1[compileJava,compileTestJava,javadoc]*.options*.encoding = 'UTF-8' 3. 忽略掉 .gradle 目录修改 .gitignore 忽略该目录： 1234567891011121314151617*.sw?.#**#*~.classpath.project.settingsbinbuildtargetdependency-reduced-pom.xml*.sublime-*/scratch.gradleREADME.html.idea*.iml 4. Maven 库中没有的 jar 该怎么管理在顶级目录增加一个 libs 文件夹，这个文件夹里面的 jar 是对所有项目都起作用的。 如果是某个项目自用的，则可以在该项目的 source 下面创建个 libs，具体实现是在顶级目录下的 build.gradle 中： 12345ext.jarTree = fileTree(dir: 'libs', include: '**/*.jar')ext.rootProjectLibs = new File(rootProject.rootDir, 'libs').getAbsolutePath()ext.jarTree += fileTree(dir: rootProjectLibs, include: '**/*.jar')compile jarTree 5. jar 包定义外移暂时还没有这样的需求，详细说明请参考 jar 包定义外移 6. 如何指定 build 输出目录和版本号12buildDir = &quot;target&quot;version = &apos;1.0&apos; 7. 在执行 Gradle 命令时如何指定参数1gradle task -P profile=development 8. Gradle 和 idea 集成时如何不自动下载依赖源码和javadoc123456idea &#123; module &#123; downloadJavadoc = false downloadSources = false &#125;&#125; 5. 参考文章 gradle多模块开发 Gradle 多项目管理示例 构建工具之 - Gradle一般使用常见问答]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>gradle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Spring Boot和Gradle创建AngularJS项目]]></title>
    <url>%2F2015%2F01%2F06%2Fbuild-app-with-spring-boot-and-gradle%2F</url>
    <content type="text"><![CDATA[Spring Boot 是由 Pivotal 团队提供的全新框架，其设计目的是用来简化新 Spring 应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。 本文主要是记录使用 Spring Boot 和 Gradle 创建项目的过程，其中会包括 Spring Boot 的安装及使用方法，希望通过这篇文章能够快速搭建一个项目。 1. 开发环境 操作系统: mac JDK：1.8 Spring Boot：2.1.3.RELEASE Gradle：2.2.1 IDE：Idea 2. 创建项目你可以通过 Spring Initializr 来创建一个空的项目，也可以手动创建，这里我使用的是手动创建 gradle 项目。 参考 使用Gradle构建项目 创建一个 ng-spring-boot 项目，执行的命令如下： 12$ mkdir ng-spring-boot &amp;&amp; cd ng-spring-boot$ gradle init ng-spring-boot 目录结构如下： 123456789101112➜ ng-spring-boot tree.├── build.gradle├── gradle│ └── wrapper│ ├── gradle-wrapper.jar│ └── gradle-wrapper.properties├── gradlew├── gradlew.bat└── settings.gradle2 directories, 6 files 然后修改 build.gradle 文件： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647buildscript &#123; ext &#123; springBootVersion = '2.1.3.RELEASE' &#125; repositories &#123; mavenCentral() &#125; dependencies &#123; classpath("org.springframework.boot:spring-boot-gradle-plugin:$&#123;springBootVersion&#125;") &#125;&#125;apply plugin: 'java'apply plugin: 'eclipse'apply plugin: 'idea'apply plugin: 'spring-boot'jar &#123; baseName = 'ng-spring-boot' version = '1.0.0-SNAPSHOT'&#125;sourceCompatibility = 1.8targetCompatibility = 1.8repositories &#123; mavenCentral() maven &#123; url "https://repo.spring.io/libs-release" &#125;&#125;dependencies &#123; compile("org.springframework.boot:spring-boot-starter-data-jpa") compile("org.springframework.boot:spring-boot-starter-web") compile("org.springframework.boot:spring-boot-starter-actuator") runtime("org.hsqldb:hsqldb") testCompile("org.springframework.boot:spring-boot-starter-test")&#125;eclipse &#123; classpath &#123; containers.remove('org.eclipse.jdt.launching.JRE_CONTAINER') containers 'org.eclipse.jdt.launching.JRE_CONTAINER/org.eclipse.jdt.internal.debug.ui.launcher.StandardVMType/JavaSE-1.7' &#125;&#125;task wrapper(type: Wrapper) &#123; gradleVersion = '2.3'&#125; 使用 spring-boot-gradle-plugin 插件可以提供一些创建可执行 jar 和从源码运行项目的任务，它还提供了 ResolutionStrategy 以方便依赖中不用写版本号。 3. 创建一个可执行的类首先，新建一个符合 Maven 规范的目录结构： 1$ mkdir -p src/main/java/com/javachen 创建一个 Sping boot 启动类： 12345678910111213141516package com.javachen;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;@Configuration@EnableAutoConfiguration@ComponentScanpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; main 方法使用了 SpringApplication 工具类。这将告诉Spring去读取 Application 的元信息，并在Spring的应用上下文作为一个组件被管理。 @Configuration 注解告诉 spring 该类定义了 application context 的 bean 的一些配置。 @ComponentScan 注解告诉 Spring 遍历带有 @Component 注解的类。这将保证 Spring 能找到并注册 GreetingController，因为它被 @RestController 标记，这也是 @Component 的一种。 @EnableAutoConfiguration 注解会基于你的类加载路径的内容切换合理的默认行为。比如，因为应用要依赖内嵌版本的 tomcat，所以一个tomcat服务器会被启动并代替你进行合理的配置。再比如，因为应用要依赖 Spring 的 MVC 框架,一个 Spring MVC 的 DispatcherServlet 将被配置并注册，并且不再需要 web.xml 文件。 你还可以添加 @EnableWebMvc 注解配置 Spring Mvc。 上面三个注解还可以用 @SpringBootApplication 代替： 1234567891011package com.javachen.examples.springboot;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplication // same as @Configuration @EnableAutoConfiguration @ComponentScanpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 你也可以修改该类的 main 方法，获取 ApplicationContext： 12345678910111213141516171819202122232425package com.javachen;import java.util.Arrays;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.context.ApplicationContext;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;@SpringBootApplication public class Application &#123; public static void main(String[] args) &#123; ApplicationContext ctx = SpringApplication.run(Application.class, args); System.out.println("Let's inspect the beans provided by Spring Boot:"); String[] beanNames = ctx.getBeanDefinitionNames(); Arrays.sort(beanNames); for (String beanName : beanNames) &#123; System.out.println(beanName); &#125; &#125;&#125; 4. 创建一个实体类创建一个实体类 src/main/java/com/javachen/model/Item.java： 123456789101112131415161718192021222324252627282930313233343536373839package com.javachen.model;import javax.persistence.*;@Entitypublic class Item &#123; @Id @GeneratedValue(strategy=GenerationType.IDENTITY) private Integer id; @Column private boolean checked; @Column private String description; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public boolean isChecked() &#123; return checked; &#125; public void setChecked(boolean checked) &#123; this.checked = checked; &#125; public String getDescription() &#123; return description; &#125; public void setDescription(String description) &#123; this.description = description; &#125;&#125; 5. 创建控制类创建一个 Restfull 的控制类，该类主要提供增删改查的方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.javachen.controller;import com.javachen.model.Item;import com.javachen.repository.ItemRepository;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.dao.EmptyResultDataAccessException;import org.springframework.http.HttpStatus;import org.springframework.web.bind.annotation.*;import javax.persistence.EntityNotFoundException;import java.util.List;@RestController@RequestMapping("/items")public class ItemController &#123; @Autowired private ItemRepository repo; @RequestMapping(method = RequestMethod.GET) public List&lt;Item&gt; findItems() &#123; return repo.findAll(); &#125; @RequestMapping(method = RequestMethod.POST) public Item addItem(@RequestBody Item item) &#123; item.setId(null); return repo.saveAndFlush(item); &#125; @RequestMapping(value = "/&#123;id&#125;", method = RequestMethod.PUT) public Item updateItem(@RequestBody Item updatedItem, @PathVariable Integer id) &#123; Item item = repo.getOne(id); item.setChecked(updatedItem.isChecked()); item.setDescription(updatedItem.getDescription()); return repo.saveAndFlush(item); &#125; @RequestMapping(value = "/&#123;id&#125;", method = RequestMethod.DELETE) @ResponseStatus(value = HttpStatus.NO_CONTENT) public void deleteItem(@PathVariable Integer id) &#123; repo.delete(id); &#125; @ResponseStatus(HttpStatus.BAD_REQUEST) @ExceptionHandler(value = &#123; EmptyResultDataAccessException.class, EntityNotFoundException.class &#125;) public void handleNotFound() &#123; &#125;&#125; Greeting 对象会被转换成 JSON 字符串，这得益于 Spring 的 HTTP 消息转换支持，你不必人工处理。由于 Jackson2 在 classpath 里，Spring的 MappingJackson2HttpMessageConverter 会自动完成这一工作。 这段代码使用 Spring4 新的注解：@RestController，表明该类的每个方法返回对象而不是视图。它实际就是 @Controller 和 @ResponseBody 混合使用的简写方法。 6. 创建 JPA 仓库使用 JAP 来持久化数据： 12345678910111213package com.javachen.repository;import com.javachen.model.Item;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.data.jpa.repository.Query;import java.util.List;public interface ItemRepository extends JpaRepository&lt;Item, Integer&gt; &#123; @Query("SELECT i FROM Item i WHERE i.checked=true") List&lt;Item&gt; findChecked();&#125; Spring Boot 可以自动配置嵌入式的数据库，包括 H2、HSQL 和 Derby，你不需要配置数据库链接的 url，只需要添加相关的依赖即可。另外，你还需要依赖 spring-jdbc，在本例中，我们是引入了对 spring-boot-starter-data-jpa 的依赖。如果你想使用其他类型的数据库，则需要配置 spring.datasource.* 属性，一个示例是在 application.properties 中配置如下属性： 1234spring.datasource.url=jdbc:mysql://localhost/testspring.datasource.username=dbuserspring.datasource.password=dbpassspring.datasource.driver-class-name=com.mysql.jdbc.Driver 创建 src/main/resources/application.properties 文件，修改 JPA 相关配置，如： 1spring.jpa.hibernate.ddl-auto=create-drop 注意： SpringApplication 会在以下路径查找 application.properties 并加载该文件： /config 目录下 当前目录 classpath 中 /config 包下 classpath 根路径下 7. 运行项目可以在项目根路径直接运行下面命令： 123$ export JAVA_OPTS=-Xmx1024m -XX:MaxPermSize=128M -Djava.security.egd=file:/dev/./urandom$ ./gradlew bootRun 也可以先 build 生成一个 jar 文件，然后执行该 jar 文件： 1$ ./gradlew build &amp;&amp; java -jar build/libs/ng-spring-boot-1.0.0-SNAPSHOT.jar 启动过程中你会看到如下内容，这部分内容是在 Application 类中打印出来的： 1234567891011121314151617181920212223242526272829303132333435Let&apos;s inspect the beans provided by Spring Boot:applicationbeanNameHandlerMappingdefaultServletHandlerMappingdispatcherServletembeddedServletContainerCustomizerBeanPostProcessorhandlerExceptionResolverhelloControllerhttpRequestHandlerAdaptermessageSourcemvcContentNegotiationManagermvcConversionServicemvcValidatororg.springframework.boot.autoconfigure.MessageSourceAutoConfigurationorg.springframework.boot.autoconfigure.PropertyPlaceholderAutoConfigurationorg.springframework.boot.autoconfigure.web.EmbeddedServletContainerAutoConfigurationorg.springframework.boot.autoconfigure.web.EmbeddedServletContainerAutoConfiguration$DispatcherServletConfigurationorg.springframework.boot.autoconfigure.web.EmbeddedServletContainerAutoConfiguration$EmbeddedTomcatorg.springframework.boot.autoconfigure.web.ServerPropertiesAutoConfigurationorg.springframework.boot.context.embedded.properties.ServerPropertiesorg.springframework.context.annotation.ConfigurationClassPostProcessor.enhancedConfigurationProcessororg.springframework.context.annotation.ConfigurationClassPostProcessor.importAwareProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessororg.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalRequiredAnnotationProcessororg.springframework.web.servlet.config.annotation.DelegatingWebMvcConfigurationpropertySourcesBinderpropertySourcesPlaceholderConfigurerrequestMappingHandlerAdapterrequestMappingHandlerMappingresourceHandlerMappingsimpleControllerHandlerAdaptertomcatEmbeddedServletContainerFactoryviewControllerHandlerMapping 你也可以启动远程调试： 1234$ ./gradlew build $ java -Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=8000,suspend=n \ -jar build/libs/spring-boot-examples-1.0.0-SNAPSHOT.jar 接下来，打开浏览器访问 http://localhost:8080/items，你会看到页面输出一个空的数组。然后，你可以使用浏览器的 Restfull 插件来添加、删除、修改数据。 8. 添加前端库文件这里主要使用 Bower 来管理前端依赖，包括 angular 和 bootstrap。 配置 Bower ，需要在项目根目录下创建 .bowerrc 和 bower.json 两个文件。 .bowerrc 文件制定下载的依赖存放路径： 1234&#123; "directory": "src/main/resources/static/bower_components", "json": "bower.json"&#125; bower.json 文件定义依赖关系： 12345678&#123; "name": "ng-spring-boot", "dependencies": &#123; "angular": "~1.3.0", "angular-resource": "~1.3.0", "bootstrap-css-only": "~3.2.0" &#125;&#125; 如果你没有安装 Bower，则运行下面命令进行安装： 1npm install -g bower 安装之后下载依赖： 1bower install 运行成功之后，查看 src/main/resources/static/bower_components 目录结构： 1234src/main/resources/static/bower_components├── angular├── angular-resource└── bootstrap-css-only 我们可以将bower_components目录改名为lib，同时修改.bowerrc文件如下： 1234&#123; &quot;directory&quot;: &quot;src/main/resources/static/libs&quot;, &quot;json&quot;: &quot;bower.json&quot;&#125; 9. 创建前端页面 注意： 前端页面和 js 存放到 public 目录下，是因为 Spring Boot 会自动在 /static 或者 /public 或者 /resources 或者 /META-INF/resources 加载静态页面。 创建 index.html创建 public 目录存放静态页面 index.html： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt; &lt;head&gt; &lt;link rel="stylesheet" href="./lib/bootstrap-css-only/css/bootstrap.min.css" /&gt; &lt;/head&gt; &lt;body ng-app="myApp"&gt; &lt;div class="container" ng-controller="AppController"&gt; &lt;div class="page-header"&gt; &lt;h1&gt;A checklist&lt;/h1&gt; &lt;/div&gt; &lt;div class="alert alert-info" role="alert" ng-hide="items &amp;amp;&amp;amp; items.length &gt; 0"&gt; There are no items yet. &lt;/div&gt; &lt;form class="form-horizontal" role="form" ng-submit="addItem(newItem)"&gt; &lt;div class="form-group" ng-repeat="item in items"&gt; &lt;div class="checkbox col-xs-9"&gt; &lt;label&gt; &lt;input type="checkbox" ng-model="item.checked" ng-change="updateItem(item)"/&gt; &#123;&#123;item.description&#125;&#125; &lt;/label&gt; &lt;/div&gt; &lt;div class="col-xs-3"&gt; &lt;button class="pull-right btn btn-danger" type="button" title="Delete" ng-click="deleteItem(item)"&gt; &lt;span class="glyphicon glyphicon-trash"&gt;&lt;/span&gt; &lt;/button&gt; &lt;/div&gt; &lt;/div&gt; &lt;hr /&gt; &lt;div class="input-group"&gt; &lt;input type="text" class="form-control" ng-model="newItem" placeholder="Enter the description..." /&gt; &lt;span class="input-group-btn"&gt; &lt;button class="btn btn-default" type="submit" ng-disabled="!newItem" title="Add"&gt; &lt;span class="glyphicon glyphicon-plus"&gt;&lt;/span&gt; &lt;/button&gt; &lt;/span&gt; &lt;/div&gt; &lt;/form&gt; &lt;/div&gt; &lt;script type="text/javascript" src="./lib/angular/angular.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" src="./lib/angular-resource/angular-resource.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" src="./lib/lodash/dist/lodash.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" src="./app/app.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" src="./app/controllers.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" src="./app/services.js"&gt;&lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 初始化 AngularJS这里使用闭包的方式来初始化 AngularJS，代码见 public/app/app.js ： 12345(function(angular) &#123; angular.module("myApp.controllers", []); angular.module("myApp.services", []); angular.module("myApp", ["ngResource", "myApp.controllers", "myApp.services"]);&#125;(angular)); 创建 resource factory代码见 public/app/services.js ： 1234567891011121314151617(function(angular) &#123; var ItemFactory = function($resource) &#123; return $resource('/items/:id', &#123; id: '@id' &#125;, &#123; update: &#123; method: "PUT" &#125;, remove: &#123; method: "DELETE" &#125; &#125;); &#125;; ItemFactory.$inject = ['$resource']; angular.module("myApp.services").factory("Item", ItemFactory);&#125;(angular)); 创建控制器代码见 public/app/controllers.js ： 123456789101112131415161718192021222324252627282930(function(angular) &#123; var AppController = function($scope, Item) &#123; Item.query(function(response) &#123; $scope.items = response ? response : []; &#125;); $scope.addItem = function(description) &#123; new Item(&#123; description: description, checked: false &#125;).$save(function(item) &#123; $scope.items.push(item); &#125;); $scope.newItem = ""; &#125;; $scope.updateItem = function(item) &#123; item.$update(); &#125;; $scope.deleteItem = function(item) &#123; item.$remove(function() &#123; $scope.items.splice($scope.items.indexOf(item), 1); &#125;); &#125;; &#125;; AppController.$inject = ['$scope', 'Item']; angular.module("myApp.controllers").controller("AppController", AppController);&#125;(angular)); 10. 测试前端页面再一次打开浏览器，访问 http://localhost:8080/ 进行测试。 11. 前后端分离部署对于AngularJS应用，我们可以使用nodejs或者其他容器部署应用，例如，这里可以使用groovy服务来启动前端服务，然后通过http请求调用后端服务的接口。 先创建app.groovy文件： 1@Controller class JsApp &#123; &#125; 修改services.js文件中的请求地址为 http://localhost:8080/items/:id： 1234567891011121314151617(function(angular) &#123; var ItemFactory = function($resource) &#123; return $resource('http://localhost:8080/items/:id', &#123; id: '@id' &#125;, &#123; update: &#123; method: "PUT" &#125;, remove: &#123; method: "DELETE" &#125; &#125;); &#125;; ItemFactory.$inject = ['$resource']; angular.module("myApp.services").factory("Item", ItemFactory);&#125;(angular)); 然后，启动groovy服务来托管静态文件，这里使用9000端口： 1spring run app.groovy -- --server.port=9000 这时候，就可以服务 http://localhost:9000/ 来查看前端页面了。 这样，前后端就是分别部署属于两个不同的应用了，为了解决跨域的问题，需要添加一个过滤器，如下： 12345678910111213141516171819202122232425package com.javachen.filter;import org.springframework.stereotype.Component;import javax.servlet.*;import javax.servlet.http.HttpServletResponse;import java.io.IOException;@Componentpublic class SimpleCORSFilter implements Filter &#123; public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException &#123; HttpServletResponse response = (HttpServletResponse) res; response.setHeader("Access-Control-Allow-Origin", "*"); response.setHeader("Access-Control-Allow-Methods", "POST, GET, PUT, OPTIONS, DELETE"); response.setHeader("Access-Control-Max-Age", "3600"); response.setHeader("Access-Control-Allow-Headers", "Origin, X-Requested-With, Content-Type, Accept"); chain.doFilter(req, res); &#125; public void init(FilterConfig filterConfig) &#123;&#125; public void destroy() &#123;&#125;&#125; 然后，重启应用就可以了。 12. 总结本文主要是记录快速使用 Spring Boot 和 Gradle 创建 AngularJS 项目的过程，并介绍了如何将前后端进行分离和解决跨域访问的问题，希望能对你有所帮助。 文中相关的源码在 ng-spring-boot，你可以下载该项目，然后编译、运行代码。 13. 参考文章 Rapid prototyping with Spring Boot and AngularJS Building an Application with Spring Boot Building a RESTful Web Service Enabling Cross Origin Requests for a RESTful Web Service]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring boot</tag>
        <tag>spring</tag>
        <tag>gradle</tag>
        <tag>angular.js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中的多线程]]></title>
    <url>%2F2014%2F12%2F23%2Fthread-in-python%2F</url>
    <content type="text"><![CDATA[线程模块Python 通过两个标准库 thread 和 threading 提供对线程的支持。Python 的 thread 模块是比较底层的模块，Python 的 threading 模块是对 thread 做了一些包装的，可以更加方便的被使用。 thread 模块提供的其他方法： start_new_thread(function,args,kwargs=None)：生一个新线程，在新线程中用指定参数和可选的 kwargs 调用 function 函数 allocate_lock()：分配一个 LockType 类型的锁对象（注意，此时还没有获得锁） interrupt_main()：在其他线程中终止主线程 get_ident()：获得一个代表当前线程的魔法数字，常用于从一个字典中获得线程相关的数据。这个数字本身没有任何含义，并且当线程结束后会被新线程复用 exit()：退出线程 LockType 类型锁对象的函数： acquire(wait=None)：尝试获取锁对象 locked()：如果获得了锁对象返回 True，否则返回 False release()：释放锁 thread 还提供了一个 ThreadLocal 类用于管理线程相关的数据，名为 thread._local，threading 中引用了这个类。 由于 thread 提供的线程功能不多，无法在主线程结束后继续运行，不提供条件变量等等原因，一般不使用 thread 模块。 threading 模块提供的其他方法： threading.currentThread()：返回当前的线程变量。 threading.enumerate()：返回一个包含正在运行的线程的 list。正在运行指线程启动后、结束前，不包括启动前和终止后的线程。 threading.activeCount()：返回正在运行的线程数量，与 len(threading.enumerate()) 有相同的结果。 除了使用方法外，线程模块同样提供了 Thread 类来处理线程，构造方法： 1Thread(group=None, target=None, name=None, args=(), kwargs=&#123;&#125;) 参数说明： group：线程组，目前还没有实现，库引用中提示必须是 None； target：要执行的方法； name：线程名； args/kwargs：要传入方法的参数。 Thread 类提供了以下实例方法: run()：用以表示线程活动的方法。 start()：启动线程活动。 join([timeout])：阻塞当前上下文环境的线程，直到调用此方法的线程终止或到达指定的 timeout（可选参数）。 isAlive()：返回线程是否活动的。 getName()：返回线程名。 setName()：设置线程名。 threading 模块提供的类：Thread, Lock, Rlock, Condition, [Bounded]Semaphore, Event, Timer, local。 使用 thread 模块创建线程函数式： 调用 thread 模块中的 start_new_thread() 函数来产生新线程。语法如下: 1thread.start_new_thread ( function, args[, kwargs] ) 参数说明： function - 线程函数。 args - 传递给线程函数的参数，他必须是个 tuple 类型。 kwargs - 可选参数。 例子1： 1234567891011121314151617181920212223# -* - coding: UTF-8 -* -import threadimport time# 为线程定义一个函数def print_time( threadName, delay): count = 0 while count &lt; 5: time.sleep(delay) count += 1 print "%s: %s" % ( threadName, time.ctime(time.time()) )# 创建两个线程try: thread.start_new_thread( print_time, ("Thread-1", 1, ) ) thread.start_new_thread( print_time, ("Thread-2", 2, ) )except: print "Error: unable to start thread"while 1: pass 输出结果如下： 12345678910Thread-1: Tue Dec 23 14:54:51 2014Thread-2: Tue Dec 23 14:54:52 2014Thread-1: Tue Dec 23 14:54:52 2014Thread-1: Tue Dec 23 14:54:53 2014Thread-2: Tue Dec 23 14:54:54 2014Thread-1: Tue Dec 23 14:54:54 2014Thread-1: Tue Dec 23 14:54:55 2014Thread-2: Tue Dec 23 14:54:56 2014Thread-2: Tue Dec 23 14:54:58 2014Thread-2: Tue Dec 23 14:55:00 2014 主线程没有结束，是因为有个 while 循环一直在执行 pass 语句，导致程序一直没有退出。如果想要主线程主动结束，可以在线程函数中调用 thread.exit()，他抛出SystemExit exception，达到退出线程的目的。 使用 Threading 模块创建线程使用 Threading 模块创建线程，直接从 threading.Thread 继承，然后重写 __init__ 方法和run 方法。 例子2： 12345678910111213141516171819202122232425262728293031323334# -* - coding: UTF-8 -* -#!/usr/bin/pythonfrom threading import Threadimport time#继承父类threading.Threadclass myThread(Thread): def __init__(self, name, delay): Thread.__init__(self) self.name = name self.delay = delay def run(self): #把要执行的代码写到run函数里面 线程在创建后会直接运行run函数 print "Starting " + self.name print_time(self.name, self.delay, 5) print "Exiting " + self.namedef print_time(threadName, delay, counter): while counter: time.sleep(delay) print "%s: %s" % (threadName, time.ctime(time.time())) counter -= 1# 创建新线程thread1 = myThread("Thread-1", 1)thread2 = myThread("Thread-2", 2)# 开启线程thread1.start()thread2.start()print "Exiting Main Thread" 执行结果如下： 123456789101112Starting Thread-1Starting Thread-2Starting Thread-3Thread-3 processing OneThread-2 processing TwoThread-3 processing ThreeThread-1 processing FourThread-2 processing FiveExiting Thread-3Exiting Thread-1Exiting Thread-2Exiting Main Thread 线程同步如果多个线程共同对某个数据修改，则可能出现不可预料的结果，为了保证数据的正确性，需要对多个线程进行同步。 使用 Thread 对象的 Lock 和 Rlock 可以实现简单的线程同步，这两个对象都有 acquire 方法和 release 方法，对于那些需要每次只允许一个线程操作的数据，可以将其操作放到 acquire 和 release 方法之间。 Lock（指令锁）是可用的最低级的同步指令。Lock 处于锁定状态时，不被特定的线程拥有。Lock 包含两种状态——锁定和非锁定，以及两个基本的方法。 可以认为 Lock 有一个锁定池，当线程请求锁定时，将线程至于池中，直到获得锁定后出池。池中的线程处于状态图中的同步阻塞状态。 构造方法： 1Lock() 实例方法： acquire([timeout])：使线程进入同步阻塞状态，尝试获得锁定。 release()：释放锁。使用前线程必须已获得锁定，否则将抛出异常。 RLock（可重入锁）是一个可以被同一个线程请求多次的同步指令。RLock 使用了拥有的线程和递归等级的概念，处于锁定状态时，RLock 被某个线程拥有。拥有RLock的线程可以再次调用acquire()，释放锁时需要调用 release() 相同次数。 可以认为 RLock 包含一个锁定池和一个初始值为0的计数器，每次成功调用 acquire()/release()，计数器将 +1/-1，为0时锁处于未锁定状态。 例子3： 1234567891011121314151617181920212223242526272829303132333435363738394041424344# -* - coding: UTF-8 -* -#!/usr/bin/pythonfrom threading import Thread,Lockimport timethreadLock = Lock()class myThread (Thread): def __init__(self, name, delay): Thread.__init__(self) self.name = name self.delay = delay def run(self): print "Starting " + self.name # 获得锁，成功获得锁定后返回True # 可选的timeout参数不填时将一直阻塞直到获得锁定 # 否则超时后将返回False threadLock.acquire() print_time(self.name, self.delay, 3) # 释放锁 threadLock.release()def print_time(threadName, delay, counter): while counter: time.sleep(delay) print "%s: %s" % (threadName, time.ctime(time.time())) counter -= 1# 创建新线程thread1 = myThread( "Thread-1", 1)thread2 = myThread("Thread-2", 2)# 开启新线程thread1.start()thread2.start()# 等待所有线程完成thread1.join()thread2.join()print "Exiting Main Thread" 运行结果： 123456789Starting Thread-1Starting Thread-2Thread-1: Tue Dec 23 16:06:32 2014Thread-1: Tue Dec 23 16:06:33 2014Thread-1: Tue Dec 23 16:06:34 2014Thread-2: Tue Dec 23 16:06:36 2014Thread-2: Tue Dec 23 16:06:38 2014Thread-2: Tue Dec 23 16:06:40 2014Exiting Main Thread 例子3和例子2的区别在于，例子上中 print_time 方法前后添加了 threadLock 的两个方法，并且在主线程调用了两个线程的 join 方法，使得主线程阻塞直到两个子线程运行完成。待子线程运行完成之后，最后才会打印 Exiting Main Thread ，即表示主线程运行完成。 除了使用 Lock 类获取锁之外，我们还可以使用 Condition 类，condition 的 acquire() 和 release() 方法内部调用了 lock 的 acquire() 和 release()，所以我们可以用 condiction 实例取代 lock 实例，但 lock 的行为不会改变。 线程优先级队列Python 的 Queue 模块中提供了同步的、线程安全的队列类，包括 FIFO（先入先出)队列 Queue，LIFO（后入先出）队列 LifoQueue，和优先级队列 PriorityQueue。这些队列都实现了锁原语，能够在多线程中直接使用。可以使用队列来实现线程间的同步。 Queue模块中的常用方法: Queue.qsize()：返回队列的大小 Queue.empty()：如果队列为空，返回 True，反之 False Queue.full()：如果队列满了，返回 True，反之 False Queue.full：与 maxsize 大小对应 Queue.get([block[, timeout]])：获取队列，timeout 等待时间 Queue.get_nowait()：相当 Queue.get(False) Queue.put(item)：写入队列，timeout 等待时间 Queue.put_nowait(item)：相当 Queue.put(item, False) Queue.task_done()：在完成一项工作之后，Queue.task_done() 函数向任务已经完成的队列发送一个信号 Queue.join()：实际上意味着等到队列为空，再执行别的操作 实例4： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# -* - coding: UTF-8 -* -#!/usr/bin/pythonfrom threading import Thread,Lockimport Queueimport timethreadList = ["Thread-1", "Thread-2", "Thread-3"]nameList = ["One", "Two", "Three", "Four", "Five"]workQueue = Queue.Queue(10)queueLock = Lock()threads = []exitFlag = 0class myThread (Thread): def __init__(self, name, q): Thread.__init__(self) self.name = name self.q = q def run(self): print "Starting " + self.name process_data(self.name, self.q) print "Exiting " + self.namedef process_data(threadName, q): while not exitFlag: queueLock.acquire() if not workQueue.empty(): data = q.get() queueLock.release() print "%s processing %s" % (threadName, data) else: queueLock.release() time.sleep(1)# 创建新线程for tName in threadList: thread = myThread(tName, workQueue) thread.start() threads.append(thread)# 填充队列queueLock.acquire()for word in nameList: workQueue.put(word)queueLock.release()# 等待队列清空while not workQueue.empty(): pass# 通知线程是时候退出exitFlag = 1# 等待所有线程完成for t in threads: t.join()print "Exiting Main Thread" 例子4中创建了3个线程读取队列的数据，当队列为空时候，三个线程停止运行，另外主线程会一直阻塞直到三个子线程运行完毕，最后再打印 “Exiting Main Thread”。 生产者和消费者模型 生产者的工作是产生一块数据，放到 buffer 中，如此循环。与此同时，消费者在消耗这些数据（例如从 buffer 中把它们移除），每次一块。这个为描述了两个共享固定大小缓冲队列的进程，即生产者和消费者。 示例5： 123456789101112131415161718192021222324252627282930313233343536373839404142from threading import Thread, Conditionimport timeimport randomqueue = []MAX_NUM = 10condition = Condition()class ProducerThread(Thread): def run(self): nums = range(5) global queue while True: condition.acquire() if len(queue) == MAX_NUM: print "Queue full, producer is waiting" condition.wait() print "Space in queue, Consumer notified the producer" num = random.choice(nums) queue.append(num) print "Produced", num condition.notify() condition.release() time.sleep(random.random())class ConsumerThread(Thread): def run(self): global queue while True: condition.acquire() if not queue: print "Nothing in queue, consumer is waiting" condition.wait() print "Producer added something to queue and notified the consumer" num = queue.pop(0) print "Consumed", num condition.notify() condition.release() time.sleep(random.random())ProducerThread().start()ConsumerThread().start() 上面例子中使用了 Condition，Condition（条件变量）通常与一个锁关联。需要在多个 Contidion 中共享一个锁时，可以传递一个 Lock/RLock 实例给构造方法，否则它将自己生成一个RLock实例。 可以认为，除了 Lock 带有的锁定池外，Condition 还包含一个等待池，池中的线程处于状态图中的等待阻塞状态，直到另一个线程调用 notify()/notifyAll() 通知；得到通知后线程进入锁定池等待锁定。 构造方法： 1Condition([lock/rlock]) 实例方法： acquire([timeout])/release()：调用关联的锁的相应方法。 wait([timeout])：调用这个方法将使线程进入 Condition 的等待池等待通知，并释放锁。使用前线程必须已获得锁定，否则将抛出异常。 notify()：调用这个方法将从等待池挑选一个线程并通知，收到通知的线程将自动调用 acquire() 尝试获得锁定（进入锁定池）；其他线程仍然在等待池中。调用这个方法不会释放锁定。使用前线程必须已获得锁定，否则将抛出异常。 notifyAll()：调用这个方法将通知等待池中所有的线程，这些线程都将进入锁定池尝试获得锁定。调用这个方法不会释放锁定。使用前线程必须已获得锁定，否则将抛出异常。 例子5中生产者和消费者共享一个 list 集合，其实也可以换成 queue。 例子6： 12345678910111213141516171819202122232425262728from threading import Threadimport timeimport randomfrom Queue import Queuequeue = Queue(3)class ProducerThread(Thread): def run(self): nums = range(5) global queue while True: num = random.choice(nums) queue.put(num) print "Produced", num time.sleep(random.random())class ConsumerThread(Thread): def run(self): global queue while True: num = queue.get() queue.task_done() print "Consumed", num time.sleep(random.random())ProducerThread().start()ConsumerThread().start() 解释： 在原来使用 list 的位置，改为使用 Queue 实例（下称队列）。 这个队列有一个 condition ，它有自己的 lock。如果你使用 Queue，你不需要为 condition 和 lock 而烦恼。 生产者调用队列的 put 方法来插入数据。 put() 在插入数据前有一个获取 lock 的逻辑。 同时，put() 也会检查队列是否已满。如果已满，它会在内部调用 wait()，生产者开始等待。 消费者使用get方法。 get() 从队列中移出数据前会获取 lock。 get() 会检查队列是否为空，如果为空，消费者进入等待状态。 get() 和 put() 都有适当的 notify()。 总结本文主要介绍了 Python 中创建多线程的两种方法，并简单说了 threading 模块中的 Lock、RLock、Condition 三个类以及 Queue 类的使用方法，另外，还通过代码实现了生产者和消费者模型。 参考文章 Python多线程 Python中的生产者消费者问题]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JPA的使用]]></title>
    <url>%2F2014%2F12%2F02%2Fsome-usages-of-jpa%2F</url>
    <content type="text"><![CDATA[JPA，Java 持久化规范，是从EJB2.x以前的实体 Bean 分离出来的，EJB3 以后不再有实体 bean，而是将实体 bean 放到 JPA 中实现。 JPA 是 sun 提出的一个对象持久化规范，各 JavaEE 应用服务器自主选择具体实现，JPA 的设计者是 Hibernate 框架的作者，因此Hibernate作为Jboss服务器中JPA的默认实现，Oracle的Weblogic使用EclipseLink(以前叫TopLink)作为默认的JPA实现，IBM的Websphere和Sun的Glassfish默认使用OpenJPA(Apache的一个开源项目)作为其默认的JPA实现。 JPA 的底层实现是一些流行的开源 ORM 框架，因此JPA其实也就是java实体对象和关系型数据库建立起映射关系，通过面向对象编程的思想操作关系型数据库的规范。 1. JPA 历史早期版本的EJB，定义持久层结合使用 javax.ejb.EntityBean 接口作为业务逻辑层。 同时引入 EJB3.0 的持久层分离，并指定为JPA1.0（Java持久性API）。这个API规范随着 JAVA EE5 对2006年5月11日使用JSR220规范发布。 JPA2.0 的JAVA EE 6规范发布于2009年12月10日并成 Java Community Process JSR317 的一部分。 JPA2.1 使用 JSR338 的 JAVA EE7的规范发布于2013年4月22日。 2. JPA 架构下图显示了JPA核心类和JPA接口。 类或接口 描述 EntityManagerFactory 这是一个 EntityManager 的工厂类。它创建并管理多个 EntityManager 实例。 EntityManager 这是一个接口，它管理的持久化操作的对象。它的工作原理类似工厂的查询实例。 Entity 实体是持久性对象是存储在数据库中的记录。 EntityTransaction 它与 EntityManager 是一对一的关系。对于每一个 EntityManager ，操作是由 EntityTransaction 类维护。 Persistence 这个类包含静态方法来获取 EntityManagerFactory 实例。 Query 该接口由每个 JPA 供应商，能够获得符合标准的关系对象。 在上述体系结构中，类和接口之间的关系属于javax.persistence包。下图显示了它们之间的关系。 EntityManagerFactory 和 EntityManager 的关系是1对多。这是一个工厂类 EntityManager 实例。 EntityManager 和 EntityTransaction 之间的关系是1对1。对于每个 EntityManager 操作，只有一个 EntityTransaction 实例。 EntityManager 和 Query 之间的关系是1对多。查询数众多可以使用一个 EntityManager 实例执行。 EntityManager 实体之间的关系是1对多。一个 EntityManager 实例可以管理多个实体。 搭建 JPA 开发环境创建 maven 工程创建一个空的 maven 工程，然后编写 pom.xml 文件，添加下面配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778&lt;properties&gt; &lt;spring.version&gt;4.1.2.RELEASE&lt;/spring.version&gt; &lt;hibernate.version&gt;4.1.9.Final&lt;/hibernate.version&gt; &lt;hibernate-jpa.version&gt;2.0-cr-1&lt;/hibernate-jpa.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate.java-persistence&lt;/groupId&gt; &lt;artifactId&gt;jpa-api&lt;/artifactId&gt; &lt;version&gt;$&#123;hibernate-jpa.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-entitymanager&lt;/artifactId&gt; &lt;version&gt;$&#123;hibernate.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.h2database&lt;/groupId&gt; &lt;artifactId&gt;h2&lt;/artifactId&gt; &lt;version&gt;1.3.156&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建实体123456789101112131415161718192021222324252627282930313233343536373839404142package com.javachen.spring4.jpa.entity;import javax.persistence.*;import javax.validation.constraints.Max;import javax.validation.constraints.Min;import javax.validation.constraints.NotNull;import javax.validation.constraints.Size;import java.io.Serializable;@Entity@NamedQuery(query = "Select e from Person e where e.id = :id", name = "find person by id")@Table(name = "T_PERSON")public class Person implements Serializable &#123; private static final long serialVersionUID = 1L; @Id @Column(name = "PERSON_ID") @GeneratedValue private Integer id; @Column(name = "PERSON_NAME") @Size(min = 1, max = 30) @NotNull private String name; @Column(name = "AGE") @Min(1) @Max(200) @NotNull private Integer age; @Column(name = "salary") private Double salary; //省略 set、get 方法 @Override public String toString() &#123; return "Person [id=" + id + ", name=" + name + ", age=" + age + ",salary="+salary+"]"; &#125;&#125; persistence.xml在这个文件中，我们将注册数据库，并指定实体类。另外，在上述所示的包的层次结构，根据JPA的内容包含在 persistence.xml 。 在 src/main/resources/META-INF/ 目录创建一个文件，名称为 persistence.xml，内容为： 1234567891011121314151617181920212223242526&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;&lt;persistence xmlns="http://java.sun.com/xml/ns/persistence" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="2.0" xsi:schemaLocation="http://java.sun.com/xml/ns/persistence http://java.sun.com/xml/ns/persistence/persistence_2_0.xsd" &gt; &lt;persistence-unit name="db1-unit" transaction-type="RESOURCE_LOCAL"&gt; &lt;class&gt;com.javachen.spring4.jpa.entity.Person&lt;/class&gt; &lt;properties&gt; &lt;property name="hibernate.connection.driver_class" value="net.sf.log4jdbc.DriverSpy"/&gt; &lt;property name="hibernate.connection.url" value="jdbc:log4jdbc:h2:mem:example"/&gt; &lt;property name="hibernate.connection.username" value="sa"/&gt; &lt;property name="hibernate.connection.password" value=""/&gt; &lt;property name="hibernate.dialect" value="org.hibernate.dialect.H2Dialect"/&gt; &lt;property name="hibernate.jdbc.batch_size" value="30" /&gt; &lt;property name="hibernate.use_sql_comments" value="true" /&gt; &lt;property name="hibernate.hbm2ddl.auto" value="create" /&gt; &lt;property name="hibernate.show_sql" value="true" /&gt; &lt;property name="hibernate.ejb.naming_strategy" value="org.hibernate.cfg.ImprovedNamingStrategy" /&gt; &lt;property name="hibernate.connection.charSet" value="UTF-8" /&gt; &lt;property name="hibernate.current_session_context_class" value="thread"/&gt; &lt;/properties&gt; &lt;/persistence-unit&gt;&lt;/persistence&gt; 如果有多个持久化单元，则可以配置多个 persistence-unit 节点。 以下是 persistence.xml 所有配置项的一个示例说明： 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;persistence version="1.0" xmlns:persistence="http://java.sun.com/xml/ns/persistence" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://java.sun.com/xml/ns/persistence persistence_1_0.xsd "&gt; &lt;!-- Name属性用于定义持久化单元的名字 (name必选,空值也合法); transaction-type 指定事务类型(可选：JTA、RESOURCE_LOCAL)--&gt; &lt;persistence-unit name="unitName" transaction-type="JTA"&gt; &lt;!-- 描述信息.(可选) --&gt; &lt;description&gt; &lt;/description&gt; &lt;!-- javax.persistence.PersistenceProvider接口的一个实现类(可选) --&gt; &lt;provider&gt;&lt;/provider&gt; &lt;!-- Jta-data-source和 non-jta-data-source用于分别指定持久化提供商使用的JTA和/或non-JTA数据源的全局JNDI名称(可选) --&gt; &lt;jta-data-source&gt;java:/MySqlDS&lt;/jta-data-source&gt; &lt;non-jta-data-source&gt;&lt;/non-jta-data-source&gt; &lt;!-- 声明orm.xml所在位置.(可选) --&gt; &lt;mapping-file&gt;product.xml&lt;/mapping-file&gt; &lt;!-- 以包含persistence.xml的jar文件为基准的相对路径,添加额外的jar文件.(可选) --&gt; &lt;jar-file&gt;../lib/model.jar&lt;/jar-file&gt; &lt;!-- 显式列出实体类,在Java SE 环境中应该显式列出.(可选) --&gt; &lt;class&gt;com.domain.User&lt;/class&gt; &lt;!-- 声明是否扫描jar文件中标注了@Enity类加入到上下文.若不扫描,则如下:(可选) --&gt; &lt;exclude-unlisted-classes/&gt; &lt;!-- 厂商专有属性(可选) --&gt; &lt;properties&gt; &lt;!-- hibernate.hbm2ddl.auto= create-drop / create / update --&gt; &lt;property name="hibernate.hbm2ddl.auto" value="update" /&gt; &lt;property name="hibernate.show_sql" value="true" /&gt; &lt;/properties&gt; &lt;/persistence-unit&gt; &lt;/persistence&gt; 持久化操作1234567891011121314151617181920212223242526272829package com.javachen.spring4.jpa.entity;import javax.persistence.EntityManager;import javax.persistence.EntityManagerFactory;import javax.persistence.Persistence;import javax.persistence.Query;import java.util.List;public class PersonTest &#123; public static void main(String[] args) &#123; EntityManagerFactory emfactory = Persistence. createEntityManagerFactory( "db1-unit" ); EntityManager entitymanager = emfactory. createEntityManager( ); entitymanager.getTransaction( ).begin( ); Person person=new Person(); person.setAge(18); person.setSalary(121d); person.setName("zhangsan"); entitymanager.persist( person ); entitymanager.getTransaction( ).commit( ); entitymanager.close(); emfactory.close( ); &#125;&#125; 在上面的代码中 createEntityManagerFactory() 通过提供我们在 persistent.xml 文件提供持久化单元相同唯一的名称创建一个持久性单元。 EntityManagerFactory对象将由usingcreateEntityManager()方法创建entitymanger实例。 EntityManager对象创建 entitytransactioninstance 事务管理。通过使用 EntityManager 对象，我们可以持久化实体到数据库中。 3. JPQLJPQL 代表 Java 持久化查询语言。它被用来创建针对实体的查询存储在关系数据库中。 JPQL 是基于 SQL 语法的发展。但它不会直接影响到数据库。 JPQL 可以检索使用 SELECT 子句中的数据，可以使用 UPDATE 子句做批量 UPDATE 和 DELETE 子句。 3.1 标准查询结构示例代码： 123456789101112131415161718192021222324EntityManagerFactory emfactory = Persistence. createEntityManagerFactory( "db1-unit" );EntityManager entitymanager = emfactory. createEntityManager( );entitymanager.getTransaction( ).begin( );//Scalar functionQuery query = entitymanager. createQuery("Select UPPER(e.name) from Person e");List&lt;String&gt; list=query.getResultList();for(String e:list)&#123; System.out.println("Person name :"+e);&#125;//Aggregate functionQuery query1 = entitymanager. createQuery("Select MAX(e.salary) from Person e");Double result=(Double) query1.getSingleResult();System.out.println("Max Person Salary :"+result);entitymanager.close();emfactory.close( ); 3.2 命名查询@NamedQuery 注解被定义为一个预定义的查询字符串，它是不可改变的查询。@NamedQuery 注解加在实体之上，例如： 123456@Entity@NamedQuery(query = "Select e from Person e where e.id = :id", name = "find person by id")@Table(name = "T_PERSON")public class Person implements Serializable &#123;&#125; 命名查询使用方法： 1234567891011121314151617181920212223import java.util.List;import javax.persistence.EntityManager;import javax.persistence.EntityManagerFactory;import javax.persistence.Persistence;import javax.persistence.Query;import com.yiibai.eclipselink.entity.Employee;public class NamedQueries&#123; public static void main( String[ ] args )&#123; EntityManagerFactory emfactory = Persistence. createEntityManagerFactory( "db1-unit" ); EntityManager entitymanager = emfactory. createEntityManager(); Query query = entitymanager.createNamedQuery( "find person by id"); query.setParameter("id", 1); List&lt;Person&gt; list = query.getResultList( ); for( Person e:list )&#123; System.out.print("Person ID :"+e.getId( )); System.out.println("\t Person Name :"+e.getName( )); &#125; &#125;&#125; 3.3 动态查询下面使用简单的条件动态查询返回数据源中的实体类的所有实例。 1234567EntityManager em = ...;CriteriaBuilder cb = em.getCriteriaBuilder();CriteriaQuery&lt;Entity class&gt; cq = cb.createQuery(Entity.class);Root&lt;Entity&gt; from = cq.from(Entity.class);cq.select(Entity);TypedQuery&lt;Entity&gt; q = em.createQuery(cq);List&lt;Entity&gt; allitems = q.getResultList(); 查询演示了基本的步骤来创建一个标准。 EntityManager 实例被用来创建一个 CriteriaBuilder 对象。 CriteriaQuery 实例是用来创建一个查询对象。这个查询对象的属性将与该查询的细节进行修改。 CriteriaQuery.form 方法被调用来设置查询根。 CriteriaQuery.select 被调用来设置结果列表类型。 TypedQuery&lt;T&gt; 实例是用来准备一个查询执行和指定的查询结果的类型。 在 TypedQuery&lt;T&gt; 对象 getResultList 方法来执行查询。该查询返回实体的集合，结果存储在一个列表中。 4. 实体映射关系4.1 注解在实体中使用到的注解列表如下： 注解 描述 @Entity 声明类为实体或表。 @Table 声明表名。 @Basic 指定非约束明确的各个字段。 @Embedded 指定类或它的值是一个可嵌入的类的实例的实体的属性。 @Id 指定的类的属性，用于标识主键。 @GeneratedValue 指定主键生成方式，例如自动，手动，或从序列表中获得的值。 @Transient 该值永远不会存储在数据库中。 @Lob 将属性持久化为 Blob 或者 Clob 类型。 @Column 指定字段属性。 @SequenceGenerator 指定在 @GeneratedValue 注解中指定的属性的值。它创建了一个序列。 @TableGenerator 指定在 @GeneratedValue 批注指定属性的值发生器。它创造了的值生成的表。 @AccessType 这种类型的注释用于设置访问类型。 @JoinColumn 指定一个实体组织或实体的集合。这是用在多对一和一对多关联。 @UniqueConstraint 指定的字段和用于主要或辅助表的唯一约束。 @ColumnResult 参考使用 select 子句的 SQL 查询中的列名。 @ManyToMany 定义了连接表之间的多对多一对多的关系。 @ManyToOne 定义了连接表之间的多对一的关系。 @OneToMany 定义了连接表之间存在一个一对多的关系。 @OneToOne 定义了连接表之间有一个一对一的关系。 @NamedQueries 指定命名查询的列表。 @NamedQuery 指定使用静态名称的命名查询。 1、 @OneToOne： 一对一映射注解，双向的一对一关系需要在关系维护端(owner side)的 @OneToOne 注解中添加 mappedBy 属性，建表时在关系被维护端(inverse side)建立外键指向关系维护端的主键列。 用法：@OneToOne(optional=true,casecade=CasecadeType.ALL,mappedBy=”被维护端外键”) 2、 @OneToMany： 一对多映射注解，双向一对多关系中，一端是关系维护端(owner side)，只能在一端添加 mapped 属性。多端是关系被维护端(inverse side)。建表时在关系被维护端(多端)建立外键指向关系维护端(一端)的主键列。 用法： @OneToMany(mappedBy = &quot;维护端(一端)主键&quot;, cascade=CascadeType.ALL) 注意：在Hibernate中有个术语叫做维护关系反转，即由对方维护关联关系，使用 inverse=false 来表示关系维护放，在JPA的注解中，mappedBy就相当于inverse=false，即由mappedBy来维护关系。 3、＠ManyToOne： 多对一映射注解，在双向的一对多关系中，一端一方使用 @OneToMany 注解，多端的一方使用 @ManyToOne 注解。多对一注解用法很简单，它不用维护关系。 用法： @ManyToOne(optional = false, fetch = FetchType.EAGER) 4、 @ManyToMany： 多对多映射，采取中间表连接的映射策略，建立的中间关系表分别引入两边的主键作为外键，形成两个多对一关系。 双向的多对多关系中，在关系维护端(owner side)的 @ManyToMany 注解中添加 mappedBy 属性，另一方是关系的被维护端(inverse side)，关系的被维护端不能加 mappedBy 属性，建表时，根据两个多端的主键生成一个中间表，中间表的外键是两个多端的主键。 用法： 关系维护端——&gt; @ManyToMany(mappedBy=&quot;另一方的关系引用属性&quot;) 关系被维护端——&gt; @ManyToMany(cascade=CascadeType.ALL ,fetch = FetchType.Lazy) 4.2 实体关联映射策略4.2.1 一对一关联映射(1).一对一主键关联： 一对一关联映射中，主键关联策略不会在两个关联实体对应的数据库表中添加外键字段，两个实体的表公用同一个主键(主键相同)，其中一个实体的主键既是主键又是外键。 主键关联映射：在实体关联属性或方法上添加 @OneToOne 注解的同时添加 @PrimaryKeyJoinColumn 注解(在一对一注解关联映射的任意一端实体添加即可)。 (2).一对一唯一外键关联： 一对一关联关系映射中，唯一外键关联策略会在其中一个实体对应数据库表中添加外键字段指向另一个实体表的主键，也是一对一映射关系中最常用的映射策略。 唯一外键关联：在关联属性或字段上添加 @OneToOne 注解的同时添加 @JoinColumn(name=”数据表列名”，unique=true) 注解。 4.2.2 一对多关联映射在JPA中两个实体之间是一对多关系的称为一对多关联关系映射，如班级和学生关系。 (1).一对多单向关联映射： 在一对多单向关联映射中，JPA 会在数据库中自动生成公有的中间表记录关联关系的情况。在一端关联集合属性或字段上添加 @OneToMany 注解即可。 (2).一对多双向关联映射： 在一对多双向关联映射中，JPA 不会在数据库中生成公有中间表。在一端关联集合属性或字段上添加 @OneToMany 注解，同时指定其 mappedBy 属性。在多端关联属性或字段上添加 @ManyToOne 注解。 注意：一对多关系映射中，mappedBy 只能添加在 OneToMany 注解中，即在多端生成外键。 4.2.3 多对多关联映射在JPA中两个实体之间是多对多关系的称为多对多关联关系映射，如学生和教师关系。 (1).多对多单向映射： 在其中任意实体一方关联属性或字段上添加 @ManyToMany 注解。 (2).多对多双向映射： 关系维护端关联属性或字段上添加 @ManyToMany 注解，同时指定该注解的 mappedBy 属性。 关系被维护端关联属性或字段上添加 @ManyToMany 注解。 4.3 实体继承映射策略在 JPA 中，实体继承关系的映射策略共有三种：单表继承策略、Joined 策略和 TABLE_PER_CLASS 策略。 4.3.1 单表继承策略：单表继承策略，父类实体和子类实体共用一张数据库表，在表中通过一列辨别字段来区别不同类别的实体。具体做法如下： a.在父类实体的 @Entity 注解下添加如下的注解： 123@Inheritance(Strategy=InheritanceType.SINGLE_TABLE) @DiscriminatorColumn(name="辨别字段列名") @DiscriminatorValue(父类实体辨别字段列值) b.在子类实体的 @Entity 注解下添加如下的注解： 1@DiscriminatorValue(子类实体辨别字段列值) 4.3.2 Joined 策略：Joined 策略，父类实体和子类实体分别对应数据库中不同的表，子类实体的表中只存在其扩展的特殊属性，父类的公共属性保存在父类实体映射表中。 具体做法：只需在父类实体的 @Entity 注解下添加如下注解： 1@Inheritance(Strategy=InheritanceType.JOINED) 子类实体不需要特殊说明。 4.3.3 TABLE_PER_CLASS 策略：TABLE_PER_CLASS 策略，父类实体和子类实体每个类分别对应一张数据库中的表，子类表中保存所有属性，包括从父类实体中继承的属性。 具体做法：只需在父类实体的 @Entity 注解下添加如下注解： 1@Inheritance(Strategy=InheritanceType.TABLE_PER_CLASS) 子类实体不需要特殊说明。 5. 参考文章 [1] JPA教程 [2] JPA学习笔记1——JPA基础]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群部署权限总结]]></title>
    <url>%2F2014%2F11%2F25%2Fquikstart-for-config-kerberos-ldap-and-sentry-in-hadoop%2F</url>
    <content type="text"><![CDATA[这是一篇总结的文章，主要介绍 Hadoop 集群快速部署权限的步骤以及一些注意事项。如果你想了解详细的过程，请参考本博客中其他的文章。 1. 开始之前hadoop 集群一共有三个节点，每个节点的 ip、hostname、角色如下： 123192.168.56.121 cdh1 NameNode、kerberos-server、ldap-server、sentry-store192.168.56.122 cdh2 DataNode、yarn、hive、impala192.168.56.123 cdh3 DataNode、yarn、hive、impala 一些注意事项： 操作系统为 CentOs6.2 Hadoop 版本为 CDH5.2 hostname 请使用小写，因为 kerberos 中区分大小写，而 hadoop 中会使用 hostname 的小写替换 _HOST，impala 直接使用 hostname 替换 _HOST。 开始之前，请确认 hadoop 集群部署安装成功，不管是否配置 HA，请规划好每个节点的角色。我这里为了简单，以三个节点的集群为例做说明，你可以参考本文并结合你的实际情况做调整。 请确认防火墙关闭，以及集群内和 kerberos 以及 ldap 服务器保持时钟同步。 cdh1 为管理节点，故需要做好 cdh1 到集群所有节点的无密码登陆，包括其本身。 集群中每个节点的 hosts 如下： 123456$ cat /etc/hosts127.0.0.1 localhost192.168.56.121 cdh1192.168.56.122 cdh2192.168.56.123 cdh3 为了方便管理集群，使用 cdh1 作为管理节点，并在 /opt/shell 目录编写了几脚本，/opt/shell/cmd.sh 用于批量执行命令： 12345678$ cat /opt/shell/cmd.sh#!/bin/shfor node in 121 122 123;do echo "==============="192.168.56.$node"===============" ssh 192.168.56.$node $1done /opt/shell/cmd.sh 用于批量执行命令： 12345678$ cat /opt/shell/syn.sh#!/bin/shfor node in 121 122 123;do echo "==============="192.168.56.$node"===============" scp -r $1 192.168.56.$node:$2done /opt/shell/cluster.sh 用于批量维护集群各个服务： 123456$ cat /opt/shell/cluster.sh#!/bin/shfor node in 121 122 123;do echo "==============="192.168.56.$node"===============" ssh 192.168.56.$node 'for src in `ls /etc/init.d|grep '$1'`;do service $src '$2'; done'done 2. 安装 kerberos在 cdh1 节点修改 /etc/krb5.conf 如下： 123456789101112131415161718192021222324252627[logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log[libdefaults] default_realm = JAVACHEN.COM dns_lookup_realm = false dns_lookup_kdc = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true default_tgs_enctypes = aes256-cts-hmac-sha1-96 default_tkt_enctypes = aes256-cts-hmac-sha1-96 permitted_enctypes = aes256-cts-hmac-sha1-96 clockskew = 120 udp_preference_limit = 1[realms] JAVACHEN.COM = &#123; kdc = cdh1 admin_server = cdh1 &#125;[domain_realm] .javachen.com = JAVACHEN.COM javachen.com = JAVACHEN.COM 修改 /var/kerberos/krb5kdc/kdc.conf 如下： 123456789101112131415[kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88[realms] JAVACHEN.COM = &#123; #master_key_type = aes256-cts acl_file = /var/kerberos/krb5kdc/kadm5.acl dict_file = /usr/share/dict/words max_renewable_life = 7d max_life = 1d admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal default_principal_flags = +renewable, +forwardable &#125; 修改 /var/kerberos/krb5kdc/kadm5.acl 如下： 1*/admin@JAVACHEN.COM * 将 cdh1 上的 /etc/krb5.conf 同步到集群各个节点上： 1sh /opt/shell/syn.sh /etc/krb5.conf /etc/krb5.conf 写了一个脚本安装和初始化 kerberos，供大家参考（详细的脚本，请参考 install_kerberos.sh 和 init_kerberos.sh ）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445# install the kerberos componentsyum install -y krb5-serveryum install -y openldap-clientsyum install -y krb5-workstationrm -rf /var/kerberos/krb5kdc/*.keytab /var/kerberos/krb5kdc/prin*kdb5_util create -r JAVACHEN.COM -schkconfig --level 35 krb5kdc onchkconfig --level 35 kadmin onservice krb5kdc restartservice kadmin restartecho -e "root\nroot" | kadmin.local -q "addprinc root/admin"DNS=JAVACHEN.COMHOSTNAME=`hostname -i`#读取/etc/host文件中ip为 192.168.56 开头的机器名称并排除自己（kerberos 服务器）for host in `cat /etc/hosts|grep 192.168.56|grep -v $HOSTNAME|awk '&#123;print $2&#125;'` ;do for user in hdfs; do kadmin.local -q "addprinc -randkey $user/$host@$DNS" kadmin.local -q "xst -k /var/kerberos/krb5kdc/$user-un.keytab $user/$host@$DNS" done for user in HTTP hive yarn mapred impala zookeeper zkcli hbase llama sentry solr hue; do kadmin.local -q "addprinc -randkey $user/$host@$DNS" kadmin.local -q "xst -k /var/kerberos/krb5kdc/$user.keytab $user/$host@$DNS" donedone# 合并cd /var/kerberos/krb5kdc/echo -e "rkt hdfs-un.keytab\nrkt HTTP.keytab\nwkt hdfs.keytab" | ktutil#kerberos 重新初始化之后，还需要添加下面代码用于集成 ldapkadmin.local -q "addprinc ldapadmin@JAVACHEN.COM"kadmin.local -q "addprinc -randkey ldap/cdh1@JAVACHEN.COM"kadmin.local -q "ktadd -k /etc/openldap/ldap.keytab ldap/cdh1@JAVACHEN.COM"/etc/init.d/slapd restart#测试 ldap 是否可以正常使用ldapsearch -x -b 'dc=javachen,dc=com' 运行上面的脚本，然后将上面生成的 keytab 同步到其他节点并设置权限： 123456789101112131415161718sh /opt/shell/syn.sh /opt/keytab/hdfs.keytab /etc/hadoop/conf/sh /opt/shell/syn.sh /opt/keytab/mapred.keytab /etc/hadoop/conf/sh /opt/shell/syn.sh /opt/keytab/yarn.keytab /etc/hadoop/conf/sh /opt/shell/syn.sh /opt/keytab/hive.keytab /etc/hive/conf/sh /opt/shell/syn.sh /opt/keytab/impala.keytab /etc/impala/conf/sh /opt/shell/syn.sh /opt/keytab/zookeeper.keytab /etc/zookeeper/conf/sh /opt/shell/syn.sh /opt/keytab/zkcli.keytab /etc/zookeeper/conf/sh /opt/shell/syn.sh /opt/keytab/sentry.keytab /etc/sentry/conf/sh /opt/shell/cmd.sh "chown hdfs:hadoop /etc/hadoop/conf/hdfs.keytab ;chmod 400 /etc/hadoop/conf/*.keytab"sh /opt/shell/cmd.sh "chown mapred:hadoop /etc/hadoop/conf/mapred.keytab ;chmod 400 /etc/hadoop/conf/*.keytab"sh /opt/shell/cmd.sh "chown yarn:hadoop /etc/hadoop/conf/yarn.keytab ;chmod 400 /etc/hadoop/conf/*.keytab"sh /opt/shell/cmd.sh "chown hive:hadoop /etc/hive/conf/hive.keytab ;chmod 400 /etc/hive/conf/*.keytab"sh /opt/shell/cmd.sh "chown impala:hadoop /etc/impala/conf/impala.keytab ;chmod 400 /etc/impala/conf/*.keytab"sh /opt/shell/cmd.sh "chown zookeeper:hadoop /etc/zookeeper/conf/*.keytab ;chmod 400 /etc/zookeeper/conf/*.keytab"# sentry 只安装在 cdh1 节点chown sentry:hadoop /etc/sentry/conf/*.keytab ;chmod 400 /etc/sentry/conf/*.keytab 在集群中每个节点安装 kerberos 客户端： 1sh /opt/shell/cmd.sh "yum install krb5-workstation -y" 批量获取 root/admin 用户的 ticket 1sh /opt/shell/cmd.sh "echo root|kinit root/admin" 3. hadoop 集成 kerberos更新每个节点上的 JCE 文件并修改 /etc/default/hadoop-hdfs-datanode，并且修改 hdfs、yarn、mapred、hive 的配置文件。 如果配置了 HA，则先配置 zookeeper 集成 kerberos。 同步配置文件： 123456sh /opt/shell/syn.sh /etc/hadoop/conf /etc/hadoopsh /opt/shell/syn.sh /etc/zookeeper/conf /etc/zookeepersh /opt/shell/cmd.sh "cd /etc/hadoop/conf/; chown root:yarn container-executor.cfg ; chmod 400 container-executor.cfg"sh /opt/shell/syn.sh /etc/hive/conf /etc/hive 接下来就是依次获取每个服务对应的 ticket 并启动对应的服务，我创建了一个脚本 /opt/shell/manager_cluster.sh 来做这件事： 123456789101112131415161718192021222324252627282930313233343536373839404142434445#!/bin/bashrole=$1dir=$rolecommand=$2if [ X"$role" == X"hdfs" ];then dir=hadoopfiif [ X"$role" == X"yarn" ];then dir=hadoopfiif [ X"$role" == X"mapred" ];then dir=hadoopfiecho $dir $role $commandfor node in 121 122 123 ;do echo "========192.168.56.$node========" ssh 192.168.56.$node ' host=`hostname -f| tr "[:upper:]" "[:lower:]"` path="'$role'/$host" #echo $path principal=`klist -k /etc/'$dir'/conf/'$role'.keytab | grep $path | head -n1 | cut -d " " -f5` echo $principal if [ X"$principal" == X ]; then principal=`klist -k /etc/'$dir'/conf/'$role'.keytab | grep $path | head -n1 | cut -d " " -f4` echo $principal if [ X"$principal" == X ]; then echo "Failed to get hdfs Kerberos principal" exit 1 fi fi kinit -r 24l -kt /etc/'$dir'/conf/'$role'.keytab $principal if [ $? -ne 0 ]; then echo "Failed to login as hdfs by kinit command" exit 1 fi kinit -R for src in `ls /etc/init.d|grep '$role'`;do service $src '$command'; done 'done 启动命令： 12345678910111213141516# 启动 zookeepersh /opt/shell/manager_cluster.sh zookeeper restart# 获取 hdfs 服务的 ticketsh /opt/shell/manager_cluster.sh hdfs status# 使用普通脚本依次启动 hadoop-hdfs-zkfc、hadoop-hdfs-journalnode、hadoop-hdfs-namenode、hadoop-hdfs-datanodesh /opt/shell/cluster.sh hadoop-hdfs-zkfc restartsh /opt/shell/cluster.sh hadoop-hdfs-journalnode restartsh /opt/shell/cluster.sh hadoop-hdfs-namenode restartsh /opt/shell/cluster.sh hadoop-hdfs-datanode restartsh /opt/shell/manager_cluster.sh yarn restartsh /opt/shell/manager_cluster.sh mapred restartsh /opt/shell/manager_cluster.sh hive restart 修改 impala 配置文件并同步到其他节点，然后启动 impala 服务： 1234567\cp /etc/hadoop/conf/core-site.xml /etc/impala/conf/\cp /etc/hadoop/conf/hdfs-site.xml /etc/impala/conf/\cp /etc/hive/conf/hive-site.xml /etc/impala/conf/sh /opt/shell/syn.sh /etc/impala/conf /etc/impala/sh /opt/shell/syn.sh /etc/default/impala /etc/default/impalash /opt/shell/manager_cluster.sh impala restart 到此，集群应该启动成功了。 3 使用 java 代码测试 kerberos在 hdfs 中集成 kerberos 之前，可以先使用下面代码(Krb.java)进行测试： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import com.sun.security.auth.module.Krb5LoginModule;import javax.security.auth.Subject;import java.io.File;import java.io.FileInputStream;import java.io.InputStream;import java.util.HashMap;import java.util.Map;import java.util.Properties;public class Krb &#123; private void loginImpl(final String propertiesFileName) throws Exception &#123; System.out.println("NB: system property to specify the krb5 config: [java.security.krb5.conf]"); //System.setProperty("java.security.krb5.conf", "/etc/krb5.conf"); System.out.println(System.getProperty("java.version")); System.setProperty("sun.security.krb5.debug", "true"); final Subject subject = new Subject(); final Krb5LoginModule krb5LoginModule = new Krb5LoginModule(); final Map&lt;String,String&gt; optionMap = new HashMap&lt;String,String&gt;(); if (propertiesFileName == null) &#123; //optionMap.put("ticketCache", "/tmp/krb5cc_1000"); optionMap.put("keyTab", "/etc/krb5.keytab"); optionMap.put("principal", "foo"); // default realm optionMap.put("doNotPrompt", "true"); optionMap.put("refreshKrb5Config", "true"); optionMap.put("useTicketCache", "true"); optionMap.put("renewTGT", "true"); optionMap.put("useKeyTab", "true"); optionMap.put("storeKey", "true"); optionMap.put("isInitiator", "true"); &#125; else &#123; File f = new File(propertiesFileName); System.out.println("======= loading property file ["+f.getAbsolutePath()+"]"); Properties p = new Properties(); InputStream is = new FileInputStream(f); try &#123; p.load(is); &#125; finally &#123; is.close(); &#125; optionMap.putAll((Map)p); &#125; optionMap.put("debug", "true"); // switch on debug of the Java implementation krb5LoginModule.initialize(subject, null, new HashMap&lt;String,String&gt;(), optionMap); boolean loginOk = krb5LoginModule.login(); System.out.println("======= login: " + loginOk); boolean commitOk = krb5LoginModule.commit(); System.out.println("======= commit: " + commitOk); System.out.println("======= Subject: " + subject); &#125; public static void main(String[] args) throws Exception &#123; System.out.println("A property file with the login context can be specified as the 1st and the only paramater."); final Krb krb = new Krb(); krb.loginImpl(args.length == 0 ? null : args[0]); &#125;&#125; 创建一个配置文件krb5.properties： 12345678910keyTab=/etc/hadoop/conf/hdfs.keytabprincipal=hdfs/cdh1@JAVACHEN.COMdoNotPrompt=truerefreshKrb5Config=trueuseTicketCache=truerenewTGT=trueuseKeyTab=truestoreKey=trueisInitiator=true 编译 java 代码并运行： 12345678# 先销毁当前 ticket$ kdestroy$ javac Krb.java$ java -cp . Krb ./krb5.properties 4. 安装 ldap使用下面命令在 cdh1 节点快速安装 ldap-server： 12345678910111213141516171819202122232425262728293031323334353637yum install db4 db4-utils db4-devel cyrus-sasl* krb5-server-ldap -yyum install openldap openldap-servers openldap-clients openldap-devel compat-openldap -y# 更新配置库：rm -rf /var/lib/ldap/*cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIGchown -R ldap.ldap /var/lib/ldap# 备份原来的 slapd-confcp -rf /etc/openldap/slapd.d /etc/openldap/slapd.d.bakcp /usr/share/doc/krb5-server-ldap-1.10.3/kerberos.schema /etc/openldap/schema/touch /etc/openldap/slapd.confecho "include /etc/openldap/schema/corba.schemainclude /etc/openldap/schema/core.schemainclude /etc/openldap/schema/cosine.schemainclude /etc/openldap/schema/duaconf.schemainclude /etc/openldap/schema/dyngroup.schemainclude /etc/openldap/schema/inetorgperson.schemainclude /etc/openldap/schema/java.schemainclude /etc/openldap/schema/misc.schemainclude /etc/openldap/schema/nis.schemainclude /etc/openldap/schema/openldap.schemainclude /etc/openldap/schema/ppolicy.schemainclude /etc/openldap/schema/collective.schemainclude /etc/openldap/schema/kerberos.schema" &gt; /etc/openldap/slapd.confecho -e "pidfile /var/run/openldap/slapd.pid\nargsfile /var/run/openldap/slapd.args" &gt;&gt; /etc/openldap/slapd.confslaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.dchown -R ldap:ldap /etc/openldap/slapd.d &amp;&amp; chmod -R 700 /etc/openldap/slapd.d#重启服务chkconfig --add slapdchkconfig --level 345 slapd on/etc/init.d/slapd restart 集成 kerberos： 123456789# 创建管理员用户kadmin.local -q "addprinc ldapadmin@JAVACHEN.COM"kadmin.local -q "addprinc -randkey ldap/cdh1@JAVACHEN.COM"rm -rf /etc/openldap/ldap.keytabkadmin.local -q "ktadd -k /etc/openldap/ldap.keytab ldap/cdh1@JAVACHEN.COM"chown -R ldap:ldap /etc/openldap/ldap.keytab/etc/init.d/slapd restart 创建 modify.ldif 文件用于更新数据库： 12345678910111213141516171819202122232425262728dn: olcDatabase=&#123;2&#125;bdb,cn=configchangetype: modifyreplace: olcSuffixolcSuffix: dc=javachen,dc=comdn: olcDatabase=&#123;2&#125;bdb,cn=configchangetype: modifyreplace: olcRootDN# Temporary lines to allow initial setupolcRootDN: uid=ldapadmin,ou=people,dc=javachen,dc=comdn: olcDatabase=&#123;2&#125;bdb,cn=configchangetype: modifyadd: olcRootPWolcRootPW: secretdn: cn=configchangetype: modifyadd: olcAuthzRegexpolcAuthzRegexp: uid=([^,]*),cn=GSSAPI,cn=auth uid=$1,ou=people,dc=javachen,dc=comdn: olcDatabase=&#123;2&#125;bdb,cn=configchangetype: modifyadd: olcAccess# Everyone can read everythingolcAccess: &#123;0&#125;to dn.base=&quot;&quot; by * read# The ldapadm dn has full write accessolcAccess: &#123;1&#125;to * by dn=&quot;uid=ldapadmin,ou=people,dc=javachen,dc=com&quot; write by * read 运行下面命令更新数据库： 1ldapmodify -Y EXTERNAL -H ldapi:/// -f modify.ldif 添加用户和组，创建 setup.ldif 如下： 123456789101112131415161718192021222324252627dn: dc=javachen,dc=comobjectClass: topobjectClass: dcObjectobjectclass: organizationo: javachen comdc: javachendn: ou=people,dc=javachen,dc=comobjectclass: organizationalUnitou: peopledescription: Usersdn: ou=group,dc=javachen,dc=comobjectClass: organizationalUnitou: groupdn: uid=ldapadmin,ou=people,dc=javachen,dc=comobjectClass: inetOrgPersonobjectClass: posixAccountobjectClass: shadowAccountcn: LDAP admin accountuid: ldapadminsn: ldapadminuidNumber: 1001gidNumber: 100homeDirectory: /home/ldaploginShell: /bin/bash 运行下面命令导入到数据库： 1ldapadd -x -D "uid=ldapadmin,ou=people,dc=javachen,dc=com" -w secret -f setup.ldif 接下来，可以在 ldap 服务器上创建一些本地系统用户，然后将这些用户导入到 ldap 服务中。 先安装 migrationtools 然后修改 /usr/share/migrationtools/migrate_common.ph 文件中的 defalut DNS domain 和 defalut base。 123456789101112131415161718# 创建 admin 组groupadd admin# 创建 test 和 hive 用户，用于后面测试 sentryuseradd test hiveusermod -G admin testusermod -G admin hive# 将关键用户导入到 ldapgrep -E "bi_|hive|test" /etc/passwd &gt;/opt/passwd.txt/usr/share/migrationtools/migrate_passwd.pl /opt/passwd.txt /opt/passwd.ldifldapadd -x -D "uid=ldapadmin,ou=people,dc=javachen,dc=com" -w secret -f /opt/passwd.ldif# 将 admin 组导入到 ldapgrep -E "admin" /etc/group &gt;/opt/group.txt/usr/share/migrationtools/migrate_group.pl /opt/group.txt /opt/group.ldifldapadd -x -D "uid=ldapadmin,ou=people,dc=javachen,dc=com" -w secret -f /opt/group.ldif 然后，你可以依次为每个用户设置密码，使用下面命令： 1ldappasswd -x -D 'uid=ldapadmin,ou=people,dc=javachen,dc=com' -w secret "uid=hive,ou=people,dc=javachen,dc=com" -S 另外，这些用户和组都是存在于 ldap 服务器上的，需要将其远程挂载到 hadoop 的每个节点上，否则，你需要在每个节点创建对应的用户和组（目前，测试是这样的）。 6. 集成 sentry这部分建议使用数据库的方式存储规则，不建议生产环境使用文件保存方式。 详细的配置，请参考 Impala和Hive集成Sentry 通过 beeline 使用 hive/cdh1@JAVACHEN.COM 连接 hive-server2 创建一些角色和组： 123456789create role admin_role;GRANT ALL ON SERVER server1 TO ROLE admin_role;GRANT ROLE admin_role TO GROUP admin;GRANT ROLE admin_role TO GROUP hive;create role test_role;GRANT ALL ON DATABASE testdb TO ROLE test_role;GRANT ALL ON DATABASE default TO ROLE test_role;GRANT ROLE test_role TO GROUP test; 上面 amdin 和 hive 组具有所有数据库的管理员权限，而 test 组只有 testdb 和 default 库的读写权限。 在 impala-shell 中通过 ldap 的方式传入不同的用户，可以测试读写权限。 7. 如何添加新用户并设置权限？下面以 test2 账号为例，说明如何添加新的用户并设置访问权限。test2 需要具有以下权限 dw_default 库：读权限 dw_user 库 t1表：读权限 dw_user 库 t2 表：读权限 在 LDAP 服务器上 上添加 LDAP 用户并设置密码，首先添加系统用户： 1useradd test2 然后使用 LDAP 工具将该用户导入到 LDAP： 123grep -E "test2" /etc/passwd &gt;/opt/passwd.txt/usr/share/migrationtools/migrate_passwd.pl /opt/passwd.txt /opt/passwd.ldifldapadd -x -D "uid=ldapadmin,ou=people,dc=javachen,dc=com" -w secret -f /opt/passwd.ldif 给 test2 用户生成一个随机密码，然后修改 LDAP 中 test2 的密码： 1ldappasswd -x -D 'uid=ldapadmin,ou=people,dc=javachen,dc=com' -w secret "uid=test2,ou=people,dc=javachen,dc=com" -S 在每台datanode机器上创建 test2 用户和 secure_analyst 分组，test2 属于 secure_analyst 分组： 1sh /opt/shell/cmd.sh "groupadd secure_analyst ; useradd test2; usermod -G secure_analyst,test2 test2" 在 hive 中创建角色： 运行 beeline -u &quot;jdbc:hive2://cdh1:10000/default;principal=hive/cdh1@JAVACHEN.COM&quot;，然后输入下面语句在 sentry 中创建角色和授予权限给 secure_analyst 组： 12345678910111213create role dw_default_r;GRANT SELECT ON DATABASE dw_default TO ROLE dw_default_r; create role dw_user;GRANT SELECT ON DATABASE dw_user TO ROLE dw_user_r; use dw_user;create role dw_user_secure_r;GRANT SELECT ON table t1 TO ROLE dw_user_secure_r;GRANT SELECT ON table t2 TO ROLE dw_user_secure_r; GRANT ROLE dw_default_r TO GROUP secure_analyst;GRANT ROLE dw_user_secure_r TO GROUP secure_analyst; 然后，需要 impala 刷新元数据，然后进行测试，可能会需要一些时间 impala-catalog 才能刷新过来。 最后进行测试，这部分略。]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>kerberos</tag>
        <tag>ldap</tag>
        <tag>sentry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring集成JPA2.0]]></title>
    <url>%2F2014%2F11%2F24%2Fspring-with-jpa2%2F</url>
    <content type="text"><![CDATA[JPA 全称 Java Persistence API，是Java EE 5标准之一，是一个 ORM 规范，由厂商来实现该规范，目前有 Hibernate、OpenJPA、TopLink、EclipseJPA 等实现。Spring目前提供集成Hibernate、OpenJPA、TopLink、EclipseJPA四个JPA标准实现。 1. 集成方式Spring提供三种方法集成JPA： LocalEntityManagerFactoryBean：适用于那些仅使用JPA进行数据访问的项目。 从JNDI中获取：用于从Java EE服务器中获取指定的EntityManagerFactory，这种方式在Spring事务管理时一般要使用JTA事务管理。 LocalContainerEntityManagerFactoryBean：适用于所有环境的FactoryBean，能全面控制EntityManagerFactory配置，非常适合那种需要细粒度定制的环境。 1.1 LocalEntityManagerFactoryBean仅在简单部署环境中只使用这种方式，比如独立的应用程序和集成测试。该 FactoryBean 根据 JPA PersistenceProvider自动检测配置文件进行工作，一般从 META-INF/persistence.xml 读取配置信息。这种方式最简单，但是不能设置 Spring 中定义的 DataSource，且不支持 Spring 管理的全局事务，甚至，持久化类的织入（字节码转换）也是特定于提供者的，经常需要在启动时指定一个特定的JVM代理。这种方法实际上只适用于独立的应用程序和测试环境，不建议使用此方式。 123&lt;bean id="entityManagerFactory" class="org.springframework.orm.jpa.LocalEntityManagerFactoryBean"&gt; &lt;property name="persistenceUnitName" value="persistenceUnit"/&gt;&lt;/bean&gt; persistenceUnit 对应 META-INF/persistence.xml 中 persistence-unit 节点的 name 属性值。 1.2 JNDI中获取Spring 中的配置： 123456789101112&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:jee="http://www.springframework.org/schema/jee" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/jee http://www.springframework.org/schema/jee/spring-jee-3.0.xsd"&gt; &lt;jee:jndi-lookup id="entityManagerFactory" jndi-name="persistence/persistenceUnit"/&gt;&lt;/beans&gt; 此处需要使用 jee 命名标签，且使用 &lt;jee:jndi-lookup&gt; 标签进行 JNDI 查找，jndi-name 属性用于指定 JNDI 名字。 在标准的 Java EE 5启动过程中，Java EE服务器自动检测持久化单元（例如应用程序文件包中的 META-INF/persistence.xml） ，以及J ava EE 部署描述符中定义给那些持久化单元命名上下文位置的环境的 persistence-unit-ref 项（例如 web.xml）。 在这种情况下，整个持久化单元部署，包括持久化类的织入（字码码转换）都取决于 Java EE 服务器。 JDBC DataSource 通过在 META-INF/persistence.xml 文件中的 JNDI 位置进行定义；EntityManager 事务与服务器的 JTA 子系统整合。Spring 仅仅用获得的 EntityManagerFactory ，通过依赖注入将它传递给应用程序对象，并为它管理事务（一般通过 JtaTransactionManager）。 注意，如果在同一个应用程序中使用了多个持久化单元，JNDI 获取的这种持久化单元的 bean 名称 应该与应用程序用来引用它们的持久化单元名称相符（例如 @PersistenceUnit 和 @PersistenceContext 注解）。 在部署到 Java EE 5 服务器时使用该方法。关于如何将自定义 JPA 提供者部署到服务器，以及允许使用服务器提供的缺省提供者之外的 JPA 提供者，请查看服务器文档的相关说明。 1.3 LocalContainerEntityManagerFactoryBeanLocalContainerEntityManagerFactoryBean 提供了对JPA EntityManagerFactory 的全面控制，非常适合那种需要细粒度定制的环境。LocalContainerEntityManagerFactoryBean 将基于 persistence.xml 文件创建 PersistenceUnitInfo 类，并提供 dataSourceLookup 策略和 loadTimeWeaver。 因此它可以在JNDI之外的用户定义的数据源之上工作，并控制织入流程。 Spring 中的配置： 12345&lt;bean id="entityManagerFactory" class="org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean"&gt; &lt;property name="persistenceUnitName" value="persistenceUnit" /&gt; &lt;property name="dataSource" ref="dataSource" /&gt; &lt;property name="jpaVendorAdapter" ref="jpaVendorAdapter" /&gt;&lt;/bean&gt; 这是最为强大的JPA配置方式，允许在应用程序中灵活进行本地配置。它支持连接现有JDBC DataSource ， 支持本地事务和全局事务等等。然而，它也将需求强加到了运行时环境中，例如，如果持久化提供者需要字节码转换，则必须有织入ClassLoader的能力。 注意，这个选项可能与 Java EE 5 服务器内建的 JPA 功能相冲突。因此，当运行在完全 Java EE 5 环境中时， 要考虑从 JNDI 获取 EntityManagerFactory。另一种可以替代的方法是，在 LocalContainerEntityManagerFactoryBean 定义中通过 persistenceXmlLocation 指定相关位置， 例如 META-INF/my-persistence.xml，并且只将包含该名称的描述符放在应用程序包文件中。因为 Java EE 5 服务器将只 查找默认的 META-INF/persistence.xml 文件，它会忽略这种定制的持久化单元，因而避免与前面 Spring 驱动的 JPA 配置冲突。 一个配置实例： 12345678910111213141516171819202122232425262728293031&lt;bean id="entityManagerFactory" class="org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean"&gt; &lt;property name="dataSource" ref="dataSource"/&gt; &lt;property name="persistenceXmlLocation" value="test/persistence.xml"/&gt; &lt;!-- gDickens: BOTH Persistence Unit and Packages to Scan are NOT compatible, persistenceUnit will win --&gt; &lt;property name="persistenceUnitName" value="persistenceUnit"/&gt; &lt;property name="packagesToScan" value="com.javachen.example.springmvc"/&gt; &lt;property name="persistenceProvider" ref="persistenceProvider"/&gt; &lt;property name="jpaVendorAdapter" ref="jpaVendorAdapter"/&gt; &lt;property name="jpaDialect" ref="jpaDialect"/&gt; &lt;property name="jpaPropertyMap" ref="jpaPropertyMap"/&gt;&lt;/bean&gt;&lt;util:map id="jpaPropertyMap"&gt; &lt;entry key="dialect" value="$&#123;hibernate.dialect&#125;"/&gt; &lt;entry key="hibernate.ejb.naming_strategy" value="$&#123;hibernate.ejb.naming_strategy&#125;"/&gt; &lt;entry key="hibernate.hbm2ddl.auto" value="$&#123;hibernate.hbm2ddl.auto&#125;"/&gt; &lt;entry key="hibernate.cache.use_second_level_cache" value="false"/&gt; &lt;entry key="hibernate.cache.use_query_cache" value="false"/&gt; &lt;entry key="hibernate.generate_statistics" value="false"/&gt; &lt;entry key="show_sql" value="$&#123;hibernate.show_sql&#125;"/&gt; &lt;entry key="format_sql" value="$&#123;hibernate.format_sql&#125;"/&gt;&lt;/util:map&gt;&lt;bean id="persistenceProvider" class="org.hibernate.ejb.HibernatePersistence"/&gt;&lt;bean id="jpaVendorAdapter" class="org.springframework.orm.jpa.vendor.HibernateJpaVendorAdapter"&gt; &lt;property name="generateDdl" value="false" /&gt; &lt;property name="showSql" value="false" /&gt; &lt;property name="database" value="HSQL"/&gt;&lt;/bean&gt;&lt;bean id="jpaDialect" class="org.springframework.orm.jpa.vendor.HibernateJpaDialect"/&gt; 说明： `LocalContainerEntityManagerFactoryBean：指定使用本地容器管理 EntityManagerFactory，从而进行细粒度控制； dataSource：属性指定使用 Spring 定义的数据源； persistenceXmlLocation：指定 JPA 配置文件为 test/persistence.xml，且该配置文件非常简单，具体配置完全在Spring中进行； persistenceUnitName：指定持久化单元名字，即 JPA 配置文件中指定的; packagesToScan：指定扫描哪个包下的类，当 persistenceUnitName 和 packagesToScan 属性同时存在时，会使用 persistenceUnitName 属性 persistenceProvider：指定 JPA 持久化提供商，此处使用 Hibernate 实现 HibernatePersistence类； jpaVendorAdapter：指定实现厂商专用特性，即 generateDdl= false 表示不自动生成 DDL，database= HSQL 表示使用 hsqld b数据库； jpaDialect：如果指定 jpaVendorAdapter 此属性可选，此处为 HibernateJpaDialect； jpaPropertyMap：此处指定一些属性。 处理多持久化单元对于那些依靠多个持久化单元位置(例如存放在 classpath 中的多个 jar 中)的应用程序， Spring 提供了作为中央仓库的 PersistenceUnitManager， 避免了持久化单元查找过程。缺省实现允许指定多个位置 (默认情况下 classpath 会搜索 META-INF/persistence.xml 文件)，它们会被解析然后通过持久化单元名称被获取： 123456789101112131415161718192021&lt;bean id="persistenceUnitManager" class="org.springframework.orm.jpa.persistenceunit.DefaultPersistenceUnitManager"&gt; &lt;property name="persistenceXmlLocation"&gt; &lt;list&gt; &lt;value&gt;org/springframework/orm/jpa/domain/persistence-multi.xml&lt;/value&gt; &lt;value&gt;classpath:/my/package/**/custom-persistence.xml&lt;/value&gt; &lt;value&gt;classpath*:META-INF/persistence.xml&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name="dataSources"&gt; &lt;map&gt; &lt;entry key="localDataSource" value-ref="local-db"/&gt; &lt;entry key="remoteDataSource" value-ref="remote-db"/&gt; &lt;/map&gt; &lt;/property&gt; &lt;!-- if no datasource is specified, use this one --&gt; &lt;property name="defaultDataSource" ref="remoteDataSource"/&gt;&lt;/bean&gt;&lt;bean id="entityManagerFactory" class="org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean"&gt; &lt;property name="persistenceUnitManager" ref="persistenceUnitManager"/&gt;&lt;/bean&gt;]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper配置Kerberos认证]]></title>
    <url>%2F2014%2F11%2F18%2Fconfig-kerberos-in-cdh-zookeeper%2F</url>
    <content type="text"><![CDATA[参考 使用yum安装CDH Hadoop集群 安装 hadoop 集群，集群包括三个节点，每个节点的ip、主机名和部署的组件分配如下： 123192.168.56.121 cdh1 NameNode、Hive、ResourceManager、HBase、impala-state-store、impala-catalog、Kerberos Server、zookeeper-server192.168.56.122 cdh2 DataNode、SSNameNode、NodeManager、HBase、impala-server、zookeeper-server192.168.56.123 cdh3 DataNode、HBase、NodeManager、impala-server、zookeeper-server 1. 配置 ZooKeeper Server1.1 生成 keytab在 cdh1 节点，即 KDC server 节点上执行下面命令： 123456789$ cd /var/kerberos/krb5kdc/kadmin.local -q "addprinc -randkey zookeeper/cdh1@JAVACHEN.COM "kadmin.local -q "addprinc -randkey zookeeper/cdh2@JAVACHEN.COM "kadmin.local -q "addprinc -randkey zookeeper/cdh3@JAVACHEN.COM "kadmin.local -q "xst -k zookeeper.keytab zookeeper/cdh1@JAVACHEN.COM "kadmin.local -q "xst -k zookeeper.keytab zookeeper/cdh2@JAVACHEN.COM "kadmin.local -q "xst -k zookeeper.keytab zookeeper/cdh3@JAVACHEN.COM " 拷贝 zookeeper.keytab 文件到其他节点的 /etc/zookeeper/conf 目录： 123$ scp zookeeper.keytab cdh1:/etc/zookeeper/conf$ scp zookeeper.keytab cdh2:/etc/zookeeper/conf$ scp zookeeper.keytab cdh3:/etc/zookeeper/conf 并设置权限，分别在 cdh1、cdh2、cdh3 上执行： 123$ ssh cdh1 "cd /etc/zookeeper/conf/;chown zookeeper:hadoop zookeeper.keytab ;chmod 400 *.keytab"$ ssh cdh2 "cd /etc/zookeeper/conf/;chown zookeeper:hadoop zookeeper.keytab ;chmod 400 *.keytab"$ ssh cdh3 "cd /etc/zookeeper/conf/;chown zookeeper:hadoop zookeeper.keytab ;chmod 400 *.keytab" 由于 keytab 相当于有了永久凭证，不需要提供密码(如果修改 kdc 中的 principal 的密码，则该 keytab 就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 keytab 中指定的用户身份访问 hadoop，所以 keytab 文件需要确保只对 owner 有读权限(0400) 1.2 修改 zookeeper 配置文件在 cdh1 节点上修改 /etc/zookeeper/conf/zoo.cfg 文件，添加下面内容： 12authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProviderjaasLoginRenew=3600000 将修改的上面文件同步到其他节点：cdh2、cdh3： 12$ scp /etc/zookeeper/conf/zoo.cfg cdh2:/etc/zookeeper/conf/zoo.cfg$ scp /etc/zookeeper/conf/zoo.cfg cdh3:/etc/zookeeper/conf/zoo.cfg 1.3 创建 JAAS 配置文件在 cdh1 的配置文件目录创建 jaas.conf 文件，内容如下： 12345678Server &#123; com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=&quot;/etc/zookeeper/conf/zookeeper.keytab&quot; storeKey=true useTicketCache=false principal=&quot;zookeeper/cdh1@JAVACHEN.COM&quot;;&#125;; 同样，在 cdh2 和 cdh3 节点也创建该文件，注意每个节点的 principal 有所不同。 然后，在 /etc/zookeeper/conf/ 目录创建 java.env，内容如下： 1export JVMFLAGS="-Djava.security.auth.login.config=/etc/zookeeper/conf/jaas.conf" 并将该文件同步到其他节点： 12$ scp /etc/zookeeper/conf/java.env cdh2:/etc/zookeeper/conf/java.env$ scp /etc/zookeeper/conf/java.env cdh3:/etc/zookeeper/conf/java.env 1.4 重启服务依次重启，并观察日志： 1/etc/init.d/zookeeper-server restart 2. 配置 ZooKeeper Client2.1 生成 keytab在 cdh1 节点，即 KDC server 节点上执行下面命令： 12345678$ cd /var/kerberos/krb5kdc/kadmin.local -q "addprinc -randkey zkcli/cdh1@JAVACHEN.COM "kadmin.local -q "addprinc -randkey zkcli/cdh2@JAVACHEN.COM "kadmin.local -q "addprinc -randkey zkcli/cdh3@JAVACHEN.COM "kadmin.local -q "xst -k zkcli.keytab zkcli/cdh1@JAVACHEN.COM "kadmin.local -q "xst -k zkcli.keytab zkcli/cdh2@JAVACHEN.COM "kadmin.local -q "xst -k zkcli.keytab zkcli/cdh3@JAVACHEN.COM " 拷贝 zkcli.keytab 文件到其他节点的 /etc/zookeeper/conf 目录： 123$ scp zkcli.keytab cdh1:/etc/zookeeper/conf$ scp zkcli.keytab cdh2:/etc/zookeeper/conf$ scp zkcli.keytab cdh3:/etc/zookeeper/conf 并设置权限，分别在 cdh1、cdh2、cdh3 上执行： 123$ ssh cdh1 "cd /etc/zookeeper/conf/;chown zookeeper:hadoop zkcli.keytab ;chmod 400 *.keytab"$ ssh cdh2 "cd /etc/zookeeper/conf/;chown zookeeper:hadoop zkcli.keytab ;chmod 400 *.keytab"$ ssh cdh3 "cd /etc/zookeeper/conf/;chown zookeeper:hadoop zkcli.keytab ;chmod 400 *.keytab" 由于 keytab 相当于有了永久凭证，不需要提供密码(如果修改 kdc 中的 principal 的密码，则该 keytab 就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 keytab 中指定的用户身份访问 hadoop，所以 keytab 文件需要确保只对 owner 有读权限(0400) 2.2 创建 JAAS 配置文件在 cdh1 的配置文件目录 /etc/zookeeper/conf/ 创建 client-jaas.conf 文件，内容如下： 12345678Client &#123; com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=&quot;/etc/zookeeper/conf/zkcli.keytab&quot; storeKey=true useTicketCache=false principal=&quot;zkcli@JAVACHEN.COM&quot;;&#125;; 同步到其他节点： 12$ scp client-jaas.conf cdh2:/etc/zookeeper/conf$ scp client-jaas.conf cdh3:/etc/zookeeper/conf 然后，在 /etc/zookeeper/conf/ 目录创建或者修改 java.env，内容如下： 1export CLIENT_JVMFLAGS="-Djava.security.auth.login.config=/etc/zookeeper/conf/client-jaas.conf" 如果，zookeeper-client 和 zookeeper-server 安装在同一个节点上，则 java.env 中的 java.security.auth.login.config 参数会被覆盖，这一点从 zookeeper-client 命令启动日志可以看出来。 并将该文件同步到其他节点： 12$ scp /etc/zookeeper/conf/java.env cdh2:/etc/zookeeper/conf/java.env$ scp /etc/zookeeper/conf/java.env cdh3:/etc/zookeeper/conf/java.env 2.3 验证启动客户端： 1$ zookeeper-client -server cdh1:2181 创建一个 znode 节点： 12k: cdh1:2181(CONNECTED) 0] create /znode1 sasl:zkcli@JAVACHEN.COM:cdwra Created /znode1 验证该节点是否创建以及其 ACL： 123[zk: cdh1:2181(CONNECTED) 1] getAcl /znode1 'world,'anyone : cdrwa 3 相关文章 HDFS配置Kerberos认证 YARN配置Kerberos认证 Hive配置Kerberos认证 Impala配置Kerberos认证 Zookeeper配置Kerberos认证 Hadoop配置LDAP集成Kerberos 配置安全的Hive集群集成Sentry 配置安全的Impala集群集成Sentry Hadoop集群部署权限总结]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
        <tag>kerberos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置安全的Impala集群集成Sentry]]></title>
    <url>%2F2014%2F11%2F14%2Fconfig-secured-impala-with-sentry%2F</url>
    <content type="text"><![CDATA[本文主要记录配置安全的Impala集群集成Sentry的过程。Impala集群上配置了Kerberos认证，并且需要提前配置好Hive与Kerberos和Sentry的集成： 使用yum安装CDH Hadoop集群 Hive配置kerberos认证 Impala配置kerberos认证 配置安全的Hive集群集成Sentry 1. 环境说明系统环境： 操作系统：CentOs 6.6 Hadoop版本：CDH5.4 JDK版本：1.7.0_71 运行用户：root 集群各节点角色规划为： 123192.168.56.121 cdh1 NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impala statestore、Sentry 192.168.56.122 cdh2 DataNode、NodeManager、HBase、Hiveserver2、Impala Server192.168.56.123 cdh3 DataNode、HBase、NodeManager、Hiveserver2、Impala Server 2. 修改Impala配置修改 /etc/default/impala 文件中的 IMPALA_SERVER_ARGS 参数，添加： 12-server_name=server1-sentry_config=/etc/hive/conf/sentry-site.xml 在 IMPALA_CATALOG_ARGS 中添加： 1-sentry_config=/etc/hive/conf/sentry-site.xml /etc/hive/conf/sentry-site.xml 内容如下： 1234567891011121314151617181920212223242526272829303132333435&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;sentry.service.client.server.rpc-port&lt;/name&gt; &lt;value&gt;8038&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.service.client.server.rpc-address&lt;/name&gt; &lt;value&gt;cdh1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.service.client.server.rpc-connection-timeout&lt;/name&gt; &lt;value&gt;200000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.provider&lt;/name&gt; &lt;value&gt;org.apache.sentry.provider.file.HadoopGroupResourceAuthorizationProvider&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.hive.provider.backend&lt;/name&gt; &lt;value&gt;org.apache.sentry.provider.db.SimpleDBProviderBackend&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.metastore.service.users&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt;&lt;!--queries made by hive user (beeline) skip meta store check--&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.hive.server&lt;/name&gt; &lt;value&gt;server1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.hive.testing.mode&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3. 重启Impala服务在cdh1节点 4. 测试5. 其他说明如果要使用基于文件存储的方式配置Sentry store，则需要修改 /etc/default/impala 文件中的 IMPALA_SERVER_ARGS 参数，添加： 123-server_name=server1-authorization_policy_file=/user/hive/sentry/sentry-provider.ini-authorization_policy_provider_class=org.apache.sentry.provider.file.LocalGroupResourceAuthorizationProvider 创建 sentry-provider.ini 文件并将其上传到 hdfs 的 /user/hive/sentry/ 目录： 1234567891011121314151617181920212223$ cat /tmp/sentry-provider.ini[databases]# Defines the location of the per DB policy file for the customers DB/schema#db1 = hdfs://cdh1:8020/user/hive/sentry/db1.ini[groups]admin = any_operationhive = any_operationtest = select_filtered[roles]any_operation = server=server1-&gt;db=*-&gt;table=*-&gt;action=*select_filtered = server=server1-&gt;db=filtered-&gt;table=*-&gt;action=SELECTselect_us = server=server1-&gt;db=filtered-&gt;table=events_usonly-&gt;action=SELECT[users]test = testhive= hive$ hdfs dfs -rm -r /user/hive/sentry/sentry-provider.ini$ hdfs dfs -put /tmp/sentry-provider.ini /user/hive/sentry/$ hdfs dfs -chown hive:hive /user/hive/sentry/sentry-provider.ini$ hdfs dfs -chmod 640 /user/hive/sentry/sentry-provider.ini 注意：server1 必须和 sentry-provider.ini 文件中的保持一致。 6. 参考文章 Securing Impala for analysts 7. 相关文章 HDFS配置Kerberos认证 YARN配置Kerberos认证 Hive配置Kerberos认证 Impala配置Kerberos认证 Zookeeper配置Kerberos认证 Hadoop配置LDAP集成Kerberos 配置安全的Hive集群集成Sentry 配置安全的Impala集群集成Sentry Hadoop集群部署权限总结]]></content>
      <categories>
        <category>impala</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>kerberos</tag>
        <tag>sentry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置安全的Hive集群集成Sentry]]></title>
    <url>%2F2014%2F11%2F14%2Fconfig-secured-hive-with-sentry%2F</url>
    <content type="text"><![CDATA[本文主要记录配置安全的Hive集群集成Sentry的过程。Hive上配置了Kerberos认证，配置的过程请参考： 使用yum安装CDH Hadoop集群 Hive配置kerberos认证 1. 环境说明系统环境： 操作系统：CentOs 6.6 Hadoop版本：CDH5.4 JDK版本：1.7.0_71 运行用户：root 集群各节点角色规划为： 123192.168.56.121 cdh1 NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impala statestore、Sentry 192.168.56.122 cdh2 DataNode、NodeManager、HBase、Hiveserver2、Impala Server192.168.56.123 cdh3 DataNode、HBase、NodeManager、Hiveserver2、Impala Server cdh1作为master节点，其他节点作为slave节点，我们在cdh1节点安装kerberos Server，在其他节点安装kerberos client。 2. 安装和配置Sentry这部分内容，请参考安装和配置Sentry，因为集群中配置了kerberos，所以需要在KDC节点上（cdh1）生成 Sentry 服务的 principal 并导出为 ticket： 12345678$ cd /etc/sentry/conf$ kadmin.local -q "addprinc -randkey sentry/cdh1@JAVACHEN.COM "$ kadmin.local -q "xst -k sentry.keytab sentry/cdh1@JAVACHEN.COM "$ chown sentry:hadoop sentry.keytab ; chmod 400 *.keytab$ cp sentry.keytab /etc/sentry/conf 然后，修改/etc/sentry/conf/sentry-site.xml 中下面的参数： 123456789101112&lt;property&gt; &lt;name&gt;sentry.service.security.mode&lt;/name&gt; &lt;value&gt;kerberos&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;sentry.service.server.principal&lt;/name&gt; &lt;value&gt;sentry/cdh1@JAVACHEN.COM&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;sentry.service.server.keytab&lt;/name&gt; &lt;value&gt;/etc/sentry/conf/sentry.keytab&lt;/value&gt;&lt;/property&gt; 获取Sentry的ticket再启动sentry-store服务： 12$ kinit -k -t /etc/sentry/conf/sentry.keytab sentry/cdh1@JAVACHEN.COM$ /etc/init.d/sentry-store start 3. 配置HiveHive Metastore集成Sentry需要在 /etc/hive/conf/hive-site.xml中添加： 12345678&lt;property&gt; &lt;name&gt;hive.metastore.pre.event.listeners&lt;/name&gt; &lt;value&gt;org.apache.sentry.binding.metastore.MetastoreAuthzBinding&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.event.listeners&lt;/name&gt; &lt;value&gt;org.apache.sentry.binding.metastore.SentryMetastorePostEventListener&lt;/value&gt;&lt;/property&gt; Hive-server2集成Sentry在Hive配置了Kerberos认证之后，Hive-server2集成Sentry有以下要求： 修改 /user/hive/warehouse 权限： 1234$ kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/cdh1@JAVACHEN.COM$ hdfs dfs -chmod -R 770 /user/hive/warehouse$ hdfs dfs -chown -R hive:hive /user/hive/warehouse 禁止 HiveServer2 impersonation： 1234&lt;property&gt; &lt;name&gt;hive.server2.enable.impersonation&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; 确认 /etc/hadoop/conf/container-executor.cfg 文件中 min.user.id=0。 修改 /etc/hive/conf/hive-site.xml： 12345678910111213141516&lt;property&gt; &lt;name&gt;hive.server2.enable.impersonation&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.security.authorization.task.factory&lt;/name&gt; &lt;value&gt;org.apache.sentry.binding.hive.SentryHiveAuthorizationTaskFactoryImpl&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.session.hook&lt;/name&gt; &lt;value&gt;org.apache.sentry.binding.hive.HiveAuthzBindingSessionHook&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.sentry.conf.url&lt;/name&gt; &lt;value&gt;file:///etc/hive/conf/sentry-site.xml&lt;/value&gt;&lt;/property&gt; 另外，因为集群配置了kerberos，故需要/etc/hive/conf/sentry-site.xml添加以下内容： 12345678910111213&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;property&gt; &lt;name&gt;sentry.service.security.mode&lt;/name&gt; &lt;value&gt;kerberos&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.service.server.principal&lt;/name&gt; &lt;value&gt;sentry/_HOST@JAVACHEN.COM&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.service.server.keytab&lt;/name&gt; &lt;value&gt;/etc/sentry/conf/sentry.keytab&lt;/value&gt; &lt;/property&gt; 参考模板sentry-site.xml.hive-client.template在 /etc/hive/conf/ 目录创建 sentry-site.xml： 12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;sentry.service.client.server.rpc-port&lt;/name&gt; &lt;value&gt;8038&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.service.client.server.rpc-address&lt;/name&gt; &lt;value&gt;cdh1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.service.client.server.rpc-connection-timeout&lt;/name&gt; &lt;value&gt;200000&lt;/value&gt; &lt;/property&gt; &lt;!--以下是客户端配置--&gt; &lt;property&gt; &lt;name&gt;sentry.provider&lt;/name&gt; &lt;value&gt;org.apache.sentry.provider.file.HadoopGroupResourceAuthorizationProvider&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.hive.provider.backend&lt;/name&gt; &lt;value&gt;org.apache.sentry.provider.db.SimpleDBProviderBackend&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.metastore.service.users&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt;&lt;!--queries made by hive user (beeline) skip meta store check--&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.hive.server&lt;/name&gt; &lt;value&gt;server1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.hive.testing.mode&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 注意：这里sentry.hive.provider.backend配置的是org.apache.sentry.provider.db.SimpleDBProviderBackend方式，关于org.apache.sentry.provider.file.SimpleFileProviderBackend的配置方法，后面再作说明。 hive添加对sentry的依赖，创建软连接： 1$ ln -s /usr/lib/sentry/lib/sentry-binding-hive.jar /usr/lib/hive/lib/sentry-binding-hive.jar 重启HiveServer2在cdh1上启动或重启hiveserver2： 123$ kinit -k -t /etc/hive/conf/hive.keytab hive/cdh1@JAVACHEN.COM$ /etc/init.d/hive-server2 restart 4. 准备测试数据参考 Securing Impala for analysts，准备测试数据： 123456$ cat /tmp/events.csv10.1.2.3,US,android,createNote10.200.88.99,FR,windows,updateNote10.1.2.3,US,android,updateNote10.200.88.77,FR,ios,createNote10.1.4.5,US,windows,updateTag 然后，在hive中运行下面 sql 语句： 12345678910create database sensitive;create table sensitive.events ( ip STRING, country STRING, client STRING, action STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';load data local inpath '/tmp/events.csv' overwrite into table sensitive.events;create database filtered;create view filtered.events as select country, client, action from sensitive.events;create view filtered.events_usonly as select * from filtered.events where country = 'US'; 在 cdh1上通过 beeline 连接 hiveserver2，运行下面命令创建角色和组： 1$ beeline -u "jdbc:hive2://cdh1:10001/default;principal=hive/cdh1@JAVACHEN.COM" 创建 role、group 等等，执行下面的 sql 语句： 12345678create role admin_role;GRANT ALL ON SERVER server1 TO ROLE admin_role;GRANT ROLE admin_role TO GROUP admin;GRANT ROLE admin_role TO GROUP hive;create role test_role;GRANT ALL ON DATABASE filtered TO ROLE test_role;GRANT ROLE test_role TO GROUP test; 上面创建了两个角色： admin_role，具有管理员权限，可以读写所有数据库，并授权给 admin 和 hive 组（对应操作系统上的组） test_role，只能读写 filtered 数据库，并授权给 test 组。 5. 测试使用 kerberos 测试以 test 用户为例，通过 beeline 连接 hive-server2： 12345$ su test$ kinit -k -t test.keytab test/cdh1@JAVACHEN.COM$ beeline -u "jdbc:hive2://cdh1:10001/default;principal=test/cdh1@JAVACHEN.COM" 接下来运行一些sql查询，查看是否有权限。 使用 ldap 用户测试在 ldap 服务器上创建系统用户 yy_test，并使用 migrationtools 工具将该用户导入 ldap，最后设置 ldap 中该用户密码。 12345678910# 创建 yy_test用户useradd yy_testgrep -E "yy_test" /etc/passwd &gt;/opt/passwd.txt/usr/share/migrationtools/migrate_passwd.pl /opt/passwd.txt /opt/passwd.ldifldapadd -x -D "uid=ldapadmin,ou=people,dc=lashou,dc=com" -w secret -f /opt/passwd.ldif#使用下面语句修改密码，填入上面生成的密码，输入两次：ldappasswd -x -D 'uid=ldapadmin,ou=people,dc=lashou,dc=com' -w secret "uid=yy_test,ou=people,dc=lashou,dc=com" -S 在每台 datanode 机器上创建 test 分组，并将 yy_test 用户加入到 test 分组： 1groupadd test ; useradd yy_test; usermod -G test,yy_test yy_test 运行 beeline 查看是否能够使用 ldap 用户连接 hiveserver2： 1$ beeline -u "jdbc:hive2://cdh1:10001/" -n yy_test -p yy_test -d org.apache.hive.jdbc.HiveDriver 6. 其他说明如果要使用基于文件存储的方式配置Sentry store，则需要修改/etc/hive/conf/sentry-site.xml为： 12345678910111213141516171819&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.sentry.server&lt;/name&gt; &lt;value&gt;server1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;sentry.hive.provider.backend&lt;/name&gt; &lt;value&gt;org.apache.sentry.provider.file.SimpleFileProviderBackend&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.sentry.provider&lt;/name&gt; &lt;value&gt;org.apache.sentry.provider.file.LocalGroupResourceAuthorizationProvider&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.sentry.provider.resource&lt;/name&gt; &lt;value&gt;/user/hive/sentry/sentry-provider.ini&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 创建 sentry-provider.ini 文件并将其上传到 hdfs 的 /user/hive/sentry/ 目录： 1234567891011121314151617181920212223$ cat /tmp/sentry-provider.ini[databases]# Defines the location of the per DB policy file for the customers DB/schema#db1 = hdfs://cdh1:8020/user/hive/sentry/db1.ini[groups]admin = any_operationhive = any_operationtest = select_filtered[roles]any_operation = server=server1-&gt;db=*-&gt;table=*-&gt;action=*select_filtered = server=server1-&gt;db=filtered-&gt;table=*-&gt;action=SELECTselect_us = server=server1-&gt;db=filtered-&gt;table=events_usonly-&gt;action=SELECT[users]test = testhive= hive$ hdfs dfs -rm -r /user/hive/sentry/sentry-provider.ini$ hdfs dfs -put /tmp/sentry-provider.ini /user/hive/sentry/$ hdfs dfs -chown hive:hive /user/hive/sentry/sentry-provider.ini$ hdfs dfs -chmod 640 /user/hive/sentry/sentry-provider.ini 关于 sentry-provider.ini 文件的语法说明，请参考官方文档。这里我指定了 Hive 组有全部权限，并指定 Hive 用户属于 Hive 分组，而其他两个分组只有部分权限。 7. 参考文章 Securing Impala for analysts 8. 相关文章 HDFS配置Kerberos认证 YARN配置Kerberos认证 Hive配置Kerberos认证 Impala配置Kerberos认证 Zookeeper配置Kerberos认证 Hadoop配置LDAP集成Kerberos 配置安全的Hive集群集成Sentry 配置安全的Impala集群集成Sentry Hadoop集群部署权限总结]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>kerberos</tag>
        <tag>sentry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop配置LDAP集成Kerberos]]></title>
    <url>%2F2014%2F11%2F12%2Fconfig-ldap-with-kerberos-in-cdh-hadoop%2F</url>
    <content type="text"><![CDATA[本文主要记录 cdh hadoop 集群集成 ldap 的过程，这里 ldap 安装的是 OpenLDAP 。LDAP 用来做账号管理，Kerberos作为认证。授权一般来说是由应用来决定的，通过在 LDAP 数据库中配置一些属性可以让应用程序来进行授权判断。 关于 Kerberos 的安装和 HDFS 配置 kerberos 认证，请参考 HDFS配置kerberos认证。 1. 环境说明系统环境： 操作系统：CentOs 6.6 Hadoop版本：CDH5.4 JDK版本：1.7.0_71 OpenLDAP 版本：2.4.39 Kerberos 版本：1.10.3 运行用户：root 集群各节点角色规划为： 123192.168.56.121 cdh1 NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impala statestore、Sentry 192.168.56.122 cdh2 DataNode、NodeManager、HBase、Hiveserver2、Impala Server192.168.56.123 cdh3 DataNode、HBase、NodeManager、Hiveserver2、Impala Server cdh1作为master节点，其他节点作为slave节点，我们在cdh1节点安装kerberos Server，在其他节点安装kerberos client。 2. 安装服务端2.1 安装同安装 kerberos 一样，这里使用 cdh1 作为服务端安装 openldap。 12$ yum install db4 db4-utils db4-devel cyrus-sasl* krb5-server-ldap -y$ yum install openldap openldap-servers openldap-clients openldap-devel compat-openldap -y 查看安装的版本： 12345$ rpm -qa openldapopenldap-2.4.39-8.el6.x86_64$ rpm -qa krb5-server-ldapkrb5-server-ldap-1.10.3-33.el6.x86_64 2.2 OpenSSL 如果，你不配置ssl，这部分内容可以略过，实际安装过程中，我也没有详细去操作这部分内容。 OpenLDAP 默认使用 Mozilla NSS，安装后已经生成了一份证书，可使用 certutil -d /etc/openldap/certs/ -L -n &#39;OpenLDAP Server&#39; 命令查看。使用如下命令生成RFC格式CA证书并分发到客户机待用。 12345$ certutil -d /etc/openldap/certs/ -L -a -n 'OpenLDAP Server' -f /etc/openldap/certs/password &gt; /etc/openldap/ldapCA.rfc# 拷贝到其他节点$ scp /etc/openldap/ldapCA.rfc cdh2:/tmp$ scp /etc/openldap/ldapCA.rfc cdh3:/tmp 附，生成自签名证书的命令供参考： 1$ certutil -d /etc/openldap/certs -S -n 'test cert' -x -t 'u,u,u' -s 'C=XX, ST=Default Province, L=Default City, O=Default Company Ltd, OU=Default Unit, CN=cdh1' -k rsa -v 120 -f /etc/openldap/certs/password 修改 /etc/sysconfig/ldap，开启 ldaps： 123# Run slapd with -h "... ldaps:/// ..."# yes/no, default: noSLAPD_LDAPS=yes 2.3 LDAP 服务端配置更新配置库： 123rm -rf /var/lib/ldap/*cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIGchown -R ldap.ldap /var/lib/ldap 在2.4以前的版本中，OpenLDAP 使用 slapd.conf 配置文件来进行服务器的配置，而2.4开始则使用 slapd.d 目录保存细分后的各种配置，这一点需要注意，其数据存储位置即目录 /etc/openldap/slapd.d 。尽管该系统的数据文件是透明格式的，还是建议使用 ldapadd, ldapdelete, ldapmodify 等命令来修改而不是直接编辑。 默认配置文件保存在 /etc/openldap/slapd.d，将其备份： 1cp -rf /etc/openldap/slapd.d /etc/openldap/slapd.d.bak 添加一些基本配置，并引入 kerberos 和 openldap 的 schema： 1234567891011121314151617181920212223$ cp /usr/share/doc/krb5-server-ldap-1.10.3/kerberos.schema /etc/openldap/schema/$ touch /etc/openldap/slapd.conf$ echo "include /etc/openldap/schema/corba.schemainclude /etc/openldap/schema/core.schemainclude /etc/openldap/schema/cosine.schemainclude /etc/openldap/schema/duaconf.schemainclude /etc/openldap/schema/dyngroup.schemainclude /etc/openldap/schema/inetorgperson.schemainclude /etc/openldap/schema/java.schemainclude /etc/openldap/schema/misc.schemainclude /etc/openldap/schema/nis.schemainclude /etc/openldap/schema/openldap.schemainclude /etc/openldap/schema/ppolicy.schemainclude /etc/openldap/schema/collective.schemainclude /etc/openldap/schema/kerberos.schema" &gt; /etc/openldap/slapd.conf$ echo -e "pidfile /var/run/openldap/slapd.pid\nargsfile /var/run/openldap/slapd.args" &gt;&gt; /etc/openldap/slapd.conf#更新slapd.d$ slaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.d$ chown -R ldap:ldap /etc/openldap/slapd.d &amp;&amp; chmod -R 700 /etc/openldap/slapd.d 2.4 启动服务启动 LDAP 服务： 1234chkconfig --add slapdchkconfig --level 345 slapd on/etc/init.d/slapd start 查看状态，验证服务端口： 123456$ ps aux | grep slapd | grep -v grep ldap 9225 0.0 0.2 581188 44576 ? Ssl 15:13 0:00 /usr/sbin/slapd -h ldap:/// -u ldap$ netstat -tunlp | grep :389 tcp 0 0 0.0.0.0:389 0.0.0.0:* LISTEN 8510/slapd tcp 0 0 :::389 :::* LISTEN 8510/slapd 如果启动失败，则运行下面命令来启动 slapd 服务并查看日志： 1$ slapd -h ldap://127.0.0.1 -d 481 待查明原因之后，停止该进程使用正常方式启动 slapd 服务。 2.5 LDAP 和 Kerberos在Kerberos安全机制里，一个principal就是realm里的一个对象，一个principal总是和一个密钥（secret key）成对出现的。 这个principal的对应物可以是service，可以是host，也可以是user，对于Kerberos来说，都没有区别。 Kdc(Key distribute center)知道所有principal的secret key，但每个principal对应的对象只知道自己的那个secret key。这也是 “共享密钥” 的由来。 为了使 Kerberos 能够绑定到 OpenLDAP 服务器，请创建一个管理员用户和一个 principal，并生成 keytab 文件，设置该文件的权限为 LDAP 服务运行用户可读（ LDAP 服务运行用户一般为 ldap）： 12345$ kadmin.local -q "addprinc ldapadmin@JAVACHEN.COM"$ kadmin.local -q "addprinc -randkey ldap/cdh1@JAVACHEN.COM"$ kadmin.local -q "ktadd -k /etc/openldap/ldap.keytab ldap/cdh1@JAVACHEN.COM"$ chown ldap:ldap /etc/openldap/ldap.keytab &amp;&amp; chmod 640 /etc/openldap/ldap.keytab ktadd 后面的-k 指定把 key 存放在一个本地文件中。 使用 ldapadmin 用户测试： 1kinit ldapadmin 系统会提示输入密码，如果一切正常，那么会安静的返回。实际上，你已经通过了kerberos的身份验证，且获得了一个Service TGT(Ticket-Granting Ticket). Service TGT的意义是， 在一段时间内，你都可以用此TGT去请求某些service，比如ldap service，而不需要再次通过kerberos的认证。 确保 LDAP 启动时使用上一步中创建的keytab文件，在 /etc/sysconfig/ldap 增加 KRB5_KTNAME 配置： 1export KRB5_KTNAME=/etc/openldap/ldap.keytab 然后，重启 slapd 服务。 2.6 创建数据库进入到 /etc/openldap/slapd.d 目录，查看 etc/openldap/slapd.d/cn\=config/olcDatabase={2}bdb.ldif 可以看到一些默认的配置，例如： 123olcRootDN: cn=Manager,dc=my-domain,dc=com olcRootPW: secret olcSuffix: dc=my-domain,dc=com 接下来更新这三个配置，建立 modify.ldif 文件，内容如下： 12345678910111213141516171819202122232425262728dn: olcDatabase=&#123;2&#125;bdb,cn=configchangetype: modifyreplace: olcSuffixolcSuffix: dc=javachen,dc=comdn: olcDatabase=&#123;2&#125;bdb,cn=configchangetype: modifyreplace: olcRootDN# Temporary lines to allow initial setupolcRootDN: uid=ldapadmin,ou=people,dc=javachen,dc=comdn: olcDatabase=&#123;2&#125;bdb,cn=configchangetype: modifyadd: olcRootPWolcRootPW: secretdn: cn=configchangetype: modifyadd: olcAuthzRegexpolcAuthzRegexp: uid=([^,]*),cn=GSSAPI,cn=auth uid=$1,ou=people,dc=javachen,dc=comdn: olcDatabase=&#123;2&#125;bdb,cn=configchangetype: modifyadd: olcAccess# Everyone can read everythingolcAccess: &#123;0&#125;to dn.base="" by * read# The ldapadm dn has full write accessolcAccess: &#123;1&#125;to * by dn="uid=ldapadmin,ou=people,dc=javachen,dc=com" write by * read 说明： 上面的密码使用的是明文密码 secret ，你也可以使用 slappasswd -s secret 生成的字符串作为密码。 上面的权限中指明了只有用户 uid=ldapadmin,ou=people,dc=javachen,dc=com 有写权限。 使用下面命令导入更新配置： 1$ ldapmodify -Y EXTERNAL -H ldapi:/// -f modify.ldif 这时候数据库没有数据，需要添加数据，你可以手动编写 ldif 文件来导入一些用户和组，或者使用 migrationtools 工具来生成 ldif 模板。创建 setup.ldif 文件如下： 123456789101112131415161718192021222324252627dn: dc=javachen,dc=comobjectClass: topobjectClass: dcObjectobjectclass: organizationo: javachen comdc: javachendn: ou=people,dc=javachen,dc=comobjectclass: organizationalUnitou: peopledescription: Usersdn: ou=group,dc=javachen,dc=comobjectClass: organizationalUnitou: groupdn: uid=ldapadmin,ou=people,dc=javachen,dc=comobjectClass: inetOrgPersonobjectClass: posixAccountobjectClass: shadowAccountcn: LDAP admin accountuid: ldapadminsn: ldapadminuidNumber: 1001gidNumber: 100homeDirectory: /home/ldaploginShell: /bin/bash 使用下面命令导入数据，密码是前面设置的 secret 。 1$ ldapadd -x -D "uid=ldapadmin,ou=people,dc=javachen,dc=com" -w secret -f setup.ldif 参数说明： -w 指定密码 -x 是使用一个匿名的绑定 2.7 LDAP 的使用导入系统用户接下来你可以从 /etc/passwd, /etc/shadow, /etc/groups 中生成 ldif 更新 ldap 数据库，这需要用到 migrationtools 工具。 安装： 1$ yum install migrationtools -y 利用迁移工具生成模板，先修改默认的配置： 123456$ vim /usr/share/migrationtools/migrate_common.ph#line 71 defalut DNS domain$DEFAULT_MAIL_DOMAIN = "javachen.com";#line 74 defalut base$DEFAULT_BASE = "dc=javachen,dc=com"; 生成模板文件： 1/usr/share/migrationtools/migrate_base.pl &gt; /opt/base.ldif 然后，可以修改该文件，然后执行导入命令： 1$ ldapadd -x -D "uid=ldapadmin,ou=people,dc=javachen,dc=com" -w secret -f /opt/base.ldif 将当前节点上的用户导入到 ldap 中，可以有选择的导入指定的用户： 123456# 先添加用户$ useradd test hive# 查找系统上的 test、hive 等用户$ grep -E "test|hive" /etc/passwd &gt;/opt/passwd.txt$ /usr/share/migrationtools/migrate_passwd.pl /opt/passwd.txt /opt/passwd.ldif$ ldapadd -x -D "uid=ldapadmin,ou=people,dc=javachen,dc=com" -w secret -f /opt/passwd.ldif 将用户组导入到 ldap 中： 1234# 生成用户组的 ldif 文件，然后导入到 ldap$ grep -E "test|hive" /etc/group &gt;/opt/group.txt$ /usr/share/migrationtools/migrate_group.pl /opt/group.txt /opt/group.ldif$ ldapadd -x -D "uid=ldapadmin,ou=people,dc=javachen,dc=com" -w secret -f /opt/group.ldif 查询查询新添加的 test 用户： 123456789101112$ ldapsearch -LLL -x -D 'uid=ldapadmin,ou=people,dc=javachen,dc=com' -w secret -b 'dc=javachen,dc=com' 'uid=test' dn: uid=test,ou=people,dc=javachen,dc=com objectClass: inetOrgPerson objectClass: posixAccount objectClass: shadowAccount cn: test account sn: test uid: test uidNumber: 1001 gidNumber: 100 homeDirectory: /home/test loginShell: /bin/bash 可以看到，通过指定 ‘uid=test’，我们只查询这个用户的数据，这个查询条件叫做filter。有关 filter 的使用可以查看 ldapsearch 的 manpage。 修改用户添加好以后，需要给其设定初始密码，运行命令如下： 1$ ldappasswd -x -D 'uid=ldapadmin,ou=people,dc=javachen,dc=com' -w secret "uid=test,ou=people,dc=javachen,dc=com" -S 删除删除用户或组条目： 12$ ldapdelete -x -w secret -D 'uid=ldapadmin,ou=people,dc=javachen,dc=com' "uid=test,ou=people,dc=javachen,dc=com"$ ldapdelete -x -w secret -D 'uid=ldapadmin,ou=people,dc=javachen,dc=com' "cn=test,ou=group,dc=javachen,dc=com" 3. 客户端配置在 cdh2 和 cdh3上，使用下面命令安装openldap客户端 1$ yum install openldap-clients -y 修改 /etc/openldap/ldap.conf 以下两个配置 12BASE dc=javachen,dc=comURI ldap://cdh1 然后，运行下面命令测试： 1234567#先删除 ticket$ kdestroy$ ldapsearch -b 'dc=javachen,dc=com' SASL/GSSAPI authentication started ldap_sasl_interactive_bind_s: Local error (-2) additional info: SASL(-1): generic failure: GSSAPI Error: Unspecified GSS failure. Minor code may provide more information (No credentials cache found) 重新获取 ticket： 12345678910111213$ kinit root/admin$ ldapsearch -b 'dc=javachen,dc=com' # 没有报错了$ ldapwhoami SASL/GSSAPI authentication started SASL username: root/admin@JAVACHEN.COM SASL SSF: 56 SASL installing layers dn:uid=root/admin,ou=people,dc=javachen,dc=com Result: Success (0)# 直接输入 ldapsearch 不会报错$ ldapsearch 使用 LDAP 客户端工具进行测试，这里我使用的是 LDAP Browser/Editor： 4. 配置 Hive 集成 LDAP 说明： CDH5.2 之前 hive-server2 支不支持集成 ldap，故需要升级 cdh 版本到高版本，如 cdh5.3，该版本支持 ldap。 修改配置文件这部分内容参考自Using LDAP Username/Password Authentication with HiveServer2。 我这使用的是 OpenLDAP ，故修改 hive-site.xml 配置文件如下： 123456789101112&lt;property&gt; &lt;name&gt;hive.server2.authentication&lt;/name&gt; &lt;value&gt;LDAP&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.authentication.ldap.url&lt;/name&gt; &lt;value&gt;ldap://cdh1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.authentication.ldap.baseDN&lt;/name&gt; &lt;value&gt;ou=people,dc=javachen,dc=com&lt;/value&gt;&lt;/property&gt; 为什么这样配置，可以参考 LdapAuthenticationProviderImpl.java 源码。 测试重启服务： 1/etc/init.d/hive-server2 restart 然后使用 beeline 测试： 12345beeline --verbose=truebeeline&gt; !connect jdbc:hive2://cdh1:10000/defaultConnecting to jdbc:hive2://cdh1:10000/default;Enter username for jdbc:hive2://cdh1:10000/default;: hiveEnter password for jdbc:hive2://cdh1:10000/default;: **** 5. 配置 Impala 集成 LDAP修改配置文件修改 /etc/default/impala 中的 IMPALA_SERVER_ARGS 参数，添加 123-enable_ldap_auth=true \-ldap_uri=ldaps://cdh1 \-ldap_baseDN=ou=people,dc=javachen,dc=com 注意： 如果没有开启 ssl，则添加 -ldap_passwords_in_clear_ok=true，同样如果开启了 ssl，则 ldap_uri 值为 ldaps://XXXX ldap_baseDN 的值是 ou=people,dc=javachen,dc=com，因为 impala 会将其追加到 uid={用户名}, 后面 测试重启服务： 1$ /etc/init.d/impala-server restart 然后使用 impala-shell 测试： 1234567891011$ impala-shell -l -u test Starting Impala Shell using LDAP-based authentication LDAP password for test: Connected to cdh1:21000 Server version: impalad version 2.0.0-cdh5 RELEASE (build ecf30af0b4d6e56ea80297df2189367ada6b7da7) Welcome to the Impala shell. Press TAB twice to see a list of available commands. Copyright (c) 2012 Cloudera, Inc. All rights reserved. (Shell build version: Impala Shell v2.0.0-cdh5 (ecf30af) built on Sat Oct 11 13:56:06 PDT 2014) [cdh1:21000] &gt; 使用 beeline 通过 ldap 方式来连接 jdbc 进行测试： 123456789101112131415161718$ beeline -u "jdbc:hive2://cdh1:21050/default;" -n test -p test scan complete in 2ms Connecting to jdbc:hive2://cdh1:21050/default; Connected to: Impala (version 2.0.0-cdh5) Driver: Hive JDBC (version 0.13.1-cdh5.2.0) Transaction isolation: TRANSACTION_REPEATABLE_READ Beeline version 0.13.1-cdh5.2.0 by Apache Hive 0: jdbc:hive2://cdh1:21050/default&gt;show tables; +-----------------------------+--+ | name | +-----------------------------+--+ | t1 | | tab1 | | tab2 | | tab3 | +-----------------------------+--+ 4 rows selected (0.325 seconds) 6. 参考文章 New in CDH 5.2: Impala Authentication with LDAP and Kerberos 使用 LDAP + Kerberos 实现集中用户认证及授权系统 Linux NFS服务器的安装与配置 linux的LDAP认证服务器的配置及客户端pam网络验证实例 Kerberos and LDAP RHEL6配置简单LDAP服务器 使用 LDAP 和 Kerberos kerberos与openldap整合 LDAP配置示例 centos下yum安装配置openldap 2.4.23-32外送svn的apache下配置 Integrating LDAP and Kerberos: Part Two (LDAP) Debian GNU and Ubuntu: Setting up MIT Kerberos 7 相关文章 HDFS配置Kerberos认证 YARN配置Kerberos认证 Hive配置Kerberos认证 Impala配置Kerberos认证 Zookeeper配置Kerberos认证 Hadoop配置LDAP集成Kerberos 配置安全的Hive集群集成Sentry 配置安全的Impala集群集成Sentry Hadoop集群部署权限总结]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>kerberos</tag>
        <tag>openldap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive配置Kerberos认证]]></title>
    <url>%2F2014%2F11%2F06%2Fconfig-kerberos-in-cdh-hive%2F</url>
    <content type="text"><![CDATA[1. 环境说明系统环境： 操作系统：CentOs 6.6 Hadoop版本：CDH5.4 JDK版本：1.7.0_71 运行用户：root 集群各节点角色规划为： 123192.168.56.121 cdh1 NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impala statestore、Sentry 192.168.56.122 cdh2 DataNode、SecondaryNameNode、NodeManager、HBase、Hive Server2、Impala Server192.168.56.123 cdh3 DataNode、HBase、NodeManager、Hive Server2、Impala Server cdh1作为master节点，其他节点作为slave节点，hostname 请使用小写，要不然在集成 kerberos 时会出现一些错误。 2. 生成 keytab在 cdh1 节点，即 KDC server 节点上执行下面命令： 123456789$ cd /var/kerberos/krb5kdc/kadmin.local -q "addprinc -randkey hive/cdh1@JAVACHEN.COM "kadmin.local -q "addprinc -randkey hive/cdh2@JAVACHEN.COM "kadmin.local -q "addprinc -randkey hive/cdh3@JAVACHEN.COM "kadmin.local -q "xst -k hive.keytab hive/cdh1@JAVACHEN.COM "kadmin.local -q "xst -k hive.keytab hive/cdh2@JAVACHEN.COM "kadmin.local -q "xst -k hive.keytab hive/cdh3@JAVACHEN.COM " 拷贝 hive.keytab 文件到其他节点的 /etc/hive/conf 目录 123$ scp hive.keytab cdh1:/etc/hive/conf$ scp hive.keytab cdh2:/etc/hive/conf$ scp hive.keytab cdh3:/etc/hive/conf 并设置权限，分别在 cdh1、cdh2、cdh3 上执行： 123$ ssh cdh1 "cd /etc/hive/conf/;chown hive:hadoop hive.keytab ;chmod 400 *.keytab"$ ssh cdh2 "cd /etc/hive/conf/;chown hive:hadoop hive.keytab ;chmod 400 *.keytab"$ ssh cdh3 "cd /etc/hive/conf/;chown hive:hadoop hive.keytab ;chmod 400 *.keytab" 由于 keytab 相当于有了永久凭证，不需要提供密码(如果修改 kdc 中的 principal 的密码，则该 keytab 就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 keytab 中指定的用户身份访问 hadoop，所以 keytab 文件需要确保只对 owner 有读权限(0400) 3. 修改 hive 配置文件修改 hive-site.xml，添加下面配置： 12345678910111213141516171819202122232425&lt;property&gt; &lt;name&gt;hive.server2.authentication&lt;/name&gt; &lt;value&gt;KERBEROS&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.authentication.kerberos.principal&lt;/name&gt; &lt;value&gt;hive/_HOST@JAVACHEN.COM&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.authentication.kerberos.keytab&lt;/name&gt; &lt;value&gt;/etc/hive/conf/hive.keytab&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.sasl.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.kerberos.keytab.file&lt;/name&gt; &lt;value&gt;/etc/hive/conf/hive.keytab&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.kerberos.principal&lt;/name&gt; &lt;value&gt;hive/_HOST@JAVACHEN.COM&lt;/value&gt;&lt;/property&gt; 在 core-site.xml 中添加： 123456789101112131415161718192021222324&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hive.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hive.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hdfs.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hdfs.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.HTTP.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.HTTP.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; 记住将修改的上面文件同步到其他节点：cdh2、cdh3，并再次一一检查权限是否正确。 12$ scp /etc/hive/conf/hive-site.xml cdh2:/etc/hive/conf/$ scp /etc/hive/conf/hive-site.xml cdh3:/etc/hive/conf/ 4. 启动服务启动 Hive MetaStorehive-metastore 是通过 hive 用户启动的，故在 cdh1 上先获取 hive 用户的 ticket 再启动服务： 12$ kinit -k -t /etc/hive/conf/hive.keytab hive/cdh1@JAVACHEN.COM$ service hive-metastore start 然后查看日志，确认是否启动成功。 启动 Hive Server2hive-server2 是通过 hive 用户启动的，故在 cdh2 和 cdh3 上先获取 hive 用户的 ticket 再启动服务： 12$ kinit -k -t /etc/hive/conf/hive.keytab hive/cdh1@JAVACHEN.COM$ service hive-server2 start 然后查看日志，确认是否启动成功。 5. 测试Hive CLI在没有配置 kerberos 之前，想要通过 hive 用户运行 hive 命令需要执行sudo，现在配置了 kerberos 之后，不再需要 sudo 了，hive 会通过 ticket 中的用户去执行该命令： 1234567891011$ klistTicket cache: FILE:/tmp/krb5cc_0Default principal: hdfs/dn5.h.lashou-inc.com@lashou_hadoopValid starting Expires Service principal11/06/14 11:39:09 11/07/14 11:39:09 krbtgt/lashou_hadoop@lashou_hadoop renew until 11/08/14 11:39:09Kerberos 4 ticket cache: /tmp/tkt0klist: You have no tickets cached 运行Hive cli： 12345678910111213$ hivehive&gt; set system:user.name;system:user.name=roothive&gt; create table t(id int);OKTime taken: 2.183 secondshive&gt; show tables;OKtTime taken: 1.349 secondshive&gt; select * from t;OKTime taken: 1.116 seconds 可以看到在获取了 hdfs 用户的 ticket 之后，进入 hive cli 可以执行查看表、查询数据等命令。当然，你也可以获取 hive 的 ticket 之后再来运行 hive 命令。 另外，如果你想通过普通用户来访问 hive，则需要 kerberos 创建规则和导出 ticket，然后把这个 ticket 拷贝到普通用户所在的家目录，在获取 ticket 了之后，再运行 hive 命令即可。 JDBC 客户端客户端通过 jdbc 代码连结 hive-server2： 12String url = "jdbc:hive2://cdh1:10000/default;principal=hive/cdh1@JAVACHEN.COM"Connection con = DriverManager.getConnection(url); BeelineBeeline 连结 hive-server2： 12345678910111213141516171819202122$ beelinebeeline&gt; !connect jdbc:hive2://cdh1:10000/default;principal=hive/cdh1@JAVACHEN.COMscan complete in 4msConnecting to jdbc:hive2://localhost:10000/default;principal=hive/cdh1@JAVACHEN.COM;Enter username for jdbc:hive2://localhost:10000/default;principal=hive/cdh1@JAVACHEN.COM;:Enter password for jdbc:hive2://localhost:10000/default;principal=hive/cdh1@JAVACHEN.COM;:Connected to: Apache Hive (version 0.14.0)Driver: Hive (version 0.14.0-cdh5.4.0)Transaction isolation: TRANSACTION_REPEATABLE_READ0: jdbc:hive2://cdh1:10000/default&gt; select * from t;+-------+--+| t.id |+-------+--++-------+--+No rows selected (1.575 seconds)0: jdbc:hive2://cdh1:10000/default&gt; desc t;+-----------+------------+----------+--+| col_name | data_type | comment |+-----------+------------+----------+--+| id | int | |+-----------+------------+----------+--+1 row selected (0.24 seconds) 6. 相关文章 HDFS配置Kerberos认证 YARN配置Kerberos认证 Hive配置Kerberos认证 Impala配置Kerberos认证 Zookeeper配置Kerberos认证 Hadoop配置LDAP集成Kerberos 配置安全的Hive集群集成Sentry 配置安全的Impala集群集成Sentry Hadoop集群部署权限总结]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>cdh</tag>
        <tag>hive</tag>
        <tag>kerberos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Impala配置Kerberos认证]]></title>
    <url>%2F2014%2F11%2F06%2Fconfig-kerberos-in-cdh-impala%2F</url>
    <content type="text"><![CDATA[1. 环境说明系统环境： 操作系统：CentOs 6.6 Hadoop版本：CDH5.4 JDK版本：1.7.0_71 运行用户：root 集群各节点角色规划为： 123192.168.56.121 cdh1 NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impala statestore、Sentry 192.168.56.122 cdh2 DataNode、SecondaryNameNode、NodeManager、HBase、Hive Server2、Impala Server192.168.56.123 cdh3 DataNode、HBase、NodeManager、Hive Server2、Impala Server cdh1作为master节点，其他节点作为slave节点，hostname 请使用小写，要不然在集成 kerberos 时会出现一些错误。 2. 安装必须的依赖在每个节点上运行下面的命令： 12$ yum install python-devel openssl-devel python-pip cyrus-sasl cyrus-sasl-gssapi cyrus-sasl-devel -y$ pip-python install ssl 3. 生成 keytab在 cdh1 节点，即 KDC server 节点上执行下面命令： 123456789$ cd /var/kerberos/krb5kdc/kadmin.local -q "addprinc -randkey impala/cdh1@JAVACHEN.COM "kadmin.local -q "addprinc -randkey impala/cdh2@JAVACHEN.COM "kadmin.local -q "addprinc -randkey impala/cdh3@JAVACHEN.COM "kadmin.local -q "xst -k impala-unmerge.keytab impala/cdh1@JAVACHEN.COM "kadmin.local -q "xst -k impala-unmerge.keytab impala/cdh2@JAVACHEN.COM "kadmin.local -q "xst -k impala-unmerge.keytab impala/cdh3@JAVACHEN.COM " 另外，如果你使用了haproxy来做负载均衡，参考官方文档Using Impala through a Proxy for High Availability，还需生成 proxy.keytab： 123456$ cd /var/kerberos/krb5kdc/# proxy 为安装了 haproxy 的机器kadmin.local -q "addprinc -randkey impala/proxy@JAVACHEN.COM "kadmin.local -q "xst -k proxy.keytab impala/proxy@JAVACHEN.COM " 合并 proxy.keytab 和 impala-unmerge.keytab 生成 impala.keytab： 12345$ ktutilktutil: rkt proxy.keytabktutil: rkt impala-unmerge.keytabktutil: wkt impala.keytabktutil: quit 拷贝 impala.keytab 和 proxy_impala.keytab 文件到其他节点的 /etc/impala/conf 目录 123$ scp impala.keytab cdh1:/etc/impala/conf$ scp impala.keytab cdh2:/etc/impala/conf$ scp impala.keytab cdh3:/etc/impala/conf 并设置权限，分别在 cdh1、cdh2、cdh3 上执行： 123$ ssh cdh1 "cd /etc/impala/conf/;chown impala:hadoop *.keytab ;chmod 400 *.keytab"$ ssh cdh2 "cd /etc/impala/conf/;chown impala:hadoop *.keytab ;chmod 400 *.keytab"$ ssh cdh3 "cd /etc/impala/conf/;chown impala:hadoop *.keytab ;chmod 400 *.keytab" 由于 keytab 相当于有了永久凭证，不需要提供密码(如果修改 kdc 中的 principal 的密码，则该 keytab 就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 keytab 中指定的用户身份访问 hadoop，所以 keytab 文件需要确保只对 owner 有读权限(0400) 4. 修改 impala 配置文件修改 cdh1 节点上的 /etc/default/impala，在 IMPALA_CATALOG_ARGS 、IMPALA_SERVER_ARGS 和 IMPALA_STATE_STORE_ARGS 中添加下面参数： 123-kerberos_reinit_interval=60-principal=impala/_HOST@JAVACHEN.COM-keytab_file=/etc/impala/conf/impala.keytab 如果使用了 HAProxy（关于 HAProxy 的配置请参考 Hive使用HAProxy配置HA），则 IMPALA_SERVER_ARGS 参数需要修改为（proxy为 HAProxy 机器的名称，这里我是将 HAProxy 安装在 cdh1 节点上）： 1234-kerberos_reinit_interval=60-be_principal=impala/_HOST@JAVACHEN.COM-principal=impala/proxy@JAVACHEN.COM-keytab_file=/etc/impala/conf/impala.keytab 在 IMPALA_CATALOG_ARGS 中添加： 1-state_store_host=$&#123;IMPALA_STATE_STORE_HOST&#125; \ 将修改的上面文件同步到其他节点。最后，/etc/default/impala 文件如下，这里，为了避免 hostname 存在大写的情况，使用 hostname 变量替换 _HOST： 123456789101112131415161718192021222324252627282930313233343536IMPALA_CATALOG_SERVICE_HOST=cdh1IMPALA_STATE_STORE_HOST=cdh1IMPALA_STATE_STORE_PORT=24000IMPALA_BACKEND_PORT=22000IMPALA_LOG_DIR=/var/log/impalaIMPALA_MEM_DEF=$(free -m |awk 'NR==2&#123;print $2-5120&#125;')hostname=`hostname -f |tr "[:upper:]" "[:lower:]"`IMPALA_CATALOG_ARGS=" -log_dir=$&#123;IMPALA_LOG_DIR&#125; -state_store_host=$&#123;IMPALA_STATE_STORE_HOST&#125; \ -kerberos_reinit_interval=60\ -principal=impala/$&#123;hostname&#125;@JAVACHEN.COM \ -keytab_file=/etc/impala/conf/impala.keytab"IMPALA_STATE_STORE_ARGS=" -log_dir=$&#123;IMPALA_LOG_DIR&#125; -state_store_port=$&#123;IMPALA_STATE_STORE_PORT&#125;\ -statestore_subscriber_timeout_seconds=15 \ -kerberos_reinit_interval=60 \ -principal=impala/$&#123;hostname&#125;@JAVACHEN.COM \ -keytab_file=/etc/impala/conf/impala.keytab"IMPALA_SERVER_ARGS=" \ -log_dir=$&#123;IMPALA_LOG_DIR&#125; \ -catalog_service_host=$&#123;IMPALA_CATALOG_SERVICE_HOST&#125; \ -state_store_port=$&#123;IMPALA_STATE_STORE_PORT&#125; \ -use_statestore \ -state_store_host=$&#123;IMPALA_STATE_STORE_HOST&#125; \ -be_port=$&#123;IMPALA_BACKEND_PORT&#125; \ -kerberos_reinit_interval=60 \ -be_principal=impala/$&#123;hostname&#125;@JAVACHEN.COM \ -principal=impala/cdh1@JAVACHEN.COM \ -keytab_file=/etc/impala/conf/impala.keytab \ -mem_limit=$&#123;IMPALA_MEM_DEF&#125;m"ENABLE_CORE_DUMPS=false 将修改的上面文件同步到其他节点：cdh2、cdh3： 12$ scp /etc/default/impala cdh2:/etc/default/impala$ scp /etc/default/impala cdh3:/etc/default/impala 更新 impala 配置文件下的文件并同步到其他节点： 123456cp /etc/hadoop/conf/core-site.xml /etc/impala/conf/cp /etc/hadoop/conf/hdfs-site.xml /etc/impala/conf/cp /etc/hive/conf/hive-site.xml /etc/impala/conf/scp -r /etc/impala/conf cdh2:/etc/impalascp -r /etc/impala/conf cdh3:/etc/impala 5. 启动服务启动 impala-state-storeimpala-state-store 是通过 impala 用户启动的，故在 cdh1 上先获取 impala 用户的 ticket 再启动服务： 12$ kinit -k -t /etc/impala/conf/impala.keytab impala/cdh1@JAVACHEN.COM$ service impala-state-store start 然后查看日志，确认是否启动成功。 1$ tailf /var/log/impala/statestored.INFO 启动 impala-catalogimpala-catalog 是通过 impala 用户启动的，故在 cdh1 上先获取 impala 用户的 ticket 再启动服务： 12$ kinit -k -t /etc/impala/conf/impala.keytab impala/cdh1@JAVACHEN.COM$ service impala-catalog start 然后查看日志，确认是否启动成功。 1$ tailf /var/log/impala/catalogd.INFO 启动 impala-serverimpala-server 是通过 impala 用户启动的，故在 cdh1 上先获取 impala 用户的 ticket 再启动服务： 12$ kinit -k -t /etc/impala/conf/impala.keytab impala/cdh1@JAVACHEN.COM$ service impala-server start 然后查看日志，确认是否启动成功。 1$ tailf /var/log/impala/impalad.INFO 6. 测试测试 impala-shell在启用了 kerberos 之后，运行 impala-shell 时，需要添加 -k 参数： 12345678910111213141516171819202122$ impala-shell -kStarting Impala Shell using Kerberos authenticationUsing service name 'impala'Connected to cdh1:21000Server version: impalad version 1.3.1-cdh4 RELEASE (build 907481bf45b248a7bb3bb077d54831a71f484e5f)Welcome to the Impala shell. Press TAB twice to see a list of available commands.Copyright (c) 2012 Cloudera, Inc. All rights reserved.(Shell build version: Impala Shell v1.3.1-cdh4 (907481b) built on Wed Apr 30 14:23:48 PDT 2014)[cdh1:21000] &gt;[cdh1:21000] &gt; show tables;Query: show tables+------+| name |+------+| a || b || c || d |+------+Returned 4 row(s) in 0.08s 7. 排错如果出现下面异常： 123456[cdh1:21000] &gt; select * from test limit 10;Query: select * from test limit 10ERROR: AnalysisException: Failed to load metadata for table: default.testCAUSED BY: TableLoadingException: Failed to load metadata for table: testCAUSED BY: TTransportException: java.net.SocketTimeoutException: Read timed outCAUSED BY: SocketTimeoutException: Read timed out 则需要在 hive-site.xml 中将 hive.metastore.client.socket.timeout 值设置大一些： 1234&lt;property&gt; &lt;name&gt;hive.metastore.client.socket.timeout&lt;/name&gt; &lt;value&gt;36000&lt;/value&gt;&lt;/property&gt; 8. 相关文章 HDFS配置Kerberos认证 YARN配置Kerberos认证 Hive配置Kerberos认证 Impala配置Kerberos认证 Zookeeper配置Kerberos认证 Hadoop配置LDAP集成Kerberos 配置安全的Hive集群集成Sentry 配置安全的Impala集群集成Sentry Hadoop集群部署权限总结]]></content>
      <categories>
        <category>impala</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>impala</tag>
        <tag>kerberos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YARN配置Kerberos认证]]></title>
    <url>%2F2014%2F11%2F05%2Fconfig-kerberos-in-cdh-yarn%2F</url>
    <content type="text"><![CDATA[关于 Kerberos 的安装和 HDFS 配置 kerberos 认证，请参考 HDFS配置kerberos认证。 1. 环境说明系统环境： 操作系统：CentOs 6.6 Hadoop版本：CDH5.4 JDK版本：1.7.0_71 运行用户：root 集群各节点角色规划为： 123192.168.56.121 cdh1 NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impala statestore、Sentry 192.168.56.122 cdh2 DataNode、SecondaryNameNode、NodeManager、HBase、Hive Server2、Impala Server192.168.56.123 cdh3 DataNode、HBase、NodeManager、Hive Server2、Impala Server cdh1作为master节点，其他节点作为slave节点，hostname 请使用小写，要不然在集成 kerberos 时会出现一些错误。 2. 生成 keytab在 cdh1 节点，即 KDC server 节点上执行下面命令： 1234567891011121314151617cd /var/kerberos/krb5kdc/kadmin.local -q "addprinc -randkey yarn/cdh1@JAVACHEN.COM "kadmin.local -q "addprinc -randkey yarn/cdh2@JAVACHEN.COM "kadmin.local -q "addprinc -randkey yarn/cdh3@JAVACHEN.COM "kadmin.local -q "addprinc -randkey mapred/cdh1@JAVACHEN.COM "kadmin.local -q "addprinc -randkey mapred/cdh2@JAVACHEN.COM "kadmin.local -q "addprinc -randkey mapred/cdh3@JAVACHEN.COM "kadmin.local -q "xst -k yarn.keytab yarn/cdh1@JAVACHEN.COM "kadmin.local -q "xst -k yarn.keytab yarn/cdh2@JAVACHEN.COM "kadmin.local -q "xst -k yarn.keytab yarn/cdh3@JAVACHEN.COM "kadmin.local -q "xst -k mapred.keytab mapred/cdh1@JAVACHEN.COM "kadmin.local -q "xst -k mapred.keytab mapred/cdh2@JAVACHEN.COM "kadmin.local -q "xst -k mapred.keytab mapred/cdh3@JAVACHEN.COM " 拷贝 yarn.keytab 和 mapred.keytab 文件到其他节点的 /etc/hadoop/conf 目录 123$ scp yarn.keytab mapred.keytab cdh1:/etc/hadoop/conf$ scp yarn.keytab mapred.keytab cdh2:/etc/hadoop/conf$ scp yarn.keytab mapred.keytab cdh3:/etc/hadoop/conf 并设置权限，分别在 cdh1、cdh2、cdh3 上执行： 123$ ssh cdh1 "cd /etc/hadoop/conf/;chown yarn:hadoop yarn.keytab;chown mapred:hadoop mapred.keytab ;chmod 400 *.keytab"$ ssh cdh2 "cd /etc/hadoop/conf/;chown yarn:hadoop yarn.keytab;chown mapred:hadoop mapred.keytab ;chmod 400 *.keytab"$ ssh cdh3 "cd /etc/hadoop/conf/;chown yarn:hadoop yarn.keytab;chown mapred:hadoop mapred.keytab ;chmod 400 *.keytab" 由于 keytab 相当于有了永久凭证，不需要提供密码(如果修改 kdc 中的 principal 的密码，则该 keytab 就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 keytab 中指定的用户身份访问 hadoop，所以 keytab 文件需要确保只对 owner 有读权限(0400) 3. 修改 YARN 配置文件修改 yarn-site.xml，添加下面配置： 12345678910111213141516171819202122232425&lt;property&gt; &lt;name&gt;yarn.resourcemanager.keytab&lt;/name&gt; &lt;value&gt;/etc/hadoop/conf/yarn.keytab&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.principal&lt;/name&gt; &lt;value&gt;yarn/_HOST@JAVACHEN.COM&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.keytab&lt;/name&gt; &lt;value&gt;/etc/hadoop/conf/yarn.keytab&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.principal&lt;/name&gt; &lt;value&gt;yarn/_HOST@JAVACHEN.COM&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.container-executor.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.linux-container-executor.group&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 如果想要 YARN 开启 SSL，则添加： 1234&lt;property&gt; &lt;name&gt;yarn.http.policy&lt;/name&gt; &lt;value&gt;HTTPS_ONLY&lt;/value&gt;&lt;/property&gt; 修改 mapred-site.xml，添加如下配置： 12345678&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.keytab&lt;/name&gt; &lt;value&gt;/etc/hadoop/conf/mapred.keytab&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.principal&lt;/name&gt; &lt;value&gt;mapred/_HOST@JAVACHEN.COM&lt;/value&gt;&lt;/property&gt; 如果想要 mapreduce jobhistory 开启 SSL，则添加： 1234&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.http.policy&lt;/name&gt; &lt;value&gt;HTTPS_ONLY&lt;/value&gt;&lt;/property&gt; 在 /etc/hadoop/conf 目录下创建 container-executor.cfg 文件，内容如下： 12345678#configured value of yarn.nodemanager.linux-container-executor.groupyarn.nodemanager.linux-container-executor.group=yarn#comma separated list of users who can not run applicationsbanned.users=bin#Prevent other super-usersmin.user.id=0#comma separated list of system users who CAN run applicationsallowed.system.users=root,nobody,impala,hive,hdfs,yarn 设置该文件权限： 12345$ chown root:yarn container-executor.cfg$ chmod 400 container-executor.cfg$ ll container-executor.cfg-r-------- 1 root yarn 354 11-05 14:14 container-executor.cfg 注意： container-executor.cfg 文件读写权限需设置为 400，所有者为 root:yarn。 yarn.nodemanager.linux-container-executor.group 要同时配置在 yarn-site.xml 和 container-executor.cfg，且其值需要为运行 NodeManager 的用户所在的组，这里为 yarn。 banned.users 不能为空，默认值为 hfds,yarn,mapred,bin min.user.id 默认值为 1000，在有些 centos 系统中，用户最小 id 为500，则需要修改该值 确保 yarn.nodemanager.local-dirs 和 yarn.nodemanager.log-dirs 对应的目录权限为 755 。 设置 /usr/lib/hadoop-yarn/bin/container-executor 读写权限为 6050 如下： 12345$ chown root:yarn /usr/lib/hadoop-yarn/bin/container-executor$ chmod 6050 /usr/lib/hadoop-yarn/bin/container-executor$ ll /usr/lib/hadoop-yarn/bin/container-executor---Sr-s--- 1 root yarn 333 11-04 19:11 container-executor 测试是否配置正确： 1$ /usr/lib/hadoop-yarn/bin/container-executor --checksetup 如果提示错误，则查看 NodeManger 的日志，然后对照 YARN ONLY: Container-executor Error Codes 查看错误对应的问题说明。 关于 LinuxContainerExecutor 的详细说明，可以参考 http://hadoop.apache.org/docs/r2.5.0/hadoop-project-dist/hadoop-common/SecureMode.html#LinuxContainerExecutor。 记住将修改的上面文件同步到其他节点：cdh2、cdh3，并再次一一检查权限是否正确。 1234567$ cd /etc/hadoop/conf/$ scp yarn-site.xml mapred-site.xml container-executor.cfg cdh2:/etc/hadoop/conf/$ scp yarn-site.xml mapred-site.xml container-executor.cfg cdh3:/etc/hadoop/conf/$ ssh cdh2 "cd /etc/hadoop/conf/; chown root:yarn container-executor.cfg ; chmod 400 container-executor.cfg"$ ssh cdh3 "cd /etc/hadoop/conf/; chown root:yarn container-executor.cfg ; chmod 400 container-executor.cfg" 4. 启动服务启动 ResourceManagerresourcemanager 是通过 yarn 用户启动的，故在 cdh1 上先获取 yarn 用户的 ticket 再启动服务： 12$ kinit -k -t /etc/hadoop/conf/yarn.keytab yarn/cdh1@JAVACHEN.COM$ service hadoop-yarn-resourcemanager start 然后查看日志，确认是否启动成功。 启动 NodeManagerresourcemanager 是通过 yarn 用户启动的，故在 cdh2 和 cdh3 上先获取 yarn 用户的 ticket 再启动服务： 12$ ssh cdh2 "kinit -k -t /etc/hadoop/conf/yarn.keytab yarn/cdh2@JAVACHEN.COM ;service hadoop-yarn-nodemanager start"$ ssh cdh3 "kinit -k -t /etc/hadoop/conf/yarn.keytab yarn/cdh3@JAVACHEN.COM ;service hadoop-yarn-nodemanager start" 启动 MapReduce Job History Serverresourcemanager 是通过 mapred 用户启动的，故在 cdh1 上先获取 mapred 用户的 ticket 再启动服务： 12$ kinit -k -t /etc/hadoop/conf/mapred.keytab mapred/cdh1@JAVACHEN.COM$ service hadoop-mapreduce-historyserver start 5. 测试检查 web 页面是否可以访问：http://cdh1:8088/cluster 运行一个 mapreduce 的例子： 12345678910111213$ klist Ticket cache: FILE:/tmp/krb5cc_1002 Default principal: yarn/cdh1@JAVACHEN.COM Valid starting Expires Service principal 11/10/14 11:18:55 11/11/14 11:18:55 krbtgt/cdh1@JAVACHEN.COM renew until 11/17/14 11:18:55 Kerberos 4 ticket cache: /tmp/tkt1002 klist: You have no tickets cached$ hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar pi 10 10000 如果没有报错，则说明配置成功。最后运行的结果为： 12Job Finished in 54.56 secondsEstimated value of Pi is 3.14120000000000000000 如果出现下面错误，请检查环境变量中 HADOOP_YARN_HOME 是否设置正确，并和 yarn.application.classpath 中的保持一致。 123456789101112131415161718192021222324252614/11/13 11:41:02 INFO mapreduce.Job: Job job_1415849491982_0003 failed with state FAILED due to: Application application_1415849491982_0003 failed 2 times due to AM Container for appattempt_1415849491982_0003_000002 exited with exitCode: 1 due to: Exception from container-launch.Container id: container_1415849491982_0003_02_000001Exit code: 1Stack trace: ExitCodeException exitCode=1: at org.apache.hadoop.util.Shell.runCommand(Shell.java:538) at org.apache.hadoop.util.Shell.run(Shell.java:455) at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:702) at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:281) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:299) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:81) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) at java.util.concurrent.FutureTask.run(FutureTask.java:138) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662)Shell output: main : command provided 1main : user is yarnmain : requested yarn user is yarnContainer exited with a non-zero exit code 1.Failing this attempt.. Failing the application.14/11/13 11:41:02 INFO mapreduce.Job: Counters: 0Job Finished in 13.428 secondsjava.io.FileNotFoundException: File does not exist: hdfs://cdh1:8020/user/yarn/QuasiMonteCarlo_1415850045475_708291630/out/reduce-out 6. 相关文章 HDFS配置Kerberos认证 YARN配置Kerberos认证 Hive配置Kerberos认证 Impala配置Kerberos认证 Zookeeper配置Kerberos认证 Hadoop配置LDAP集成Kerberos 配置安全的Hive集群集成Sentry 配置安全的Impala集群集成Sentry Hadoop集群部署权限总结]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>yarn</tag>
        <tag>kerberos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS配置Kerberos认证]]></title>
    <url>%2F2014%2F11%2F04%2Fconfig-kerberos-in-cdh-hdfs%2F</url>
    <content type="text"><![CDATA[本文主要记录 CDH Hadoop 集群上配置 HDFS 集成 Kerberos 的过程，包括 Kerberos 的安装和 Hadoop 相关配置修改说明。 1. 环境说明系统环境： 操作系统：CentOs 6.6 Hadoop版本：CDH5.4 JDK版本：1.7.0_71 运行用户：root 集群各节点角色规划为： 123192.168.56.121 cdh1 NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impala statestore、Sentry 192.168.56.122 cdh2 DataNode、SecondaryNameNode、NodeManager、HBase、Hive Server2、Impala Server192.168.56.123 cdh3 DataNode、HBase、NodeManager、Hive Server2、Impala Server cdh1作为master节点，其他节点作为slave节点，我们在cdh1节点安装kerberos Server，在其他节点安装kerberos client。 2. 准备工作确认添加主机名解析到 /etc/hosts 文件中。 123456$ cat /etc/hosts127.0.0.1 localhost192.168.56.121 cdh1192.168.56.122 cdh2192.168.56.123 cdh3 注意：hostname 请使用小写，要不然在集成 kerberos 时会出现一些错误。 3. 安装 Kerberos在 cdh1 上安装包 krb5、krb5-server 和 krb5-client。 1$ yum install krb5-server -y 在其他节点（cdh1、cdh2、cdh3）安装 krb5-devel、krb5-workstation ： 1234#使用无密码登陆$ ssh cdh1 "yum install krb5-devel krb5-workstation -y"$ ssh cdh2 "yum install krb5-devel krb5-workstation -y"$ ssh cdh3 "yum install krb5-devel krb5-workstation -y" 4. 修改配置文件kdc 服务器涉及到三个配置文件： 123/etc/krb5.conf/var/kerberos/krb5kdc/kdc.conf/var/kerberos/krb5kdc/kadm5.acl 配置 Kerberos 的一种方法是编辑配置文件 /etc/krb5.conf。默认安装的文件中包含多个示例项。 123456789101112131415161718192021222324252627[logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log[libdefaults] default_realm = JAVACHEN.COM dns_lookup_realm = false dns_lookup_kdc = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true default_tgs_enctypes = aes256-cts-hmac-sha1-96 default_tkt_enctypes = aes256-cts-hmac-sha1-96 permitted_enctypes = aes256-cts-hmac-sha1-96 clockskew = 120 udp_preference_limit = 1[realms] JAVACHEN.COM = &#123; kdc = cdh1 admin_server = cdh1 &#125;[domain_realm] .javachen.com = JAVACHEN.COM javachen.com = JAVACHEN.COM 说明： [logging]：表示 server 端的日志的打印位置 [libdefaults]：每种连接的默认配置，需要注意以下几个关键的小配置 default_realm = JAVACHEN.COM：设置 Kerberos 应用程序的默认领域。如果您有多个领域，只需向 [realms] 节添加其他的语句。 ticket_lifetime： 表明凭证生效的时限，一般为24小时。 renew_lifetime： 表明凭证最长可以被延期的时限，一般为一个礼拜。当凭证过期之后，对安全认证的服务的后续访问则会失败。 clockskew：时钟偏差是不完全符合主机系统时钟的票据时戳的容差，超过此容差将不接受此票据。通常，将时钟扭斜设置为 300 秒（5 分钟）。这意味着从服务器的角度看，票证的时间戳与它的偏差可以是在前后 5 分钟内。 udp_preference_limit= 1：禁止使用 udp 可以防止一个 Hadoop 中的错误 [realms]：列举使用的 realm。 kdc：代表要 kdc 的位置。格式是 机器:端口 admin_server：代表 admin 的位置。格式是 机器:端口 default_domain：代表默认的域名 [appdefaults]：可以设定一些针对特定应用的配置，覆盖默认配置。 修改 /var/kerberos/krb5kdc/kdc.conf ，该文件包含 Kerberos 的配置信息。例如，KDC 的位置，Kerbero 的 admin 的realms 等。需要所有使用的 Kerberos 的机器上的配置文件都同步。这里仅列举需要的基本配置。详细介绍参考：krb5conf 123456789101112131415[kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88[realms] JAVACHEN.COM = &#123; #master_key_type = aes256-cts acl_file = /var/kerberos/krb5kdc/kadm5.acl dict_file = /usr/share/dict/words max_renewable_life = 7d max_life = 1d admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal default_principal_flags = +renewable, +forwardable &#125; 说明： JAVACHEN.COM： 是设定的 realms。名字随意。Kerberos 可以支持多个 realms，会增加复杂度。大小写敏感，一般为了识别使用全部大写。这个 realms 跟机器的 host 没有大关系。 master_key_type：和 supported_enctypes 默认使用 aes256-cts。JAVA 使用 aes256-cts 验证方式需要安装 JCE 包，见下面的说明。为了简便，你可以不使用 aes256-cts 算法，这样就不需要安装 JCE 。 acl_file：标注了 admin 的用户权限，需要用户自己创建。文件格式是：Kerberos_principal permissions [target_principal] [restrictions] supported_enctypes：支持的校验方式。 admin_keytab：KDC 进行校验的 keytab。 关于AES-256加密： 对于使用 centos5. 6 及以上的系统，默认使用 AES-256 来加密的。这就需要集群中的所有节点上安装 JCE，如果你使用的是 JDK1.6 ，则到 Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files for JDK/JRE 6 页面下载，如果是 JDK1.7，则到 Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files for JDK/JRE 7 下载。 下载的文件是一个 zip 包，解开后，将里面的两个文件放到下面的目录中：$JAVA_HOME/jre/lib/security 为了能够不直接访问 KDC 控制台而从 Kerberos 数据库添加和删除主体，请对 Kerberos 管理服务器指示允许哪些主体执行哪些操作。通过编辑文件 /var/lib/kerberos/krb5kdc/kadm5.acl 完成此操作。ACL（访问控制列表）允许您精确指定特权。 12$ cat /var/kerberos/krb5kdc/kadm5.acl */admin@JAVACHEN.COM * 5. 同步配置文件将 kdc 中的 /etc/krb5.conf 拷贝到集群中其他服务器即可。 12$ scp /etc/krb5.conf cdh2:/etc/krb5.conf$ scp /etc/krb5.conf cdh3:/etc/krb5.conf 请确认集群如果关闭了 selinux。 6. 创建数据库在 cdh1 上运行初始化数据库命令。其中 -r 指定对应 realm。 1$ kdb5_util create -r JAVACHEN.COM -s 出现 Loading random data 的时候另开个终端执行点消耗CPU的命令如 cat /dev/sda &gt; /dev/urandom 可以加快随机数采集。该命令会在 /var/kerberos/krb5kdc/ 目录下创建 principal 数据库。 如果遇到数据库已经存在的提示，可以把 /var/kerberos/krb5kdc/ 目录下的 principal 的相关文件都删除掉。默认的数据库名字都是 principal。可以使用 -d 指定数据库名字。 7. 启动服务在 cdh1 节点上运行： 1234$ chkconfig --level 35 krb5kdc on$ chkconfig --level 35 kadmin on$ service krb5kdc start$ service kadmin start 8. 创建 kerberos 管理员关于 kerberos 的管理，可以使用 kadmin.local 或 kadmin，至于使用哪个，取决于账户和访问权限： 如果有访问 kdc 服务器的 root 权限，但是没有 kerberos admin 账户，使用 kadmin.local 如果没有访问 kdc 服务器的 root 权限，但是用 kerberos admin 账户，使用 kadmin 在 cdh1 上创建远程管理的管理员： 12345678910#手动输入两次密码，这里密码为 root$ kadmin.local -q "addprinc root/admin"# 也可以不用手动输入密码$ echo -e "root\nroot" | kadmin.local -q "addprinc root/admin"# 或者运行下面命令$ kadmin.local &lt;&lt;eojaddprinc -pw root root/admineoj 系统会提示输入密码，密码不能为空，且需妥善保存。 9. 测试 kerberos查看当前的认证用户： 1234567891011121314151617# 查看principals$ kadmin: list_principals # 添加一个新的 principal kadmin: addprinc user1 WARNING: no policy specified for user1@JAVACHEN.COM; defaulting to no policy Enter password for principal "user1@JAVACHEN.COM": Re-enter password for principal "user1@JAVACHEN.COM": Principal "user1@JAVACHEN.COM" created. # 删除 principal kadmin: delprinc user1 Are you sure you want to delete the principal "user1@JAVACHEN.COM"? (yes/no): yes Principal "user1@JAVACHEN.COM" deleted. Make sure that you have removed this principal from all ACLs before reusing. kadmin: exit 也可以直接通过下面的命令来执行： 123456789# 提示需要输入密码$ kadmin -p root/admin -q "list_principals"$ kadmin -p root/admin -q "addprinc user2"$ kadmin -p root/admin -q "delprinc user2"# 不用输入密码$ kadmin.local -q "list_principals"$ kadmin.local -q "addprinc user2"$ kadmin.local -q "delprinc user2" 创建一个测试用户 test，密码设置为 test： 1$ echo -e "test\ntest" | kadmin.local -q "addprinc test" 获取 test 用户的 ticket： 1234567891011121314# 通过用户名和密码进行登录$ kinit testPassword for test@JAVACHEN.COM:$ klist -eTicket cache: FILE:/tmp/krb5cc_0Default principal: test@JAVACHEN.COMValid starting Expires Service principal11/07/14 15:29:02 11/08/14 15:29:02 krbtgt/JAVACHEN.COM@JAVACHEN.COM renew until 11/17/14 15:29:02, Etype (skey, tkt): aes256-cts-hmac-sha1-96, aes256-cts-hmac-sha1-96Kerberos 4 ticket cache: /tmp/tkt0klist: You have no tickets cached 销毁该 test 用户的 ticket： 1234567$ kdestroy$ klistklist: No credentials cache found (ticket cache FILE:/tmp/krb5cc_0)Kerberos 4 ticket cache: /tmp/tkt0klist: You have no tickets cached 更新 ticket： 1234567891011121314151617181920212223242526$ kinit root/admin Password for root/admin@JAVACHEN.COM:$ klist Ticket cache: FILE:/tmp/krb5cc_0 Default principal: root/admin@JAVACHEN.COM Valid starting Expires Service principal 11/07/14 15:33:57 11/08/14 15:33:57 krbtgt/JAVACHEN.COM@JAVACHEN.COM renew until 11/17/14 15:33:57 Kerberos 4 ticket cache: /tmp/tkt0 klist: You have no tickets cached$ kinit -R$ klist Ticket cache: FILE:/tmp/krb5cc_0 Default principal: root/admin@JAVACHEN.COM Valid starting Expires Service principal 11/07/14 15:34:05 11/08/14 15:34:05 krbtgt/JAVACHEN.COM@JAVACHEN.COM renew until 11/17/14 15:33:57 Kerberos 4 ticket cache: /tmp/tkt0 klist: You have no tickets cached 抽取密钥并将其储存在本地 keytab 文件 /etc/krb5.keytab 中。这个文件由超级用户拥有，所以您必须是 root 用户才能在 kadmin shell 中执行以下命令： 1234567891011$ kadmin.local -q "ktadd kadmin/admin"$ klist -k /etc/krb5.keytab Keytab name: FILE:/etc/krb5.keytab KVNO Principal ---- -------------------------------------------------------------------------- 3 kadmin/admin@LASHOU-INC.COM 3 kadmin/admin@LASHOU-INC.COM 3 kadmin/admin@LASHOU-INC.COM 3 kadmin/admin@LASHOU-INC.COM 3 kadmin/admin@LASHOU-INC.COM 10. HDFS 上配置 kerberos10.1 创建认证规则在 Kerberos 安全机制里，一个 principal 就是 realm 里的一个对象，一个 principal 总是和一个密钥（secret key）成对出现的。 这个 principal 的对应物可以是 service，可以是 host，也可以是 user，对于 Kerberos 来说，都没有区别。 Kdc(Key distribute center) 知道所有 principal 的 secret key，但每个 principal 对应的对象只知道自己的那个 secret key 。这也是“共享密钥“的由来。 对于 hadoop，principals 的格式为 username/fully.qualified.domain.name@YOUR-REALM.COM。 通过 yum 源安装的 cdh 集群中，NameNode 和 DataNode 是通过 hdfs 启动的，故为集群中每个服务器节点添加两个principals：hdfs、HTTP。 在 KCD server 上（这里是 cdh1）创建 hdfs principal： 123kadmin.local -q "addprinc -randkey hdfs/cdh1@JAVACHEN.COM"kadmin.local -q "addprinc -randkey hdfs/cdh2@JAVACHEN.COM"kadmin.local -q "addprinc -randkey hdfs/cdh3@JAVACHEN.COM" -randkey 标志没有为新 principal 设置密码，而是指示 kadmin 生成一个随机密钥。之所以在这里使用这个标志，是因为此 principal 不需要用户交互。它是计算机的一个服务器帐户。 创建 HTTP principal： 123kadmin.local -q "addprinc -randkey HTTP/cdh1@JAVACHEN.COM"kadmin.local -q "addprinc -randkey HTTP/cdh2@JAVACHEN.COM"kadmin.local -q "addprinc -randkey HTTP/cdh3@JAVACHEN.COM" 创建完成后，查看： 1$ kadmin.local -q "listprincs" 10.2 创建keytab文件keytab 是包含 principals 和加密 principal key 的文件。keytab 文件对于每个 host 是唯一的，因为 key 中包含 hostname。keytab 文件用于不需要人工交互和保存纯文本密码，实现到 kerberos 上验证一个主机上的 principal。因为服务器上可以访问 keytab 文件即可以以 principal 的身份通过 kerberos 的认证，所以，keytab 文件应该被妥善保存，应该只有少数的用户可以访问。 创建包含 hdfs principal 和 host principal 的 hdfs keytab： 1xst -norandkey -k hdfs.keytab hdfs/fully.qualified.domain.name host/fully.qualified.domain.name 创建包含 mapred principal 和 host principal 的 mapred keytab： 1xst -norandkey -k mapred.keytab mapred/fully.qualified.domain.name host/fully.qualified.domain.name 注意： 上面的方法使用了xst的norandkey参数，有些kerberos不支持该参数。当不支持该参数时有这样的提示：Principal -norandkey does not exist.，需要使用下面的方法来生成keytab文件。 在 cdh1 节点，即 KDC server 节点上执行下面命令： 123456789$ cd /var/kerberos/krb5kdc/kadmin.local -q "xst -k hdfs-unmerged.keytab hdfs/cdh1@JAVACHEN.COM"kadmin.local -q "xst -k hdfs-unmerged.keytab hdfs/cdh2@JAVACHEN.COM"kadmin.local -q "xst -k hdfs-unmerged.keytab hdfs/cdh3@JAVACHEN.COM"kadmin.local -q "xst -k HTTP.keytab HTTP/cdh1@JAVACHEN.COM"kadmin.local -q "xst -k HTTP.keytab HTTP/cdh2@JAVACHEN.COM"kadmin.local -q "xst -k HTTP.keytab HTTP/cdh3@JAVACHEN.COM" 这样，就会在 /var/kerberos/krb5kdc/ 目录下生成 hdfs-unmerged.keytab 和 HTTP.keytab 两个文件，接下来使用 ktutil 合并者两个文件为 hdfs.keytab。 1234567$ cd /var/kerberos/krb5kdc/$ ktutilktutil: rkt hdfs-unmerged.keytabktutil: rkt HTTP.keytabktutil: wkt hdfs.keytabktutil: exit 使用 klist 显示 hdfs.keytab 文件列表： 12345678910111213141516171819202122232425262728293031323334353637383940$ klist -ket hdfs.keytabKeytab name: FILE:hdfs.keytabKVNO Timestamp Principal---- ----------------- -------------------------------------------------------- 2 11/13/14 10:40:18 hdfs/cdh1@JAVACHEN.COM (aes256-cts-hmac-sha1-96) 2 11/13/14 10:40:18 hdfs/cdh1@JAVACHEN.COM (aes128-cts-hmac-sha1-96) 2 11/13/14 10:40:18 hdfs/cdh1@JAVACHEN.COM (des3-cbc-sha1) 2 11/13/14 10:40:18 hdfs/cdh1@JAVACHEN.COM (arcfour-hmac) 2 11/13/14 10:40:18 hdfs/cdh1@JAVACHEN.COM (des-hmac-sha1) 2 11/13/14 10:40:18 hdfs/cdh1@JAVACHEN.COM (des-cbc-md5) 2 11/13/14 10:40:18 hdfs/cdh2@JAVACHEN.COM (aes256-cts-hmac-sha1-96) 2 11/13/14 10:40:18 hdfs/cdh2@JAVACHEN.COM (aes128-cts-hmac-sha1-96) 2 11/13/14 10:40:18 hdfs/cdh2@JAVACHEN.COM (des3-cbc-sha1) 2 11/13/14 10:40:18 hdfs/cdh2@JAVACHEN.COM (arcfour-hmac) 2 11/13/14 10:40:18 hdfs/cdh2@JAVACHEN.COM (des-hmac-sha1) 2 11/13/14 10:40:18 hdfs/cdh2@JAVACHEN.COM (des-cbc-md5) 2 11/13/14 10:40:18 hdfs/cdh3@JAVACHEN.COM (aes256-cts-hmac-sha1-96) 2 11/13/14 10:40:18 hdfs/cdh3@JAVACHEN.COM (aes128-cts-hmac-sha1-96) 2 11/13/14 10:40:18 hdfs/cdh3@JAVACHEN.COM (des3-cbc-sha1) 2 11/13/14 10:40:18 hdfs/cdh3@JAVACHEN.COM (arcfour-hmac) 2 11/13/14 10:40:18 hdfs/cdh3@JAVACHEN.COM (des-hmac-sha1) 2 11/13/14 10:40:18 hdfs/cdh3@JAVACHEN.COM (des-cbc-md5) 2 11/13/14 10:40:18 HTTP/cdh1@JAVACHEN.COM (aes256-cts-hmac-sha1-96) 2 11/13/14 10:40:18 HTTP/cdh1@JAVACHEN.COM (aes128-cts-hmac-sha1-96) 2 11/13/14 10:40:18 HTTP/cdh1@JAVACHEN.COM (des3-cbc-sha1) 2 11/13/14 10:40:18 HTTP/cdh1@JAVACHEN.COM (arcfour-hmac) 2 11/13/14 10:40:18 HTTP/cdh1@JAVACHEN.COM (des-hmac-sha1) 2 11/13/14 10:40:18 HTTP/cdh1@JAVACHEN.COM (des-cbc-md5) 2 11/13/14 10:40:18 HTTP/cdh2@JAVACHEN.COM (aes256-cts-hmac-sha1-96) 2 11/13/14 10:40:18 HTTP/cdh2@JAVACHEN.COM (aes128-cts-hmac-sha1-96) 2 11/13/14 10:40:18 HTTP/cdh2@JAVACHEN.COM (des3-cbc-sha1) 2 11/13/14 10:40:18 HTTP/cdh2@JAVACHEN.COM (arcfour-hmac) 2 11/13/14 10:40:18 HTTP/cdh2@JAVACHEN.COM (des-hmac-sha1) 2 11/13/14 10:40:18 HTTP/cdh2@JAVACHEN.COM (des-cbc-md5) 2 11/13/14 10:40:18 HTTP/cdh3@JAVACHEN.COM (aes256-cts-hmac-sha1-96) 2 11/13/14 10:40:18 HTTP/cdh3@JAVACHEN.COM (aes128-cts-hmac-sha1-96) 2 11/13/14 10:40:18 HTTP/cdh3@JAVACHEN.COM (des3-cbc-sha1) 2 11/13/14 10:40:18 HTTP/cdh3@JAVACHEN.COM (arcfour-hmac) 2 11/13/14 10:40:18 HTTP/cdh3@JAVACHEN.COM (des-hmac-sha1) 2 11/13/14 10:40:18 HTTP/cdh3@JAVACHEN.COM (des-cbc-md5) 验证是否正确合并了key，使用合并后的keytab，分别使用hdfs和host principals来获取证书。 12$ kinit -k -t hdfs.keytab hdfs/cdh1@JAVACHEN.COM$ kinit -k -t hdfs.keytab HTTP/cdh1@JAVACHEN.COM 如果出现错误：kinit: Key table entry not found while getting initial credentials， 则上面的合并有问题，重新执行前面的操作。 10.3 部署kerberos keytab文件拷贝 hdfs.keytab 文件到其他节点的 /etc/hadoop/conf 目录 12345$ cd /var/kerberos/krb5kdc/$ scp hdfs.keytab cdh1:/etc/hadoop/conf$ scp hdfs.keytab cdh2:/etc/hadoop/conf$ scp hdfs.keytab cdh3:/etc/hadoop/conf 并设置权限，分别在 cdh1、cdh2、cdh3 上执行： 123$ ssh cdh1 "chown hdfs:hadoop /etc/hadoop/conf/hdfs.keytab ;chmod 400 /etc/hadoop/conf/hdfs.keytab"$ ssh cdh2 "chown hdfs:hadoop /etc/hadoop/conf/hdfs.keytab ;chmod 400 /etc/hadoop/conf/hdfs.keytab"$ ssh cdh3 "chown hdfs:hadoop /etc/hadoop/conf/hdfs.keytab ;chmod 400 /etc/hadoop/conf/hdfs.keytab" 由于 keytab 相当于有了永久凭证，不需要提供密码(如果修改kdc中的principal的密码，则该keytab就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 keytab 中指定的用户身份访问 hadoop，所以 keytab 文件需要确保只对 owner 有读权限(0400) 10.4 修改 hdfs 配置文件先停止集群： 1234$ for x in `cd /etc/init.d ; ls hive-*` ; do sudo service $x stop ; done$ for x in `cd /etc/init.d ; ls impala-*` ; do sudo service $x stop ; done$ for x in `cd /etc/init.d ; ls hadoop-*` ; do sudo service $x stop ; done$ for x in `cd /etc/init.d ; ls zookeeper-*` ; do sudo service $x stop ; done 在集群中所有节点的 core-site.xml 文件中添加下面的配置: 123456789&lt;property&gt; &lt;name&gt;hadoop.security.authentication&lt;/name&gt; &lt;value&gt;kerberos&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.security.authorization&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 在集群中所有节点的 hdfs-site.xml 文件中添加下面的配置： 12345678910111213141516171819202122232425262728293031323334353637383940&lt;property&gt; &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir.perm&lt;/name&gt; &lt;value&gt;700&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt; &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt; &lt;value&gt;hdfs/_HOST@JAVACHEN.COM&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.kerberos.https.principal&lt;/name&gt; &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.address&lt;/name&gt; &lt;value&gt;0.0.0.0:1004&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.http.address&lt;/name&gt; &lt;value&gt;0.0.0.0:1006&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.keytab.file&lt;/name&gt; &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.kerberos.principal&lt;/name&gt; &lt;value&gt;hdfs/_HOST@JAVACHEN.COM&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.kerberos.https.principal&lt;/name&gt; &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;&lt;/property&gt; 如果想开启 SSL，请添加（本文不对这部分做说明）： 1234&lt;property&gt; &lt;name&gt;dfs.http.policy&lt;/name&gt; &lt;value&gt;HTTPS_ONLY&lt;/value&gt;&lt;/property&gt; 如果 HDFS 配置了 QJM HA，则需要添加（另外，你还要在 zookeeper 上配置 kerberos）： 123456789101112&lt;property&gt; &lt;name&gt;dfs.journalnode.keytab.file&lt;/name&gt; &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.journalnode.kerberos.principal&lt;/name&gt; &lt;value&gt;hdfs/_HOST@JAVACHEN.COM&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.journalnode.kerberos.internal.spnego.principal&lt;/name&gt; &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;&lt;/property&gt; 如果配置了 WebHDFS，则添加： 1234567891011121314&lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.web.authentication.kerberos.principal&lt;/name&gt; &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.web.authentication.kerberos.keytab&lt;/name&gt; &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;&lt;/property&gt; 配置中有几点要注意的： dfs.datanode.address表示 data transceiver RPC server 所绑定的 hostname 或 IP 地址，如果开启 security，端口号必须小于 1024(privileged port)，否则的话启动 datanode 时候会报 Cannot start secure cluster without privileged resources 错误 principal 中的 instance 部分可以使用 _HOST 标记，系统会自动替换它为全称域名 如果开启了 security, hadoop 会对 hdfs block data(由 dfs.data.dir 指定)做 permission check，方式用户的代码不是调用hdfs api而是直接本地读block data，这样就绕过了kerberos和文件权限验证，管理员可以通过设置 dfs.datanode.data.dir.perm 来修改 datanode 文件权限，这里我们设置为700 10.5 检查集群上的 HDFS 和本地文件的权限请参考 Verify User Accounts and Groups in CDH 5 Due to Security 或者 Hadoop in Secure Mode。 10.6 启动 NameNode启动之前，请确认 JCE jar 已经替换，请参考前面的说明。 在每个节点上获取 root 用户的 ticket，这里 root 为之前创建的 root/admin 的密码。 123$ ssh cdh1 "echo root|kinit root/admin"$ ssh cdh1 "echo root|kinit root/admin"$ ssh cdh1 "echo root|kinit root/admin" 获取 cdh1的 ticket： 1$ kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/cdh1@JAVACHEN.COM 如果出现下面异常 kinit: Password incorrect while getting initial credentials，则重新导出 keytab 再试试。 然后启动服务，观察日志： 1$ /etc/init.d/hadoop-hdfs-namenode start 验证 NameNode 是否启动，一是打开 web 界面查看启动状态，一是运行下面命令查看 hdfs： 123456$ hadoop fs -ls /Found 4 itemsdrwxrwxrwx - yarn hadoop 0 2014-06-26 15:24 /logrootdrwxrwxrwt - hdfs hadoop 0 2014-11-04 10:44 /tmpdrwxr-xr-x - hdfs hadoop 0 2014-08-10 10:53 /userdrwxr-xr-x - hdfs hadoop 0 2013-05-20 22:52 /var 如果在你的凭据缓存中没有有效的 kerberos ticket，执行上面命令将会失败，将会出现下面的错误： 123414/11/04 12:08:12 WARN ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException:GSS initiate failed [Caused by GS***ception: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]Bad connection to FS. command aborted. exception: Call to cdh1/192.168.56.121:8020 failed on local exception: java.io.IOException:javax.security.sasl.SaslException: GSS initiate failed [Caused by GS***ception: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)] 10.7 启动DataNodeDataNode 需要通过 JSVC 启动。首先检查是否安装了 JSVC 命令，然后配置环境变量。 在 cdh1 节点查看是否安装了 JSVC： 12$ ls /usr/lib/bigtop-utils/bigtop-detect-classpath bigtop-detect-javahome bigtop-detect-javalibs jsvc 然后编辑 /etc/default/hadoop-hdfs-datanode，取消对下面的注释并添加一行设置 JSVC_HOME，修改如下： 12345export HADOOP_SECURE_DN_USER=hdfsexport HADOOP_SECURE_DN_PID_DIR=/var/run/hadoop-hdfsexport HADOOP_SECURE_DN_LOG_DIR=/var/log/hadoop-hdfsexport JSVC_HOME=/usr/lib/bigtop-utils 将该文件同步到其他节点： 12$ scp /etc/default/hadoop-hdfs-datanode cdh2:/etc/default/hadoop-hdfs-datanode$ scp /etc/default/hadoop-hdfs-datanode cdh3:/etc/default/hadoop-hdfs-datanode 分别在 cdh2、cdh3 获取 ticket 然后启动服务： 1234#root 为 root/admin 的密码$ ssh cdh1 "kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/cdh1@JAVACHEN.COM; service hadoop-hdfs-datanode start"$ ssh cdh2 "kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/cdh2@JAVACHEN.COM; service hadoop-hdfs-datanode start"$ ssh cdh3 "kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/cdh3@JAVACHEN.COM; service hadoop-hdfs-datanode start" 观看 cdh1 上 NameNode 日志，出现下面日志表示 DataNode 启动成功： 1214/11/04 17:21:41 INFO security.UserGroupInformation:Login successful for user hdfs/cdh2@JAVACHEN.COM using keytab file /etc/hadoop/conf/hdfs.keytab 11. 总结本文介绍了 CDH Hadoop 集成 kerberos 认证的过程，其中主要需要注意以下几点： 配置 hosts，hostname 请使用小写 确保 kerberos 客户端和服务端连通 替换 JRE 自带的 JCE jar 包 为 DataNode 设置运行用户并配置 JSVC_HOME 启动服务前，先获取 ticket 再运行相关命令 上面的过程比较繁琐，我总结了上面的过程并写了一些自动化的脚本方便快速安装、配置以及管理 kerberos，请参考Hadoop集群部署权限总结。 12. 参考文章 How-to: Quickly Configure Kerberos for Your Apache Hadoop Cluster Hadoop的kerberos的实践部署 hadoop 添加kerberos认证 YARN &amp; HDFS2 安装和配置Kerberos Kerberos basics and installing a KDC Hadoop, Hbase, Zookeeper安全实践 13. 相关文章 HDFS配置Kerberos认证 YARN配置Kerberos认证 Hive配置Kerberos认证 Impala配置Kerberos认证 Zookeeper配置Kerberos认证 Hadoop配置LDAP集成Kerberos 配置安全的Hive集群集成Sentry 配置安全的Impala集群集成Sentry Hadoop集群部署权限总结]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>cdh</tag>
        <tag>kerberos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac上使用homebrew安装PostgreSql]]></title>
    <url>%2F2014%2F10%2F30%2Finstall-postgresql-on-mac-using-homebrew%2F</url>
    <content type="text"><![CDATA[安装brew 安装 postgresql ： 1$ brew install postgresql 查看安装的版本： 12$ pg_ctl -Vpg_ctl (PostgreSQL) 9.3.5 安装成功之后，安装路径为：/usr/local/var/postgres 接下来，初始化数据库： 1234567891011121314151617181920212223242526272829303132333435363738394041$ initdb /usr/local/var/postgresThe files belonging to this database system will be owned by user "june".This user must also own the server process.The database cluster will be initialized with locale "zh_CN.UTF-8".The default database encoding has accordingly been set to "UTF8".initdb: could not find suitable text search configuration for locale "zh_CN.UTF-8"The default text search configuration will be set to "simple".Data page checksums are disabled.creating directory /usr/local/var/postgres ... okcreating subdirectories ... okselecting default max_connections ... 100selecting default shared_buffers ... 128MBcreating configuration files ... okcreating template1 database in /usr/local/var/postgres/base/1 ... okinitializing pg_authid ... okinitializing dependencies ... okcreating system views ... okloading system objects' descriptions ... okcreating collations ... okcreating conversions ... okcreating dictionaries ... oksetting privileges on built-in objects ... okcreating information schema ... okloading PL/pgSQL server-side language ... okvacuuming database template1 ... okcopying template1 to template0 ... okcopying template1 to postgres ... oksyncing data to disk ... okWARNING: enabling "trust" authentication for local connectionsYou can change this by editing pg_hba.conf or using the option -A, or--auth-local and --auth-host, the next time you run initdb.Success. You can now start the database server using: postgres -D /usr/local/var/postgresor pg_ctl -D /usr/local/var/postgres -l logfile start 配置开机登陆（可选）： 123$ mkdir -p ~/Library/LaunchAgents$ cp /usr/local/Cellar/postgresql/9.3.5_1/homebrew.mxcl.postgresql.plist ~/Library/LaunchAgents/$ launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist 手动启动 postgresql 1$ pg_ctl -D /usr/local/var/postgres -l /usr/local/var/postgres/server.log start 查看状态： 1pg_ctl -D /usr/local/var/postgres -l /usr/local/var/postgres/server.log status 停止： 1$ pg_ctl -D /usr/local/var/postgres -l /usr/local/var/postgres/server.log stop -s -m fast 查看进程： 12345678$ ps auxwww | grep postgresjune 56126 0.0 0.0 2432772 644 s000 S+ 5:01下午 0:00.00 grep postgresjune 56058 0.0 0.0 2467360 584 ?? Ss 5:00下午 0:00.00 postgres: stats collector processjune 56057 0.0 0.0 2611808 1744 ?? Ss 5:00下午 0:00.00 postgres: autovacuum launcher processjune 56056 0.0 0.0 2611676 696 ?? Ss 5:00下午 0:00.00 postgres: wal writer processjune 56055 0.0 0.0 2611676 944 ?? Ss 5:00下午 0:00.01 postgres: writer processjune 56054 0.0 0.0 2611676 756 ?? Ss 5:00下午 0:00.00 postgres: checkpointer processjune 56044 0.0 0.2 2611676 14096 s000 S 5:00下午 0:00.02 /usr/local/Cellar/postgresql/9.3.5_1/bin/postgres -D /usr/local/var/postgres 创建用户和数据库： 123456#createuser will prompt you for a password, enter it twice.$ createuser -P test$ createdb -Otest -Eutf8 test_db$ psqlpostgres=# GRANT ALL PRIVILEGES ON test TO test;postgres=# \q 进入命令行模式： 1$ psql -U test test_db -h localhost -W 如果出现 FATAL: Ident authentication failed for user，是因为： This is because by default PostgreSQL uses ‘ident’ authentication i.e it checks if the username exists on the system. You need to change authentication mode to ‘trust’ as we do not want to add a system user.Modify the settings in “pg_hba.conf” to use ‘trust’ authentication. 请修改 /usr/local/var/postgres/pg_hba.conf 为： 123host all all 127.0.0.1/32 trust# IPv6 local connections:host all all ::1/128 trust 安装 pgadmin，下载地址：http://www.pgadmin.org/download/macosx.php 卸载1$ brew uninstall postgres 如果配置了开机登陆： 12$ launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist$ rm -rf ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>postgresql</tag>
        <tag>mac</tag>
        <tag>homebrew</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Impala查询功能测试]]></title>
    <url>%2F2014%2F10%2F24%2Fimpala-query-table-tutorial%2F</url>
    <content type="text"><![CDATA[关于 Impala 使用方法的一些测试，包括加载数据、查看数据库、聚合关联查询、子查询等等。 1. 准备测试数据以下测试以 impala 用户来运行： 12345678910$ su - impala-bash-4.1$ whoamiimpala$ hdfs dfs -ls /userFound 5 itemsdrwxr-xr-x - hdfs hadoop 0 2014-09-22 18:36 /user/hdfsdrwxrwxrwt - mapred hadoop 0 2014-07-23 21:37 /user/historydrwxr-xr-x - hive hadoop 0 2014-08-04 16:57 /user/hivedrwxr-xr-x - impala hadoop 0 2014-10-24 10:13 /user/impaladrwxr-xr-x - root hadoop 0 2014-09-22 10:22 /user/root 准备一些测试数据，tab1.csv 文件内容如下： 123451,true,123.123,2012-10-24 08:55:00 2,false,1243.5,2012-10-25 13:40:003,false,24453.325,2008-08-22 09:33:21.1234,false,243423.325,2007-05-12 22:32:21.334545,true,243.325,1953-04-22 09:11:33 tab1.csv 文件内容如下： 1234567891,true,12789.1232,false,1243.53,false,24453.3254,false,2423.325450,true,243.32560,false,243565423.32570,true,243.32580,false,243423.32590,true,243.325 将这两个表上传到 hdfs： 1234567891011$ hdfs dfs -mkdir -p sample_data/tab1 sample_data/tab2$ hdfs dfs -put tab1.csv /user/impala/sample_data/tab1$ hdfs dfs -ls /user/impala/sample_data/tab1Found 1 items-rw-r--r-- 3 impala hadoop 193 2014-10-24 10:13 /user/impala/sample_data/tab1/tab1.csv$ hdfs dfs -put tab2.csv /user/impala/sample_data/tab2$ hdfs dfs -ls /user/impala/sample_data/tab2Found 1 items-rw-r--r-- 3 impala hadoop 158 2014-10-24 10:13 /user/impala/sample_data/tab2/tab2.csv 在 impala 中建表，建表语句如下： 12345678910111213141516171819202122232425262728DROP TABLE IF EXISTS tab1;CREATE EXTERNAL TABLE tab1 ( id INT, col_1 BOOLEAN, col_2 DOUBLE, col_3 TIMESTAMP)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','LOCATION '/user/impala/sample_data/tab1';DROP TABLE IF EXISTS tab2;CREATE EXTERNAL TABLE tab2 ( id INT, col_1 BOOLEAN, col_2 DOUBLE)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','LOCATION '/user/impala/sample_data/tab2';DROP TABLE IF EXISTS tab3;CREATE TABLE tab3 ( id INT, col_1 BOOLEAN, col_2 DOUBLE, month INT, day INT)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; 其中 tab1 和 tab2 都是外部表，tab3 是内部表。 将上面 sql 保存在 init.sql 语句，然后运行下面命令进行创建表： 1$ impala-shell -i localhost -f init.sql 也可以进入到 impala-shell 命令行模式，直接运行 sql 语句。 2. 查看表结构查看所有数据库： 12345678910[192.168.56.121:21000] &gt; show databases;Query: show databases+------------------+| name |+------------------+| _impala_builtins || default | | testdb |+------------------+Returned 3 row(s) in 0.05s 查看默认数据库下的所有表： 12345678910[192.168.56.121:21000] &gt; show tables;Query: show tables+------+| name |+------+| tab1 || tab2 || tab3 |+------+Returned 3 row(s) in 0.01s 查看 tab1 表结构： 1234567891011[192.168.56.121:21000] &gt; describe tab1;Query: describe tab1+-------+-----------+---------+| name | type | comment |+-------+-----------+---------+| id | int | || col_1 | boolean | || col_2 | double | || col_3 | timestamp | |+-------+-----------+---------+Returned 4 row(s) in 0.07s 3. impala-shell 命令使用 impala-shell 进入命令行交互模式： 1$ impala-shell -i localhost 传入一个文件： 1$ impala-shell -i localhost -f init.sql 执行指定的 sql： 1$ impala-shell -i localhost -q 'select count(*) from tab1;' 4. 导入数据并查询导入数据： 准备数据 创建表 加数据导入到创建的表 查询数据： 12345678910111213141516171819202122232425262728293031323334353637383940[192.168.56.121:21000] &gt; SELECT * FROM tab1;Query: select * FROM tab1+----+-------+------------+-------------------------------+| id | col_1 | col_2 | col_3 |+----+-------+------------+-------------------------------+| 1 | true | 123.123 | 2012-10-24 08:55:00 || 2 | false | 1243.5 | 2012-10-25 13:40:00 || 3 | false | 24453.325 | 2008-08-22 09:33:21.123000000 || 4 | false | 243423.325 | 2007-05-12 22:32:21.334540000 || 5 | true | 243.325 | 1953-04-22 09:11:33 |+----+-------+------------+-------------------------------+Returned 5 row(s) in 0.24s[192.168.56.121:21000] &gt; SELECT * FROM tab2;Query: select * FROM tab2+----+-------+---------------+| id | col_1 | col_2 |+----+-------+---------------+| 1 | true | 12789.123 || 2 | false | 1243.5 || 3 | false | 24453.325 || 4 | false | 2423.3254 || 50 | true | 243.325 || 60 | false | 243565423.325 || 70 | true | 243.325 || 80 | false | 243423.325 || 90 | true | 243.325 |+----+-------+---------------+Returned 9 row(s) in 0.44s[192.168.56.121:21000] &gt; SELECT * FROM tab2 LIMIT 5;Query: select * FROM tab2 LIMIT 5+----+-------+-----------+| id | col_1 | col_2 |+----+-------+-----------+| 1 | true | 12789.123 || 2 | false | 1243.5 || 3 | false | 24453.325 || 4 | false | 2423.3254 || 50 | true | 243.325 |+----+-------+-----------+Returned 5 row(s) in 0.44s 带 OFFSET 语句查询 带 OFFSET 语句查询，需要和 order by 一起使用，起始编号从 0 开始往后偏移，offset 为 0 时，其结果和去掉 offset 的 limit 结果一致。 测试如下： 1234567891011121314151617181920[192.168.56.121:21000] &gt; SELECT * FROM tab2 order by id LIMIT 3 offset 0;Query: select * FROM tab2 order by id LIMIT 3 offset 0+----+-------+-----------+| id | col_1 | col_2 |+----+-------+-----------+| 1 | true | 12789.123 || 2 | false | 1243.5 || 3 | false | 24453.325 |+----+-------+-----------+Returned 3 row(s) in 0.45s[192.168.56.121:21000] &gt; SELECT * FROM tab2 order by id LIMIT 3 offset 2;Query: select * FROM tab2 order by id LIMIT 3 offset 2+----+-------+-----------+| id | col_1 | col_2 |+----+-------+-----------+| 3 | false | 24453.325 || 4 | false | 2423.3254 || 50 | true | 243.325 |+----+-------+-----------+Returned 3 row(s) in 0.45s 5. join 连接查询5.1 左外连接：1234567891011[192.168.56.121:21000] &gt; SELECT tab1.id,tab1.col_1,tab2.col_2 FROM tab1 LEFT OUTER JOIN tab2 USING (id);+----+-------+-----------+| id | col_1 | col_2 |+----+-------+-----------+| 1 | true | 12789.123 || 2 | false | 1243.5 || 3 | false | 24453.325 || 4 | false | 2423.3254 || 5 | true | NULL |+----+-------+-----------+Returned 5 row(s) in 1.12s 以上 SQL 语句等同于下面语句，用法同样适用于多个字段： 1SELECT tab1.id,tab1.col_1,tab2.col_2 FROM tab1 LEFT OUTER JOIN tab2 where tab1.id=tab2.id; 由上可以看到左边表 tab1 的记录都查询出来了，右边表 tab2 只查询出跟 tab1 关联的记录。 5.2 内连接12345678910[192.168.56.121:21000] &gt; SELECT tab1.id,tab1.col_1,tab2.col_2 FROM tab1 INNER JOIN tab2 USING (id);+----+-------+-----------+| id | col_1 | col_2 |+----+-------+-----------+| 1 | true | 12789.123 || 2 | false | 1243.5 || 3 | false | 24453.325 || 4 | false | 2423.3254 |+----+-------+-----------+Returned 4 row(s) in 0.53s 以上语句可以修改为： 123-- 下面语句都是内连接SELECT tab1.id,tab1.col_1,tab2.col_2 FROM tab1 JOIN tab2 USING (id);SELECT tab1.id,tab1.col_1,tab2.col_2 FROM tab1 , tab2 where tab1.id=tab2.id ; 查询结果为： 1234567891011[192.168.56.121:21000] &gt; SELECT tab1.id,tab1.col_1,tab2.col_2 FROM tab1 , tab2 where tab1.id=tab2.id ;Query: select tab1.id,tab1.col_1,tab2.col_2 FROM tab1 , tab2 where tab1.id=tab2.id+----+-------+-----------+| id | col_1 | col_2 |+----+-------+-----------+| 1 | true | 12789.123 || 2 | false | 1243.5 || 3 | false | 24453.325 || 4 | false | 2423.3254 |+----+-------+-----------+Returned 4 row(s) in 0.38s 如果去掉 where 语句，会提示错误： 123[192.168.56.121:21000] &gt; select tab1.id,tab1.col_1,tab2.col_2 FROM tab1 , tab2;Query: select tab1.id,tab1.col_1,tab2.col_2 FROM tab1 , tab2ERROR: NotImplementedException: Join with 'default.tab2' requires at least one conjunctive equality predicate. To perform a Cartesian product between two tables, use a CROSS JOIN. 5.3 自连接impala 允许自连接，例如： 12-- Combine fields from both parent and child rows.SELECT lhs.id, rhs.parent, lhs.c1, rhs.c2 FROM tree_data lhs, tree_data rhs WHERE lhs.id = rhs.parent; 5.4 交叉连接为了避免产生大量的结果集，impala 不允许下面形式的笛卡尔连接： 12SELECT ... FROM t1 JOIN t2;SELECT ... FROM t1, t2; 如果，你的确想使用笛卡尔连接，建议使用 cross join： 12345678910111213141516171819202122232425[192.168.56.121:21000] &gt; select tab1.id,tab1.col_1,tab2.col_2 FROM tab1 CROSS JOIN tab2 where tab1.id&lt;3;Query: select tab1.id,tab1.col_1,tab2.col_2 FROM tab1 CROSS JOIN tab2 where tab1.id&lt;3+----+-------+---------------+| id | col_1 | col_2 |+----+-------+---------------+| 1 | true | 12789.123 || 1 | true | 1243.5 || 1 | true | 24453.325 || 1 | true | 2423.3254 || 1 | true | 243.325 || 1 | true | 243565423.325 || 1 | true | 243.325 || 1 | true | 243423.325 || 1 | true | 243.325 || 2 | false | 12789.123 || 2 | false | 1243.5 || 2 | false | 24453.325 || 2 | false | 2423.3254 || 2 | false | 243.325 || 2 | false | 243565423.325 || 2 | false | 243.325 || 2 | false | 243423.325 || 2 | false | 243.325 |+----+-------+---------------+Returned 18 row(s) in 0.41s 5.5 等值连接和非等值连接默认地，impala的两表连接需要一个等值的比较，或者使用 ON、USING、WHERE 语句。在Impala 1.2.2 之后，非等值连接也支持。同样需要避免因为产生大量的结果集而造成内存溢出。一旦你想使用非等值连接，建议使用 cross 连接并增加额外的 where 语句。 12345678910111213141516171819202122[192.168.56.121:21000] &gt; select tab1.id,tab1.col_1,tab1.col_2,tab2.col_2 FROM tab1 CROSS JOIN tab2 where tab1.col_2 &gt;tab2.col_2 ;+----+-------+------------+-----------+| id | col_1 | col_2 | col_2 |+----+-------+------------+-----------+| 2 | false | 1243.5 | 243.325 || 2 | false | 1243.5 | 243.325 || 2 | false | 1243.5 | 243.325 || 3 | false | 24453.325 | 12789.123 || 3 | false | 24453.325 | 1243.5 || 3 | false | 24453.325 | 2423.3254 || 3 | false | 24453.325 | 243.325 || 3 | false | 24453.325 | 243.325 || 3 | false | 24453.325 | 243.325 || 4 | false | 243423.325 | 12789.123 || 4 | false | 243423.325 | 1243.5 || 4 | false | 243423.325 | 24453.325 || 4 | false | 243423.325 | 2423.3254 || 4 | false | 243423.325 | 243.325 || 4 | false | 243423.325 | 243.325 || 4 | false | 243423.325 | 243.325 |+----+-------+------------+-----------+Returned 16 row(s) in 0.41s 查询出来的结果会有一些重复的记录，这个时候可以通过 distinct 去重。 5.6 半连接左半连接是为了实现 in 语句，左边的记录会查询出来，而不管右边表有多少匹配的记录。Impala 2.0版本之后，支持右半连接。 1234567891011[192.168.56.121:21000] &gt; SELECT tab1.id,tab1.col_1,tab2.col_2 FROM tab1 LEFT SEMI JOIN tab2 USING (id);Query: select tab1.id,tab1.col_1,tab2.col_2 FROM tab1 LEFT SEMI JOIN tab2 USING (id)+----+-------+-----------+| id | col_1 | col_2 |+----+-------+-----------+| 1 | true | 12789.123 || 2 | false | 1243.5 || 3 | false | 24453.325 || 4 | false | 2423.3254 |+----+-------+-----------+Returned 4 row(s) in 0.41s 5.7 自然连接（不支持）Impala 不支持 NATURAL JOIN 操作，以避免产生不一致或者大量的结果。自然连接不适应 ON 和 USING 语句，而是自动的关联所有列相同值的记录。这种连接是不建议的，特别是当表结构发生变化的时候，如添加或者删除列的时候，会产生不一样的结果集。 123456789-- 'NATURAL' is interpreted as an alias for 't1' and Impala attempts an inner join,-- resulting in an error because inner joins require explicit comparisons between columns.SELECT t1.c1, t2.c2 FROM t1 NATURAL JOIN t2;ERROR: NotImplementedException: Join with 't2' requires at least one conjunctive equality predicate. To perform a Cartesian product between two tables, use a CROSS JOIN.-- If you expect the tables to have identically named columns with matching values,-- list the corresponding column names in a USING clause.SELECT t1.c1, t2.c2 FROM t1 JOIN t2 USING (id, type_flag, name, address); 5.8 反连接（Impala 2.0 / CDH 5.2 以上版本）Impala 2.0 / CDH 5.2 以上版本中支持反连接，包括左反连接和右反连接。左反连接的意思是返回左边表不在右边表中的记录。 找出 tab2 的 id 不在 tab1 中的记录： 1234567891011[192.168.56.121:21000] &gt; SELECT tab2.id FROM tab2 LEFT ANTI JOIN tab1 USING (id);+----+| id |+----+| 50 || 60 || 70 || 80 || 90 |+----+Returned 5 row(s) in 0.41s 6. 聚合查询聚合关联查询： 1234567[192.168.56.121:21000] &gt; select tab1.col_1, MAX(tab2.col_2), MIN(tab2.col_2) FROM tab2 JOIN tab1 USING (id) GROUP BY col_1 ORDER BY 1 LIMIT 5 ;+-------+-----------------+-----------------+| col_1 | max(tab2.col_2) | min(tab2.col_2) |+-------+-----------------+-----------------+| false | 24453.325 | 1243.5 || true | 12789.123 | 12789.123 |+-------+-----------------+-----------------+ 聚合关联子查询： 12345678[192.168.56.121:21000] &gt; select tab2.* FROM tab2, (SELECT tab1.col_1, MAX(tab2.col_2) AS max_col2 FROM tab2, tab1 WHERE tab1.id = tab2.id GROUP BY col_1) subquery1 WHERE subquery1.max_col2 = tab2.col_2 ;+----+-------+-----------+| id | col_1 | col_2 |+----+-------+-----------+| 1 | true | 12789.123 || 3 | false | 24453.325 |+----+-------+-----------+Returned 2 row(s) in 0.54s Impala 2版本中，支持where 条件子查询，包括 IN 、EXISTS 和比较符的子查询： 123select tab2.* from tab2 where tab2.id IN (select max(id) from tab1)select tab2.* from tab2 where tab2.id EXISTS (select max(id) from tab1)select tab2.* from tab2 where tab2.id &gt; (select max(id) from tab1) 插入查询： 12[192.168.56.121:21000] &gt; insert OVERWRITE TABLE tab3 SELECT id, col_1, col_2, MONTH(col_3), DAYOFMONTH(col_3) FROM tab1 WHERE YEAR(col_3) = 2012 ;Inserted 2 rows in 0.44s 这时候查询 tab3 的记录： 1234567[192.168.56.121:21000] &gt; SELECT * FROM tab3;+----+-------+---------+-------+-----+| id | col_1 | col_2 | month | day |+----+-------+---------+-------+-----+| 1 | true | 123.123 | 10 | 24 || 2 | false | 1243.5 | 10 | 25 |+----+-------+---------+-------+-----+ 7. 参考文章 Cloudera Impala Guide - Impala Joins]]></content>
      <categories>
        <category>impala</category>
      </categories>
      <tags>
        <tag>impala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[当前数据仓库建设过程]]></title>
    <url>%2F2014%2F10%2F23%2Fhive-warehouse-in-2014%2F</url>
    <content type="text"><![CDATA[一个典型的企业数据仓库通常包含数据采集、数据加工和存储、数据展现等几个过程，本篇文章将按照这个顺序记录部门当前建设数据仓库的过程。 1. 数据采集和存储采集数据之前，先要定义数据如何存放在 hadoop 以及一些相关约束。约束如下： 所有的日志数据都存放在 hdfs 上的 /logroot 路径下面 hive 中数据库命名方式为 dw_XXXX，例如：dw_srclog 存放外部来源的原始数据，dw_stat 存放统计结果的数据 原始数据都加工成为结构化的文本文件，字段分隔符统一使用制表符，并在 lzo 压缩之后上传到 hdfs 中。 hive 中使用外部表保存数据，数据存放在 /logroot 下，如果不是分区表，则文件名为表名；如果是分区表，则按月和天分区，每天分区下的文件名为表名_日期，例如：test_20141023 数据采集的来源可能是关系数据库或者一些系统日志，采集工具可以是日志采集系统，例如：flume、sqoop 、storm以及一些 ETL 工具等等。 目前，主要是从 mysql 中导出数据然后在导入到 hdfs 中，对于存储不需要按天分区的表，这部分过程代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#!/bin/bashif [ "$1" ]; then DAY="$1"else DAY="yesterday"fidatestr=`date +%Y-%m-%d -d"$DAY"`;logday=`date +%Y%m%d -d"$DAY"`;logmonth=`date +%Y%m -d"$DAY"`#hive tabletable=test#mysql db config filesrcdb=db_namesql="select * from test"hql="use dw_srclog;create external table if not exists test ( id int, name int)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'STORED AS INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'LOCATION '/logroot/test';"#beginchmod +x $srcdb.sql. ./$srcdb.sqlfile="$&#123;table&#125;"sql_var=" -r -quick --default-character-set=utf8 --skip-column"mysql $sql_var -h$&#123;db_host&#125; -u$&#123;db_user&#125; -p$&#123;db_pass&#125; -P$&#123;db_port&#125; -D$&#123;db_name&#125; -e "$sql" | sed "s/NULL/\\\\N/g"&gt; $file 2&gt;&amp;1lzop -U $filehadoop fs -mkdir -p /logroot/$tablehadoop fs -ls /logroot/$table |grep lzo|awk '&#123;print $8&#125;'|xargs -i hadoop fs -rm &#123;&#125; hadoop fs -moveFromLocal $file.lzo /logroot/$table/hadoop jar /usr/lib/hadoop/lib/hadoop-lzo.jar com.hadoop.compression.lzo.LzoIndexer /logroot/$table/$file.lzo 2&gt;&amp;1echo "create table if not exists"hive -v -e "$hql;" 2&gt;&amp;1 上面 bash 代码逻辑如下： 1、判断是否输入参数，如果没有参数，则取昨天，意思是每天读取 mysql 数据库中昨天的数据。 2、定义 mysql 中 select 查询语句 3、定义 hive 中建表语句 4、读取 mysql 数据库连接信息，上例中为从 db_name.sql 中读取 db_host、db_user、db_pass、db_port、db_name 五个变量 5、运行 mysql 命令导出指定 sql 查询的结果，并将结果中的 NULL 字段转换为 \\N，因为 \ 在 bash 中是转义字符，故需要使用两个 \ 6、lzo 压缩文件并上传到 hdfs，并且创建 lzo 索引 7、最后删除本地文件 对于分区表来说，建表语句如下： 12345678910111213use dw_srclog;create external table if not exists test_p ( id int, name int)partitioned by (key_ym int, key_ymd int)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'STORED AS INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'LOCATION '/logroot/test_p'; 从 mysql 导出文件并上传到 hdfs 命令如下： 12345678910111213141516#beginchmod +x $srcdb.sql. ./$srcdb.sqlfile="$&#123;table&#125;_$logday"sql_var=" -r -quick --default-character-set=utf8 --skip-column"mysql $sql_var -h$&#123;db_host&#125; -u$&#123;db_user&#125; -p$&#123;db_pass&#125; -P$&#123;db_port&#125; -D$&#123;db_name&#125; -e "$sql" | sed "s/NULL/\\\\N/g"&gt; $file 2&gt;&amp;1lzop -U $filehadoop fs -mkdir -p /logroot/$table/key_ym=$logmonth/key_ymd=$logdayhadoop fs -ls /logroot/$table/key_ym=$logmonth/key_ymd=$logday/ |grep lzo|awk '&#123;print $8&#125;'|xargs -i hadoop fs -rm &#123;&#125; 2&gt;&amp;1hadoop fs -moveFromLocal $file.lzo /logroot/$table/key_ym=$logmonth/key_ymd=$logday/hadoop jar /usr/lib/hadoop/lib/hadoop-lzo.jar com.hadoop.compression.lzo.LzoIndexer /logroot/$table/key_ym=$logmonth/key_ymd=$logday/$file.lzo 2&gt;&amp;1hive -v -e "$hql;ALTER TABLE $table ADD IF NOT EXISTS PARTITION(key_ym=$logmonth,key_ymd=$logday) location '/logroot/$table/key_ym=$logmonth/key_ymd=$logday' " 2&gt;&amp;1 通过上面的两个命令就可以实现将 mysql 中的数据导入到 hdfs 中。 这里需要注意以下几点： 1、 hive 中原始日志使用默认的 textfile 方式存储，是为了保证日志的可读性，方便以后从 hdfs 下载来之后能够很方便的转换为结构化的文本文件并能浏览文件内容。 2、使用 lzo 压缩是为了节省存储空间 3、使用外包表建表，在删除表结构之后数据不会删，方便修改表结构和分区。 使用 Sqoop使用 sqoop 主要是用于从 oracle 中通过 jdbc 方式导出数据到 hdfs，sqoop 命令如下： 1sqoop import --connect jdbc:oracle:thin:@192.168.56.121:2154:db --username bi_user_limit --password 'XXXX' --query "select * from test where \$CONDITIONS" --split-by id -m 5 --fields-terminated-by '\t' --lines-terminated-by '\n' --null-string '\\N' --null-non-string '\\N' --target-dir "/logroot/test/key_ymd=20140315" --delete-target-dir 2. 数据加工对于数据量比较小任务可以使用 impala 处理，对于数据量大的任务使用 hive hql 来处理。 impala 处理数据： 1impala-shell -i '192.168.56.121:21000' -r -q "$sql;" 有时候需要使用 impala 导出数据： 12impala-shell -i &apos;192.168.56.121:21000&apos; -r -q &quot;$sql;&quot; -B --output_delimiter=&quot;\t&quot; -o $filesed -i &apos;1d&apos; $file #导出的第一行有不可见的字符 使用 hive 处理数据生成结果表： 12345678910111213141516171819202122232425262728293031323334353637#!/bin/bashif [ "$1" ]; then DAY="$1"else DAY="yesterday"fiecho "DAY=$DAY"datestr=`date +%Y-%m-%d -d"$DAY"`;logday=`date +%Y%m%d -d"$DAY"`;logmonth=`date +%Y%m -d"$DAY"`#target tabletable=stat_test_psql="use dw_srclog;insert OVERWRITE table stat_test_p partition(key_ym=$logmonth,key_ymd=$logday)select id,count(name) from test_p where key_ymd=$logday group by id"hql="use dw_web;create external table if not exists goods_sales_info_day ( id int, count int) partitioned by (key_ym int, key_ymd int)STORED AS RCFILELOCATION '/logroot/stat_test_p';"#beginhive -v -e "$hql;SET hive.exec.compress.output=true;SET mapreduce.input.fileinputformat.split.maxsize=128000000;SET mapred.output.compression.type=BLOCK;SET mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;$sql" 2&gt;&amp;1 这里主要是先判断是否创建外包表（外包表存储为 RCFILE 格式），然后设置 map 的输出结果使用 snappy 压缩，并设置每个 map 的大小，最后运行 insert 语句。结果表存储为 RCFILE 的原因是，在 CDH 5.2 之前，该格式的表可以被 impala 读取。 任务调度当任务多了之后，每个任务之间会有一些依赖，为了保证任务的先后执行顺序，这里使用的是 azkaban 任务调度框架。 该框架的使用方式很简单： 首先创建一个 bi_etl 目录，用于存放执行脚本。 在 bi_etl 目录下创建一个 properties 文件，文件名称任意，文件内容为：DAY=yesterday，这是一个系统默认参数，即默认 DAY 变量的值为 yesterday，该变量在运行时可以被覆盖：在 azkaban 的 web 管理界面，运行一个 Flow 时，添加一个 Flow Parameters 参数，Name 为 DAY，Value 为你想要指定的值，例如：20141023。 创建一个 bash 脚本 test.sh，文件内容如第一章节内容，需要注意的是该脚本中会判断是否有输出参数。 针对 bash 脚本，创建 azkaban 需要的 job 文件，文件内容如下（azkaban 运行该 job 时候，会替换 ${DAY} 变量为实际的值 ）： 1234type=commandcommand=sh test.sh $&#123;DAY&#125;failure.emails=XXX@163.comdependencies=xxx 最后，将 bi_etl 目录打包成 zip 文件，然后上传到 azkaban 管理界面上去，就可以运行或者是设置调度任务了。 使用上面的方式编写 bash 脚本和 azkaban 的 job 的好处是： azkaban 的 job 可以指定参数来控制运行哪一天的任务 job 中实际上运行的是 bash 脚本，这些脚本脱离了 azkaban 也能正常运行，同样也支持传参数。 3. 数据展现目前是将 hive 或者 impala 的处理结果推送到关系数据库中，由传统的 BI 报表工具展示数据或者直接通过 impala 查询数据生成报表并发送邮件。 为了保证报表的正常发送，需要监控任务的正常运行，当任务失败的时候能够发送邮件，这部分通过 azkaban 可以做到。另外，还需要监控每天运行的任务同步的记录数，下面脚本是统计记录数为0的任务： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#!/bin/bashif [ "$1" ]; then DAY="$1"else DAY="yesterday"fiecho "DAY=$DAY"datestr=`date +%Y-%m-%d -d"$DAY"`;logday=`date +%Y%m%d -d"$DAY"`;logmonth=`date +%Y%m -d"$DAY"`datemod=`date +%w -d "yesterday"`rm -rf /tmp/stat_table_day_count_$logdaytouch /tmp/stat_table_day_count_$logdayfor db in `hadoop fs -ls /user/hive/warehouse|grep -vE 'testdb|dw_etl'|grep '.db'|awk '&#123;print $8&#125;'|awk -F '/' '&#123;print $5&#125;' |awk -F '.' '&#123;print $1&#125;'`;do for table in `hive -S -e "set hive.cli.print.header=false; use $db;show tables" ` ;do count_new="" result=`hive -S -e "set hive.cli.print.header=false; use $db;show create table $table;" 2&gt;&amp;1 | grep PARTITIONED` if [ $&#123;#result&#125; -gt 0 ];then is_part=1 count_new=`impala-shell -k -i 10.168.35.127:21089 --quiet -B --output_delimiter="\t" -q "select count(1) from $&#123;db&#125;.$table where key_ymd=$logday "` else is_part=0 count_new=`impala-shell -k -i 10.168.35.127:21089 --quiet -B --output_delimiter="\t" -q "select count(1) from $&#123;db&#125;.$table; "` fi echo "$db,$table,$is_part,$count_new" &gt;&gt; /tmp/stat_table_day_count_$logday donedone#mail -s "The count of the table between old and new cluster in $datestr" -c $mails &lt; /tmp/stat_table_day_count_$logdaysed -i 's/1034h//g' /tmp/stat_table_day_count_$logdaysed -i 's/\[//g' /tmp/stat_table_day_count_$logdaysed -i 's/\?//g' /tmp/stat_table_day_count_$logdaysed -i 's/\x1B//g' /tmp/stat_table_day_count_$logdayres=`cat /tmp/stat_table_day_count_$logday|grep -E '1,0|0,0'|grep -v stat_table_day_count`echo $reshive -e "use dw_default;LOAD DATA LOCAL INPATH '/tmp/stat_table_day_count_$logday' overwrite INTO TABLE stat_table_day_count PARTITION (key_ym=$logmonth,key_ymd=$logday)"python mail.py "Count is 0 in $datestr" "$res" 4. 总结上面介绍了数据采集、加工和任务调度的过程，有些地方还可以改进： 引入 ETL 工具实现关系数据库导入到 hadoop，例如：Kettle 工具 目前是每天一次从 mysql 同步数据到 hadoop，以后需要修改同步频率，做到更实时 hive 和 impala 在字段类型、存储方式、函数的兼容性上存在一些问题]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码整体架构]]></title>
    <url>%2F2014%2F09%2F29%2Fspring-source-codes%2F</url>
    <content type="text"><![CDATA[前言Spring 是一个开源框架，是为了解决企业应用程序开发复杂性而创建的。框架的主要优势之一就是其分层架构，分层架构允许您选择使用哪一个组件，同时为 J2EE 应用程序开发提供集成的框架。 从这篇文章开始，我讲开始阅读并介绍 Spring 源码的设计思想，希望能改对 Spring 框架有一个初步的全面的认识，并且学习其架构设计方面的一些理念和方法。 Spring 源码地址：https://github.com/spring-projects/spring-framework 概述Spring的整体架构Spring 总共有十几个组件，其中核心组件只有三个：Core、Context 和 Beans。以下是 Spring3的总体架构图。 组成 Spring 框架的每个模块（或组件）都可以单独存在，或者与其他一个或多个模块联合实现。每个模块的功能如下： 核心容器：核心容器提供 Spring 框架的基本功能。核心容器的主要组件是 BeanFactory，它是工厂模式的实现。BeanFactory 使用控制反转 （IOC） 模式将应用程序的配置和依赖性规范与实际的应用程序代码分开。 Spring 上下文：Spring 上下文是一个配置文件，向 Spring 框架提供上下文信息。Spring 上下文包括企业服务，例如：JNDI、EJB、电子邮件、国际化、校验和调度功能。 Spring AOP：通过配置管理特性，Spring AOP 模块直接将面向方面的编程功能集成到了 Spring 框架中。所以，可以很容易地使 Spring 框架管理的任何对象支持 AOP。Spring AOP 模块为基于 Spring 的应用程序中的对象提供了事务管理服务。通过使用 Spring AOP，不用依赖 EJB 组件，就可以将声明性事务管理集成到应用程序中。 Spring DAO：JDBC DAO 抽象层提供了有意义的异常层次结构，可用该结构来管理异常处理和不同数据库供应商抛出的错误消息。异常层次结构简化了错误处理，并且极大地降低了需要编写的异常代码数量（例如打开和关闭连接）。Spring DAO 的面向 JDBC 的异常遵从通用的 DAO 异常层次结构。 Spring ORM：Spring 框架插入了若干个 ORM 框架，从而提供了 ORM 的对象关系工具，其中包括 JDO、Hibernate 和 iBatis SQL Map。所有这些都遵从 Spring 的通用事务和 DAO 异常层次结构。 Spring Web 模块：Web 上下文模块建立在应用程序上下文模块之上，为基于 Web 的应用程序提供了上下文。所以，Spring 框架支持与 Jakarta Struts 的集成。Web 模块还简化了处理多部分请求以及将请求参数绑定到域对象的工作。 Spring MVC 框架：MVC 框架是一个全功能的构建 Web 应用程序的 MVC 实现。通过策略接口，MVC 框架变成为高度可配置的，MVC 容纳了大量视图技术，其中包括 JSP、Velocity、Tiles、iText 和 POI。 从下图（该图来自SPRING 3.2.X 源代码分析之二: SPRING源码的包结构）可以看出 Spring 各个模块之间的依赖关系。 从图中可以看出，IOC 的实现包 spring-beans 和 AOP 的实现包 spring-aop 也是整个框架的基础，而 spring-core 是整个框架的核心，基础的功能都在这里。 在此基础之上，spring-context 提供上下文环境，为各个模块提供粘合作用。 在 spring-context 基础之上提供了 spring-tx 和 spring-orm包，而web部分的功能，都是要依赖spring-web来实现的。 Spring 的设计理念Spring 是面向 Bean 的编程（BOP,Bean Oriented Programming），Bean 在 Spring 中才是真正的主角。Bean 在 Spring 中作用就像 Object 对 OOP 的意义一样，没有对象的概念就像没有面向对象编程，Spring 中没有 Bean 也就没有 Spring 存在的意义。Spring 提供了 IOC 容器通过配置文件或者注解的方式来管理对象之间的依赖关系。 控制反转模式（也称作依赖性介入）的基本概念是：不创建对象，但是描述创建它们的方式。在代码中不直接与对象和服务连接，但在配置文件中描述哪一个组件需要哪一项服务。容器 （在 Spring 框架中是 IOC 容器） 负责将这些联系在一起。 在典型的 IOC 场景中，容器创建了所有对象，并设置必要的属性将它们连接在一起，决定什么时间调用方法。 面向方面的编程面向方面的编程，即 AOP，是一种编程技术，它允许程序员对横切关注点或横切典型的职责分界线的行为（例如日志和事务管理）进行模块化。AOP 的核心构造是方面，它将那些影响多个类的行为封装到可重用的模块中。 AOP 和 IOC 是补充性的技术，它们都运用模块化方式解决企业应用程序开发中的复杂问题。在典型的面向对象开发方式中，可能要将日志记录语句放在所有方法和 Java 类中才能实现日志功能。在 AOP 方式中，可以反过来将日志服务模块化，并以声明的方式将它们应用到需要日志的组件上。当然，优势就是 Java 类不需要知道日志服务的存在，也不需要考虑相关的代码。所以，用 Spring AOP 编写的应用程序代码是松散耦合的。 AOP 的功能完全集成到了 Spring 事务管理、日志和其他各种特性的上下文中。 IOC 容器Spring 设计的核心是 org.springframework.beans 包，它的设计目标是与 JavaBean 组件一起使用。这个包通常不是由用户直接使用，而是由服务器将其用作其他多数功能的底层中介。下一个最高级抽象是 BeanFactory 接口，它是工厂设计模式的实现，允许通过名称创建和检索对象。BeanFactory 也可以管理对象之间的关系。 BeanFactory 支持两个对象模型。 单例。 模型提供了具有特定名称的对象的共享实例，可以在查询时对其进行检索。Singleton 是默认的也是最常用的对象模型。对于无状态服务对象很理想。 原型。 模型确保每次检索都会创建单独的对象。在每个用户都需要自己的对象时，原型模型最适合。 bean 工厂的概念是 Spring 作为 IOC 容器的基础。IOC 将处理事情的责任从应用程序代码转移到框架。正如我将在下一个示例中演示的那样，Spring 框架使用 JavaBean 属性和配置数据来指出必须设置的依赖关系 Spring4 的系统架构图 Spring 4.0.x对比Spring3.2.x的系统架构变化（以下文字摘抄于SPRING 3.2.X 源代码分析之三: SPRING源码的整体架构分析）: 从图中可以看出，总体的层次结构没有太大变化，变化的是 Spring 4.0.3去掉了 struts 模块(spring-struts包)。现在的 Spring mvc的确已经足够优秀了，大量的 web 应用均已经使用了 Spring mvc。而 struts1.x 的架构太落后了，struts2.x 是 struts 自身提供了和 Spring 的集成包，但是由于之前版本的 struts2 存在很多致命的安全漏洞，所以，大大影响了其使用度，好在最新的2.3.16版本的 struts 安全有所改善，希望不会再出什么大乱子。 web 部分去掉了 struts 模块，但是增加 WebSocket 模块(spring-websocket包)，增加了对 WebSocket、SockJS 以及 STOMP 的支持，它与 JSR-356 Java WebSocket API 兼容。另外，还提供了基于 SockJS（对 WebSocket 的模拟）的回调方案，以适应不支持 WebSocket 协议的浏览器。 同时，增加了 messaging 模块(spring-messaging)，提供了对 STOMP 的支持，以及用于路由和处理来自 WebSocket 客户端的 STOMP 消息的注解编程模型。spring-messaging 模块中还 包含了 Spring Integration 项目中的核心抽象类，如 Message、MessageChannel、MessageHandler。 如果去看源代码的话，还可以发现还有一个新增的包，加强了 beans 模块，就是 spring-beans-groovy。应用可以部分或完全使用 Groovy 编写。借助于 Spring 4.0，能够使用 Groovy DSL 定义外部的 Bean 配置，这类似于 XML Bean 声明，但是语法更为简洁。使用Groovy还能够在启动代码中直接嵌入Bean的声明。 还有一些： 删除过时的包和方法。具体API变动可以参考变动报告，第三方类库至少使用2010/2011年发布的版本，尤其是Hibernate 3.6+, EhCache 2.1+, Quartz 1.8+, Groovy 1.8+, and Joda-Time 2.0+。Hibernate Validator要求使用4.3+，Jackson 2.0+。 Java 8支持。当然也支持Java6和Java7，但最好在使用Spring框架3.X或4.X时，将JDK升级到Java7，因为有些版本至少需要Java7。 Java EE 6和7。使用Spring4.x时Java EE版本至少要6或以上，且需要JPA 2.0和Servlet 3.0 的支持，所以服务器，web容器需要做相应的升级。一个更具前瞻性的注意是，Spring4.0支持J2EE 7的适用级规范，比如JMS 2.0， JTA 1.2， JPA 2.1， Bean Validation 1.1和JSR-236并发工具包，在选择这些jar包时需要注意版本。 使用Groovy DSL定义外部Bean。 核心容器提升。 1、支持Bean的泛型注入，比如：@Autowired Repository&lt;Customer&gt; customerRepository 2、使用元注解开发暴露指定内部属性的自定义注解。 3、通过 @Ordered 注解或Ordered 接口对注入集合或数组的 Bean 进行排序。 4、@Lazy 注解可以用在注入点或 @Bean 定义上。 5、为开发者引入 @Description 注解。 6、引入 @Conditional 注解进行有条件的 Bea n过滤。 7、基于 CGLIB 的代理类不需要提供默认构造器，因为 Spring 框架将 CGLIB 整合到内部了。 8、框架支持时区管理，比如 LocalContext。 Web提升。 1、增加新的 @RestController 注解，这样就不需要在每个 @RequestMapping 方法中添加 @ResponseBody 注解。 2、添加 AsyncRestTemplate，在开发 REST 客户端时允许非阻塞异步支持。 3、为 Spring MVC 应用程序开发提供全面的时区支持。 WebSocket，SockJS 和 STOMP 消息。 测试提升。 1、spring-test 模块里的几乎所有注解都能被用做元注解去创建自定义注解，来减少跨测试集时的重复配置。 2、活跃的 bean 定义配置文件可以编程方式解析。 3、spring-core 模块里引入一个新的 SocketUtils 类，用于扫描本地可使用的 TCP 和 UDP 服务端口。一般用于测试需要 socket 的情况，比如测试开启内存 SMTP 服务，FTP 服务，Servlet 容器等。 4、由于 Spring4.0 的原因，org.springframework.mock.web 包现在基于 Servlet 3.0 API。 阅读过程因为 Spring 是分模块的，所以阅读 Spring 3.2.11 版本的源码过程打算先从最底层的模块开始，然后再由下向上分析每一个模块的实现过程。在阅读过程中，随着对代码的理解加深，也会重新阅读已经读过的代码。 大概的阅读顺序： spring-core：了解 Spring 提供哪些工具类以及一些基础的功能，如对资源文件的封装、对 Propertis 文件操作的封装、对 Environment 的封装等等。 spring-context：通过分析 Spring 的启动方式，了解 Spring xml 文件的解析过程，bean 的初始化过程 spring-orm spring-tx spring-web 注意：这里使用的Spring 版本是 3.2.11，下载地址在：https://github.com/spring-projects/spring-framework/releases 搭建测试环境Spring 源码编译过程，这里不做说明。 为了测试简单，可以单独创建 java 项目，编写一些测试用例或者例子来测试 Spring 的功能。我使用的测试环境是： Idea 13 Maven 3.0.5 JDK1.6 生成 maven 项目命令： 12$ mvn archetype:generate -DgroupId=com.javachen.spring.core -DartifactId=Spring3Example -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false 转换成 idea 项目： 1mvn idea:idea 然后，根据使用情况添加 Spring 的依赖包： 123456789101112131415161718192021222324252627282930313233&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.javachen.spring.core&lt;/groupId&gt; &lt;artifactId&gt;Spring3Example&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;Spring3Example&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;spring.version&gt;3.2.11.RELEASE&lt;/spring.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- Spring 3 dependencies --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 记下来就可以编写测试类进行测试并关联 Spring 源代码跟踪调试代码。 Spring 的一些教程 http://www.mkyong.com/tutorials/spring-tutorials/ http://www.springbyexample.org/examples/index.html http://www.dzone.com/tutorials/java/spring/spring-tutorial/spring-tutorial.html http://viralpatel.net/blogs/category/spring/ 参考文章 Spring 框架的设计理念与设计模式分析 Spring 系列: Spring 框架简介 SPRING 3.2.X 源代码分析之二: SPRING源码的包结构 SPRING 3.2.X 源代码分析之三: SPRING源码的整体架构分析]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译Dubbo源码并测试]]></title>
    <url>%2F2014%2F09%2F24%2Fcompile-and-test-dubbo%2F</url>
    <content type="text"><![CDATA[Dubbo是阿里巴巴内部的SOA服务化治理方案的核心框架，每天为2000+ 个服务提供3,000,000,000+ 次访问量支持，并被广泛应用于阿里巴巴集团的各成员站点。Dubbo自2011年开源后，已被许多非阿里系公司使用。 项目主页：http://alibaba.github.io/dubbo-doc-static/Home-zh.htm 项目源码：https://github.com/alibaba/dubbo 1. 安装首先从 github 下载源代码并阅读 readme.md ，参考该文档，首先下载 opensesame，并编译： 123$ git clone git@github.com:alibaba/opensesame.git$ cd opensesame$ mvn install 然后，下载 dubbo 并编译： 123$ git clone git@github.com:alibaba/dubbo.git$ cd dubbo$ mvn clean install -Dmaven.test.skip 编译成功之后，生成 idea 相关配置文件： 1$ mvn idea:idea 接下来，将代码通过 maven 的方式导入到 idea ide 中。 2. 测试安装之后，现在来搭一个测试环境。搭建一个测试环境，需要下面三个角色： 消息提供者，示例工程见：dubbo-demo-provider 消息注册中心，有四种类型：multicast、zookeeper、redis、dubbo 消息消费者，示例工程见：dubbo-demo-consumer 作为测试，这里消息注册中心使用 Multicast 注册中心，以下操作是在 idea 中运行。 首先，修改 Dubbo/dubbo-demo/dubbo-demo-provider/src/test/resources/dubbo.properties 文件如下： 12345678910111213dubbo.container=log4j,springdubbo.application.name=demo-providerdubbo.application.owner=dubbo.registry.address=multicast://224.5.6.7:1234?unicast=false#dubbo.registry.address=zookeeper://127.0.0.1:2181#dubbo.registry.address=redis://127.0.0.1:6379#dubbo.registry.address=dubbo://10.1.19.41:20880#dubbo.monitor.protocol=registrydubbo.protocol.name=dubbodubbo.protocol.port=20880dubbo.service.loadbalance=roundrobin#dubbo.log4j.file=logs/dubbo-demo-consumer.log#dubbo.log4j.level=WARN 注意： 消息提供者和消息消费者建议在不同机器上运行，如果在同一机器上，需设置 unicast=false：即：multicast://224.5.6.7:1234?unicast=false，否则发给消费者的单播消息可能被提供者抢占，两个消费者在同一台机器也一样，只有 multicast 注册中心有此问题。 然后，修改 Dubbo/dubbo-demo/dubbo-demo-consumer/src/test/resources/dubbo.properties 文件如下： 12345678910dubbo.container=log4j,springdubbo.application.name=demo-consumerdubbo.application.owner=dubbo.registry.address=multicast://224.5.6.7:1234?unicast=false#dubbo.registry.address=zookeeper://127.0.0.1:2181#dubbo.registry.address=redis://127.0.0.1:6379#dubbo.registry.address=dubbo://10.1.19.41:20880dubbo.monitor.protocol=registry#dubbo.log4j.file=logs/dubbo-demo-consumer.log#dubbo.log4j.level=WARN 接下来，就可以运行 dubbo-demo-provider 和 dubbo-demo-consumer 了。 在 idea 中右键运行 Dubbo/dubbo-demo/dubbo-demo-provider/src/test/java/com/alibaba/dubbo/demo/provider/DemoProvider.java 类，以启动 dubbo-demo-provider 。 在 idea 中右键运行 Dubbo/dubbo-demo/dubbo-demo-consumer/src/test/java/com/alibaba/dubbo/demo/consumer/DemoConsumer.java 类，以启动 dubbo-demo-consumer 。 最后，观察终端输出的日志，dubbo-demo-provider 中输出如下内容： 1234[17:13:19] Hello world458, request from consumer: /10.1.19.41:57319, cookie:iamsorry[17:13:21] Hello world459, request from consumer: /10.1.19.41:57319, cookie:iamsorry[17:13:23] Hello world460, request from consumer: /10.1.19.41:57319, cookie:iamsorry[17:13:25] Hello world461, request from consumer: /10.1.19.41:57319, cookie:iamsorry 而 dubbo-demo-consumer 中输出如下内容 1234567891011121314[17:13:17] Hello world458, response form provider: 10.1.19.41:20880cookie-&gt;iamsorryabc-&gt;17:13:19Key 1-&gt;1Key 2-&gt;2codec-&gt;negoutput-&gt;135[17:13:20] Hello world459, response form provider: 10.1.19.41:20880cookie-&gt;iamsorryabc-&gt;17:13:21Key 1-&gt;1Key 2-&gt;2codec-&gt;negoutput-&gt;135 接下来，你可以试试使用其他的消息注册方式。 使用类似的方式，你也可以启动 dubbo-admin 和 dubbo-monitor-simple，需要注意的是，如果你是在一台机器上启动这两个服务，则需要修改 dubbo.properties 中的端口以避免端口冲突。 3. 其他简单谈谈个人对 dubbo 项目的看法： 项目导入到 IDE 之后，使用的是 jdk 1.5 进行编译，需要手动一个一个地修改为 1.6。 项目没有使用统一的 code-template ，代码风格不统一。 文档不够规范，缺少一些能够快速上手的用户文档。 dubbo 是获取第一个网卡的 ip 地址，当有多个网卡或者使用 VPN 时候会存在问题。 dubbo 依赖的 Spring 和 Netty 版本都较低 有些类和注解中的属性过多，显得比较臃肿，当然，这是强迫性症了。 以上仅仅代表个人意见。]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mahout推荐引擎介绍]]></title>
    <url>%2F2014%2F09%2F22%2Fmahout-recommend-engine%2F</url>
    <content type="text"><![CDATA[Mahout 是一个来自 Apache 的、开源的机器学习软件库，他主要关注于推荐引擎（协同过滤）、聚类和分类。 推荐一般是基于物品或者用户进行推荐相关。 聚类是讲大量的事物组合为拥有类似属性的簇，借以在一些规模较大或难于理解的数据集上发现层次结构和顺序，以揭示一些有用的模式或让数据集更易于理解。 分类有助于判断一个新的输入或新的事物是否于以前观察到的模式相匹配，它通常还被用于筛选异常的行为或模式，来检测可疑的网络活动或欺骗行为。 推荐系统推荐引擎算法应用最广的两大类：基于用户和基于物品的推荐。这两者都是协同过滤的范畴：仅仅通过了解用户于物品之间的关系进行推荐。这些技术无需了解物品自身的属性。还有基于内容的推荐技术，这需要和特定的领域相结合，mahout 中没有讨论此类算法。 推荐引擎分为5个主要部分组成：数据模型，相似度算法，近邻算法，推荐算法，算法评分器。 数据模型： GenericDataModel：用户ID，物品ID，用户对物品的打分(UserID,ItemID,PreferenceValue) GenericBooleanPrefDataModel: 用户ID，物品ID (UserID,ItemID)，这种方式表达用户是否浏览过该物品，但并未对物品进行打分。 内存级 DataModel 基于文件的 DataModel 基于数据库的 DataModel 相似度算法PearsonCorrelationSimilarity: 皮尔逊相似度原理：其值为两个序列协方差与二者方差乘积的比值。它度量两个用户针对同一物品的偏好值变化趋势的一致性：都偏高或都偏低。 范围：[-1,1]，绝对值越大，说明相关性越强，负相关对于推荐的意义小。 问题： 1.没有考虑两个用户同时给出偏好值的物品数目。 2.如果两个用户的交集仅包含一个物品，则无法计算相关性。 3.只要任何一个序列中出现偏好值相同的情况，相关系数都是未定义的。 该相似度并不是最好的选择，也不是最坏的选择，只是因为其容易理解，在早期研究中经常被提起。使用Pearson线性相关系数必须假设数据是成对地从正态分布中取得的，并且数据至少在逻辑范畴内必须是等间距的数据。Mahout中，为皮尔森相关计算提供了一个扩展，通过增加一个权重来使得重叠数也成为计算相似度的影响因子。 EuclideanDistanceSimilarity: 欧氏距离相似度原理：利用欧式距离d定义的相似度s，s=1 / (1+d)。 范围：[0,1]，值越大，说明d越小，也就是距离越近，则相似度越大。 说明：同皮尔森相似度一样，该相似度也没有考虑重叠数对结果的影响，同样地，Mahout通过增加一个枚举类型（Weighting）的参数来使得重叠数也成为计算相似度的影响因子。 UncenteredCosineSimilarity: 余弦相似度原理：多维空间两点与所设定的点形成夹角的余弦值。 范围：[-1,1]，值越大，说明夹角越大，两点相距就越远，相似度就越小。 说明：在数学表达中，如果对两个项的属性进行了数据中心化，计算出来的余弦相似度和皮尔森相似度是一样的，在mahout中，实现了数据中心化的过程，所以皮尔森相似度值也是数据中心化后的余弦相似度。另外在新版本中，Mahout提供了UncenteredCosineSimilarity类作为计算非中心化数据的余弦相似度。 SpearmanCorrelationSimilarity: 斯皮尔曼相关系数相似度原理：斯皮尔曼相关系数通常被认为是排名后的变量之间的皮尔逊线性相关系数。 范围：{-1.0,1.0}，当一致时为1.0，不一致时为-1.0。 说明：计算非常慢，有大量排序。针对推荐系统中的数据集来讲，用斯皮尔曼相关系数作为相似度量是不合适的。 TanimotoCoefficientSimilarity: 忽略偏好值基于谷本系数相似度原理：又名广义Jaccard系数，是对Jaccard系数的扩展。它是两个偏好物品之间的交集大小与并集大小的比值。 范围：[0,1]，完全重叠时为1，无重叠项时为0，越接近1说明越相似。 说明：处理无打分的偏好数据。 CityBlockSimilarity: 曼哈顿距离相似度原理：曼哈顿距离的实现，同欧式距离相似，都是用于多维数据空间距离的测度 范围：[0,1]，同欧式距离一致，值越小，说明距离值越大，相似度越大。 说明：比欧式距离计算量少，性能相对高。 LogLikelihoodSimilarity: 对数似然相似度原理：重叠的个数，不重叠的个数，都没有的个数，计算发生重叠的非偶然概率。 范围：具体可去百度文库中查找论文《Accurate Methods for the Statistics of Surprise and Coincidence》 说明：处理无打分的偏好数据，比Tanimoto系数的计算方法更为智能。 总结，无偏好的相似度： TanimotoCoefficientSimilarity: 忽略偏好值基于谷本系数相似度 LogLikelihoodSimilarity: 对数似然相似度 近邻算法分为2种： NearestNUserNeighborhood:指定N的个数，比如，选出前10最相似的用户。 ThresholdUserNeighborhood:指定比例，比如，选择前10%最相似的用户。 在小数据量情况下，基于物品的推荐算法比基于用户的推荐算法要快。 推荐算法Slop-one 推荐算法Slop-one 算法假设两个物品之间存在着某种线性关系，由物品 x 的偏好值可以估计出物品 y 的偏好值。Slop-one 的吸引力在于其算法的在线部分执行很快。 基于SVD 奇异值分解的推荐算法基于聚类的推荐算法运行时的推荐很快，因为几乎一切都预先计算好了。但是，这种推荐只能基于一个群组做推荐而不是为个人提供的。另外，聚类的过程非常花时间，在用户数较少时效果很好。 参考文章 [1]从源代码剖析Mahout推荐引擎]]></content>
      <categories>
        <category>mahout</category>
      </categories>
      <tags>
        <tag>mahout</tag>
        <tag>recommendation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Gradle构建项目]]></title>
    <url>%2F2014%2F09%2F15%2Fbuild-project-with-gradle%2F</url>
    <content type="text"><![CDATA[Gradle 是一款基于 Groovy 语言、免费开源的构建工具，它既保持了 Maven 的优点，又通过使用 Groovy 定义的 DSL 克服了 Maven 中使用 XML 繁冗以及不灵活的缺点。 Gradle 官方网站：http://www.gradle.org/downloads 安装一种方式是从 官方 下载解压然后配置环境变量。 Mac 上安装： 1$ brew install gradle 测试是否安装成功： 12345678910111213$ gradle -v------------------------------------------------------------Gradle 2.0------------------------------------------------------------Build time: 2014-07-01 07:45:34 UTCBuild number: noneRevision: b6ead6fa452dfdadec484059191eb641d817226cGroovy: 2.3.3Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013JVM: 1.7.0_60 (Oracle Corporation 24.60-b09)OS: Mac OS X 10.9.4 x86_64 简单使用Gradle 使用 约定优于配置(Convention over Configuration)的理念。使用与maven兼容的目录结构布局。完全按照约定的目录结构来布置工程文件，会大大简化编译配置文件。 除了常见的src/main/java等目录，默认的 web 应用程序根目录为 src/main/webapp，也就是包含 WEB-INF 目录的上一级目录。如果工程没有完全依照约定布局，可以通过脚本文件指定相应的路径。 Gradle 中有两个最基本的对象：project 和 task。每个 Gradle 的构建由一个 project 对象来表达，它代表着需要被构建的组件或者构建的整个项目。每个 project 对象由一个或者多个 task 对象组成。 Gradle 已经自带了很多 pugins，可以满足大部分的常见构建任务。 Gradle 的默认构建脚本文件为工程根目录下的 build.gradle。 创建项目作为测试，创建一个 test 目录，然后通过下面命令来初始化一个项目： 123456789101112$ mkdir test$ cd test$ gradle init:wrapper:initBUILD SUCCESSFULTotal time: 3.058 secs$ lsbuild.gradle gradle gradlew gradlew.bat settings.gradle 可以看到生成了 gradle 的一些配置文件。接下来在 build.gradle 文件中添加下面代码，可以支持生成 jar 包： 1apply plugin: 'java' 这就是你定义一个Java项目所需要做的一切。这就会在你项目里使用Java插件，该插件会给你的项目增加很多任务。 Gradle 期望在 src/main/java 路径下找到你项目的源代码，并且测试在 src/test/java 路径下的代码。同时，在src/main/resources 路径下的文件也会作为资源文件包含在JAR包中，并且 src/test/resources 下的所有文件会包含在 classpath 下以运行测试程序。所有的输出文件都生成在 build 目录下，JAR 包生成在 build/libs 目录下。 运行下面命令即可生成 jar 包： 1$ gradle jar 如果 jar 包中有一个包含 main 方法的主类，想让其打包之后能够运行其 main 方法，则需要添加下面代码： 1234567apply plugin: 'java'jar &#123; manifest &#123; attributes 'Main-Class': 'com.javachen.gradle.HelloWorld' &#125;&#125; 通常，JAR 包需要被发布到某个地方。为了完成这个功能，你需要告诉 Gradle 把 JAR 包发布到哪里。在 Gradle 中，如 JAR 之类的压缩包都被发布到库中。在我们的样例中，我们将会发布到本地仓库。你也可以发布到一个或多个远端地址。 发布 jar 包添加如下配置： 123456uploadArchives &#123; repositories &#123; flatDir &#123; dirs 'repos' &#125; &#125;&#125; 此外，你还可以添加下面代码，引入 Eclipse 插件以支持生成 Eclipse 工程： 1apply plugin: 'eclipse' 添加Maven库： 123repositories &#123; mavenCentral()&#125; 如果想添加依赖，也是非常简单，例如添加 spring 依赖： 12345apply plugin: 'java'dependencies &#123; compile 'org.springframework:spring-context:3.2.6.RELEASE' testCompile group: 'junit', name: 'junit', version: '4.+'&#125; gradle 便会自动地到 maven 服务器下载 spring-context-3.2.6.RELEASE.jar，以及它所依赖的 jar 包。 常用的依赖： （1）compile：编译生产代码的依赖环境，即src/main/下 （2）runtime：生产代码运行时的依赖（包含编译生产代码时的依赖） （3）testCompile：编译测试代码的依赖环境，即src/test下 （4）testRuntime：测试代码运行时的依赖（包含编译测试代码时的依赖） 当然，也可以直接依赖本地的 jar 包，例如： 1234apply plugin: 'java'dependencies &#123; compile fileTree(dir: 'libs', include: '*.jar')&#125; 也可以通过 buildscript{} 中添加依赖的方式，将相关 jar 包加入到 classpath 中，如： 12345678buildscript &#123; repositories &#123; mavenCentral() &#125; dependencies &#123; classpath group: 'commons-codec', name: 'commons-codec', version: '1.2' &#125; &#125; 综上，完成的配置如下： 123456789101112131415161718192021222324252627282930313233343536373839apply plugin: 'java'apply plugin: 'eclipse'jar &#123; manifest &#123; attributes 'Main-Class': 'com.javachen.gradle.HelloWorld' &#125;&#125;repositories &#123; mavenLocal() mavenCentral() mavenRepo urls: "http://repository.sonatype.org/content/groups/forge/"&#125;dependencies &#123; compile 'org.springframework:spring-context:3.2.6.RELEASE' testCompile group: 'junit', name: 'junit', version: '4.+' compile fileTree(dir: 'libs', include: '*.jar')&#125;buildscript &#123; repositories &#123; mavenLocal() mavenCentral() mavenRepo urls: "http://repository.sonatype.org/content/groups/forge/" &#125; dependencies &#123; classpath group: 'commons-codec', name: 'commons-codec', version: '1.2' &#125; &#125; uploadArchives &#123; repositories &#123; flatDir &#123; dirs 'repos' &#125; &#125;&#125; 其他使用创建项目目录结构gradle 不像 maven 那样有固定的项目结构，gradle 原声 API 是不支持的，要想做到这一点，我们可以自定义一个 task。 12345678910111213apply plugin: 'idea'apply plugin: 'java'apply plugin: 'war'task createJavaProject &lt;&lt; &#123; sourceSets*.java.srcDirs*.each &#123; it.mkdirs() &#125; sourceSets*.resources.srcDirs*.each &#123; it.mkdirs()&#125;&#125;task createWebProject(dependsOn: 'createJavaProject') &lt;&lt; &#123; def webAppDir = file("$webAppDirName") webAppDir.mkdirs()&#125; 然后运行下面命令： 1$ gradle createJavaProject 另外，还可以使用 gradle templates 创建项目目录结构，这里不做研究。 更加标准的方法是使用 gradle 自带的插件创建项目目录结构，例如创建 java 项目结构： 1$ gradle init --type java-library 这时候的目录结果如下： 12345678910111213141516171819$ tree -L 4.├── build.gradle├── gradle│ └── wrapper│ ├── gradle-wrapper.jar│ └── gradle-wrapper.properties├── gradlew├── gradlew.bat├── settings.gradle└── src ├── main │ └── java │ └── Library.java └── test └── java └── LibraryTest.java7 directories, 8 files 如果想要导入到 idea 中，先执行下面命令： 1$ gradle idea 这时候的 build.gradle 如下： 12345678910111213apply plugin: 'idea'apply plugin: 'java'apply plugin: 'war'task createJavaProject &lt;&lt; &#123; sourceSets*.java.srcDirs*.each &#123; it.mkdirs() &#125; sourceSets*.resources.srcDirs*.each &#123; it.mkdirs()&#125;&#125;task createWebProject(dependsOn: 'createJavaProject') &lt;&lt; &#123; def webAppDir = file("$webAppDirName") webAppDir.mkdirs()&#125; 将Java项目从maven迁移到gradle如何将一个 java 项目从maven迁移到 gradle 呢？gradle 集成了一个很方便的插件：Build Init Plugin，使用这个插件可以很方便地创建一个新的 gradle 项目，或者将其它类型的项目转换为 gradle 项目。 要将 maven 项目转换为 gradle 项目，只需要在项目的 pom 文件所在的目录下执行以下命令： 1$ gradle init --type pom 上面的命令会根据 pom 文件自动生成 gradle 项目所需的文件和配置，然后以 gradle 项目重新导入即可。 参考 [1] Gradle安装与简单使用 [2] gradle太好用了]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>gradle</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Groovy操作文件]]></title>
    <url>%2F2014%2F09%2F12%2Ffile-operation-in-groovy%2F</url>
    <content type="text"><![CDATA[Java 读写文件比较麻烦，那 Groovy 操作文件又如何呢？ 1. 读文件读文件内容在groovy中输出文件的内容： 1println new File("tmp.csv").text 上面代码非常简单，没有流的出现，没有资源关闭的出现，也没有异常控制的出现，所有的这些groovy已经搞定了。 读取每一行内容： 1234567891011121314File file = new File('tmp.csv')assert file.name == 'tmp.csv'assert ! file.isAbsolute()assert file.path == 'tmp.csv'assert file.parent == null//使用系统默认的编码处理文件流 file.eachLine &#123;println it &#125; //指定处理流的编码file.eachLine("UTF-8") &#123; println it &#125;file.eachLine("UTF-8",10) &#123;str,no-&gt; println str println no &#125; 对文件中每一行的内容做处理： 123456789101112file.splitEachLine("\t") &#123; println it &#125;//以大写行式输出文件内容 lineList = file.readLines(); liineList.each &#123; println it.toUpperCase(); &#125;file.filterLine &#123;String str-&gt; if (str.contains('code')) println str &#125;.writeTo(new PrintWriter(System.out)) 解析 xml 文件123456789101112&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;customers&gt; &lt;corporate&gt; &lt;customer name="bill gates" company="microsoft"&gt;&lt;/customer&gt; &lt;customer name="steve jobs" company="apple"&gt;&lt;/customer&gt; &lt;customer name="bill dyh" company="sun"&gt;&lt;/customer&gt; &lt;/corporate&gt; &lt;consumer&gt; &lt;customer name="jone Doe"&gt;&lt;/customer&gt; &lt;customer name="jane Doe"&gt;&lt;/customer&gt; &lt;/consumer&gt; &lt;/customers&gt; 12345def customers = new XmlSlurper().parse(new File("customers.xml")) /*对文件进行解析*/ for(customer in customers.corporate.customer)&#123; println "$&#123;customer.@name&#125; works for$&#123;customer.@company&#125;"; &#125; 解析 propeties 文件参考 groovy: How to access to properties file?，代码如下： 12345678910def props = new Properties()new File("message.properties").withInputStream &#123; stream -&gt; props.load(stream) &#125;// accessing the property from Properties object using Groovy's map notationprintln "capacity.created=" + props["capacity.created"]def config = new ConfigSlurper().parse(props)// accessing the property from ConfigSlurper object using GPath expressionprintln "capacity.created=" + config.capacity.created 另外一种方式： 1def config = new ConfigSlurper().parse(new File("message.groovy").toURL()) message.groovy 内容如下： 1234capacity &#123; created="x" modified="y"&#125; 2. 操作目录列出目录所有文件（包含子文件夹，子文件夹内文件） ： 123456789def dir = new File(dirName) if (dir.isDirectory()) &#123; dir.eachFileRecurse &#123; file -&gt; println file &#125; &#125; dir.eachFileMatch(~/.*\.txt/) &#123;File it-&gt; println it.name &#125; //使正则表达式匹配文件名 dir.eachFileMatch(FILES, ~/.*\.txt/) &#123; File it-&gt; println it.name &#125; 3. 写文件1234567891011121314151617import java.io.File def writeFile(fileName) &#123; def file = new File(fileName) if (file.exists()) file.delete() def printWriter = file.newPrintWriter() // printWriter.write('The first content of file') printWriter.write('\n') printWriter.write('The first content of file') printWriter.flush() printWriter.close() &#125; 除了 file.newPrintWriter() 可以得到一个 PrintWriter，类似方法还有 file.newInputStream()、 file.newObjectInputStream()等。 更简洁写法： 123new File(fileName).withPrintWriter &#123; printWriter -&gt; printWriter.println('The first content of file') &#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>groovy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Llama的使用]]></title>
    <url>%2F2014%2F09%2F09%2Fllama%2F</url>
    <content type="text"><![CDATA[1. 介绍 Llama (Low Latency Application MAster) 是一个 Yarn 的 Application Master，用于协调 Impala 和 Yarn 之间的集群资源的管理和监控。Llama 使 Impala 能够获取、使用和释放资源配额，而不需要 Impala 使用 Yarn 管理的 container 进程。Llama 提供了 Thrift API 来和 Yarn 交互。 个人理解，Llama 的作用就是使 Impala 能够工作在 YARN 之上，使得 Impala 和 YARN 共享集群资源，提供低延迟的查询。 Llama 官网地址：http://cloudera.github.io/llama/ Llama 源码：https://github.com/cloudera/llama 2. 架构3. Llama 安装3.1 安装 llamaLlama 需要安装在装有 Yarn 的节点上。 在 rhel 系统上安装： 1$ sudo yum install llama-master 3.2 配置 Llama 只能和 Yarn 配合工作，不能用于 MRv1。 Llama 的配置文件在 /etc/llama/conf/ 目录，llama-site.xml 默认配置在 http://cloudera.github.io/llama/llama-site.html。 3.3 启动和停止启动： 1$ sudo service llama start 停止： 1$ sudo service llama stop 3.4 配置 HALlama 使用 Zookeeper 来实现 HA，任一时刻，只有一个 Llama-master 实例是 active的以确保资源不会被分区。 为了从 Yarn 获取资源，Llama 启动 YARN application 并且运行未管理的ApplicationMaster。当一个 Llama 实例宕掉的时候，分配给该实例启动的 application 的所有资源将会被回首，直到这些 application 超时（默认超时时间为10分钟）。当 Llama 运行失败的时候，这些资源将会被杀掉他启动的application的 Llama 回收。 HA 相关配置参数在 /etc/llama/conf/llama-site.xml： 属性 描述 默认值 llama.am.cluster.id Cluster ID of the Llama pair, used to differentiate between different Llamas llama llama.am.ha.enabled Whether to enable Llama HA false llama.am.ha.zk-quorum ZooKeeper quorum to use for leader election and fencing llama.am.ha.zk-base Base znode for leader election and fencing data /llama llama.am.ha.zk-timeout-ms The session timeout, in milliseconds, for connections to ZooKeeper quorum 10000 llama.am.ha.zk-acl ACLs to control access to ZooKeeper world:anyone:rwcda llama.am.ha.zk-auth Authorization information to go with the ACLs 上面必填的两个参数为： llama.am.ha.enabled ： true llama.am.ha.zk-quorum ： cdh1:21088,cdh2:21088 3.5 修改 Impala 启动参数使用 jdbc 方式提交查询到 Impala 时，会出现 number of running queries 20 is over limit 20 的异常，这时候在 impala的 源代码中搜索关键字 number of running queries，可以找到https://github.com/cloudera/Impala/blob/cdh5-1.4_5.1.2/be/src/scheduling/admission-controller.cc，从源代码中可以看到出现该问题和 Llama 有关系，在找不到 llama 的相关配置时，impala 一个队列中能够接受的最大请求数为 20。代码见:RequestPoolService.java 1234567891011121314151617181920212223@VisibleForTesting TPoolConfigResult getPoolConfig(String pool) &#123; TPoolConfigResult result = new TPoolConfigResult(); int maxMemoryMb = allocationConf_.get().getMaxResources(pool).getMemory(); result.setMem_limit( maxMemoryMb == Integer.MAX_VALUE ? -1 : (long) maxMemoryMb * ByteUnits.MEGABYTE); if (llamaConf_ == null) &#123; //llama配置为空 result.setMax_requests(LLAMA_MAX_PLACED_RESERVATIONS_DEFAULT); result.setMax_queued(LLAMA_MAX_QUEUED_RESERVATIONS_DEFAULT); &#125; else &#123; // Capture the current llamaConf_ in case it changes while we're using it. Configuration currentLlamaConf = llamaConf_; result.setMax_requests(getLlamaPoolConfigValue(currentLlamaConf, pool, LLAMA_MAX_PLACED_RESERVATIONS_KEY, LLAMA_MAX_PLACED_RESERVATIONS_DEFAULT)); //20 result.setMax_queued(getLlamaPoolConfigValue(currentLlamaConf, pool, LLAMA_MAX_QUEUED_RESERVATIONS_KEY, LLAMA_MAX_QUEUED_RESERVATIONS_DEFAULT)); &#125; LOG.trace("getPoolConfig(pool=&#123;&#125;): mem_limit=&#123;&#125;, max_requests=&#123;&#125;, max_queued=&#123;&#125;", new Object[] &#123; pool, result.mem_limit, result.max_requests, result.max_queued &#125;); return result; &#125; 目前，参考 Admission Control and Query Queuing，在不安装和使用 llama 情况下，找到的一种解决办法是： 修改 impala 启动参数（/etc/default/impala），添加 -default_pool_max_requests=-1，该参数设置每一个队列的最大请求数，如果为-1，则表示不做限制。 4. 使用4.1 Llama Application Master4.2 Llama Admin Command Line tool4.3 Llama Node Manager Auxiliary Service5. 参考文章 [1] http://cloudera.github.io/llama [2] Admission Control and Query Queuing]]></content>
      <categories>
        <category>impala</category>
      </categories>
      <tags>
        <tag>impala</tag>
        <tag>yarn</tag>
        <tag>llama</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始创建Grails应用]]></title>
    <url>%2F2014%2F09%2F09%2Fcreate-a-grails-app-step-by-step%2F</url>
    <content type="text"><![CDATA[本篇文章主要介绍如何从零开始一步一步创建一个 Grails 应用程序。整个过程中，你将学到如何改变 Grails 运行的端口，了解 Grails 应用的基础组成部分(领域类、控制器和视图)、指定字段的缺省值，以及其他许多内容。 1. 安装下载压缩包然后解压或者通过rpm、deb发行包安装。 这里，我在 mac 上安装 grails ： 1$ brew install grails 2. 创建应用以 blog 为例，创建一个应用程序，在命令行输入 grails create-app blog ： 12345$ grails create-app blog2014-9-9 10:43:29 org.codehaus.groovy.runtime.m12n.MetaInfExtensionModule newModule警告: Module [groovy-all] - Unable to load extension class [org.codehaus.groovy.runtime.NioGroovyMethods]2014-9-9 10:43:29 org.codehaus.groovy.runtime.m12n.MetaInfExtensionModule newModule| Created Grails Application at /Users/june/workspace/groovy/blog 这样，就创建了一个空的应用程序，进入到 blog 目录，输入 grails run-app : 123456$ cd blog$ grails run-app...2014-9-9 10:45:31 org.codehaus.groovy.runtime.m12n.MetaInfExtensionModule newModule警告: Module [groovy-all] - Unable to load extension class [org.codehaus.groovy.runtime.NioGroovyMethods]| Server running. Browse to http://localhost:8080/blog 一切顺刟癿话，你应该可以在浏觅器中访问 http://localhost:8080/blog/ 并查看欢迎页面。 如果发现端口冲突，你可以修改默认端口： 在启动时添加参数：grails -Dserver.port=9090 run-app 在 grails-app/conf 目录修改 BuildConfig.groovy，添加 grails.server.port.http=9090 或 server.port=9090 修改用户的默认设置。修改 ~/.grails/settings.groovy，添加 grails.server.port.http = 9000 修改Grails程序的默认设置。在 $GRAILS_HOME/scripts/_GrailsSettings.groovy 中添加：serverPort = getPropertyValue(&quot;server.port&quot;, 9000).toInteger() 测试应用： 1grails test-app 如果想部署应用： 1grails war 上面命令默认使用的是 production 环境，也可以添加参数使用 dev 环境： 1grails dev war 当部署应用时候，最后是设置 jvm 内存： 1-server -Xmx512M -XX:MaxPermSize=256m 在 blog 目录查看应用目录结构： 12345678910111213141516171819202122232425262728293031323334353637383940$ blog tree -L 2.├── application.properties├── grails-app│ ├── assets│ ├── conf # 配置文件(如数据源、URL 映射、遗留的 Spring 和 Hibernate 配置文件)│ ├── controllers # 控制器(MVC 中的“C”)│ ├── domain # 领域类(MVC 中的模型或“M”。该目弽中 癿每个文件在数据库中都有对应癿表。)│ ├── i18n # 国际化│ ├── migrations # 迁移│ ├── services # 服务类│ ├── taglib # 自定义标签库│ ├── utils # 工具│ └── views # 视图├── grailsw├── grailsw.bat├── lib # 存放第三方 jar├── scripts # Gant 脚本├── src # 源代码文件│ ├── groovy│ └── java├── target│ ├── classes│ ├── stacktrace.log│ └── work├── test # 单元测试│ ├── integration│ └── unit├── web-app # web 资源文件│ ├── META-INF│ ├── WEB-INF│ ├── css│ ├── images│ └── js└── wrapper ├── grails-wrapper-runtime-2.4.3.jar ├── grails-wrapper.properties └── springloaded-1.2.0.RELEASE.jar29 directories, 7 files Grails 非常强调惯例优于配置(convention over configuration)。这意味着 Grails 并不是靠配置(configuration)文件来把应用各部分组织在一起，相反，它靠的是惯例 (convention)。所有领域类都存放在 domain 目录，控制器保存在 controllers 目录，视图则=放在 views 目录，等等。 3. 创建领域类Grails 接受这些简单的类，并用它们完成许多工作。相应的数据库表会自劢为每个领域 类创建，控制器和视图会从关联的领域类中派生出相应的类。领域类还是存放验证规则、定义 “一对多”关系，以及包含其他许多信息的地方。 创建 User 领域类： 12345$ grails create-domain-class User...| Created file grails-app/domain/blog/User.groovy| Compiling 1 source files| Created file test/unit/blog/UserSpec.groovy 你会发现 Grails 同时创建了一个领域类和一个测试类，领域类中包名为 blog，其实，你也可以在创建领域类的时候自定义包名。 编辑 grails-app/domain/blog/User.groovy，添加一些属性： 123456789101112131415class User &#123; String name Integer age String sex Date dateCreated Date lastUpdated static mapping = &#123; autoTimestamp false &#125; static constraints = &#123; &#125;&#125; 以上只是添加了一些简单的属性，你还可以添加两个特殊的属性。如果你定义了一个名为 dateCreated 的日期字段，Grails 将自动在第一次向数据库保存实例的时候填上这个值。要是你创建了另一个名为 lastUpdated 的日期字段，Grails 将在每次把更新后的记录存回数据库的时候填充这个日期。这个可以在 static mapping 代码块中来配置： 12345678910class User &#123; Date dateCreated Date lastUpdated static mapping = &#123; autoTimestamp false &#125;&#125; 另外，你也可以定义一些领域类的生命周期事件来做一些复杂的事情： 123456789101112131415class User &#123; // ... def beforeInsert = &#123; // your code goes here &#125; def beforeUpdate = &#123; // your code goes here &#125; def beforeDelete = &#123; // your code goes here &#125; def onLoad = &#123; // your code goes here &#125;&#125; static mapping 还可以做一些其他事情，如制定列表排列顺序： 123456class User &#123; static mapping = &#123; sort &quot;name&quot; &#125; // ... &#125; 关亍 static mapping 代码块更多的信息，请参见：http://grails.org/GORM+-+Mapping+DSL。 4. 创建控制器和视图在命令行下，输入 grails create-controller User： 123456$ grails create-controller User| Compiling 1 source files| Created file grails-app/controllers/blog/UserController.groovy| Created file grails-app/views/user| Compiling 1 source files| Created file test/unit/blog/UserControllerSpec.groovy 查看 User 的控制器 grails-app/controllers/blog/UserController.groovy，你会发现没有多少内容 ： 123456package blogclass UserController &#123; def index() &#123; &#125;&#125; 修改一下内容： 12345678package blogclass UserController &#123; def index=&#123; render "Hello World" &#125;&#125; 这时候启动应用，访问 http://localhost:8080/blog/user 你会看到 “Hello World”。 在控制器中定义的任何一个闭包都会暴露成一个 url。 重新修改控制器类代码如下： 12345package blogclass UserController &#123; def scaffold = User&#125; 当 Grails 看到控制器中的 scaffold 属性,它会动态地产生针对指定领域类的控制器逻辑和必要的CRUD视图。所有这些只需要这一行代码。 进入 http://localhost:8080/blog/user/create 页面，你会发现表单字段排列顺序没有按照预想的情况排列，这时候可在 static constraints 代码块中指定表单顺序： 1234567891011121314151617181920class User &#123; String name Integer age String sex Date dateCreated Date lastUpdated static mapping = &#123; autoTimestamp false sort "name" &#125; static constraints = &#123; name() age() sex() dateCreated() lastUpdated() &#125;&#125; 当然，也可以增加一些约束条件： 12345678910111213141516171819202122class User &#123; String name Integer age String sex Date activeDate Date dateCreated Date lastUpdated static mapping = &#123; autoTimestamp false sort "name" &#125; static constraints = &#123; name(blank:false, maxSize:50) age(min:0) sex(inList:["F", "M"]) activeDate() dateCreated() lastUpdated() &#125;&#125; Grails 所有可用的验证选项： blank、nullable creditCard display email password inList matches min, max minSize,maxSize,size notEqual range scale unique url validator 而校验失败的国际化消息保存在 grails-app/i18n 目录下的 messages.properties 文件里。 你可以通过 validator 来指定自定义的校验器，例如，startDate要大于当前时间： 1234static constraints = &#123; // ... activeDate(validator: &#123;return (it &gt; new Date())&#125;) // ...&#125; 然后在 messages.properties 文件里添加一行： 1user.activeDate.validator.invalid=Sorry, but the date is the past. 5. 对象关联映射不管愿不愿意,我们都已经创建了领域类，从来没有操心过这些数据的保存和位置。之所以我们能够如此享福，这都得感谢 GORM。Grails 对象-关系映射(Grails Object-Relational Mapping)API 得以让我们放心地以对象方式去思考问题——而不至于陷入到关系数据库相关的 SQL 当中。 5.1 DataSourceGORM 缺醒设置在 grails-app/conf/DataSource.groovy： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657dataSource &#123; pooled = true jmxExport = true driverClassName = "org.h2.Driver" username = "sa" password = ""&#125;hibernate &#123; cache.use_second_level_cache = true cache.use_query_cache = false// cache.region.factory_class = 'net.sf.ehcache.hibernate.EhCacheRegionFactory' // Hibernate 3 cache.region.factory_class = 'org.hibernate.cache.ehcache.EhCacheRegionFactory' // Hibernate 4 singleSession = true // configure OSIV singleSession mode flush.mode = 'manual' // OSIV session flush mode outside of transactional context&#125;// environment specific settingsenvironments &#123; development &#123; dataSource &#123; dbCreate = "create-drop" // one of 'create', 'create-drop', 'update', 'validate', '' url = "jdbc:h2:mem:devDb;MVCC=TRUE;LOCK_TIMEOUT=10000;DB_CLOSE_ON_EXIT=FALSE" &#125; &#125; test &#123; dataSource &#123; dbCreate = "update" url = "jdbc:h2:mem:testDb;MVCC=TRUE;LOCK_TIMEOUT=10000;DB_CLOSE_ON_EXIT=FALSE" &#125; &#125; production &#123; dataSource &#123; dbCreate = "update" url = "jdbc:h2:prodDb;MVCC=TRUE;LOCK_TIMEOUT=10000;DB_CLOSE_ON_EXIT=FALSE" properties &#123; // See http://grails.org/doc/latest/guide/conf.html#dataSource for documentation jmxEnabled = true initialSize = 5 maxActive = 50 minIdle = 5 maxIdle = 25 maxWait = 10000 maxAge = 10 * 60000 timeBetweenEvictionRunsMillis = 5000 minEvictableIdleTimeMillis = 60000 validationQuery = "SELECT 1" validationQueryTimeout = 3 validationInterval = 15000 testOnBorrow = true testWhileIdle = true testOnReturn = false jdbcInterceptors = "ConnectionState" defaultTransactionIsolation = java.sql.Connection.TRANSACTION_READ_COMMITTED &#125; &#125; &#125;&#125; 从上面可以看到，Grails 的环境分为三种模式： development：开发模式 test：测试模式 production：生产模式 并且每种模式下的数据源配置有些许差异，如：dbCreate 值不一样， Grails 默认使用的是 H2内存数据库来保存数据，并三种模式下使用的 JDBC url（内存或者文件）不一样，等等。 如果在 DataSource 上设置dbCreate属性为”update”, “create” or “create-drop”, Grails 会为你自动生成/修改数据表格。 你也可以修改该文件，使用其他的数据库，从而清楚的看到 Grails 创建的表以及其中每一个字段。 12345678910111213141516171819202122232425262728293031323334353637dataSource &#123; pooled = true jmxExport = true driverClassName = "com.mysql.jdbc.Driver" username = "grails" password = "grails"&#125;hibernate &#123; cache.use_second_level_cache = true cache.use_query_cache = false// cache.region.factory_class = 'net.sf.ehcache.hibernate.EhCacheRegionFactory' // Hibernate 3 cache.region.factory_class = 'org.hibernate.cache.ehcache.EhCacheRegionFactory' // Hibernate 4 singleSession = true // configure OSIV singleSession mode flush.mode = 'manual' // OSIV session flush mode outside of transactional context&#125;// environment specific settingsenvironments &#123; development &#123; dataSource &#123; dbCreate = "create-drop" // one of 'create', 'create-drop', 'update', 'validate', '' url = "jdbc:mysql://localhost:3306/grails?autoreconnect=true" &#125; &#125; test &#123; dataSource &#123; dbCreate = "update" url = "jdbc:mysql://localhost:3306/grails?autoreconnect=true" &#125; &#125; production &#123; dataSource &#123; dbCreate = "update" url = "jdbc:mysql://localhost:3306/grails?autoreconnect=true" &#125; &#125;&#125; 当然，你还需要在 mysql 中创建 grails 用户和 grails 数据库，并将 mysql 的 jdbc 驱动拷贝到 lib 目录下。 然后，启动应用观察日志中是否有报错。 5.2 One-to-many创建 Blog 领域类，并设置 User 和 Blog 的 一对多 的关系。 先创建 Blog 领域类： 1$ grails create-domain-class Blog 然后修改 User 领域类： 1234class User &#123; // ... static hasMany = [blogs:Blog] &#125; 上述代码创建了一个名为 blogs 的新字段，类型是 java.util.Set。如果有多个关系，可以用逗号分隔。 Grails中默认使用的fetch策略是 “lazy”, 意思就是集合将被延迟初始化。 如果你不小心，这会导致 n+1 问题 。如果需要”eager” 抓取 ，需要使用 [ORM DSL](http://www.cjsdn.net/doc/jvm/grails/docs/1.1/guide/single.html#5.5.2 Custom ORM Mapping) 或者指定立即抓取作为query的一部分 默认的级联行为是级联保存和更新，但不删除，除非 belongsTo 被指定: 1234class Blog &#123; // ... static belongsTo = [user:User] &#125; 这不仅形成了闭环，它还强制了级联更新和删除。 如果在one-to-many的多方拥有2个同类型的属性，必须使用mappedBy 指定哪个集合被映射： 12345678class User &#123; static hasMany = [blogs:Blog] static mappedBy = [blogs:"users1"]&#125;class Blog &#123; User users1 User users2&#125; 如果多方拥有多个集合被映射到不同的属性，也是一样的： 12345678class User &#123; static hasMany = [blogs1:Blog, blogs2:Blog] static mappedBy = [blogs1:"users1", blogs2:"users2"]&#125;class Blog &#123; User users1 User users2&#125; 另外，为了代码的可读性，我们可以修改领域类的 toString() 方法： 123456class User &#123; // ... String toString()&#123; return "$&#123;name&#125;, $&#123;activeDate.format('MM/dd/yyyy')&#125;" &#125;&#125; 5.3 Many-to-manyGrails 支持 many-to-many 关联，通过在关联双方定义 hasMany ，并在关联拥有方定义 belongsTo ： 123456789class Book &#123; static belongsTo = Author static hasMany = [authors:Author] String title&#125;class Author &#123; static hasMany = [books:Book] String name&#125; Grails 在数据库层使用一个连接表来映射 many-to-many，在这种情况下，Author 负责持久化关联，并且是唯一可以级联保存另一端的一方 。 5.4 数据初始化在 grails-app/conf 目录下有一个文件叫做 BootStrap.groovy，可以用来做一些初始化的工作： 1234class BootStrap &#123; def init = &#123; servletContext -&gt; &#125; def destroy = &#123;&#125; &#125; init 闭包会每次在 Grails 启动时被调用; destroy 闭包会每次在 Grails 停止时被调用。 6. 参考 [1] The Grails Framework - Reference Documentation-v1.1 [2] Grails Tutorial for Beginners [3] 使用 Grails 快速开发 Web 应用程序]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>grails</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Groovy语法介绍]]></title>
    <url>%2F2014%2F09%2F05%2Fabout-groovy%2F</url>
    <content type="text"><![CDATA[1. 介绍Groovy 是基于 JRE 的脚本语言，和Perl，Python等等脚本语言一样，它能以快速简洁的方式来完成一些工作：如访问数据库，编写单元测试用例，快速实现产品原型等等。 Groovy 是由James Strachan 和 Bob McWhirter 这两位天才发明的（JSR 241 2004 年 3 月）。Groovy 完全以Java API为基础，使用了Java开发人员最熟悉的功能和库。Groovy 的语法近似Java，并吸收了 Ruby 的一些特点，因此 Groovy 在某些场合可以扮演一种 “咖啡伴侣”的角色。 官网地址：http://groovy.codehaus.org/ Groovy的主要特性： Closure（闭包）的支持 本地的 List 和 Map 语法 Groovy 标记：支持多种标记语言，如 XML、HTML、SAX、W3C DOM Groovy Path 表达式语言：类似 Xpath Groovlet：用简单的 Groovy 脚本实现 Servlet Groovy SQL：使得和 SQL 一起工作更简单 Groovy Bean：和 Bean 一起工作的简单语法 Groovy模版引擎：简单使用，集成了 Gpath 和编译成字节码 Ant 脚本化 正则表达式：简洁的脚本语法使用正则表达式 操作符重载：使 Collection 和 Map 的数据类型简单化 多形式的 iteration 和 Autoboxing 直接编译成 Java 字节码，很干净的和所有已存在的 Java 对象和类库一起工作 Groovy 可以作为 javac 的一种可选编译器来生成标准的 Java 字节码，在任何 Java 工程 中使用。Groovy 可以作为一种动态的可选语言，如脚本化 Java对 象、模版化、编写单元测试用例。 工具介绍： Groovy ：运行 groovy 脚本文件。 Groovyc：编译 groovy 脚本成 java 的 bytecode 文件（.class） Groovysh：运行命令行的控制台，可以输入 groovy 语句直接执行 GroovyConsole：GUI 形式的控制台，相当于简单的编辑器 2. 使用使用压缩包安装，下载地址为 http://groovy.codehaus.org/Download，下载然后解压配置环境变量。 在 mac 上安装 groovy： 1brew install groovy Groovy 脚本可以直接用 Groovy 解析执行： 1groovy hello.groovy 编译为字节码： 1groovyc -d classes hello.groovy 运行编译好的groovy脚本： 1java -cp %GROOVY_HOM E%/embeddable/groovy-all.jar;classes hello 你会发现其实就是运行 java 的 class 文件。 3. 语法Groovy 的语法融合了 Ruby、Python 和 Smalltalk 的一些最有用的功能，同时保留了基于 Java 语言的核心语法。对于Java 开发人员，Groovy 提供了更简单的替代语言，且几乎不需要学习时间。 Groovy在语法上跟java有几点不同： Groovy中，”= =”等同于java中的equals方法；”= = =”等同于java中的”= =”。 Groovy中缺省的标志符是public。 3.1 关键字在 Groovy 可以用 def 定义无类型的变量(定义变量方面 def 与 JavaScript 中的 var 相似)，和返回值为无类型的方法 123456class Man &#123; def name = "javachen" def introduce() &#123; return "I'm $name" // return可以省略 &#125;&#125; 3.2 语句Groovy的语句和Java类似，但是有一些特殊的地方。例如语句的分号是可选的。如果每行一个语句，就可以省略分号；如果一行上有多个语句，则需要用分号来分隔。 另外return关键字在方法的最后是可选的；同样，返回类型也是可选（缺省是Object）。 调用方法时可以不用括号，只要有参数，并且没有歧义。 一个示例： 1234567package com.javachen.groovy.testclass Hello&#123; static main(args)&#123; println "hello world" &#125;&#125; 和Java一样，程序会从这个类的main方法开始执行，和Java的区别是： class 前省略 public 修饰； main 方法前省略 public 修饰； main 方法省略返回值类型 void； main 方法形参列表省略类型 String[]； 当然，这只是 Groovy 代码的一种写法，实际上执行 Groovy 代码完全可以不必需要一个类或 main 方法，所以更简单的写法如下： 123package com.javachen.groovy.testprintln "hello world" 将上述代码保存为 hello.groovy，然后运行： 12$ groovy hello.groovyhello world 可以看到正确的输出了 “hello world” 3.3 变量和类型像其他 Script 一样，Groovy 不需要显式声明类型。在 Groovy 中，一个对象的类型是在运行时动态发现的，这极大地减少了要编写的代码数量。 在 Groovy 中，类型对于值(varibles)、属性(properties)、方法(method)和闭包(closure)参数、返回值都是可有可无的，只有在给定值的时候，才会决定它的类型，(当然声明了类型的除外）。 Groovy 对 boolean 类型放宽了限制： 常量true和false分别表示“真”和“假”； null表示false，非null表示true； 空字符串””表示false，非空字符串表示true； 0表示false，非0表示true； 空集合表示false，非空集合表示true； 3.4 字符串Groovy 中的字符串允许使用双引号和单引号。 当使用双引号时，可以在字符串内嵌入一些运算式，Groovy 允许您使用 与 bash 类似的 ${expression} 语法进行替换。可以在字符串中包含任意的 Groovy 表达式。 大块文本： 如果有一大块文本（例如 HTML 和 XML）不想编码，你可以使用Here-docs。 here-docs 是创建格式化字符串的一种便利机制。它需要类似 Python 的三重引号(“””)开头，并以三重引号结尾。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package com.javachen.groovy.test// groovy中对字符串的使用做了大量的简化// 获取字符串中的字符s = "Hello"println s[0] // 输出'H'// 遍历字符串中的所有字符s.each &#123; print it + ", " // 遍历字符串中的所有字符&#125;println ""// 截取字符串s1 = s[1..3] // 截取s字符串标号从1到3的3个字符，组成新的字符串赋予s1 // 该语法是String类的substring方法的简化println s1// 模板式字符串n = 100s1 = "The number n is $&#123;n&#125;" // $&#123;n&#125;表示将变量n的值放在字符串该位置println s1// 带格式的长字符串// """和"""之间的所有字符都会被算做字符串内容，包括// /*以及回车，制表符等s = """大家好欢迎大家学习Groovy编程Groovy is a better Java"""println s// groovy中单引号的作用// 在不定义类型时，单引号也表示字符串c1 = 'A'println c1.getClass().getName()// 要明确的定义字符类型，需要给变量增加定义char c2 = 'A'println c2.getClass().getName()// 取消转义字符s = 'c:\\windows\\system'println ss = /c:\windows\system/ // 利用/字符串/定义的字符串println s// 字符串运算s = "hello"s = s + " world" // +运算符用于连接字符串println ss -= "world" // -可以从字符串中去掉一部分println ss = s * 2 // *可以让字符串重复n次println s// 字符串比较s1 = "Abc"s2 = "abc"println s1 == s2 ? "Same" : "Different" // 执行s1.equals(s2)println s1 != s2 ? "Different" : "Same" // 执行!s1.equals(s2)println s1 &gt; s2 ? "Great" : "Less" // 执行s1.compareTo(s2) &gt; 0println s1 &lt; s2 ? "Less" : "Great" // 执行s1.compareTo(s2) &lt; 0println s1 &lt;=&gt; s2 == 1 ? "Same" : "Different" // 执行s1.compareTo(s2) Groovy增加了对字符串的如下操作： 集合操作，Groovy 将字符串看为字符的集合，可以通过 [n] 运算符直接访问字符串内的字符，也可以通过 each 循环遍历字符串的每一个字符； 截取子字符串的 substring 方法被简化为使用数值范围来进行截取，”hello”[1..3]表示截取字符串 “hello” 从下标为1到下标为3的部分，结果为 “ell”； Groovy 增加了一个新的字符串类型 GString，这种字符串可以进行格式化，在 GString 字符串中使用 ${变量}，可以将该变量的值放入字符串的相应位置； 带格式的字符串，使用 “””字符串内容”””(连续的三个引号)，这种字符串中可以包含直接输入的回车，TAB键，//或/*等字符，而这些在 Java 原本的字符串里，都必须通过转义字符来表示，例如只能用 \n 表示回车； 单引号问题，和 Javascript 和 PHP 类似，Groovy 中无论是单引号还是双引号都表示是字符串类型，例如 ‘a’ 和”a”都是字符串类型，所以如果要确定存储一个 char 类型变量，就必须使用 char 类型定义强类型变量；实际上 Groovy 认为 char 类型并不是必须的，大部分时候字符串类型更方便一些； 用 / 包围的字符串，即 /字符串内容/，可以避免在字符串中使用转义字符，但 \n 字符不包含在内； Java 中对字符串的运算只有+运算，在 Groovy 中，字符串还可以使用 - 运算 和 * 运算，减法运算可以从一个字符串中删除一部分，乘法运算可以将一个字符串重复n次； Groovy 还为字符串加入了所有关系运算符，包括 ==, !=, &gt;, &lt;, &gt;=, &lt;=，这要归功于 Groovy 允许运算符重载，对于 == 和 !=，将调用 String 类的 equals 方法，对于 &gt;, &gt;=, &lt;, &lt;=，将调用 String 类的 compareTo 方法；Groovy 还增加了一个特殊的运算符&lt;=&gt;，这个运算符也会调用 compareTo 方法，返回 compareTo 方法的返回值； 3.5 switch语句Groovy 的 switch 语句兼容 Java 代码，但是更灵活，Groovy 的 switch 语句能够处理各种类型的 switch 值，可以做各种类型的匹配： case 值为类名，匹配 switch 值为类实例 case 值为正则表达式，匹配 switch 值的字符串匹配该正则表达式 case 值为集合，匹配 switch 值包含在集合中，包括 ranges 除了上面的，case值与switch值相等才匹配。 Switch 语句的工作原理：switch 语句在做 case 值匹配时，会调用 isCase(switchValue) 方法，Groovy 提供了各种类型，如类，正则表达式、集合等等的重载。可以创建自定义的匹配类，增加 isCase(switchValue) 方法来提供自定义的匹配类型。 3.6 循环1234567891011121314151617181920212223242526package com.javachen.groovy.loopspublic class LoopTest&#123; public static void main(args)&#123; def list = ["Lars", "Ben", "Jack"] // using a variable assignment list.each&#123;firstName-&gt; println firstName &#125; // using the it variable list.each&#123;println it&#125; 5.times &#123;println "Times + $it "&#125; 1.upto(3) &#123;println "Up + $it "&#125; 4.downto(1) &#123;print "Down + $it "&#125; def sum = 0 1.upto(100) &#123;sum += 1&#125; print sum (1..6).each &#123;print "Range $it"&#125; for (i in 0..9) &#123; println ("Hello $i") &#125; &#125; &#125; Groovy对Java循环结构作了如下的修整： 对于 for 循环：除了传统三表达式的 for 循环和用于迭代的 for each 循环外，Groovy 允许 for 循环遍历一个范围（Range），例如 for (i in 1..10)，表示循环10次，i在1至10之间取值； 对于整数，Groovy 增加了如下几个方法来进行循环： upto：n.upto(m) 函数，表示循环 m- n 次，并且会有一个循环变量it，从 n 开始，每次循环增加1，直到 m。循环体写在upto方法之后大括号中，表示一个闭包，在闭包中，it 作为循环变量，值从 a 增长到 n； times：n.times 函数，表示循环 n 次，循环变量 it 从0开始到n结束。 step：n.step(x, y) 函数，表示循环变量从 n 开始到 x 结束，每次循环后循环变量增加 y，所以整个循环次数为 (x - n) / y次； 3.7 集合参考文章 [1] Groovy基本语法(1) [2] Groovy基本语法(2) [3] Groovy基本语法(3) [4] Groovy with Eclipse - Tutorial]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>groovy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装Azkaban]]></title>
    <url>%2F2014%2F08%2F25%2Finstall-azkaban%2F</url>
    <content type="text"><![CDATA[Azkaban 是由 Linkedin 开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban 定义了一种 KV 文件格式来建立任务之间的依赖关系，并提供一个易于使用的 web 用户界面维护和跟踪你的工作流。 Azkaban 官网地址：http://azkaban.github.io/Azkaban 的下载地址：http://azkaban.github.io/downloads.html Azkaban 包括三个关键组件： 关系数据库：使用 Mysql数据库，主要用于保存流程、权限、任务状态、任务计划等信息。 AzkabanWebServer：为用户提供管理留存、任务计划、权限等功能。 AzkabanExecutorServer：执行任务，并把任务执行的输出日志保存到 Mysql；可以同时启动多个 AzkabanExecutorServer ，他们通过 mysql 获取流程状态来协调工作。 在 2.5 版本之后，Azkaban 提供了两种模式来安装： 一种是 standalone 的 “solo-server” 模式； 另一种是两个 server 的模式，分别为 AzkabanWebServer 和 AzkabanExecutorServer。 这里主要介绍第二种模式的安装方法。 1. 安装过程1.1 安装 MySql目前 Azkaban 只支持 MySql ，故需安装 MySql 服务器，安装 MySql 的过程这里不作介绍。 安装之后，创建 azkaban 数据库，并创建 azkaban 用户，密码为 azkaban，并设置权限。 123456# Example database creation command, although the db name doesn't need to be 'azkaban'mysql&gt; CREATE DATABASE azkaban;# Example database creation command. The user name doesn't need to be 'azkaban'mysql&gt; CREATE USER 'azkaban'@'%' IDENTIFIED BY 'azkaban';# Replace db, username with the ones created by the previous steps.mysql&gt; GRANT SELECT,INSERT,UPDATE,DELETE ON azkaban.* to 'azkaban'@'%' WITH GRANT OPTION; 修改 /etc/my.cnf 文件，设置 max_allowed_packet 值： 123[mysqld]...max_allowed_packet=1024M 然后重启 MySql。 解压缩 azkaban-sql-2.5.0.tar.gz文件，并进入到 azkaban-sql-script目录，然后进入 mysql 命令行模式： 123$ mysql -uazkaban -pazkabanmysql&gt; use azkabanmysql&gt; source create-all-sql-2.5.0.sql 1.2 安装 azkaban-web-server解压缩 azkaban-web-server-2.5.0.tar.gz，创建 SSL 配置，命令：keytool -keystore keystore -alias jetty -genkey -keyalg RSA 完成上述工作后，将在当前目录生成 keystore 证书文件，将 keystore 考贝到 azkaban web 目录中。 修改 azkaban web 服务器配置，主要包括： a. 修改时区和首页名称: 1234567# Azkaban Personalization Settingsazkaban.name=ETL Taskazkaban.label=By BIazkaban.color=#FF3601azkaban.default.servlet.path=/indexweb.resource.dir=web/default.timezone.id=Asia/Shanghai b. 修改 MySql 数据库配置 1234567database.type=mysqlmysql.port=3306mysql.host=localhostmysql.database=azkabanmysql.user=azkabanmysql.password=azkabanmysql.numconnections=100 c. 修改 Jetty 服务器属性，包括 keystore 的相关配置 12345678910# Azkaban Jetty server properties.jetty.hostname=0.0.0.0jetty.maxThreads=25jetty.ssl.port=8443jetty.port=8081jetty.keystore=keystorejetty.password=redhatjetty.keypassword=redhatjetty.truststore=keystorejetty.trustpassword=redhat d. 修改邮件设置（可选） 12345# mail settingsmail.sender=admin@javachen.commail.host=javachen.commail.user=adminmail.password=admin 1.3 安装 azkaban-executor-server解压缩 azkaban-executor-server-2.5.0.tar.gz，然后修改配置文件，包括： a. 修改时区为：default.timezone.id=Asia/Shanghai b. 修改 MySql 数据库配置 1234567database.type=mysqlmysql.port=3306mysql.host=localhostmysql.database=azkabanmysql.user=azkabanmysql.password=azkabanmysql.numconnections=100 1.4 用户设置进入 azkaban web 服务器 conf 目录，修改 azkaban-users.xml ，增加管理员用户： 1234567&lt;azkaban-users&gt; &lt;user username="azkaban" password="azkaban" roles="admin" groups="azkaban" /&gt; &lt;user username="metrics" password="metrics" roles="metrics"/&gt; &lt;user username="admin" password="admin" roles="admin,metrics" /&gt; &lt;role name="admin" permissions="ADMIN" /&gt; &lt;role name="metrics" permissions="METRICS"/&gt;&lt;/azkaban-users&gt; 1.5 启动服务azkaban-web-server，需要在 azkaban-web-server 目录下执行下面命令： 1sh bin/azkaban-web-start.sh azkaban-executor-server，需要在 azkaban-executor-server 目录下执行下面命令： 1sh bin/azkaban-executor-start.sh 1.6 配置插件下载 HDFS Browser 插件，解压然后重命名为 hdfs，然后将其拷贝到 azkaban-web-server/plugins/viewer 目录下。 下载 Job Types Plugins 插件，解压然后重命名为 jobtype，然后将其拷贝到 azkaban-executor-server/plugins/ 目录下，然后修改以下文件，设置 hive.home： plugins/jobtypes/commonprivate.properties plugins/jobtypes/common.properties plugins/jobtypes/hive/plugin.properties 说明： 在实际使用中，这些插件都没有配置成功，故最后的生产环境没有使用这些插件，而是基于最基本的 command 或 script 方式来编写作业。 1.7 生产环境使用这部分内容详细说明见 当前数据仓库建设过程 一文中的任务调度这一章节内容。]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>azkaban</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[升级cdh4到cdh5]]></title>
    <url>%2F2014%2F08%2F19%2Fupgrading-from-cdh4-to-cdh5%2F</url>
    <content type="text"><![CDATA[本文主要记录从CDH4升级到CDH5的过程和遇到的问题，当然本文同样适用于CDH5低版本向最新版本的升级。 1. 不兼容的变化升级前，需要注意 cdh5 有哪些不兼容的变化，具体请参考：Apache Hadoop Incompatible Changes。 2. 升级过程2.1. 备份数据和停止所有服务2.1.1 让 namenode 进入安全模式在NameNode或者配置了 HA 中的 active NameNode上运行下面命令： 1$ sudo -u hdfs hdfs dfsadmin -safemode enter 保存 fsimage： 1$ sudo -u hdfs hdfs dfsadmin -saveNamespace 如果使用了kerberos，则先获取hdfs用户凭证，再执行下面代码： 123$ kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/cdh1@JAVACHEN.COM$ hdfs dfsadmin -safemode enter$ hdfs dfsadmin -saveNamespace 2.1.2 备份配置文件、数据库和其他重要文件根据你安装的cdh组件，可能需要备份的配置文件包括： 12345678/etc/hadoop/conf/etc/hive/conf/etc/hbase/conf/etc/zookeeper/conf/etc/impala/conf/etc/spark/conf/etc/sentry/conf/etc/default/impala 2.1.3 停止所有服务在每个节点上运行： 123456for x in `cd /etc/init.d ; ls hadoop-*` ; do sudo service $x stop ; donefor x in `cd /etc/init.d ; ls hbase-*` ; do sudo service $x stop ; donefor x in `cd /etc/init.d ; ls hive-*` ; do sudo service $x stop ; donefor x in `cd /etc/init.d ; ls zookeeper-*` ; do sudo service $x stop ; donefor x in `cd /etc/init.d ; ls hadoop-*` ; do sudo service $x stop ; donefor x in `cd /etc/init.d ; ls impala-*` ; do sudo service $x stop ; done 2.1.4 在每个节点上查看进程1$ ps -aef | grep java 2.2. 备份 hdfs 元数据（可选，防止在操作过程中对数据的误操作） a，查找本地配置的文件目录（属性名为 dfs.name.dir 或者 dfs.namenode.name.dir或者hadoop.tmp.dir ） 1234grep -C1 hadoop.tmp.dir /etc/hadoop/conf/hdfs-site.xml#或者grep -C1 dfs.namenode.name.dir /etc/hadoop/conf/hdfs-site.xml 通过上面的命令，可以看到类似以下信息： 1234&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/data/dfs/nn&lt;/value&gt;&lt;/property&gt; b，对hdfs数据进行备份 12cd /data/dfs/nntar -cvf /root/nn_backup_data.tar . 2.3. 更新 yum 源如果你使用的是官方的远程yum源，则下载 cloudera-cdh5.repo 文件到 /etc/yum.repos.d 目录： 1$ wget http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/cloudera-cdh5.repo -P /etc/yum.repos.d 如果你使用本地yum，则需要从 http://archive-primary.cloudera.com/cdh5/repo-as-tarball 下载最新的压缩包文件，然后解压到对于目录，以 CDH5.4 版本为例： 1234$ cd /var/ftp/pub$ rm -rf cdh$ wget http://archive-primary.cloudera.com/cdh5/repo-as-tarball/5.4.0/cdh5.4.0-centos6.tar.gz$ tar zxvf cdh5.4.0-centos6.tar.gz 然后，在 /etc/yum.repos.d 目录创建一个 repos 文件，指向本地yum源即可，详细过程请自行百度。 2.4. 升级组件在所有节点上运行： 1$ sudo yum update hadoop* hbase* hive* zookeeper* bigtop* impala* spark* llama* lzo* sqoop* parquet* sentry* avro* mahout* -y 启动ZooKeeper集群，如果配置了 HA，则在原来的所有 Journal Nodes 上启动 hadoop-hdfs-journalnode： 12345678# 在安装zookeeper-server的节点上运行$ /etc/init.d/zookeeper-server start# 在安装zkfc的节点上运行$ /etc/init.d/hadoop-hdfs-zkfc# 在安装journalnode的节点上运行$ /etc/init.d/hadoop-hdfs-journalnode start 2.5. 更新 hdfs 元数据在NameNode或者配置了 HA 中的 active NameNode上运行下面命令： 1$ sudo service hadoop-hdfs-namenode upgrade 查看日志，检查是否完成升级，例如查看日志中是否出现/var/lib/hadoop-hdfs/cache/hadoop/dfs/&lt;name&gt; is complete 1$ sudo tail -f /var/log/hadoop-hdfs/hadoop-hdfs-namenode-&lt;hostname&gt;.log 如果配置了 HA，在另一个 NameNode 节点上运行： 12345# 输入 Y$ sudo -u hdfs hdfs namenode -bootstrapStandby=====================================================Re-format filesystem in Storage Directory /data/dfs/nn ? (Y or N)$ sudo service hadoop-hdfs-namenode start 启动所有的 DataNode： 1$ sudo service hadoop-hdfs-datanode start 打开 web 界面查看 hdfs 文件是否都存在。 待集群稳定运行一段时间，可以完成升级： 1$ sudo -u hdfs hadoop dfsadmin -finalizeUpgrade 2.6. 更新 YARN更新 YARN 需要注意以下节点： yarn-site.xml 中做如下改变： yarn.nodemanager.aux-services的值从mapreduce.shuffle 修改为 mapreduce_shuffle yarn.nodemanager.aux-services.mapreduce.shuffle.class 改名为 yarn.nodemanager.aux-services.mapreduce_shuffle.class yarn.resourcemanager.resourcemanager.connect.max.wait.secs 修改为 yarn.resourcemanager.connect.max-wait.secs yarn.resourcemanager.resourcemanager.connect.retry_interval.secs 修改为 yarn.resourcemanager.connect.retry-interval.secs yarn.resourcemanager.am. max-retries 修改为 yarn.resourcemanager.am.max-attempts yarn.application.classpath 中的环境变量 YARN_HOME 属性修改为HADOOP_YARN_HOME 然后在启动 YARN 的相关服务。 2.7. 更新 HBase升级 HBase 之前，先启动 zookeeper。 在启动hbase-master进程和hbase-regionserver进程之前，更新 HBase： 1$ sudo -u hdfs hbase upgrade -execute 如果你使用了 phoenix，则请删除 HBase lib 目录下对应的 phoenix 的 jar 包。 启动 HBase： 12$ service hbase-master start$ service hbase-regionserver start 2.8. 更新 hive在启动hive之前，进入 /usr/lib/hive/bin 执行下面命令升级元数据（这里元数据使用的是postgres数据库）： 1234$ cd /usr/lib/hive/bin# ./schematool -dbType 数据库类型 -upgradeSchemaFrom 版本号# 升级之前 hive 版本为 0.14.0，下面命令会运行 /usr/lib/hive/scripts/metastore/upgrade/postgres/upgrade-0.14.0-to-1.1.0.postgres.sql$ ./schematool -dbType postgres -upgradeSchemaFrom 0.14.0 确认 /etc/hive/conf/hive-site.xml 和 /etc/hive/conf/hive-env.sh 是否需要修改，例如 /etc/hive/conf/hive-env.sh 配置了如下参数，需要修改到 cdh-5.2 对应的版本： 12# 请修改到 cdh5.4对应的 jar 包$ export HIVE_AUX_JARS_PATH=/usr/lib/hive/lib/hive-contrib-1.1.0-cdh5.4.0.jar 修改完之后，请同步到其他节点。 然后启动 hive 服务： 12$ service hive-metastore start$ service hive-server2 start 2.9 更新Sentry如果你从 CDH 5.2.0 以前的版本更新到 CDH 5.2.0 以后的版本，请更新sentry的元数据库： 1$ bin/sentry --command schema-tool --conffile &lt;sentry-site.xml&gt; --dbType &lt;db-type&gt; --upgradeSchema 3. 参考文章 Upgrading Unmanaged CDH Using the Command Line CDH Incompatible Changes]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>cdh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sqoop导入关系数据库到Hive]]></title>
    <url>%2F2014%2F08%2F04%2Fimport-data-to-hive-with-sqoop%2F</url>
    <content type="text"><![CDATA[Sqoop 是 apache 下用于 RDBMS 和 HDFS 互相导数据的工具。本文以 mysql 数据库为例，实现关系数据库导入到 hdfs 和 hive。 1. 安装 Sqoop使用 rpm 安装即可。 1yum install sqoop sqoop-metastore -y 安装完之后需要下载 mysql jar 包到 sqoop 的 lib 目录。 这里使用 hive 的 metastore 的 mysql 数据库作为关系数据库，以 TBLS 表为例，该表结构和数据如下： 12345678mysql&gt; select * from TBLS limit 3;+------+-----------+-----+----------------+-----+--------+------+---------+----------------+------------------+-------------------+|TBL_ID|CREATE_TIME|DB_ID|LAST_ACCESS_TIME|OWNER|RETENTI | SD_ID| TBL_NAME| TBL_TYPE |VIEW_EXPANDED_TEXT| VIEW_ORIGINAL_TEXT|+------+-----------+-----+----------------+-----+--------+------+---------+----------------+------------------+-------------------+| 34|1406784308 | 8| 0|root | 0| 45| test1 | EXTERNAL_TABLE | NULL | NULL || 40|1406797005 | 9| 0|root | 0| 52| test2 | EXTERNAL_TABLE | NULL | NULL || 42|1407122307 | 7| 0|root | 0| 59| test3 | EXTERNAL_TABLE | NULL | NULL |+------+-----------+-----+----------------+-----+--------+------+---------+----------------+------------------+-------------------+ 2. 使用2.1 命令说明查看 sqoop 命令说明： 12345678910111213141516$ sqoop helpusage: sqoop COMMAND [ARGS]Available commands: codegen Generate code to interact with database records create-hive-table Import a table definition into Hive eval Evaluate a SQL statement and display the results export Export an HDFS directory to a database table help List available commands import Import a table from a database to HDFS import-all-tables Import tables from a database to HDFS list-databases List available databases on a server list-tables List available tables in a database version Display version informationSee 'sqoop help COMMAND' for information on a specific command. 你也可以查看某一个命令的使用说明： 12$ sqoop import --help$ sqoop help import 你也可以使用别名来代替 sqoop (toolname)： 1$ sqoop-import sqoop import 的一个示例如下： 1$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS 你还可以使用 --options-file 来传入一个文件，使用这种方式可以重用一些配置参数： 1$ sqoop --options-file /users/homer/work/import.txt --table TEST /users/homer/work/import.txt 文件内容如下： 1234567import--connectjdbc:mysql://192.168.56.121:3306/metastore--usernamehiveuser--password redhat 2.2 导入数据到 hdfs使用 sqoop-import 命令可以从关系数据库导入数据到 hdfs。 1$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --target-dir /user/hive/result 注意： mysql jdbc url 请使用 ip 地址 如果重复执行，会提示目录已经存在，可以手动删除 如果不指定 --target-dir，导入到用户家目录下的 TBLS 目录 你还可以指定其他的参数： 参数 |说明||:—|:—|| --append |将数据追加到hdfs中已经存在的dataset中。使用该参数，sqoop将把数据先导入到一个临时目录中，然后重新给文件命名到一个正式的目录中，以避免和该目录中已存在的文件重名。|| --as-avrodatafile | 将数据导入到一个Avro数据文件中||| --as-sequencefile | 将数据导入到一个sequence文件中|| --as-textfile | 将数据导入到一个普通文本文件中，生成该文本文件后，可以在hive中通过sql语句查询出结果。|| --boundary-query &lt;statement&gt;| 边界查询，也就是在导入前先通过SQL查询得到一个结果集，然后导入的数据就是该结果集内的数据，格式如：--boundary-query &#39;select id,no from t where id = 3&#39;，表示导入的数据为id=3的记录，或者 select min(&lt;split-by&gt;), max(&lt;split-by&gt;) from &lt;table name&gt;，注意查询的字段中不能有数据类型为字符串的字段，否则会报错|| --columns&lt;col,col&gt; |指定要导入的字段值，格式如：--columns id,username| | --direct |直接导入模式，使用的是关系数据库自带的导入导出工具。官网上是说这样导入会更快|| --direct-split-size|在使用上面direct直接导入的基础上，对导入的流按字节数分块，特别是使用直连模式从PostgreSQL导入数据的时候，可以将一个到达设定大小的文件分为几个独立的文件。|| --inline-lob-limit |设定大对象数据类型的最大值|| -m,--num-mappers |启动N个map来并行导入数据，默认是4个，最好不要将数字设置为高于集群的节点数|| --query，-e &lt;sql&gt;| 从查询结果中导入数据，该参数使用时必须指定–target-dir、–hive-table，在查询语句中一定要有where条件且在where条件中需要包含 \$CONDITIONS，示例：--query &#39;select * from t where \$CONDITIONS &#39; --target-dir /tmp/t –hive-table t| | --split-by &lt;column&gt; |表的列名，用来切分工作单元，一般后面跟主键ID|| --table &lt;table-name&gt; |关系数据库表名，数据从该表中获取|| --delete-target-dir|删除目标目录|| --target-dir &lt;dir&gt; |指定hdfs路径|| --warehouse-dir &lt;dir&gt;| 与 --target-dir 不能同时使用，指定数据导入的存放目录，适用于hdfs导入，不适合导入hive目录|| --where |从关系数据库导入数据时的查询条件，示例：--where &quot;id = 2&quot;| | -z,--compress |压缩参数，默认情况下数据是没被压缩的，通过该参数可以使用gzip压缩算法对数据进行压缩，适用于SequenceFile, text文本文件, 和Avro文件|| --compression-codec |Hadoop压缩编码，默认是gzip|| --null-string &lt;null-string&gt; |可选参数，如果没有指定，则字符串null将被使用|| --null-non-string &lt;null-string&gt; |可选参数，如果没有指定，则字符串null将被使用|| 示例程序： 1$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --columns "tbl_id,create_time" --where "tbl_id &gt; 1" --target-dir /user/hive/result 使用 sql 语句参照上表，使用 sql 语句查询时，需要指定 $CONDITIONS 1$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --query 'SELECT * from TBLS where \$CONDITIONS ' --split-by tbl_id -m 4 --target-dir /user/hive/result 上面命令通过 -m 1 控制并发的 map 数。 使用 direct 模式：1$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --delete-target-dir --direct --default-character-set UTF-8 --target-dir /user/hive/result 指定文件输出格式：1$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by "\t" --lines-terminated-by "\n" --delete-target-dir --target-dir /user/hive/result 这时候查看 hdfs 中数据(观察分隔符是否为制表符)： 1234567891011121314$ hadoop fs -ls resultFound 5 items-rw-r--r-- 3 root hadoop 0 2014-08-04 16:07 result/_SUCCESS-rw-r--r-- 3 root hadoop 69 2014-08-04 16:07 result/part-m-00000-rw-r--r-- 3 root hadoop 0 2014-08-04 16:07 result/part-m-00001-rw-r--r-- 3 root hadoop 142 2014-08-04 16:07 result/part-m-00002-rw-r--r-- 3 root hadoop 62 2014-08-04 16:07 result/part-m-00003$ hadoop fs -cat result/part-m-0000034 1406784308 8 0 root 0 45 test1 EXTERNAL_TABLE null null null$ hadoop fs -cat result/part-m-0000240 1406797005 9 0 root 0 52 test2 EXTERNAL_TABLE null null null42 1407122307 7 0 root 0 59 test3 EXTERNAL_TABLE null null null 指定空字符串： 1$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by "\t" --lines-terminated-by "\n" --delete-target-dir --null-string '\\N' --null-non-string '\\N' --target-dir /user/hive/result 如果需要指定压缩： 1$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by "\t" --lines-terminated-by "\n" --delete-target-dir --null-string '\\N' --null-non-string '\\N' --compression-codec "com.hadoop.compression.lzo.LzopCodec" --target-dir /user/hive/result 附：可选的文件参数如下表。 参数 说明 --enclosed-by &lt;char&gt; 给字段值前后加上指定的字符，比如双引号，示例：--enclosed-by &#39;\&quot;&#39;，显示例子：”3”,”jimsss”,”dd@dd.com“ --escaped-by &lt;char&gt; 给双引号作转义处理，如字段值为”测试”，经过 --escaped-by &quot;\\&quot; 处理后，在hdfs中的显示值为：\&quot;测试\&quot;，对单引号无效 --fields-terminated-by &lt;char&gt; 设定每个字段是以什么符号作为结束的，默认是逗号，也可以改为其它符号，如句号.，示例如：--fields-terminated-by --lines-terminated-by &lt;char&gt; 设定每条记录行之间的分隔符，默认是换行串，但也可以设定自己所需要的字符串，示例如：--lines-terminated-by &quot;#&quot; 以#号分隔 --mysql-delimiters Mysql默认的分隔符设置，字段之间以,隔开，行之间以换行\n隔开，默认转义符号是\，字段值以单引号&#39;包含起来。 --optionally-enclosed-by &lt;char&gt; enclosed-by是强制给每个字段值前后都加上指定的符号，而--optionally-enclosed-by只是给带有双引号或单引号的字段值加上指定的符号，故叫可选的 2.3 创建 hive 表生成与关系数据库表的表结构对应的HIVE表： 1$ sqoop create-hive-table --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS 参数 说明 --hive-home &lt;dir&gt; Hive的安装目录，可以通过该参数覆盖掉默认的hive目录 --hive-overwrite 覆盖掉在hive表中已经存在的数据 --create-hive-table 默认是false，如果目标表已经存在了，那么创建任务会失败 --hive-table 后面接要创建的hive表 --table 指定关系数据库表名 2.4 导入数据到 hive执行下面的命令会将 mysql 中的数据导入到 hdfs 中，然后创建一个hive 表，最后再将 hdfs 上的文件移动到 hive 表的目录下面。 1$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by "\t" --lines-terminated-by "\n" --hive-import --hive-overwrite --create-hive-table --hive-table dw_srclog.TBLS --delete-target-dir 说明： 可以在 hive 的表名前面指定数据库名称 可以通过 --create-hive-table 创建表，如果表已经存在则会执行失败 接下来可以查看 hive 中的数据： 1234$ hive -e 'select * from dw_srclog.tbls'34 1406784308 8 0 root 0 45 test1 EXTERNAL_TABLE null null NULL40 1406797005 9 0 root 0 52 test2 EXTERNAL_TABLE null null NULL42 1407122307 7 0 root 0 59 test3 EXTERNAL_TABLE null null NULL 直接查看文件内容： 123$ hadoop fs -cat /user/hive/warehouse/dw_srclog.db/tbls/part-m-0000034140678430880root045go_goodsEXTERNAL_TABLEnullnullnull40140679700590root052merchantEXTERNAL_TABLEnullnullnull 从上面可见，数据导入到 hive 中之后分隔符为默认分隔符，参考上文你可以通过设置参数指定其他的分隔符。 另外，Sqoop 默认地导入空值（NULL）为 null 字符串，而 hive 使用 \N 去标识空值（NULL），故你在 import 或者 export 时候，需要做相应的处理。在 import 时，使用如下命令： 1$ sqoop import ... --null-string '\\N' --null-non-string '\\N' 在导出时，使用下面命令： 1$ sqoop import ... --input-null-string '' --input-null-non-string '' 一个完整的例子如下： 1$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by "\t" --lines-terminated-by "\n" --hive-import --hive-overwrite --create-hive-table --hive-table dw_srclog.TBLS --null-string '\\N' --null-non-string '\\N' --compression-codec "com.hadoop.compression.lzo.LzopCodec" 2.5 增量导入 参数 说明 --check-column (col) 用来作为判断的列名，如id --incremental (mode) append：追加，比如对大于last-value指定的值之后的记录进行追加导入。lastmodified：最后的修改时间，追加last-value指定的日期之后的记录 --last-value (value) 指定自从上次导入后列的最大值（大于该指定的值），也可以自己设定某一值 2.6 合并 hdfs 文件将HDFS中不同目录下面的数据合在一起，并存放在指定的目录中，示例如： 1sqoop merge –new-data /test/p1/person –onto /test/p2/person –target-dir /test/merged –jar-file /opt/data/sqoop/person/Person.jar –class-name Person –merge-key id 其中，–class-name 所指定的 class 名是对应于 Person.jar 中的 Person 类，而 Person.jar 是通过 Codegen 生成的 参数 说明 --new-data &lt;path&gt; Hdfs中存放数据的一个目录，该目录中的数据是希望在合并后能优先保留的，原则上一般是存放越新数据的目录就对应这个参数。 --onto &lt;path&gt; Hdfs中存放数据的一个目录，该目录中的数据是希望在合并后能被更新数据替换掉的，原则上一般是存放越旧数据的目录就对应这个参数。 --merge-key &lt;col&gt; 合并键，一般是主键ID --jar-file &lt;file&gt; 合并时引入的jar包，该jar包是通过Codegen工具生成的jar包 --class-name &lt;class&gt; 对应的表名或对象名，该class类是包含在jar包中的。 --target-dir &lt;path&gt; 合并后的数据在HDFS里的存放目录 3. 参考文章 Sqoop中文手册]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2014年7月总结]]></title>
    <url>%2F2014%2F07%2F31%2Fsummary-of-july-in-2014%2F</url>
    <content type="text"><![CDATA[在休息了将近三个月之后，7月9日终于开始上班了，新的工作还是和 hadoop 相关。7月主要的工作内容如下： 搭建新的 hadoop 集群，hadoop 版本为 CDH4.7.0，并配置 NameNode 的 QJM HA 方案。配置 HA 方法见 CDH 中配置 HDFS HA 购买了三本书： mahout实战 机器学习实战 这才是搜索引擎 调研了 flume-ng 日志采集方案 大众点评的大数据实践 analyzing-twitter-data-with-hadoop Hadoop Analysis of Apache Logs Using Flume-NG, Hive and Pig Log Files with Flume and Hive Solving Small Files Problem on CDH4 flume-ng+Kafka+Storm+HDFS 实时系统搭建 flume-ng+Hadoop实现日志收集 基于Flume的美团日志收集系统(一)架构和设计 查看关系数据库数据同步到 hadoop 的相关方法 - a. 查看sqoop 和 sqoop2 的相关文档 - b. 测试使用kettle 连接 hdfs 和 hive 熟悉原来 hadoop 数据同步和调度的代码逻辑，查看一些开源的任务调度框架： https://github.com/alibaba/zeus https://github.com/thieman/dagobah https://github.com/azkaban/azkaban http://demo.gethue.com/ 熟悉 phoenix 用法（phoenix 为 HBase 提供 sql 支持），Phoenix 快速入门见Phoenix Quick Start 测试 impala 是否支持 hive 的自定义文件格式，见Impala 新特性 完善下载日志文件并上传到 hive 的 python 脚本，见采集日志到 hive 另外萌生了一些想法，例如，开发一个java web 项目，支持 hive 查询、任务调度、查看 hdfs 等功能。可参考的资源： https://github.com/dianping/hiveweb https://github.com/dianping/polestar]]></content>
      <categories>
        <category>work</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Impala新特性]]></title>
    <url>%2F2014%2F07%2F29%2Fnew-features-in-impala%2F</url>
    <content type="text"><![CDATA[本文主要整理一下 Impala 每个版本的新特性，方便了解 Impala 做了哪些改进、修复了哪些 bug。 Impala 目前最新版本为 1.4.0，其下载地址为：http://archive.cloudera.com/impala/redhat/6/x86_64/impala/ 不得不说的事情： 1.3.1 用于 CDH4 1.4.0 用于 CDH5 1.4.0 CDH5 中增加 DECIMAL 数据类型，可以设置精度，其语法为：DECIMAL[(precision[,scale])] CDH5 中，impala 可以使用 HDFS 缓存特性加快频繁访问的数据的速度，减少 cpu 使用率。当数据缓存到 hdfs cache 中时，impala 可以直接从缓存中读取数据而不需要读磁盘并且减少额外的内存拷贝。 Centralized Cache Management in HDFS impala 中使用 HDFS Caching，参考 sing HDFS Caching with Impala (CDH 5 Only) Impala 可以使用基于 Sentry 的授权策略，详细说明可以参考：Enabling Sentry Authorization for Impala Impala 支持其他 hadoop 组件创建的 Parquet 格式的文件，你可以在建表语句中指定 Parquet 格式，Impala 中创建 parquet 格式的表，请参考：Using the Parquet File Format with Impala Tables ORDER BY 查询不再要求一个 limit 语句，如果需要排序的结果集的大小超过了内存限制，则会使用临时的磁盘空间用于排序，ORDER BY 语法为：ORDER BY col1 [, col2 ...] [ASC | DESC] [NULLS FIRST | NULLS LAST]，详细说明见：ORDER BY Clause LDAP 连接可以使用 SSL 或者 TLS 加密，详细说明参考：Enabling LDAP Authentication for Impala 增加以下内建函数： EXTRACT()，用于从一个 TIMESTAMP 字段返回一个 date 或者 time 的字段，详细说明参考：Date and Time Functions TRUNC()，用于将一个 date/time 类型的字段裁剪为一个特定格式的值，如年、月、日、小时等等，详细说明参考：Date and Time Functions ADD_MONTHS() ROUND()，对 DECIMAL 类型的值四舍五入，详细说明参考：Mathematical Functions STDDEV, STDDEV_SAMP, STDDEV_POP Functions 和 VARIANCE, VARIANCE_SAMP, VARIANCE_POP Functions MAX_INT()、MIN_SMALLINT()等，用于判断数组是否超过最大值和最小值。 IS_INF() 和 IS_NAN()，用于判断是否为数值。 SHOW PARTITIONS 语句用于查看分区情况，详细说明参考：SHOW Statement 添加 impalad 进程设置参数让你设置所有查询的初始化内存值，详细说明参考：Using YARN Resource Management with Impala (CDH 5 Only) CDH 5.1 中可以利用 Llama 高可用的特性，详细说明参考：Using Impala with a Llama High Availability Configuration CREATE TABLE 语句支持 STORED AS AVRO，详细说明参考：Using the Avro File Format with Impala Tables impala-shell 中添加 SUMMARY 命令用于查看摘要信息，详细说明参考：Using the SUMMARY Report for Performance Tuning COMPUTE STATS 语句性能改进： NDV 函数通过生成本地代码加快速度 在 1.4.0 或者更高版本，不再统计 NULL 值，其值被看做为 -1，详细说明参考：How Impala Uses Statistics for Query Optimization 分区性能改进。之前只能处理3000个分区，现在没有这个限制，详细说明参考：Partition Pruning for Queries impala-shell 支持 UTF-8 字符的输入和输出，可以通过参数 --strict_unicode 控制是否忽略不合法的 Unicode 字符。 1.3.1该版本主要是 bug 修复，可以在 CDH 4 和 CDH 5 中使用。 在 impalad 启动参数中，添加 --insert_inherit_permissions 参数用于设置创建分区的用户。默认的，INSERT 会使用 HDFS 权限为新分区创建目录，详细说明参考：INSERT Statement SHOW 函数显示每个函数的返回类型，详细说明参考：SHOW Statement CREATE TABLE 语句可以使用 FIELDS TERMINATED BY &#39;\0&#39; 语句，详细说明参考：Using Text Data Files with Impala Tables 在 1.3.1 以及更高版本后，REGEXP 和 RLIKE 的语义进行修正，和数据库中的语义进行兼容，详细说明参考：REGEXP Operator。regexp_extract() 和 regexp_replace() 可以不再使用。 1.3.0 Admission Control and Query Queuing EXPLAIN 以一种更容易读的格式显示更加详细的内容，详细说明参考：EXPLAIN Statement 和 Understanding Impala Query Performance - EXPLAIN Plans and Query Profiles UNIX_TIMESTAMP 、FROM_UNIXTIME 和 INTERVAL 增加条件函数： NULLIF()、NULLIFZERO()、 ZEROIFNULL()，详细说明参考：Conditional Functions 添加新的功能函数：CURRENT_DATABASE()，详细说明参考：Miscellaneous Functions 和 yarn 集成，只在 CDH5 中可用，详细说明参考：Using YARN Resource Management with Impala (CDH 5 Only) 1.2.4该版本用于 CDH4，主要针对 1.2.3 做了一些 bug 修复。 增加 INVALIDATE METADATA table_name 语法刷新新建的一个表 添加 catalogd 启动参数： --load_catalog_in_background，是否后台运行 --num_metadata_loading_threads，并行加载线程 1.2.3 Impala 1.2.3 works with CDH 4 and with CDH 5 beta 2. The resource management feature requires CDH 5 beta. 该版本主要是在 1.2.2 基础上修复 Parquet 兼容性，详细说明参考：Known Issues and Workarounds in Impala 1.2.2 Impala 1.2.2 works with CDH 4. Its feature set is a superset of features in the Impala 1.2.0 beta, with the exception of resource management, which relies on CDH 5. Join order optimizations，详细说明参考：Performance Considerations for Join Queries COMPUTE STATS STRAIGHT_JOIN，详细说明参考：Overriding Join Reordering with STRAIGHT_JOIN CROSS JOIN，详细说明参考：Cross Joins and Cartesian Products with the CROSS JOIN Operator LDAP 支持 添加 GROUP_CONCAT() INSERT 语句可以添加 SHUFFLE 或者 NOSHUFFLE，主要是用在插入数据到 Parquet 表的分区的时候。 添加 CAST() 用于类型转换 添加 fnv_hash() 用于计算 hash 值，详细说明参考：Mathematical Functions 支持 STORED AS PARQUET 语句。 1.2.1 添加 SHOW TABLE STATS table_name 和 SHOW COLUMN STATS table_name 语法 添加 CREATE TABLE AS SELECT 语法 支持 OFFSET 语句，用于分页查询 ORDER BY 语句中添加 NULLS FIRST 和 NULLS LAST 语法支持 添加内置函数： least(), greatest(), initcap() 添加 ndv() 函数来计算 COUNT(DISTINCT col) LIMIT 语句接受数值表达式作为参数 SHOW CREATE TABLE 添加两个参数：--idle_query_timeout 和 --idle_session_timeout，详细说明参考：Setting Timeout Periods for Daemons, Queries, and Sessions 支持 UDFs，详细说明参考：CREATE FUNCTION Statement 和 DROP FUNCTION Statement 添加新的同步元数据的机制，详细参考：The Impala Catalog Service 添加 CREATE TABLE ... AS SELECT 语法 CREATE TABLE 和 ALTER TABLE 支持 TBLPROPERTIES 和 WITH SERDEPROPERTIES 语句，详细说明参考：CREATE TABLE Statement 和 ALTER TABLE Statement EXPLAIN SHOW CREATE TABLE LIMIT 语句支持算术表达式 另外，impala 的一些不兼容的变化，请参考：Incompatible Changes in Impala Impala 一些已知的问题：Known Issues and Workarounds in Impala 已经修复的问题：Fixed Issues in Impala]]></content>
      <categories>
        <category>impala</category>
      </categories>
      <tags>
        <tag>impala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Phoenix Quick Start]]></title>
    <url>%2F2014%2F07%2F28%2Fphoenix-quick-start%2F</url>
    <content type="text"><![CDATA[1. 介绍Phoenix 是 Salesforce.com 开源的一个 Java 中间件，可以让开发者在Apache HBase 上执行 SQL 查询。Phoenix完全使用Java编写，代码位于 GitHub 上，并且提供了一个客户端可嵌入的 JDBC 驱动。 根据项目所述，Phoenix 被 Salesforce.com 内部使用，对于简单的低延迟查询，其量级为毫秒；对于百万级别的行数来说，其量级为秒。Phoenix 并不是像 HBase 那样用于 map-reduce job 的，而是通过标准化的语言来访问 HBase 数据的。 Phoenix 为 HBase 提供 SQL 的查询接口，它在客户端解析SQL语句，然后转换为 HBase native 的客户端语言，并行执行查询然后生成标准的JDBC结果集。 Phoenix 最值得关注的一些特性 有： 嵌入式的JDBC驱动，实现了大部分的java.sql接口，包括元数据API 可以通过多部行键或是键/值单元对列进行建模 完善的查询支持，可以使用多个谓词以及优化的扫描键 DDL支持：通过CREATE TABLE、DROP TABLE及ALTER TABLE来添加/删除列 版本化的模式仓库：当写入数据时，快照查询会使用恰当的模式 DML支持：用于逐行插入的UPSERT VALUES、用于相同或不同表之间大量数据传输的UPSERT SELECT、用于删除行的DELETE 通过客户端的批处理实现的有限的事务支持 表连接 和二级索引 紧跟ANSI SQL标准 SQL Support 可以参考 language reference，Phoenix 当前不支持： 完全的事物支持 嵌套查询 关联操作: Union、Intersect、Minus 各种各样的内建函数。可以参考这篇文章添加自定义函数。 2. 安装HBase 兼容性： Phoenix 2.x - HBase 0.94.x Phoenix 3.x - HBase 0.94.x Phoenix 4.x - HBase 0.98.1+ 安装已经编译好的 phoenix ： 下载对应你 hbase 版本的 phoenix-[version]-incubating.tar 并解压，下载地址：http://www.apache.org/dyn/closer.cgi/incubator/phoenix/。 添加 phoenix-core-[version]-incubating.jar 到 HBase region server 的 classpath 中，或者直接将其加载到 hbase 的 lib 目录 重启 HBase region server 添加 phoenix-[version]-incubating-client.jar 到 hadoop 客户端的 lib 目录。 3. 使用3.1 JDBCJava 客户端连接 jdbc 代码如下： 1Connection conn = DriverManager.getConnection("jdbc:phoenix:server1,server2:2181"); jdbc 的 url 类似为 jdbc:phoenix [ :&lt;zookeeper quorum&gt; [ :&lt;port number&gt; ] [ :&lt;root node&gt; ] ]，需要引用三个参数：hbase.zookeeper.quorum、hbase.zookeeper.property.clientPort、and zookeeper.znode.parent，这些参数可以缺省不填而在 hbase-site.xml 中定义。 3.2 sqlline 命令行进入解压后的 bin 目录，执行下面命令可以进入一个命令行模式： 1$ sqlline.py localhost 进入之后，可以查看表和列： 123456789101112131415sqlline version 1.1.20: jdbc:phoenix:localhost&gt; !tables+------------+-------------+------------+------------+------------+------------+---------------------------+----------------+--------+| TABLE_CAT | TABLE_SCHEM | TABLE_NAME | TABLE_TYPE | REMARKS | TYPE_NAME | SELF_REFERENCING_COL_NAME | REF_GENERATION | INDEX_ |+------------+-------------+------------+------------+------------+------------+---------------------------+----------------+--------+| null | SYSTEM | CATALOG | SYSTEM TABLE | null | null | null | null | null || null | SYSTEM | SEQUENCE | SYSTEM TABLE | null | null | null | null | null |+------------+-------------+------------+------------+------------+------------+---------------------------+----------------+--------+0: jdbc:phoenix:localhost&gt; !columns sequence+------------+-------------+------------+-------------+------------+------------+-------------+---------------+----------------+-----+| TABLE_CAT | TABLE_SCHEM | TABLE_NAME | COLUMN_NAME | DATA_TYPE | TYPE_NAME | COLUMN_SIZE | BUFFER_LENGTH | DECIMAL_DIGITS | NUM |+------------+-------------+------------+-------------+------------+------------+-------------+---------------+----------------+-----+| null | SYSTEM | SEQUENCE | TENANT_ID | 12 | VARCHAR | null | null | null | nul |+------------+-------------+------------+-------------+------------+------------+-------------+---------------+----------------+-----+0: jdbc:phoenix:localhost&gt; 从上面可以看出来，phoenix 中存在两个系统表： SYSTEM.CATALOG SYSTEM.SEQUENCE 通过 HBase Master 的 web 页面，可以看到上面两个表的建表语句，例如： 1&apos;SYSTEM.CATALOG&apos;, &#123;METHOD =&gt; &apos;table_att&apos;, coprocessor$1 =&gt; &apos;|org.apache.phoenix.coprocessor.ScanRegionObserver|1|&apos;, coprocessor$2 =&gt; &apos;|org.apache.phoenix.coprocessor.UngroupedAggregateRegionObserver|1|&apos;, coprocessor$3 =&gt; &apos;|org.apache.phoenix.coprocessor.GroupedAggregateRegionObserver|1|&apos;, coprocessor$4 =&gt; &apos;|org.apache.phoenix.coprocessor.ServerCachingEndpointImpl|1|&apos;, coprocessor$5 =&gt; &apos;|org.apache.phoenix.coprocessor.MetaDataEndpointImpl|1|&apos;, coprocessor$6 =&gt; &apos;|org.apache.phoenix.coprocessor.MetaDataRegionObserver|2|&apos;, CONFIG =&gt; &#123;&apos;SPLIT_POLICY&apos; =&gt; &apos;org.apache.phoenix.schema.MetaDataSplitPolicy&apos;, &apos;UpgradeTo30&apos; =&gt; &apos;true&apos;&#125;&#125;, &#123;NAME =&gt; &apos;0&apos;, DATA_BLOCK_ENCODING =&gt; &apos;FAST_DIFF&apos;, VERSIONS =&gt; &apos;1000&apos;, KEEP_DELETED_CELLS =&gt; &apos;true&apos;&#125; 可以用下面脚本执行一个 sql 语句： 1$ ./sqlline.py localhost ../examples/STOCK_SYMBOL.sql 执行结果如下： 1234567891011121314151617181920212223242526272829303132331/4 /** Licensed to the Apache Software Foundation (ASF) under one* or more contributor license agreements. See the NOTICE file* distributed with this work for additional information* regarding copyright ownership. The ASF licenses this file* to you under the Apache License, Version 2.0 (the* "License"); you may not use this file except in compliance* with the License. You may obtain a copy of the License at** http://www.apache.org/licenses/LICENSE-2.0** Unless required by applicable law or agreed to in writing, software* distributed under the License is distributed on an "AS IS" BASIS,* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.* See the License for the specific language governing permissions and* limitations under the License.*/-- creates stock table with single rowCREATE TABLE IF NOT EXISTS STOCK_SYMBOL (SYMBOL VARCHAR NOT NULL PRIMARY KEY, COMPANY VARCHAR);No rows affected (1.714 seconds)2/4 UPSERT INTO STOCK_SYMBOL VALUES ('CRM','SalesForce.com');1 row affected (0.035 seconds)3/4 SELECT * FROM STOCK_SYMBOL;+------------+------------+| SYMBOL | COMPANY |+------------+------------+| CRM | SalesForce.com |+------------+------------+1 row selected (0.117 seconds)4/4Closing: org.apache.phoenix.jdbc.PhoenixConnectionsqlline version 1.1.2 ../examples/STOCK_SYMBOL.sql 文件主要包括三个 sql 语句： 123CREATE TABLE IF NOT EXISTS STOCK_SYMBOL (SYMBOL VARCHAR NOT NULL PRIMARY KEY, COMPANY VARCHAR);UPSERT INTO STOCK_SYMBOL VALUES ('CRM','SalesForce.com');SELECT * FROM STOCK_SYMBOL; 第一个语句创建表 第二个语句插入一条记录 第三个语句查询数据 在 hbase shell 中查看存在的表： 12345hbase(main):001:0&gt; listTABLESTOCK_SYMBOLSYSTEM.CATALOGSYSTEM.SEQUENCE 查看 STOCK_SYMBOL 表中数据： 12345hbase(main):004:0&gt; scan 'STOCK_SYMBOL'ROW COLUMN+CELL CRM column=0:COMPANY, timestamp=1406535419510, value=SalesForce.com CRM column=0:_0, timestamp=1406535419510, value=1 row(s) in 0.0210 seconds 可以看到插入了一行记录，rowkey 为 CRM，而列族名称为 0 ，存在两列，一列为指定的COMPANY，另一列为 phoenix 插入的 _0 然后，也可以通过 sqlline.py 来查看数据： 12345670: jdbc:phoenix:localhost&gt; select * from STOCK_SYMBOL;+------------+------------+| SYMBOL | COMPANY |+------------+------------+| CRM | SalesForce.com |+------------+------------+1 row selected (0.144 seconds) 注意到：从上面查询看到的就只有两列，没有看到 _0 这一列。 从上可以知道，Phoenix 是构建在 HBase 之上的 SQL 中间层，向 HBase 发送标准 sql 语句，对 HBase 进行操作。 3.3 加载数据你可以使用 bin/psql.py 来加载 CSV 数据 或者执行 SQL 脚本，例如： 1$ ./psql.py localhost ../examples/WEB_STAT.sql ../examples/WEB_STAT.csv ../examples/WEB_STAT_QUERIES.sql 其输出结果为： 123456789101112131415161718192021222324252627282930313233343536373839no rows upsertedTime: 1.528 sec(s)csv columns from database.CSV Upsert complete. 39 rows upsertedTime: 0.122 sec(s)DOMAIN AVERAGE_CPU_USAGE AVERAGE_DB_USAGE---------- ----------------- ----------------Salesforce.com 260.727 257.636Google.com 212.875 213.75Apple.com 114.111 119.556Time: 0.062 sec(s)DAY TOTAL_CPU_USAGE MIN_CPU_USAGE MAX_CPU_USAGE------------------- --------------- ------------- -------------2013-01-01 00:00:00 35 35 352013-01-02 00:00:00 150 25 1252013-01-03 00:00:00 88 88 882013-01-04 00:00:00 26 3 232013-01-05 00:00:00 550 75 4752013-01-06 00:00:00 12 12 122013-01-08 00:00:00 345 345 3452013-01-09 00:00:00 390 35 3552013-01-10 00:00:00 345 345 3452013-01-11 00:00:00 335 335 3352013-01-12 00:00:00 5 5 52013-01-13 00:00:00 355 355 3552013-01-14 00:00:00 5 5 52013-01-15 00:00:00 720 65 6552013-01-16 00:00:00 785 785 7852013-01-17 00:00:00 1590 355 1235Time: 0.045 sec(s)HOST TOTAL_ACTIVE_VISITORS---- ---------------------EU 150NA 1Time: 0.058 sec(s) WEB_STAT.sql 中 sql 语句为： 12345678910CREATE TABLE IF NOT EXISTS WEB_STAT ( HOST CHAR(2) NOT NULL, DOMAIN VARCHAR NOT NULL, FEATURE VARCHAR NOT NULL, DATE DATE NOT NULL, USAGE.CORE BIGINT, -- 指定了列族： USAGE USAGE.DB BIGINT, -- 指定了列族： USAGE STATS.ACTIVE_VISITOR INTEGER , --指定了列族： STATS CONSTRAINT PK PRIMARY KEY (HOST, DOMAIN, FEATURE, DATE)); 从 sql 语句上可以看出来，HOST、DOMAIN、FEATURE、DATE 这四列前面并没有指定列族，并且通过约束设置这四列组成 hbase 的 rowkey，其他三列都指定了列族。 通过 sqlline.py 查询第一条记录： 1234560: jdbc:phoenix:localhost&gt; select * from WEB_STAT limit 1;+------+------------+------------+---------------------+------------+------------+----------------+| HOST | DOMAIN | FEATURE | DATE | CORE | DB | ACTIVE_VISITOR |+------+------------+------------+---------------------+------------+------------+----------------+| EU | Apple.com | Mac | 2013-01-01 | 35 | 22 | 34 |+------+------------+------------+---------------------+------------+------------+----------------+ 而通过 hbase shell 查询 WEB_STAT 表第一条记录： 12345678EUApple.com\x00Mac\x00\x80\x00\x0 column=STATS:ACTIVE_VISITOR, timestamp=1406535785946, value=\x80\x00\x00&quot;1;\xF3\xA04\xC8EUApple.com\x00Mac\x00\x80\x00\x0 column=USAGE:CORE, timestamp=1406535785946, value=\x80\x00\x00\x00\x00\x00\x00#1;\xF3\xA04\xC8EUApple.com\x00Mac\x00\x80\x00\x0 column=USAGE:DB, timestamp=1406535785946, value=\x80\x00\x00\x00\x00\x00\x00\x161;\xF3\xA04\xC8EUApple.com\x00Mac\x00\x80\x00\x0 column=USAGE:_0, timestamp=1406535785946, value=1;\xF3\xA04\xC8 通过上面对比知道： phoenix 对用户屏蔽了 rowkey 的设计细节 USAGE 列族中存在一列为 _0，而 STATS 列族中却没有，这是为什么？ Phoenix 把 rowkey 内化为 table 的 PRIMARY KEY 处理，由 HOST、DOMAIN、FEATURE、DATE 这四列拼接在一起，组成了 rowkey 其他可选的加载数据的方法： 使用 map-reduce based CSV loader 加载更大的数据 映射一个存在的 HBase 表到 Phoenix 表 以及使用 UPSERT SELECT 来创建一个新表 使用 UPSERT VALUES 插入记录 3.4 映射到存在的 HBase 表创建一张hbase表： 1234567create &apos;t1&apos;, &apos;f&apos;put &apos;t1&apos;, &quot;row1&quot;, &apos;f:q&apos;, 1put &apos;t1&apos;, &quot;row2&quot;, &apos;f:q&apos;, 2put &apos;t1&apos;, &quot;row3&quot;, &apos;f:q&apos;, 3put &apos;t1&apos;, &quot;row4&quot;, &apos;f:q&apos;, 4put &apos;t1&apos;, &quot;row5&quot;, &apos;f:q&apos;, 5 在phoenix建一张同样的表： 1234567./sqlline.py localhost CREATE TABLE IF NOT EXISTS "t1" ( row VARCHAR NOT NULL, "f"."q" VARCHAR CONSTRAINT PK PRIMARY KEY (row)); t1、f、q 需要用双引号括起来，原因主要是大小写的问题，参考 phoenix 的 wiki。 注意： 在这里， phoenix 会修改 table 的 Descriptor，然后添加 coprocessor，所以会先 disable，在 modify，最后 enable 表。 接下来就可以查询了： 1234567891011121314151617180: jdbc:phoenix:localhost&gt; select * from "t1";+------------+------------+| ROW | q |+------------+------------+| row1 | 1 || row2 | 2 || row3 | 3 || row4 | 4 || row5 | 5 |+------------+------------+5 rows selected (0.101 seconds)0: jdbc:phoenix:localhost&gt; select count(1) from "t1";+------------+| COUNT(1) |+------------+| 5 |+------------+1 row selected (0.068 seconds) 4. 总结这篇文章主要是介绍了什么是 Phoenix 、如何安装以及他的一些特性，然后介绍了他的使用方法，主要包括命令行使用、加载数据以及如何映射存在的 HBase 表，通过该篇文章对 Phoenix 有了一个初步的认识。]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
        <tag>phoenix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[采集日志到Hive]]></title>
    <url>%2F2014%2F07%2F25%2Fcollect-log-to-hive%2F</url>
    <content type="text"><![CDATA[我们现在的需求是需要将线上的日志以小时为单位采集并存储到 hive 数据库中，方便以后使用 mapreduce 或者 impala 做数据分析。为了实现这个目标调研了 flume 如何采集数据到 hive，其他的日志采集框架尚未做调研。 日志压缩flume中有个 HdfsSink 组件，其可以压缩日志进行保存，故首先想到我们的日志应该以压缩的方式进行保存，遂选择了 lzo 的压缩格式，HdfsSink 的配置如下: 1234567891011agent-1.sinks.sink_hdfs.channel = ch-1agent-1.sinks.sink_hdfs.type = hdfsagent-1.sinks.sink_hdfs.hdfs.path = hdfs://cdh1:8020/user/root/events/%Y-%m-%dagent-1.sinks.sink_hdfs.hdfs.filePrefix = logsagent-1.sinks.sink_hdfs.hdfs.inUsePrefix = .agent-1.sinks.sink_hdfs.hdfs.rollInterval = 30agent-1.sinks.sink_hdfs.hdfs.rollSize = 0agent-1.sinks.sink_hdfs.hdfs.rollCount = 0agent-1.sinks.sink_hdfs.hdfs.batchSize = 1000agent-1.sinks.sink_hdfs.hdfs.fileType = CompressedStreamagent-1.sinks.sink_hdfs.hdfs.codeC = lzop hive 目前是支持 lzo 压缩的，但是要想在 mapreduce 中 lzo 文件可以拆分，需要通过 hadoop 的 api 进行手动创建索引： 123$ lzop a.txt$ hadoop fs -put a.txt.lzo /log/dw_srclog/sp_visit_log/ptd_ymd=20140720​$ hadoop jar /usr/lib/hadoop/lib/hadoop-lzo.jar com.hadoop.compression.lzo.LzoIndexer /log/sp_visit_log/ptd_ymd=20140720/a.txt.lzo impala 目前也是在支持 lzo 压缩格式的文件的，故采用 lzo 压缩方式存储日志文件似乎是个可行方案。 自定义分隔符Hive默认创建的表字段分隔符为：\001(ctrl-A)，也可以通过 ROW FORMAT DELIMITED FIELDS TERMINATED BY 指定其他字符，但是该语法只支持单个字符。 目前，我们的日志中几乎任何单个字符都被使用了，故没法使用单个字符作为 hive 表字段的分隔符，只能使用多个字符，例如：“|||”。使用多字符来分隔字段，则需要你自定义InputFormat来实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package org.apache.hadoop.mapred;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapred.FileSplit;import org.apache.hadoop.mapred.InputSplit;import org.apache.hadoop.mapred.JobConf;import org.apache.hadoop.mapred.LineRecordReader;import org.apache.hadoop.mapred.RecordReader;import org.apache.hadoop.mapred.Reporter;import org.apache.hadoop.mapred.TextInputFormat;public class MyDemoInputFormat extends TextInputFormat &#123; @Override public RecordReader&lt;LongWritable, Text&gt; getRecordReader( InputSplit genericSplit, JobConf job, Reporter reporter) throws IOException &#123; reporter.setStatus(genericSplit.toString()); MyDemoRecordReader reader = new MyDemoRecordReader( new LineRecordReader(job, (FileSplit) genericSplit)); return reader; &#125; public static class MyDemoRecordReader implements RecordReader&lt;LongWritable, Text&gt; &#123; LineRecordReader reader; Text text; public MyDemoRecordReader(LineRecordReader reader) &#123; this.reader = reader; text = reader.createValue(); &#125; @Override public void close() throws IOException &#123; reader.close(); &#125; @Override public LongWritable createKey() &#123; return reader.createKey(); &#125; @Override public Text createValue() &#123; return new Text(); &#125; @Override public long getPos() throws IOException &#123; return reader.getPos(); &#125; @Override public float getProgress() throws IOException &#123; return reader.getProgress(); &#125; @Override public boolean next(LongWritable key, Text value) throws IOException &#123; Text txtReplace; while (reader.next(key, text)) &#123; txtReplace = new Text(); txtReplace.set(text.toString().toLowerCase().replaceAll("\\|\\|\\|", "\001")); value.set(txtReplace.getBytes(), 0, txtReplace.getLength()); return true; &#125; return false; &#125; &#125;&#125; 这时候的建表语句是： 123456789create external table IF NOT EXISTS test(id string,name string)partitioned by (day string) STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.MyDemoInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'LOCATION '/log/dw_srclog/test'; 但是，这样建表的话，是不能识别 lzo 压缩文件的，需要去扩展 lzo 的 DeprecatedLzoTextInputFormat 类，但是如何扩展，没有找到合适方法。 所以，在自定义分隔符的情况下，想支持 lzo 压缩文件，需要另外想办法。例如，使用 SERDE 的方式： 1234567891011121314create external table IF NOT EXISTS test(id string,name string)partitioned by (day string) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe' WITH SERDEPROPERTIES ( 'input.regex' = '([^ ]*)\\|\\|\\|([^ ]*)', 'output.format.string' = '%1$s %2$s') STORED AS INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'LOCATION '/log/dw_srclog/test'; 要想使用SERDE，必须添加 hive-contrib-XXXX.jar 到 classpath，在 hive-env.sh 中添加下面代码; 1$ export HIVE_AUX_JARS_PATH=/usr/lib/hive/lib/hive-contrib-0.10.0-cdh4.7.0.jar 注意： 使用 SERDE 时，字段类型只能为 string。 这种方式建表，flume 可以将日志存储为 lzo 并且 hive 能够识别出数据，但是 impala 中却不支持 SERDE 的语法，故只能放弃该方法。 最后，只能放弃 lzo 压缩文件的想法，改为不做压缩。flume 中 HdfsSink 配置参数 hdfs.fileType 目前只有三种可选值：CompressedStream、DataStream、SequenceFile，为了保持向后兼容便于扩展，这里使用了 DataStream 的方式，不做数据压缩。 Update注意： 最后又经过测试，发现 impala 不支持 hive 的自定义文件格式，详细说明请参考：SQL Differences Between Impala and Hive 日志采集使用 flume 来采集日志，只需要在应用程序服务器上安装一个 agent 就可以监听文件或者目录的改变来搜集日志，但是实际情况你不一定有权限访问应用服务器，更多的方式是应用服务器将日志推送到一个中央的日志集中存储服务器。你只有权限去从该服务器收集数据，并且该服务器对外提供 ftp 的接口供你访问。 日志采集有 pull 和 push 的两种方式，关于两种方式的一些说明，可以参考这篇文章：大规模日志收集处理项目的技术总结。 对于当前情况而言，只能从 ftp 服务器轮询文件然后下载文件到本地，最后再将其导入到 hive 中去。以前，使用 kettle 做过这种事情，现在为了简单只是写了个 python 脚本来做这件事情，一个示例代码，请参考 https://gist.github.com/javachen/6f7d14aae138c7a284e6#file-fetch-py。 该脚本会再 crontab 中每隔5分钟执行一次。 执行该脚本会往 mongodb 中记录一些状态信息，并往 logs 目录以天为单位记录日志。 暂时没有使用 flume 的原因： 对 flume 的测试于调研程度还不够 flume 中无法对数据去重 只能停止 flume 进程，才可以升级 flume，这样会丢失数据 等日志采集实时性要求变高，以及对 flume 的熟悉程度变深之后，会考虑使用 flume。]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume-ng的原理和使用]]></title>
    <url>%2F2014%2F07%2F22%2Fflume-ng%2F</url>
    <content type="text"><![CDATA[1. 介绍Flume NG是Cloudera提供的一个分布式、可靠、可用的系统，它能够将不同数据源的海量日志数据进行高效收集、聚合、移动，最后存储到一个中心化数据存储系统中。由原来的Flume OG到现在的Flume NG，进行了架构重构，并且现在NG版本完全不兼容原来的OG版本。经过架构重构后，Flume NG更像是一个轻量的小工具，非常简单，容易适应各种方式日志收集，并支持failover和负载均衡。 Flume 使用 java 编写，其需要运行在 Java1.6 或更高版本之上。 官方网站：http://flume.apache.org/ 用户文档：http://flume.apache.org/FlumeUserGuide.html 开发文档：http://flume.apache.org/FlumeDeveloperGuide.html 2. 架构Flume的架构主要有一下几个核心概念： Event：一个数据单元，带有一个可选的消息头 Flow：Event从源点到达目的点的迁移的抽象 Client：操作位于源点处的Event，将其发送到Flume Agent Agent：一个独立的Flume进程，包含组件Source、Channel、Sink Source：用来消费传递到该组件的Event Channel：中转Event的一个临时存储，保存有Source组件传递过来的Event Sink：从Channel中读取并移除Event，将Event传递到Flow Pipeline中的下一个Agent（如果有的话） 2.1 数据流Flume 的核心是把数据从数据源收集过来，再送到目的地。为了保证输送一定成功，在送到目的地之前，会先缓存数据，待数据真正到达目的地后，删除自己缓存的数据。 Flume 传输的数据的基本单位是 Event，如果是文本文件，通常是一行记录，这也是事务的基本单位。Event 从 Source，流向 Channel，再到 Sink，本身为一个 byte 数组，并可携带 headers 信息。Event 代表着一个数据流的最小完整单元，从外部数据源来，向外部的目的地去。 Flume 运行的核心是 Agent。它是一个完整的数据收集工具，含有三个核心组件，分别是 source、channel、sink。通过这些组件，Event 可以从一个地方流向另一个地方，如下图所示。 source 可以接收外部源发送过来的数据。不同的 source，可以接受不同的数据格式。比如有目录池(spooling directory)数据源，可以监控指定文件夹中的新文件变化，如果目录中有文件产生，就会立刻读取其内容。 channel 是一个存储地，接收 source 的输出，直到有 sink 消费掉 channel 中的数据。channel 中的数据直到进入到下一个channel中或者进入终端才会被删除。当 sink 写入失败后，可以自动重启，不会造成数据丢失，因此很可靠。 sink 会消费 channel 中的数据，然后送给外部源或者其他 source。如数据可以写入到 HDFS 或者 HBase 中。 2.2 核心组件2.2.1 sourceClient端操作消费数据的来源，Flume 支持 Avro，log4j，syslog 和 http post(body为json格式)。可以让应用程序同已有的Source直接打交道，如AvroSource，SyslogTcpSource。也可以 写一个 Source，以 IPC 或 RPC 的方式接入自己的应用，Avro和 Thrift 都可以(分别有 NettyAvroRpcClient 和 ThriftRpcClient 实现了 RpcClient接口)，其中 Avro 是默认的 RPC 协议。具体代码级别的 Client 端数据接入，可以参考官方手册。 对现有程序改动最小的使用方式是使用是直接读取程序原来记录的日志文件，基本可以实现无缝接入，不需要对现有程序进行任何改动。对于直接读取文件 Source,有两种方式： ExecSource: 以运行 Linux 命令的方式，持续的输出最新的数据，如 tail -F 文件名 指令，在这种方式下，取的文件名必须是指定的。 ExecSource 可以实现对日志的实时收集，但是存在Flume不运行或者指令执行出错时，将无法收集到日志数据，无法保证日志数据的完整性。 SpoolSource: 监测配置的目录下新增的文件，并将文件中的数据读取出来。需要注意两点：拷贝到 spool 目录下的文件不可以再打开编辑；spool 目录下不可包含相应的子目录。 SpoolSource 虽然无法实现实时的收集数据，但是可以使用以分钟的方式分割文件，趋近于实时。 如果应用无法实现以分钟切割日志文件的话， 可以两种收集方式结合使用。 在实际使用的过程中，可以结合 log4j 使用，使用 log4j的时候，将 log4j 的文件分割机制设为1分钟一次，将文件拷贝到spool的监控目录。 log4j 有一个 TimeRolling 的插件，可以把 log4j 分割文件到 spool 目录。基本实现了实时的监控。Flume 在传完文件之后，将会修改文件的后缀，变为 .COMPLETED（后缀也可以在配置文件中灵活指定）。 Flume Source 支持的类型： Source类型 说明 Avro Source 支持Avro协议（实际上是Avro RPC），内置支持 Thrift Source 支持Thrift协议，内置支持 Exec Source JMS Source 从JMS系统（消息、主题）中读取数据，ActiveMQ已经测试过 Spooling Directory Source 监控指定目录内数据变更 Twitter 1% firehose Source 通过API持续下载Twitter数据，试验性质 Netcat Source 监控某个端口，将流经端口的每一个文本行数据作为Event输入 Sequence Generator Source 序列生成器数据源，生产序列数据 Syslog Sources 读取syslog数据，产生Event，支持UDP和TCP两种协议 HTTP Source 基于HTTP POST或GET方式的数据源，支持JSON、BLOB表示形式 Legacy Sources 兼容老的Flume OG中Source（0.9.x版本） 2.2.2 Channel当前有几个 channel 可供选择，分别是 Memory Channel, JDBC Channel , File Channel，Psuedo Transaction Channel。比较常见的是前三种 channel。 MemoryChannel 可以实现高速的吞吐，但是无法保证数据的完整性。 MemoryRecoverChannel 在官方文档的建议上已经建义使用FileChannel来替换。 FileChannel保证数据的完整性与一致性。在具体配置FileChannel时，建议FileChannel设置的目录和程序日志文件保存的目录设成不同的磁盘，以便提高效率。 File Channel 是一个持久化的隧道（channel），它持久化所有的事件，并将其存储到磁盘中。因此，即使 Java 虚拟机当掉，或者操作系统崩溃或重启，再或者事件没有在管道中成功地传递到下一个代理（agent），这一切都不会造成数据丢失。Memory Channel 是一个不稳定的隧道，其原因是由于它在内存中存储所有事件。如果 java 进程死掉，任何存储在内存的事件将会丢失。另外，内存的空间收到 RAM大小的限制,而 File Channel 这方面是它的优势，只要磁盘空间足够，它就可以将所有事件数据存储到磁盘上。 Flume Channel 支持的类型： Channel类型 说明 Memory Channel Event数据存储在内存中 JDBC Channel Event数据存储在持久化存储中，当前Flume Channel内置支持Derby File Channel Event数据存储在磁盘文件中 Spillable Memory Channel Event数据存储在内存中和磁盘上，当内存队列满了，会持久化到磁盘文件（当前试验性的，不建议生产环境使用） Pseudo Transaction Channel 测试用途 Custom Channel 自定义Channel实现 2.2.3 sinkSink在设置存储数据时，可以向文件系统、数据库、hadoop存数据，在日志数据较少时，可以将数据存储在文件系中，并且设定一定的时间间隔保存数据。在日志数据较多时，可以将相应的日志数据存储到Hadoop中，便于日后进行相应的数据分析。 Flume Sink支持的类型 Sink类型 说明 HDFS Sink 数据写入HDFS Logger Sink 数据写入日志文件 Avro Sink 数据被转换成Avro Event，然后发送到配置的RPC端口上 Thrift Sink 数据被转换成Thrift Event，然后发送到配置的RPC端口上 IRC Sink 数据在IRC上进行回放 File Roll Sink 存储数据到本地文件系统 Null Sink 丢弃到所有数据 HBase Sink 数据写入HBase数据库 Morphline Solr Sink 数据发送到Solr搜索服务器（集群） ElasticSearch Sink 数据发送到Elastic Search搜索服务器（集群） Kite Dataset Sink 写数据到Kite Dataset，试验性质的 Custom Sink 自定义Sink实现 更多sink的内容可以参考官方手册。 2.3 可靠性Flume 的核心是把数据从数据源收集过来，再送到目的地。为了保证输送一定成功，在送到目的地之前，会先缓存数据，待数据真正到达目的地后，删除自己缓存的数据。 Flume 使用事务性的方式保证传送Event整个过程的可靠性。Sink 必须在 Event 被存入 Channel 后，或者，已经被传达到下一站agent里，又或者，已经被存入外部数据目的地之后，才能把 Event 从 Channel 中 remove 掉。这样数据流里的 event 无论是在一个 agent 里还是多个 agent 之间流转，都能保证可靠，因为以上的事务保证了 event 会被成功存储起来。而 Channel 的多种实现在可恢复性上有不同的保证。也保证了 event 不同程度的可靠性。比如 Flume 支持在本地保存一份文件 channel 作为备份，而memory channel 将 event 存在内存 queue 里，速度快，但丢失的话无法恢复。 2.4 可恢复性3. 使用场景下面，根据官网文档，我们展示几种Flow Pipeline，各自适应于什么样的应用场景： 多个 agent 顺序连接： 可以将多个Agent顺序连接起来，将最初的数据源经过收集，存储到最终的存储系统中。这是最简单的情况，一般情况下，应该控制这种顺序连接的Agent的数量，因为数据流经的路径变长了，如果不考虑failover的话，出现故障将影响整个Flow上的Agent收集服务。 多个Agent的数据汇聚到同一个Agent: 这种情况应用的场景比较多，比如要收集Web网站的用户行为日志，Web网站为了可用性使用的负载均衡的集群模式，每个节点都产生用户行为日志，可以为每个节点都配置一个Agent来单独收集日志数据，然后多个Agent将数据最终汇聚到一个用来存储数据存储系统，如HDFS上。 多路（Multiplexing）Agent 这种模式，有两种方式，一种是用来复制（Replication），另一种是用来分流（Multiplexing）。Replication方式，可以将最前端的数据源复制多份，分别传递到多个channel中，每个channel接收到的数据都是相同的。 配置格式示例如下： 12345678910111213# List the sources, sinks and channels for the agent&lt;Agent&gt;.sources = &lt;Source1&gt;&lt;Agent&gt;.sinks = &lt;Sink1&gt; &lt;Sink2&gt;&lt;Agent&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt;# set list of channels for source (separated by space)&lt;Agent&gt;.sources.&lt;Source1&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt;# set channel for sinks&lt;Agent&gt;.sinks.&lt;Sink1&gt;.channel = &lt;Channel1&gt;&lt;Agent&gt;.sinks.&lt;Sink2&gt;.channel = &lt;Channel2&gt;&lt;Agent&gt;.sources.&lt;Source1&gt;.selector.type = replicating 上面指定了selector的type的值为replication，其他的配置没有指定，使用的Replication方式，Source1会将数据分别存储到Channel1和Channel2，这两个channel里面存储的数据是相同的，然后数据被传递到Sink1和Sink2。 Multiplexing方式，selector可以根据header的值来确定数据传递到哪一个channel，配置格式，如下所示： 123456789# Mapping for multiplexing selector&lt;Agent&gt;.sources.&lt;Source1&gt;.selector.type = multiplexing&lt;Agent&gt;.sources.&lt;Source1&gt;.selector.header = &lt;someHeader&gt;&lt;Agent&gt;.sources.&lt;Source1&gt;.selector.mapping.&lt;Value1&gt; = &lt;Channel1&gt;&lt;Agent&gt;.sources.&lt;Source1&gt;.selector.mapping.&lt;Value2&gt; = &lt;Channel1&gt; &lt;Channel2&gt;&lt;Agent&gt;.sources.&lt;Source1&gt;.selector.mapping.&lt;Value3&gt; = &lt;Channel2&gt;#...&lt;Agent&gt;.sources.&lt;Source1&gt;.selector.default = &lt;Channel2&gt; 上面selector的type的值为multiplexing，同时配置selector的header信息，还配置了多个selector的mapping的值，即header的值：如果header的值为Value1、Value2，数据从Source1路由到Channel1；如果header的值为Value2、Value3，数据从Source1路由到Channel2。 实现load balance功能 Load balancing Sink Processor能够实现load balance功能，上图Agent1是一个路由节点，负责将Channel暂存的Event均衡到对应的多个Sink组件上，而每个Sink组件分别连接到一个独立的Agent上，示例配置，如下所示： 123456a1.sinkgroups = g1a1.sinkgroups.g1.sinks = k1 k2 k3a1.sinkgroups.g1.processor.type = load_balancea1.sinkgroups.g1.processor.backoff = truea1.sinkgroups.g1.processor.selector = round_robina1.sinkgroups.g1.processor.selector.maxTimeOut=10000 实现failover能 Failover Sink Processor能够实现failover功能，具体流程类似load balance，但是内部处理机制与load balance完全不同：Failover Sink Processor维护一个优先级Sink组件列表，只要有一个Sink组件可用，Event就被传递到下一个组件。如果一个Sink能够成功处理Event，则会加入到一个Pool中，否则会被移出Pool并计算失败次数，设置一个惩罚因子，示例配置如下所示： 1234567a1.sinkgroups = g1a1.sinkgroups.g1.sinks = k1 k2 k3a1.sinkgroups.g1.processor.type = failovera1.sinkgroups.g1.processor.priority.k1 = 5a1.sinkgroups.g1.processor.priority.k2 = 7a1.sinkgroups.g1.processor.priority.k3 = 6a1.sinkgroups.g1.processor.maxpenalty = 20000 4. 安装和使用Flume 的 rpm 安装方式很简单，这里不做说明。 示例1： avro 数据源安装成功之后，在 /etc/flume/conf 目录创建f1.conf 文件，内容如下: 1234567891011121314agent-1.channels.ch-1.type = memoryagent-1.sources.avro-source1.channels = ch-1agent-1.sources.avro-source1.type = avroagent-1.sources.avro-source1.bind = 0.0.0.0agent-1.sources.avro-source1.port = 41414agent-1.sources.avro-source1.threads = 5agent-1.sinks.log-sink1.channel = ch-1agent-1.sinks.log-sink1.type = loggeragent-1.channels = ch-1agent-1.sources = avro-source1agent-1.sinks = log-sink1 关于 avro-source 配置说明，请参考 avro-source 接下来启动 agent： 1$ flume-ng agent -c /etc/flume-ng/conf -f /etc/flume-ng/conf/f1.conf -Dflume.root.logger=DEBUG,console -n agent-1 参数说明： -n 指定agent名称 -c 指定配置文件目录 -f 指定配置文件 -Dflume.root.logger=DEBUG,console 设置日志等级 下面可以启动一个 avro-client 客户端生产数据： 1$ flume-ng avro-client -c /etc/flume-ng/conf -H localhost -p 41414 -F /etc/passwd -Dflume.root.logger=DEBUG,console 示例2：spooldir 数据源在 /etc/flume/conf 目录创建 f2.conf 文件，内容如下: 1234567891011121314agent-1.channels = ch-1agent-1.sources = src-1agent-1.channels.ch-1.type = memoryagent-1.sources.src-1.type = spooldiragent-1.sources.src-1.channels = ch-1agent-1.sources.src-1.spoolDir = /root/logagent-1.sources.src-1.fileHeader = trueagent-1.sinks.log-sink1.channel = ch-1agent-1.sinks.log-sink1.type = loggeragent-1.sinks = log-sink1 关于 Spooling Directory Source 配置说明，请参考 Spooling Directory Source 接下来启动 agent： 1$ flume-ng agent -c /etc/flume-ng/conf -f /etc/flume-ng/conf/f2.conf -Dflume.root.logger=DEBUG,console -n agent-1 然后，手动拷贝一个文件到 /root/log 目录，观察日志输出以及/root/log 目录下的变化。 示例3：spooldir 数据源，写入 hdfs在 /etc/flume/conf 目录创建 f3.conf 文件，内容如下: 123456789101112131415161718192021222324252627282930agent-1.channels.ch-1.type = fileagent-1.channels.ch-1.checkpointDir= /root/checkpointagent-1.channels.ch-1.dataDirs= /root/dataagent-1.sources.src-1.type = spooldiragent-1.sources.src-1.channels = ch-1agent-1.sources.src-1.spoolDir = /root/logagent-1.sources.src-1.deletePolicy= neveragent-1.sources.src-1.fileHeader = trueagent-1.sources.src-1.interceptors =i1agent-1.sources.src-1.interceptors.i1.type = timestampagent-1.sinks.sink_hdfs.channel = ch-1agent-1.sinks.sink_hdfs.type = hdfsagent-1.sinks.sink_hdfs.hdfs.path = hdfs://cdh1:8020/user/root/events/%Y-%m-%dagent-1.sinks.sink_hdfs.hdfs.filePrefix = logsagent-1.sinks.sink_hdfs.hdfs.inUsePrefix = .agent-1.sinks.sink_hdfs.hdfs.rollInterval = 30agent-1.sinks.sink_hdfs.hdfs.rollSize = 0agent-1.sinks.sink_hdfs.hdfs.rollCount = 0agent-1.sinks.sink_hdfs.hdfs.batchSize = 1000agent-1.sinks.sink_hdfs.hdfs.writeFormat = textagent-1.sinks.sink_hdfs.hdfs.fileType = DataStream#agent-1.sinks.sink_hdfs.hdfs.fileType = CompressedStream#agent-1.sinks.sink_hdfs.hdfs.codeC = lzopagent-1.channels = ch-1agent-1.sources = src-1agent-1.sinks = sink_hdfs 关于 HDFS Sink配置说明，请参考 HDFS Sink 说明： 通过 interceptors 往 header 里添加 timestamp，这样做，可以在 hdfs.path 引用系统内部的时间变量或者主机的 hostname。 通过设置 hdfs.inUsePrefix，例如设置为 .时，hdfs 会把该文件当做隐藏文件，以避免在 mr 过程中读到这些临时文件，引起一些错误 如果使用 lzo 压缩，则需要手动创建 lzo 索引，可以通过修改 HdfsSink 的代码，通过代码创建索引 FileChannel 的目录最好是和 spooldir 的数据目录处于不同磁盘。 示例4：spooldir 数据源，写入 HBase关于 HBase Sink 配置说明，请参考 HBase Sink 5. 开发相关5.1 编译源代码从 github 下载源代码并编译： 123$ git clone git@github.com:cloudera/flume-ng.git -b cdh4-1.4.0_4.7.0$ cd flume-ng$ mvn install -DskipTests -Phadoop-2 如果提示找不到 hadoop-test 的 jar 包，则修改 pom.xml 中的版本，如改为 2.0.0-mr1-cdh4.7.0，具体版本视你使用的分支版本而定，我这里是 cdh4.7.0。 如果提示找不到 uanodeset-parser 的 jarb，则在 pom.xml 中添加下面仓库： 12345678&lt;repository&gt; &lt;id&gt;tempo-db&lt;/id&gt; &lt;url&gt;http://maven.tempo-db.com/artifactory/list/twitter/ &lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt;&lt;/repository&gt; 6. 最佳实践参考基于Flume的美团日志收集系统(一)架构和设计，列出一些最佳实践： 模块命名规则：所有的 Source 以 src 开头，所有的 Channel 以 ch 开头，所有的 Sink 以 sink 开头； 模块之间内部通信统一使用 Avro 接口； 将日志采集系统系统分为三层：Agent 层，Collector 层和 Store 层，其中 Agent 层每个机器部署一个进程，负责对单机的日志收集工作；Collector 层部署在中心服务器上，负责接收Agent层发送的日志，并且将日志根据路由规则写到相应的 Store 层中；Store 层负责提供永久或者临时的日志存储服务，或者将日志流导向其它服务器。 扩展 MemoryChannel 和 FileChannel ，提供 DualChannel 的实现，以提供高吞吐和大缓存 监控 collector HdfsSink写数据到 hdfs 的速度、FileChannel 中拥堵的 events 数量，以及写 hdfs 状态（查看是否有 .tmp 文件生成） 美团对 flume 的改进代码见 github：https://github.com/javachen/mt-flume。 7. 参考文章 Flume User Guide Apache Flume - Architecture of Flume NG Flume(NG)架构设计要点及配置实践 基于Flume的美团日志收集系统(一)架构和设计 基于Flume的美团日志收集系统(二)架构和设计]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH中配置HDFS HA]]></title>
    <url>%2F2014%2F07%2F18%2Finstall-hdfs-ha-in-cdh%2F</url>
    <content type="text"><![CDATA[最近又安装 hadoop 集群， 故尝试了一下配置 HDFS 的 HA，CDH4支持Quorum-based Storage和shared storage using NFS两种HA方案，而CDH5只支持第一种方案，即 QJM 的 HA 方案。 关于 hadoop 集群的安装部署过程你可以参考 使用yum安装CDH Hadoop集群 或者 手动安装 hadoop 集群的过程。 集群规划我一共安装了三个节点的集群，对于 HA 方案来说，三个节点准备安装如下服务： cdh1：hadoop-hdfs-namenode(primary) 、hadoop-hdfs-journalnode、hadoop-hdfs-zkfc cdh2：hadoop-hdfs-namenode(standby)、hadoop-hdfs-journalnode、hadoop-hdfs-zkfc cdh3: hadoop-hdfs-journalnode 根据上面规划，在对应节点上安装相应的服务。 安装步骤停掉集群停掉集群上所有服务。 12345$ sh /opt/cmd.sh ' for x in `ls /etc/init.d/|grep spark` ; do service $x stop ; done'​$ sh /opt/cmd.sh ' for x in `ls /etc/init.d/|grep impala` ; do service $x stop ; done'$ sh /opt/cmd.sh ' for x in `ls /etc/init.d/|grep hive` ; do service $x stop ; done'$ sh /opt/cmd.sh ' for x in `ls /etc/init.d/|grep hbase` ; do service $x stop ; done'$ sh /opt/cmd.sh ' for x in `ls /etc/init.d/|grep hadoop` ; do service $x stop ; done' cmd.sh代码内容见Hadoop集群部署权限总结一文中的/opt/shell/cmd.sh。 停止客户端程序停止服务集群的所有客户端程序，包括定时任务。 备份 hdfs 元数据a，查找本地配置的文件目录（属性名为 dfs.name.dir 或者 dfs.namenode.name.dir或者hadoop.tmp.dir ） 12345grep -C1 hadoop.tmp.dir /etc/hadoop/conf/hdfs-site.xmlgrep -C1 hadoop.tmp.dir /etc/hadoop/conf/core-site.xml#或者grep -C1 dfs.namenode.name.dir /etc/hadoop/conf/hdfs-site.xml 通过上面的命令，可以看到类似以下信息： 1234&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/var/hadoop&lt;/value&gt;&lt;/property&gt; b，对hdfs数据进行备份 12cd /var/hadoop/dfs/nametar -cvf /root/name_backup_data.tar . 安装服务在 cdh1、cdh2、cdh3 上安装 hadoop-hdfs-journalnode 123$ ssh cdh1 'yum install hadoop-hdfs-journalnode -y '$ ssh cdh2 'yum install hadoop-hdfs-journalnode -y '$ ssh cdh3 'yum install hadoop-hdfs-journalnode -y ' 在 cdh1、cdh2 上安装 hadoop-hdfs-zkfc： 12ssh cdh1 "yum install hadoop-hdfs-zkfc -y "ssh cdh2 "yum install hadoop-hdfs-zkfc -y " 修改配置文件修改/etc/hadoop/conf/core-site.xml，做如下修改： 12345678&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;cdh1:21088,cdh2:21088,cdh3:21088&lt;/value&gt;&lt;/property&gt; 修改/etc/hadoop/conf/hdfs-site.xml，删掉一些原来的 namenode 配置，增加如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;!-- hadoop HA --&gt;&lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;cdh1:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;cdh2:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;cdh1:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;cdh2:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://cdh1:8485,cdh2:8485,cdh3:8485/mycluster&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/var/hadoop/dfs/jn&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence(hdfs)&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/var/lib/hadoop-hdfs/.ssh/id_rsa&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 同步配置文件将配置文件同步到集群其他节点： 1$ sh /opt/syn.sh /etc/hadoop/conf /etc/hadoop/ 在journalnode的三个节点上创建目录： 123$ ssh cdh1 'mkdir -p /var/hadoop/dfs/jn ; chown -R hdfs:hdfs /var/hadoop/dfs/jn'$ ssh cdh2 'mkdir -p /var/hadoop/dfs/jn ; chown -R hdfs:hdfs /var/hadoop/dfs/jn'$ ssh cdh3 'mkdir -p /var/hadoop/dfs/jn ; chown -R hdfs:hdfs /var/hadoop/dfs/jn' 配置无密码登陆在两个NN上配置hdfs用户间无密码登陆： 对于 cdh1： 1234$ passwd hdfs$ su - hdfs$ ssh-keygen$ ssh-copy-id cdh2 对于 cdh2： 1234$ passwd hdfs$ su - hdfs$ ssh-keygen$ ssh-copy-id cdh1 启动journalnode启动cdh1、cdh2、cdh3上的 hadoop-hdfs-journalnode 服务 123$ ssh cdh1 'service hadoop-hdfs-journalnode start'$ ssh cdh2 'service hadoop-hdfs-journalnode start'$ ssh cdh3 'service hadoop-hdfs-journalnode start' 初始化共享存储在namenode上初始化共享存储，如果没有格式化，则先格式化： 1hdfs namenode -initializeSharedEdits 启动NameNode： 1$ service hadoop-hdfs-namenode start 同步 Standby NameNodecdh2作为 Standby NameNode，在该节点上先安装namenode服务 1$ yum install hadoop-hdfs-namenode -y 再运行： 1$ sudo -u hdfs hadoop namenode -bootstrapStandby 如果是使用了kerberos，则先获取hdfs的ticket再执行： 12$ kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/cdh1@JAVACHEM.COM$ hadoop namenode -bootstrapStandby 然后，启动 Standby NameNode： 1$ service hadoop-hdfs-namenode start 配置自动切换在两个NameNode上，即cdh1和cdh2，安装hadoop-hdfs-zkfc 12$ ssh cdh1 'yum install hadoop-hdfs-zkfc -y'$ ssh cdh2 'yum install hadoop-hdfs-zkfc -y' 在任意一个NameNode上下面命令，其会创建一个znode用于自动故障转移。 1$ hdfs zkfc -formatZK 如果你想对zookeeper的访问进行加密，则请参考 Enabling HDFS HA 中 Securing access to ZooKeeper 这一节内容。 然后再两个 NameNode 节点上启动zkfc： 12$ ssh cdh1 "service hadoop-hdfs-zkfc start"$ ssh cdh2 "service hadoop-hdfs-zkfc start" 测试分别访问 http://cdh1:50070/ 和 http://cdh2:50070/ 查看谁是 active namenode，谁是 standyby namenode。 查看某Namenode的状态： 1234567#查看cdh1状态$ sudo -u hdfs hdfs haadmin -getServiceState nn1active#查看cdh2状态$ sudo -u hdfs hdfs haadmin -getServiceState nn2standby 执行手动切换： 12$ sudo -u hdfs hdfs haadmin -failover nn1 nn2Failover to NameNode at cdh2/192.168.56.122:8020 successful 再次访问 http://cdh1:50070/ 和 http://cdh2:50070/ 查看谁是 active namenode，谁是 standyby namenode。 配置HBase HA先停掉 hbase，然后修改/etc/hbase/conf/hbase-site.xml，做如下修改： 12345&lt;!-- Configure HBase to use the HA NameNode nameservice --&gt;&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://mycluster:8020/hbase&lt;/value&gt; &lt;/property&gt; 在 zookeeper 节点上运行/usr/lib/zookeeper/bin/zkCli.sh 12$ ls /hbase/splitlogs$ rmr /hbase/splitlogs 最后启动 hbase 服务。 配置 Hive HA运行下面命令将hive的metastore的root地址的HDFS nameservice。 123456789101112$ /usr/lib/hive/bin/metatool -listFSRoot Initializing HiveMetaTool..Listing FS Roots..hdfs://cdh1:8020/user/hive/warehouse $ /usr/lib/hive/bin/metatool -updateLocation hdfs://mycluster hdfs://cdh1 -tablePropKey avro.schema.url -serdePropKey schema.url $ metatool -listFSRoot Listing FS Roots..Initializing HiveMetaTool..hdfs://mycluster:8020/user/hive/warehouse 配置 Impala不需要做什么修改，但是一定要记住 core-site.xml 中 fs.defaultFS 参数值要带上端口号，在CDH中为 8020。 配置 YARN修改yarn-site.xml： 1234567891011121314151617181920212223242526272829303132&lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;cluster1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;cdh1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;cdh2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;cdh1:8088&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;cdh2:8088&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;cdh1:2181,cdh2:2181,cdh3:2181&lt;/value&gt;&lt;/property&gt; 详细说明请参考 ResourceManager High Availability。 重启yarn，然后查看状态： 12345$ yarn rmadmin -getServiceState cdh1active$ yarn rmadmin -getServiceState cdh2standby 配置 Hue暂时未使用 配置 Llama暂时未使用]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark集群安装和使用]]></title>
    <url>%2F2014%2F07%2F01%2Fspark-install-and-usage%2F</url>
    <content type="text"><![CDATA[本文主要记录 CDH5 集群中 Spark 集群模式的安装过程配置过程并测试 Spark 的一些基本使用方法。 安装环境如下： 操作系统：CentOs 6.5 Hadoop 版本：cdh-5.4.0 Spark 版本：cdh5-1.3.0_5.4.0 关于 yum 源的配置以及 Hadoop 集群的安装，请参考 使用yum安装CDH Hadoop集群。 1. 安装首先查看 Spark 相关的包有哪些： 1234567$ yum list |grep sparkspark-core.noarch 1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6spark-history-server.noarch 1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6spark-master.noarch 1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6spark-python.noarch 1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6spark-worker.noarch 1.3.0+cdh5.4.0+24-1.cdh5.4.0.p0.52.el6hue-spark.x86_64 3.7.0+cdh5.4.0+1145-1.cdh5.4.0.p0.58.el6 以上包作用如下： spark-core: spark 核心功能 spark-worker: spark-worker 初始化脚本 spark-master: spark-master 初始化脚本 spark-python: spark 的 Python 客户端 hue-spark: spark 和 hue 集成包 spark-history-server 在已经存在的 Hadoop 集群中，选择一个节点来安装 Spark Master，其余节点安装 Spark worker ，例如：在 cdh1 上安装 master，在 cdh1、cdh2、cdh3 上安装 worker： 12345# 在 cdh1 节点上运行$ sudo yum install spark-core spark-master spark-worker spark-python spark-history-server -y# 在 cdh1、cdh2、cdh3 上运行$ sudo yum install spark-core spark-worker spark-python -y 安装成功后，我的集群各节点部署如下： 123cdh1节点: spark-master、spark-worker、spark-history-servercdh2节点: spark-worker cdh3节点: spark-worker 2. 配置2.1 修改配置文件设置环境变量，在 .bashrc 或者 /etc/profile 中加入下面一行，并使其生效： 1export SPARK_HOME=/usr/lib/spark 可以修改配置文件 /etc/spark/conf/spark-env.sh，其内容如下，你可以根据需要做一些修改，例如，修改 master 的主机名称为cdh1。 12# 设置 master 主机名称export STANDALONE_SPARK_MASTER_HOST=cdh1 设置 shuffle 和 RDD 数据存储路径，该值默认为/tmp。使用默认值，可能会出现No space left on device的异常，建议修改为空间较大的分区中的一个目录。 1export SPARK_LOCAL_DIRS=/data/spark 如果你和我一样使用的是虚拟机运行 spark，则你可能需要修改 spark 进程使用的 jvm 大小（关于 jvm 大小设置的相关逻辑见 /usr/lib/spark/bin/spark-class）： 1export SPARK_DAEMON_MEMORY=256m 更多spark相关的配置参数，请参考 Spark Configuration。 2.2 配置 Spark History Server 在运行Spark应用程序的时候，driver会提供一个webUI给出应用程序的运行信息，但是该webUI随着应用程序的完成而关闭端口，也就是说，Spark应用程序运行完后，将无法查看应用程序的历史记录。Spark history server就是为了应对这种情况而产生的，通过配置，Spark应用程序在运行完应用程序之后，将应用程序的运行信息写入指定目录，而Spark history server可以将这些运行信息装载并以web的方式供用户浏览。 创建 /etc/spark/conf/spark-defaults.conf： 1cp /etc/spark/conf/spark-defaults.conf.template /etc/spark/conf/spark-defaults.conf 添加下面配置： 1234spark.master=spark://cdh1:7077spark.eventLog.dir=/user/spark/applicationHistoryspark.eventLog.enabled=truespark.yarn.historyServer.address=cdh1:18082 如果你是在hdfs上运行Spark，则执行下面命令创建/user/spark/applicationHistory目录： 1234$ sudo -u hdfs hadoop fs -mkdir /user/spark$ sudo -u hdfs hadoop fs -mkdir /user/spark/applicationHistory$ sudo -u hdfs hadoop fs -chown -R spark:spark /user/spark$ sudo -u hdfs hadoop fs -chmod 1777 /user/spark/applicationHistory 设置 spark.history.fs.logDirectory 参数： 1export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS -Dspark.history.fs.logDirectory=/tmp/spark -Dspark.history.ui.port=18082" 创建 /tmp/spark 目录： 12$ mkdir -p /tmp/spark$ chown spark:spark /tmp/spark 如果集群配置了 kerberos ，则添加下面配置： 12HOSTNAME=`hostname -f`export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS -Dspark.history.kerberos.enabled=true -Dspark.history.kerberos.principal=spark/$&#123;HOSTNAME&#125;@LASHOU.COM -Dspark.history.kerberos.keytab=/etc/spark/conf/spark.keytab -Dspark.history.ui.acls.enable=true" 2.3 和Hive集成Spark和hive集成，最好是将hive的配置文件链接到Spark的配置文件目录： 1$ ln -s /etc/hive/conf/hive-site.xml /etc/spark/conf/hive-site.xml 2.4 同步配置文件修改完 cdh1 节点上的配置文件之后，需要同步到其他节点： 12scp -r /etc/spark/conf cdh2:/etc/sparkscp -r /etc/spark/conf cdh3:/etc/spark 3. 启动和停止3.1 使用系统服务管理集群启动脚本： 123456789# 在 cdh1 节点上运行$ sudo service spark-master start# 在 cdh1 节点上运行，如果 hadoop 集群配置了 kerberos，则运行之前需要先获取 spark 用户的凭证# kinit -k -t /etc/spark/conf/spark.keytab spark/cdh1@JAVACHEN.COM$ sudo service spark-history-server start# 在cdh2、cdh3 节点上运行$ sudo service spark-worker start 停止脚本： 123$ sudo service spark-master stop$ sudo service spark-worker stop$ sudo service spark-history-server stop 当然，你还可以设置开机启动： 123$ sudo chkconfig spark-master on$ sudo chkconfig spark-worker on$ sudo chkconfig spark-history-server on 3.2 使用 Spark 自带脚本管理集群另外，你也可以使用 Spark 自带的脚本来启动和停止，这些脚本在 /usr/lib/spark/sbin 目录下： 1234$ ls /usr/lib/spark/sbinslaves.sh spark-daemons.sh start-master.sh stop-all.shspark-config.sh spark-executor start-slave.sh stop-master.shspark-daemon.sh start-all.sh start-slaves.sh stop-slaves.sh 在master节点修改 /etc/spark/conf/slaves 文件添加worker节点的主机名称，并且还需要在master和worker节点之间配置无密码登陆。 123# A Spark Worker will be started on each of the machines listed below.cdh2cdh3 然后，你也可以通过下面脚本启动 master 和 worker： 123$ cd /usr/lib/spark/sbin$ ./start-master.sh$ ./start-slaves.sh 当然，你也可以通过spark-class脚本来启动，例如，下面脚本以standalone模式启动worker： 1$ ./bin/spark-class org.apache.spark.deploy.worker.Worker spark://cdh1:18080 3.3 访问web界面你可以通过 http://cdh1:18080/ 访问 spark master 的 web 界面。 访问Spark History Server页面：http://cdh1:18082/。 注意：我这里使用的是CDH版本的 Spark，Spark master UI的端口为18080，不是 Apache Spark 的 8080 端口。CDH发行版中Spark使用的端口列表如下： 7077 – Default Master RPC port 7078 – Default Worker RPC port 18080 – Default Master web UI port 18081 – Default Worker web UI port 18080 – Default HistoryServer web UI port 4. 测试Spark可以以本地模式运行，也支持三种集群管理模式： Standalone – Spark原生的资源管理，由Master负责资源的分配。 Apache Mesos – 运行在Mesos之上，由Mesos进行资源调度 Hadoop YARN – 运行在Yarn之上，由Yarn进行资源调度。 另外 Spark 的 EC2 launch scripts 可以帮助你容易地在Amazon EC2上启动standalone cluster. 在集群不是特别大，并且没有 mapReduce 和 Spark 同时运行的需求的情况下，用 Standalone 模式效率最高。 Spark可以在应用间（通过集群管理器）和应用中（如果一个 SparkContext 中有多项计算任务）进行资源调度。 4.1 Standalone 模式该模式中，资源调度是Spark框架自己实现的，其节点类型分为Master和Worker节点，其中Driver节点运行在Master节点中，并且有常驻内存的Master进程守护，Worker节点上常驻Worker守护进程，负责与Master通信。 Standalone 模式是Master-Slaves架构的集群模式，Master存在着单点故障问题，目前，Spark提供了两种解决办法：基于文件系统的故障恢复模式，基于Zookeeper的HA方式。 Standalone 模式需要在每一个节点部署Spark应用，并按照实际情况配置故障恢复模式。 你可以使用交互式命令spark-shell、pyspark或者spark-submit script连接到集群，下面以wordcount程序为例： 12345$ spark-shell --master spark://cdh1:7077scala&gt; val file = sc.textFile("hdfs://cdh1:8020/tmp/test.txt")scala&gt; val counts = file.flatMap(line =&gt; line.split(" ")).map(word =&gt; (word, 1)).reduceByKey(_ + _)scala&gt; counts.count()scala&gt; counts.saveAsTextFile("hdfs://cdh1:8020/tmp/output") 如果运行成功，可以打开浏览器访问 http://cdh1:4040 查看应用运行情况。 运行过程中，可能会出现下面的异常： 1234567891011121314/10/24 14:51:40 WARN hdfs.BlockReaderLocal: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.14/10/24 14:51:40 ERROR lzo.GPLNativeCodeLoader: Could not load native gpl libraryjava.lang.UnsatisfiedLinkError: no gplcompression in java.library.path at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1738) at java.lang.Runtime.loadLibrary0(Runtime.java:823) at java.lang.System.loadLibrary(System.java:1028) at com.hadoop.compression.lzo.GPLNativeCodeLoader.&lt;clinit&gt;(GPLNativeCodeLoader.java:32) at com.hadoop.compression.lzo.LzoCodec.&lt;clinit&gt;(LzoCodec.java:71) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:249) at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:1836) at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1801) at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:128) 解决方法可以参考 Spark连接Hadoop读取HDFS问题小结 这篇文章，执行以下命令，然后重启服务即可： 123cp /usr/lib/hadoop/lib/native/libgplcompression.so $JAVA_HOME/jre/lib/amd64/cp /usr/lib/hadoop/lib/native/libhadoop.so $JAVA_HOME/jre/lib/amd64/cp /usr/lib/hadoop/lib/native/libsnappy.so $JAVA_HOME/jre/lib/amd64/ 使用 spark-submit 以 Standalone 模式运行 SparkPi 程序的命令如下： 1$ spark-submit --class org.apache.spark.examples.SparkPi --master spark://cdh1:7077 /usr/lib/spark/lib/spark-examples-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar 10 需要说明的是：Standalone mode does not support talking to a kerberized HDFS，如果你以 spark-shell --master spark://cdh1:7077 方式访问安装有 kerberos 的 HDFS 集群上访问数据时，会出现下面异常: 123456715/04/02 11:58:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from poolorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, bj03-bi-pro-hdpnamenn): java.io.IOException: Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]; Host Details : local host is: &quot;cdh1/192.168.56.121&quot;; destination host is: &quot;192.168.56.121&quot;:8020; org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764) org.apache.hadoop.ipc.Client.call(Client.java:1415) org.apache.hadoop.ipc.Client.call(Client.java:1364) org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206) com.sun.proxy.$Proxy17.getBlockLocations(Unknown Source) 4.2 Spark On Mesos 模式参考 http://dongxicheng.org/framework-on-yarn/apache-spark-comparing-three-deploying-ways/。 4.3 Spark on Yarn 模式Spark on Yarn 模式同样也支持两种在 Yarn 上启动 Spark 的方式，一种是 cluster 模式，Spark driver 在 Yarn 的 application master 进程中运行，客户端在应用初始化完成之后就会退出；一种是 client 模式，Spark driver 运行在客户端进程中。Spark on Yarn 模式是可以访问配置有 kerberos 的 HDFS 文件的。 CDH Spark中，以 cluster 模式启动，命令如下： 1$ spark-submit --class path.to.your.Class --deploy-mode cluster --master yarn [options] &lt;app jar&gt; [app options] CDH Spark中，以 client 模式启动，命令如下： 1$ spark-submit --class path.to.your.Class --deploy-mode client --master yarn [options] &lt;app jar&gt; [app options] 以SparkPi程序为例： 12345678910$ spark-submit --class org.apache.spark.examples.SparkPi \ --deploy-mode cluster \ --master yarn \ --num-executors 3 \ --driver-memory 4g \ --executor-memory 2g \ --executor-cores 1 \ --queue thequeue \ /usr/lib/spark/lib/spark-examples-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar \ 10 另外，运行在 YARN 集群之上的时候，可以手动把 spark-assembly 相关的 jar 包拷贝到 hdfs 上去，然后设置 SPARK_JAR 环境变量： 1234$ hdfs dfs -mkdir -p /user/spark/share/lib$ hdfs dfs -put $SPARK_HOME/lib/spark-assembly.jar /user/spark/share/lib/spark-assembly.jar$ SPARK_JAR=hdfs://&lt;nn&gt;:&lt;port&gt;/user/spark/share/lib/spark-assembly.jar 5. Spark-SQLSpark 安装包中包括了 Spark-SQL ，运行 spark-sql 命令，在 cdh5.2 中会出现下面异常： 12345678910111213141516$ cd /usr/lib/spark/bin$ ./spark-sqljava.lang.ClassNotFoundException: org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:306) at java.lang.ClassLoader.loadClass(ClassLoader.java:247) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:247) at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:319) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Failed to load Spark SQL CLI main class org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.You need to build Spark with -Phive. 在 cdh5.4 中会出现下面异常： 123456789Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.cli.CliDriver at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) ... 18 more 从上可以知道 Spark-SQL 编译时没有集成 Hive，故需要重新编译 spark 源代码。 编译 Spark-SQL以下内容参考 编译Spark源代码。 下载cdh5-1.3.0_5.4.0分支的代码： 123$ git clone git@github.com:cloudera/spark.git$ cd spark$ git checkout -b origin/cdh5-1.3.0_5.4.0 使用maven 编译，先修改根目录下的 pom.xml，添加一行 &lt;module&gt;sql/hive-thriftserver&lt;/module&gt;： 123456789101112131415161718192021&lt;modules&gt; &lt;module&gt;core&lt;/module&gt; &lt;module&gt;bagel&lt;/module&gt; &lt;module&gt;graphx&lt;/module&gt; &lt;module&gt;mllib&lt;/module&gt; &lt;module&gt;tools&lt;/module&gt; &lt;module&gt;streaming&lt;/module&gt; &lt;module&gt;sql/catalyst&lt;/module&gt; &lt;module&gt;sql/core&lt;/module&gt; &lt;module&gt;sql/hive&lt;/module&gt; &lt;module&gt;sql/hive-thriftserver&lt;/module&gt; &lt;!--添加的一行--&gt; &lt;module&gt;repl&lt;/module&gt; &lt;module&gt;assembly&lt;/module&gt; &lt;module&gt;external/twitter&lt;/module&gt; &lt;module&gt;external/kafka&lt;/module&gt; &lt;module&gt;external/flume&lt;/module&gt; &lt;module&gt;external/flume-sink&lt;/module&gt; &lt;module&gt;external/zeromq&lt;/module&gt; &lt;module&gt;external/mqtt&lt;/module&gt; &lt;module&gt;examples&lt;/module&gt; &lt;/modules&gt; 然后运行： 12$ export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"$ mvn -Pyarn -Dhadoop.version=2.6.0-cdh5.4.0 -Phive -Phive-thriftserver -DskipTests clean package 如果编译成功之后， 会在 assembly/target/scala-2.10 目录下生成：spark-assembly-1.3.0-cdh5.4.0.jar，在 examples/target/scala-2.10 目录下生成：spark-examples-1.3.0-cdh5.4.0.jar，然后将 spark-assembly-1.3.0-cdh5.4.0.jar 拷贝到 /usr/lib/spark/lib 目录，然后再来运行 spark-sql。 但是，经测试 cdh5.4.0 版本中的 spark 的 sql/hive-thriftserver 模块存在编译错误，最后无法编译成功，故需要等到 cloudera 官方更新源代码或者等待下一个 cdh 版本集成 spark-sql。 虽然 spark-sql 命令用不了，但是我们可以在 spark-shell 中使用 SQLContext 来运行 sql 语句，限于篇幅，这里不做介绍，你可以参考 http://www.infoobjects.com/spark-sql-schemardd-programmatically-specifying-schema/。 6. 总结本文主要介绍了 CDH5 集群中 Spark 的安装过程以及三种集群运行模式： Standalone – spark-shell --master spark://host:port Apache Mesos – spark-shell --master mesos://host:port Hadoop YARN – spark-shell --master yarn 如果以本地模式运行，则为 spark-shell --master local。 关于 Spark 的更多介绍可以参考官网或者一些中文翻译的文章。 7. 参考文章 Spark Standalone Mode Spark连接Hadoop读取HDFS问题小结 Apache Spark探秘：三种分布式部署方式比较]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>yarn</tag>
        <tag>spark</tag>
        <tag>mesos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase中的一些注意事项]]></title>
    <url>%2F2014%2F06%2F26%2Fsome-tips-about-hbase%2F</url>
    <content type="text"><![CDATA[1. 安装集群前 配置SSH无密码登陆 DNS。HBase使用本地 hostname 才获得IP地址，正反向的DNS都是可以的。你还可以设置 hbase.regionserver.dns.interface 来指定主接口，设置 hbase.regionserver.dns.nameserver 来指定nameserver，而不使用系统带的 安装NTP服务，并配置和检查crontab是否生效 操作系统调优，包括最大文件句柄，nproc hard 和 soft limits等等 conf/hdfs-site.xml里面的 dfs.datanode.max.xcievers 参数，至少要有4096 2. HDFS客户端配置如果你希望Hadoop集群上做HDFS 客户端配置 ，例如你的HDFS客户端的配置和服务端的不一样。按照如下的方法配置，HBase就能看到你的配置信息: 在hbase-env.sh里将 HBASE_CLASSPATH 环境变量加上 HADOOP_CONF_DIR 。 在${HBASE_HOME}/conf 下面加一个 hdfs-site.xml (或者 hadoop-site.xml) ，最好是软连接 如果你的HDFS客户端的配置不多的话，你可以把这些加到 hbase-site.xml上面. 例如HDFS的配置 dfs.replication 你希望复制5份，而不是默认的3份。如果你不照上面的做的话，Hbase只会复制3份。 3. 一些配置参数以下参数来自apache的hbase版本，如果你使用的其他厂商的hbase，有可能默认值不一样。 zookeeper.session.timeout：这个默认值是3分钟。这意味着一旦一个server宕掉了，Master至少需要3分钟才能察觉到宕机，开始恢复。你可能希望将这个超时调短，这样Master就能更快的察觉到了。在你调这个值之前，你需要确认你的JVM的GC参数，否则一个长时间的GC操作就可能导致超时。 hbase.regionserver.handler.count：这个设置决定了处理用户请求的线程数量。默认是10，这个值设的比较小，主要是为了预防用户用一个比较大的写缓冲，然后还有很多客户端并发，这样region servers会垮掉。有经验的做法是，当请求内容很大(上MB，如大puts, 使用缓存的scans)的时候，把这个值放低。请求内容较小的时候(gets, 小puts, ICVs, deletes)，把这个值放大。把这个值放大的危险之处在于，把所有的Put操作缓冲意味着对内存有很大的压力，甚至会导致OutOfMemory.一个运行在内存不足的机器的RegionServer会频繁的触发GC操作，渐渐就能感受到停顿。一段时间后，集群也会受到影响，因为所有的指向这个region的请求都会变慢。这样就会拖累集群，加剧了这个问题。 hbase.client.keyvalue.maxsize：一个KeyValue实例的最大size。如果设置为0或者更小，就会禁用这个检查。默认10MB。 hbase.regionserver.lease.period：户端租用HRegion server 期限，即超时阀值。单位是毫秒。默认情况下，客户端必须在这个时间内发一条信息，否则视为死掉。默认值为60000。 hbase.regionserver.msginterval：RegionServer 发消息给 Master 时间间隔，单位是毫秒，默认: 3000 hbase.regionserver.optionallogflushinterval：将Hlog同步到HDFS的间隔。如果Hlog没有积累到一定的数量，到了时间，也会触发同步。默认是1秒，单位毫秒。 hbase.regionserver.logroll.period：提交commit log的间隔，不管有没有写足够的值。默认: 3600000 hbase.regionserver.thread.splitcompactcheckfrequency：region server 多久执行一次split/compaction 检查。默认: 20000 hbase.balancer.period：Master执行region balancer的间隔。默认: 300000 hbase.hregion.memstore.block.multiplier：如果memstore有hbase.hregion.memstore.block.multiplier倍数的hbase.hregion.flush.size的大小，就会阻塞update操作。这是为了预防在update高峰期会导致的失控。如果不设上界，flush的时候会花很长的时间来合并或者分割，最坏的情况就是引发out of memory异常。默认: 2 hbase.hstore.compactionThreshold：当一个HStore含有多于这个值的HStoreFiles(每一个memstore flush产生一个HStoreFile)的时候，会执行一个合并操作，把这HStoreFiles写成一个。这个值越大，需要合并的时间就越长。默认: 3 hbase.hstore.blockingStoreFiles：当一个HStore含有多于这个值的HStoreFiles(每一个memstore flush产生一个HStoreFile)的时候，会执行一个合并操作，update会阻塞直到合并完成，直到超过了hbase.hstore.blockingWaitTime的值。默认: 7 4. Shell 技巧irbrc可以在你自己的Home目录下创建一个.irbrc文件，在这个文件里加入自定义的命令。有一个有用的命令就是记录命令历史，这样你就可以把你的命令保存起来。 1234$ more .irbrcrequire &apos;irb/ext/save-history&apos;IRB.conf[:SAVE_HISTORY] = 100IRB.conf[:HISTORY_FILE] = &quot;#&#123;ENV[&apos;HOME&apos;]&#125;/.irb-save-history&quot; Shell 切换成debug 模式你可以将shell切换成debug模式。这样可以看到更多的信息。例如可以看到命令异常的stack trace: 1hbase&gt; debug 想要在shell中看到 DEBUG 级别的 logging ，可以在启动的时候加上 -d 参数. 1$ ./bin/hbase shell -d 5. HBase 和 MapReduce当 MapReduce job的HBase table 使用TableInputFormat为数据源格式的时候,他的splitter会给这个table的每个region一个map。因此，如果一个table有100个region，就有100个map-tasks，不论需要scan多少个column families 。 通常建议关掉针对HBase的MapReduce job的预测执行(speculative execution)功能。这个功能也可以用每个Job的配置来完成。对于整个集群，使用预测执行意味着双倍的运算量。这可不是你所希望的。 6.HBase 的 Schema 设计flush和compaction操作是针对一个Region。 Compaction操作现在是根据一个column family下的全部文件的数量触发的，而不是根据文件大小触发的。当很多的column families在flush和compaction时,会造成很多没用的I/O负载(要想解决这个问题，需要将flush和compaction操作只针对一个column family) 行的版本的数量是HColumnDescriptor设置的，每个column family可以单独设置，默认是3。 7. 性能调优1、长时间GC停顿 Hbase中常见的两种stop-the-world的GC操作： 一种是CMS失败的模式 另一种是老一代的堆碎片导致的 要想定位第一种，只要将CMS执行的时间提前就可以了，加入 -XX:CMSInitiatingOccupancyFraction 参数，把值调低。可以先从60%和70%开始(这个值调的越低，触发的GC次数就越多，消耗的CPU时间就越长)。要想定位第二种错误，Todd加入了一个实验性的功能，将你的Configuration中的 hbase.hregion.memstore.mslab.enabled 设置为true。 2、使用压缩 3、设置合理的版本 4、控制split和compaction 8. 需要理解一些过程8.1 什么时候做split？答：根据拆分策略算法来定，具体过程见：HBase笔记：Region拆分策略 8.2 什么时候做compaction？答：当有3个小文件时候，会进行合并小文件 8.3 memstore什么时候flush，什么时候阻塞写？答：memstore满了64M就会flush，当memstore大小达到128M时候，聚会阻塞update，进行flush。 8.4 HLog什么时候会阻塞写？]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce任务运行过程]]></title>
    <url>%2F2014%2F06%2F24%2Fthe-running-process-of-mapreduce-job%2F</url>
    <content type="text"><![CDATA[下图是MapReduce任务运行过程的一个图： Map-Reduce的处理过程主要涉及以下四个部分： 客户端Client：用于提交Map-reduce任务job JobTracker：协调整个job的运行，其为一个Java进程，其main class为JobTracker TaskTracker：运行此job的task，处理input split，其为一个Java进程，其main class为TaskTracker HDFS：hadoop分布式文件系统，用于在各个进程间共享Job相关的文件 上图中主要包括以下过程： 提交作业 作业初始化 任务分配 执行任务 进度和状态更新 完成作业 1. 提交作业运行Shell命令使用hadoop提供的命令行或者通过编程接口提交任务，命令行方式如下： 12345678$ HADOOP_HOME/bin/hadoop jar job.jar \ -D mapred.job.name="task-job" \ -D mapred.reduce.tasks=3 \ -files=blacklist.txt,whitelist.xml \ -libjars=aaa.jar \ -archives=bbb.zip \ -input /test/input \ -output /test/output 当用户按上述命令格式提交作业后，命令行脚本会调用JobClient.runJob()方法提交作业 作业文件上传JobClient将作业提交到JobTracker节点上之前，需要作业写初始化工作。初始化工作由 JobClient.submitJobInternal(job) 实现，这些初始化包括获取作业的jobId、创建HDFS目录、上传作业以及生成所有的InputSplit分片的相关信息等。 MapReduce的作业文件的上传和下载都是由DistributedCache透明完成的，它是Hadoop专门开发的数据分发工具。 JobClient上传文件时可以修改文件副本数（通过参数 mapred.submit.replication 指定，默认值为10），这样的话可以分摊负载以避免产生访问热点。 产生InputSplit文件作业提交后，JobClient会调用InputFormat的getSplits()方法生成相关的split分片信息，该信息包括InputSplit元数据信息和原始的InputSplit信息，其中元数据信息被JobTracker使用，第二部分在Map Task初始化时由Mapper使用来获取自己要处理的数据，这两部分数据被保存到job.split文件和job.splitmetainfo文件中。 作业提交到JobTracker调用JobTracker的submitJob()方法将作业提交。在这一阶段会依次进行如下操作： 1）、为作业创建JobInProgress对象。JobTracker会为用户提交的每一个作业创建一个JobInProgress对象，这个对象维护了作业的运行时信息，主要用于跟踪正在运行的作业的状态和进度； 2）、检查用户是否具有指定队列的作业提交权限。Hadoop以队列为单位来管理作业和资源，每个队列分配有一定亮的资源，管理严可以为每个队列指定哪些用户有权限提交作业； 3）、检查作业配置的内存使用量是否合理。用户在提交作业时，可已分别通过参数 mapred.job.map.memory.mb 和mapred.job.reduce.memory.mb 指定Map Task和Reduce Task的内存使用量，而管理员可以给集群中的Map Task和Reduce Task分别设置中的内存使用量，一旦用户配置的内存使用量超过总的内存限制，作业就会提交失败； 4）、JobTracker收到提交的作业后，并不会马上对其进行初始化，而是会交给TaskScheduler调度器，由他按照一定的策略对作业做初始化操作。 JobTracker采用了观察者模式将“提交新作业”这一事件告诉TaskScheduler 提交任务后，runJob每隔一秒钟轮询一次job的进度，将进度返回到命令行，直到任务运行完毕。 2. 作业初始化调度器调用JobTracker.initJob()方法来对新作业做初始化的。Hadoop将每个作业分节成4中类型的任务：Setup Task，Map Task，Reduce Task和Cleanup Task，它们的运行时信息由TaskInProgress维护，因此，从某个方面将，创建这些任务就是创建TaskInProgress对象。 Setup Task。作业初始化标志性任务，它进行一些很简单的作业初始化工作。该类型任务又分为Map Setup Task和Reduce Setup Task两种，并且只能运行一次。 Map Task。Map阶段的数据处理任务。 其数目及对应的处理数据分片由应用程序中的InputFormat中间确定。 Reduce Task。Reduce阶段的处理数据的任务。其数目可以由用户通过参数 mapred.reduce.tasks 指定。Hadoop刚开始的时候只会调度Map Task任务，直到Map Task完成数目达到由参数 mapred.reduce.slowstart.completed.maps指定的百分比（默认值为0.05，即百分之5）后，才开始调度Reduce Task。 Cleanup Task。作业结束的标志性任务，主要是做一些作业清理的工作，比如删除作业在运行中产生的一些零食目录和数据等信息。 说明：可以通过参数 mapred.committer.job.setup.cleanup.needed 配置是否为作业创建Setup/Cleanup Task，以避免他们拖慢作业执行进度且降低作业的可靠性。 3. 任务分配Tasktracker 和 JobTracker 通过心跳通信分配一个任务 TaskTracker 定期发送心跳，告知 JobTracker, tasktracker 是否还存活，并充当两者之间的消息通道。 TaskTracker 主动向 JobTracker 询问是否有作业。若自己有空闲的 solt,就可在心跳阶段得到 JobTracker 发送过来的 Map 任务或 Reduce 任务。对于 map 任务和 task 任务，TaskTracker 有固定数量的任务槽，准确数量由 tasktracker 核的个数核内存的大小来确定。默认调度器在处理 reduce 任务槽之前，会填充满空闲的 map 任务槽，因此，如果 tasktracker 至少有一个空闲的 map 任务槽，tasktracker 会为它选择一个 map 任务，否则选择一个 reduce 任务。选择 map 任务时，jobTracker 会考虑数据本地化（任务运行在输入分片所在的节点），而 reduce 任务不考虑数据本地化。任务还可能是机架本地化。 4. 执行任务tasktracker 执行任务大致步骤： 被分配到一个任务后，从共享文件中把作业的jar复制到本地，并将程序执行需要的全部文件（配置信息、数据分片）复制到本地 为任务新建一个本地工作目录 内部类TaskRunner实例启动一个新的jvm运行任务 5. 进度和状态更新 状态包括：作业或认为的状态（成功，失败，运行中）、map 和 reduce 的进度、作业计数器的值、状态消息或描述 task 运行时，将自己的状态发送给 TaskTracker,由 TaskTracker 心跳机制向 JobTracker 汇报 状态进度由计数器实现 6. 完成作业当JobTracker获得最后一个task的运行成功的报告后，将job得状态改为成功。 当JobClient从JobTracker轮询的时候，发现此job已经成功结束，则向用户打印消息，从runJob函数中返回。 7. 总结以上过程通过时序图来表达过程如下： 8. 参考资料 [1] Hadoop MapReduce 工作机制]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce任务参数调优]]></title>
    <url>%2F2014%2F06%2F24%2Ftuning-in-mapreduce%2F</url>
    <content type="text"><![CDATA[本文主要记录Hadoop 2.x版本中MapReduce参数调优，不涉及Yarn的调优。 Hadoop的默认配置文件（以cdh5.0.1为例）： core-default.xml hdfs-default.xml mapred-default.xml 说明： 在hadoop2中有些参数名称过时了，例如原来的mapred.reduce.tasks改名为mapreduce.job.reduces了，当然，这两个参数你都可以使用，只是第一个参数过时了。 1. 操作系统调优 增大打开文件数据和网络连接上限，调整内核参数net.core.somaxconn，提高读写速度和网络带宽使用率 适当调整epoll的文件描述符上限，提高Hadoop RPC并发 关闭swap。如果进程内存不足，系统会将内存中的部分数据暂时写入磁盘，当需要时再将磁盘上的数据动态换置到内存中，这样会降低进程执行效率 增加预读缓存区大小。预读可以减少磁盘寻道次数和I/O等待时间 设置openfile 2. Hdfs参数调优2.1 core-default.xml：hadoop.tmp.dir： 默认值： /tmp 说明： 尽量手动配置这个选项，否则的话都默认存在了里系统的默认临时文件/tmp里。并且手动配置的时候，如果服务器是多磁盘的，每个磁盘都设置一个临时文件目录，这样便于mapreduce或者hdfs等使用的时候提高磁盘IO效率。 fs.trash.interval： 默认值： 0 说明： 这个是开启hdfs文件删除自动转移到垃圾箱的选项，值为垃圾箱文件清除时间。一般开启这个会比较好，以防错误删除重要文件。单位是分钟。 io.file.buffer.size： 默认值：4096 说明：SequenceFiles在读写中可以使用的缓存大小，可减少 I/O 次数。在大型的 Hadoop cluster，建议可设定为 65536 到 131072。 2.2 hdfs-default.xml：dfs.blocksize： 默认值：134217728 说明： 这个就是hdfs里一个文件块的大小了，CDH5中默认128M。太大的话会有较少map同时计算，太小的话也浪费可用map个数资源，而且文件太小namenode就浪费内存多。根据需要进行设置。 dfs.namenode.handler.count： 默认值：10 说明：设定 namenode server threads 的数量，这些 threads 會用 RPC 跟其他的 datanodes 沟通。当 datanodes 数量太多时会发現很容易出現 RPC timeout，解決方法是提升网络速度或提高这个值，但要注意的是 thread 数量多也表示 namenode 消耗的内存也随着增加 3. MapReduce参数调优包括以下节点： 合理设置槽位数目 调整心跳配置 磁盘块配置 设置RPC和线程数目 启用批量任务调度 3.1 mapred-default.xml：mapred.reduce.tasks（mapreduce.job.reduces）： 默认值：1 说明：默认启动的reduce数。通过该参数可以手动修改reduce的个数。 mapreduce.task.io.sort.factor： 默认值：10 说明：Reduce Task中合并小文件时，一次合并的文件数据，每次合并的时候选择最小的前10进行合并。 mapreduce.task.io.sort.mb： 默认值：100 说明： Map Task缓冲区所占内存大小。 mapred.child.java.opts： 默认值：-Xmx200m 说明：jvm启动的子线程可以使用的最大内存。建议值-XX:-UseGCOverheadLimit -Xms512m -Xmx2048m -verbose:gc -Xloggc:/tmp/@taskid@.gc mapreduce.jobtracker.handler.count： 默认值：10 说明：JobTracker可以启动的线程数，一般为tasktracker节点的4%。 mapreduce.reduce.shuffle.parallelcopies： 默认值：5 说明：reuduce shuffle阶段并行传输数据的数量。这里改为10。集群大可以增大。 mapreduce.tasktracker.http.threads： 默认值：40 说明：map和reduce是通过http进行数据传输的，这个是设置传输的并行线程数。 mapreduce.map.output.compress： 默认值：false 说明： map输出是否进行压缩，如果压缩就会多耗cpu，但是减少传输时间，如果不压缩，就需要较多的传输带宽。配合mapreduce.map.output.compress.codec使用，默认是org.apache.hadoop.io.compress.DefaultCodec，可以根据需要设定数据压缩方式。 mapreduce.reduce.shuffle.merge.percent： 默认值： 0.66 说明：reduce归并接收map的输出数据可占用的内存配置百分比。类似mapreduce.reduce.shuffle.input.buffer.percen属性。 mapreduce.reduce.shuffle.memory.limit.percent： 默认值： 0.25 说明：一个单一的shuffle的最大内存使用限制。 mapreduce.jobtracker.handler.count： 默认值： 10 说明：可并发处理来自tasktracker的RPC请求数，默认值10。 mapred.job.reuse.jvm.num.tasks（mapreduce.job.jvm.numtasks）： 默认值： 1 说明：一个jvm可连续启动多个同类型任务，默认值1，若为-1表示不受限制。 mapreduce.tasktracker.tasks.reduce.maximum： 默认值： 2 说明：一个tasktracker并发执行的reduce数，建议为cpu核数 4. 系统优化4.1 避免排序对于一些不需要排序的应用，比如hash join或者limit n，可以将排序变为可选环节，这样可以带来一些好处： 在Map Collect阶段，不再需要同时比较partition和key，只需要比较partition，并可以使用更快的计数排序（O(n)）代替快速排序（O(NlgN)） 在Map Combine阶段，不再需要进行归并排序，只需要按照字节合并数据块即可。 去掉排序之后，Shuffle和Reduce可同时进行，这样就消除了Reduce Task的屏障（所有数据拷贝完成之后才能执行reduce()函数）。 4.2 Shuffle阶段内部优化 Map端–用Netty代替Jetty Reduce端–批拷贝 将Shuffle阶段从Reduce Task中独立出来 5. 总结在运行mapreduce任务中，经常调整的参数有： mapred.reduce.tasks：手动设置reduce个数 mapreduce.map.output.compress：map输出结果是否压缩 mapreduce.map.output.compress.codec mapreduce.output.fileoutputformat.compress：job输出结果是否压缩 mapreduce.output.fileoutputformat.compress.type mapreduce.output.fileoutputformat.compress.codec]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase和Cassandra比较]]></title>
    <url>%2F2014%2F06%2F24%2Fhbase-vs-cassandra%2F</url>
    <content type="text"><![CDATA[HBase是一个开源的分布式存储系统。他可以看作是Google的Bigtable的开源实现。如同Google的Bigtable使用Google File System一样，HBase构建于和Google File System类似的Hadoop HDFS之上。 Cassandra可以看作是Amazon Dynamo的开源实现。和Dynamo不同之处在于，Cassandra结合了Google Bigtable的ColumnFamily的数据模型。可以简单地认为，Cassandra是一个P2P的，高可靠性并具有丰富的数据模型的分布式文件系统。 HBase vs Cassandra HBase Cassandra 语言 Java Java 出发点 BigTable BigTable and Dynamo License Apache Apache Protocol HTTP/REST (also Thrift) Custom, binary (Thrift) 数据分布 表划分为多个region存在不同region server上 改进的一致性哈希（虚拟节点） 存储目标 大文件 小文件 一致性 强一致性 最终一致性，Quorum NRW策略 架构 master/slave p2p 高可用性 NameNode是HDFS的单点故障点 P2P和去中心化设计，不会出现单点故障 伸缩性 Region Server扩容，通过将自身发布到Master，Master均匀分布Region 扩容需在Hash Ring上多个节点间调整数据分布 读写性能 数据读写定位可能要通过最多6次的网络RPC，性能较低。 数据读写定位非常快 数据冲突处理 乐观并发控制（optimistic concurrency control） 向量时钟 临时故障处理 Region Server宕机，重做HLog 数据回传机制：某节点宕机，hash到该节点的新数据自动路由到下一节点做 hinted handoff，源节点恢复后，推送回源节点。 永久故障恢复 Region Server恢复，master重新给其分配region Merkle 哈希树，通过Gossip协议同步Merkle Tree，维护集群节点间的数据一致性 成员通信及错误检测 Zookeeper 基于Gossip CAP 1，强一致性，0数据丢失。2，可用性低。3，扩容方便。 1，弱一致性，数据可能丢失。2，可用性高。3，扩容方便。 facebook为什么放弃Cassandra？参考：http://www.zhihu.com/question/19593207: Facebook开发Cassandra初衷是用于Inbox Search，但是后来的Message System则使用了HBase，Facebook对此给出的解释是Cassandra的最终一致性模型不适合Message System，HBase具有更简单的一致性模型，当然还有其他的原因。HBase更加的成熟，成功的案例也比较多等等。Twitter和Digg都曾经很高调的选用Cassandra，但是最后也都放弃了，当然Twitter还有部分项目也还在使用Cassandra，但是主要的Tweet已经不是了。]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>cassandra</tag>
        <tag>hbase</tag>
        <tag>nosql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive中的排序语法]]></title>
    <url>%2F2014%2F06%2F22%2Fsort-in-hive-query%2F</url>
    <content type="text"><![CDATA[ORDER BYhive中的ORDER BY语句和关系数据库中的sql语法相似。他会对查询结果做全局排序，这意味着所有的数据会传送到一个Reduce任务上，这样会导致在大数量的情况下，花费大量时间。 与数据库中 ORDER BY 的区别在于在hive.mapred.mode = strict模式下，必须指定 limit 否则执行会报错。 123hive&gt; set hive.mapred.mode=strict;hive&gt; select * from test order by id;FAILED: SemanticException 1:28 In strict mode, if ORDER BY is specified, LIMIT must also be specified. Error encountered near token 'id' 例子： 123456789101112131415hive&gt; set hive.mapred.mode=unstrict;hive&gt; select * from test order BY id ;MapReduce Jobs Launched: Job 0: Map: 1 Reduce: 1 Cumulative CPU: 1.88 sec HDFS Read: 305 HDFS Write: 32 SUCCESSTotal MapReduce CPU Time Spent: 1 seconds 880 msecOK1 a1 a2 b2 b3 c3 c4 d4 dTime taken: 24.609 seconds, Fetched: 8 row(s) 从上面的日志可以看到：启动了一个reduce进行全局排序。 SORT BYSORT BY不是全局排序，其在数据进入reducer前完成排序，因此在有多个reduce任务情况下，SORT BY只能保证每个reduce的输出有序，而不能保证全局有序。 注意：SORT BY 不受 hive.mapred.mode 参数的影响 你可以通过设置mapred.reduce.tasks的值来控制reduce的数，然后对reduce输出的结果做二次排序。 例子： 123456789101112131415hive&gt; set mapred.reduce.tasks=3;hive&gt; select * from test sort BY id ; MapReduce Jobs Launched: Job 0: Map: 1 Reduce: 3 Cumulative CPU: 4.48 sec HDFS Read: 305 HDFS Write: 32 SUCCESSTotal MapReduce CPU Time Spent: 4 seconds 480 msecOK1 a2 b3 c4 d2 b3 c4 d1 aTime taken: 29.574 seconds, Fetched: 8 row(s) 从上面的日志可以看到：启动了三个reduce分别排序，最后的结果不是有序的。 DISTRIBUTE BY with SORT BYDISTRIBUTE BY能够控制map的输出在reduce中如何划分。其可以按照指定的字段对数据进行划分到不同的输出reduce/文件中。 DISTRIBUTE BY和GROUP BY有点类似，DISTRIBUTE BY控制reduce如何处理数据，而SORT BY控制reduce中的数据如何排序。 注意：hive要求DISTRIBUTE BY语句出现在SORT BY语句之前。 例子： 12345678910111213hive&gt; select * from test distribute BY id sort by id asc; Job 0: Map: 1 Reduce: 3 Cumulative CPU: 4.24 sec HDFS Read: 305 HDFS Write: 32 SUCCESSTotal MapReduce CPU Time Spent: 4 seconds 240 msecOK3 c3 c1 a1 a4 d4 d2 b2 bTime taken: 29.89 seconds, Fetched: 8 row(s) 从上面的日志可以看到：启动了三个reduce分别排序，最后的结果不是有序的。 CLUSTER BY来代替当DISTRIBUTE BY的字段和SORT BY的字段相同时，可以用CLUSTER BY来代替 DISTRIBUTE BY with SORT BY。 注意：CLUSTER BY不能添加desc或者asc。 例子： 12hive&gt; select * from test cluster by id asc; FAILED: ParseException line 1:33 extraneous input 'asc' expecting EOF near '&lt;EOF&gt;' hive&gt; select * from test cluster by id ; MapReduce Jobs Launched: Job 0: Map: 1 Reduce: 3 Cumulative CPU: 4.58 sec HDFS Read: 305 HDFS Write: 32 SUCCESS Total MapReduce CPU Time Spent: 4 seconds 580 msec OK 3 c 3 c 1 a 1 a 4 d 4 d 2 b 2 b Time taken: 30.646 seconds, Fetched: 8 row(s) 从上面的日志可以看到：启动了三个reduce分别排序，最后的结果不是有序的。 怎样让最后的结果是有序的呢？ 可以这样做： hive&gt; select a.* from (select * from test cluster by id ) a order by a.id ; MapReduce Jobs Launched: Job 0: Map: 1 Reduce: 3 Cumulative CPU: 4.5 sec HDFS Read: 305 HDFS Write: 448 SUCCESS Job 1: Map: 1 Reduce: 1 Cumulative CPU: 1.96 sec HDFS Read: 1232 HDFS Write: 32 SUCCESS Total MapReduce CPU Time Spent: 6 seconds 460 msec OK 1 a 1 a 2 b 2 b 3 c 3 c 4 d 4 d Time taken: 118.261 seconds, Fetched: 8 row(s) 总结 ORDER BY是全局排序，但在数据量大的情况下，花费时间会很长 SORT BY是将reduce的单个输出进行排序，不能保证全局有序 DISTRIBUTE BY可以按指定字段将数据划分到不同的reduce中 当DISTRIBUTE BY的字段和SORT BY的字段相同时，可以用CLUSTER BY来代替 DISTRIBUTE BY with SORT BY。]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene介绍]]></title>
    <url>%2F2014%2F06%2F21%2Fthe-introduction-of-lucene%2F</url>
    <content type="text"><![CDATA[1. Lucene是什么Lucene 是一个开源的、成熟的全文索引与信息检索(IR)库，采用Java实现。信息检索式指文档搜索、文档内信息搜索或者文档相关的元数据搜索等操作。Lucene是apache软件基金会项目组的一个子项目，是一个开放源代码的全文检索引擎工具包，即它不是一个完整的搜索应用程序，而是为你的应用程序提供索引和搜索功能。 Lucene的目的是为软件开发人员提供一个简单易用的工具包，以方便的在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎。全文检索（Full-Text Retrieval）是指以文本作为检索对象，找出含有指定词汇的文本。全面、准确和快速是衡量全文检索系统的关键指标。 官方网站：http://lucene.apache.org/ 2. 历史Lucene最初是由Doug Cutting开发的，在SourceForge的网站上提供下载。在2001年9月做为高质量的开源Java产品加入到Apache软件基金会的 Jakarta家族中。随着每个版本的发布，这个项目得到明显的增强，也吸引了更多的用户和开发人员 3. Lucene能做什么Lucene使你可以为你的应用程序添加索引和搜索能力。Lucene可以索引并能使得可以转换成文本格式的任何数据能够被搜索。 同样，利用Lucene你可以索引存放于数据库中的数据，提供给用户很多数据库没有提供的全文搜索的能力。 Lucene作为一个全文检索引擎，其具有如下突出的优点： 索引文件格式独立于应用平台。Lucene定义了一套以8位字节为基础的索引文件格式，使得兼容系统或者不同平台的应用能够共享建立的索引文件。 在传统全文检索引擎的倒排索引的基础上，实现了分块索引，能够针对新的文件建立小文件索引，提升索引速度。然后通过与原有索引的合并，达到优化的目的。 优秀的面向对象的系统架构，使得对于Lucene扩展的学习难度降低，方便扩充新功能。 设计了独立于语言和文件格式的文本分析接口，索引器通过接受Token流完成索引文件的创立，用户扩展新的语言和文件格式，只需要实现文本分析的接口。 已经默认实现了一套强大的查询引擎，用户无需自己编写代码即使系统可获得强大的查询能力，Lucene的查询实现中默认实现了布尔操作、模糊查询、分组查询等等。 4. Lucene 原理Lucene的检索算法属于索引检索，即用空间来换取时间，对需要检索的文件、字符流进行全文索引，在检索的时候对索引进行快速的检索，得到检索位置，这个位置记录检索词出现的文件路径或者某个关键词。 5. 索引组件为了快速搜索大量的文本，你必须首先简历针对文本索引，将文本内容转换成能够进行快速搜索的格式，从而消除慢速顺序扫描处理所带来的影响。这个过程就叫做索引操作，它的输出就叫做索引。 Lucene 采用的是一种称为反向索引（inverted index）的机制。反向索引就是说我们维护了一个词/短语表，对于这个表中的每个词/短语，都有一个链表描述了有哪些文档包含了这个词/短语。这样在用户输入查询条件的时候，就能非常快的得到搜索结果。 整个索引过程包括： 获取内容 建立文档 文档分析 文档索引 6. 搜索组件搜索是在一个索引中查找单词来找出它们所出现的文档的过程。搜索质量主要由查准率和查全率来衡量。查全率用来衡量搜索系统朝赵相关文档的能力；而查准率用来衡量搜索系统过滤费相关文档的能力。 搜索引擎的典型组件： 用户搜索界面 建立查询 搜索查询 展现结果 7. 一些概念7.1 AnalyzerAnalyzer是分析器，它的作用是把一个字符串按某种规则划分成一个个词语，并去除其中的无效词语，这里说的无效词语是指英文中的“of”、“the”，中文中的“的”、“地”等词语，这些词语在文章中大量出现，但是本身不包含什么关键信息，去掉有利于缩小索引文件、提高效率、提高命中率。 分词的规则千变万化，但目的只有一个：按语义划分。这点在英文中比较容易实现，因为英文本身就是以单词为单位的，已经用空格分开；而中文则必须以某种方法将连成一片的句子划分成一个个词语。 7.2 Document用户提供的源是一条条记录，它们可以是文本文件、字符串或者数据库表的一条记录等等。一条记录经过索引之后，就是以一个Document的形式存储在索引文件中的。用户进行搜索，也是以Document列表的形式返回。 7.3 Field一个Document可以包含多个信息域，例如一篇文章可以包含“标题”、“正文”、“最后修改时间”等信息域，这些信息域就是通过Field在Document中存储的。 Field有两个属性可选：存储和索引。通过存储属性你可以控制是否对这个Field进行存储；通过索引属性你可以控制是否对该Field进行索引。这看起来似乎有些废话，事实上对这两个属性的正确组合很重要，。 面举例说明：还是以刚才的文章为例子，我们需要对标题和正文进行全文搜索，所以我们要把索引属性设置为真，同时我们希望能直接从搜索结果中提取文章标题，所以我们把标题域的存储属性设置为真，但是由于正文域太大了，我们为了缩小索引文件大小，将正文域的存储属性设置为假，当需要时再直接读取文件；我们只是希望能从搜索解果中提取最后修改时间，不需要对它进行搜索，所以我们把最后修改时间域的存储属性设置为真，索引属性设置为假。上面的三个域涵盖了两个属性的三种组合，还有一种全为假的没有用到，事实上Field不允许你那么设置，因为既不存储又不索引的域是没有意义的。 7.4 Termterm是搜索的最小单位，它表示文档的一个词语，term由两部分组成：它表示的词语和这个词语所出现的field。 7.5 Tockentocken是term的一次出现，它包含trem文本和相应的起止偏移，以及一个类型字符串。一句话中可以出现多次相同的词语，它们都用同一个term表示，但是用不同的tocken，每个tocken标记该词语出现的地方。 7.6 Segment添加索引时并不是每个document都马上添加到同一个索引文件，它们首先被写入到不同的小文件，然后再合并成一个大索引文件，这里每个小文件都是一个segment。 8. Lucene示例Github上的一些Lucene例子： https://github.com/yusuke/lucene-examples https://github.com/yozhao/lucene-examples]]></content>
      <categories>
        <category>search-engine</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm集群安装部署步骤]]></title>
    <url>%2F2014%2F06%2F19%2Fhow-to-install-and-deploy-a-storm-cluster%2F</url>
    <content type="text"><![CDATA[开始学习Storm，本文主要记录Storm集群安装部署步骤，不包括对Storm的介绍。 安装storm集群，需要依赖以下组件： Zookeeper Python Zeromq Storm JDK JZMQ 故安装过程根据上面的组件分为以下几步： 安装JDK 安装Zookeeper集群 安装Python及依赖 安装Storm 另外，操作系统环境为：Centos6.4，安装用户为：root。 1. 安装JDK安装jdk有很多方法，可以参考文博客使用yum安装CDH Hadoop集群中的jdk安装步骤，需要说明的是下面的zookeeper集群安装方法也可以参考此文。 不管你用什么方法，最后需要配置JAVA_HOME并检测当前jdk版本： 1234$ java -versionjava version "1.6.0_31"Java(TM) SE Runtime Environment (build 1.6.0_31-b04)Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode) 2. 安装Zookeeper集群可以参考文博客使用yum安装CDH Hadoop集群中的Zookeeper集群安装步骤。 3. 安装Python及依赖一般操作系统上都安装了Python，查看当前Python版本： 12$ python -VPython 2.6.6 3.1 下载Zeromq1234$ wget http://download.zeromq.org/zeromq-4.0.4.tar.gz$ tar zxvf zeromq-4.0.4.tar.gz$ ./configure$ make &amp; make install 3.2 安装Jzmq12345$ git clone git://github.com/nathanmarz/jzmq.git$ cd jzmq$ ./autogen.sh$ ./configure$ make &amp; make install 4. 安装Storm下载稳定版本的storm，然后解压将其拷贝到/usr/lib/storm目录： 123$ wget https://github.com/downloads/nathanmarz/storm/storm-0.8.1.zip$ unzip storm-0.8.1.zip $ mv storm-0.8.1 /usr/lib/storm 接下来，配置环境变量： 12export STORM_HOME=/usr/lib/stormexport PATH=$PATH:$STORM_HOME/bin 建立storm存储目录： 1$ mkdir /tmp/storm 修改配置文件/usr/lib/storm/conf/storm.yaml，修改为如下： 123456789101112storm.zookeeper.servers: - "cdh1" - "cdh2" - "cdh3"ui.port: 8081nimbus.host: "cdh2"storm.local.dir: "/tmp/storm"supervisor.slots.ports: - 6700 - 6701 - 6702 - 6703 其中，配置参数说明： storm.zookeeper.servers：Storm集群使用的Zookeeper集群地址，如果Zookeeper集群使用的不是默认端口，那么还需要storm.zookeeper.port选项 ui.port：Storm UI的服务端口 storm.local.dir：Nimbus和Supervisor进程用于存储少量状态，如jars、confs等的本地磁盘目录 java.library.path: Storm使用的本地库（ZMQ和JZMQ）加载路径，默认为”/usr/local/lib:/opt/local/lib:/usr/lib”，一般来说ZMQ和JZMQ默认安装在/usr/local/lib下，因此不需要配置即可。 nimbus.host: Storm集群Nimbus机器地址 supervisor.slots.ports: 对于每个Supervisor工作节点，需要配置该工作节点可以运行的worker数量。每个worker占用一个单独的端口用于接收消息，该配置选项即用于定义哪些端口是可被worker使用的。默认情况下，每个节点上可运行4个workers，分别在6700、6701、6702和6703端口 更多配置参数，请参考Storm配置项详解。 最后，启动Storm各个后台进程： 主控节点上启动nimbus： 1$ storm nimbus &gt;/dev/null 2&gt;&amp;1 &amp; 在Storm各个工作节点上运行： 1$ storm supervisor &gt;/dev/null 2&gt;&amp;1 &amp; 在Storm主控节点上启动ui： 1$ storm ui &gt;/dev/null 2&gt;&amp;1 &amp; 然后，你可以访问http://cdh2:8081/查看集群的worker资源使用情况、Topologies的运行状态等信息。]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Effective Java 笔记]]></title>
    <url>%2F2014%2F06%2F17%2Fnote-about-effective-java%2F</url>
    <content type="text"><![CDATA[创建和销毁对象NO.1 考虑用静态工厂方法代替构造函数静态工厂方法好处： 1、构造函数有命名的限制，而静态方法有自己的名字，更加易于理解。 2、静态工厂方法在每次调用的时候不要求创建一个新的对象。这种做法对于一个要频繁创建相同对象的程序来说，可以极大的提高性能。它使得一个类可以保证是一个singleton；他使非可变类可以保证“不会有两个相等的实例存在”。 3、静态工厂方法在选择返回类型时有更大的灵活性。使用静态工厂方法，可以通过调用方法时使用不同的参数创建不同类的实例，还可以创建非公有类的对象，这就封装了类的实现细节。 4、在创建参数化类型实例的时候，他们使代码变的更加简洁。 例如： 12345public static Boolean valueOf(boolean f)&#123; return b ? Boolean.TRUE : Booleab.FALSE;&#125;Map&lt;String,List&lt;String&gt;&gt; m=HashMap.newInstance(); 静态工厂方法坏处： 1、如果一个类是通过静态工厂方法来取得实例的，并且该类的构造函数都不是公有的或者保护的，那该类就不可能有子类（被继承），子类的构造函数需要首先调用父类的构造函数，因为父类的构造函数是private的，所以即使我们假设继承成功的话，那么子类也根本没有权限去调用父类的私有构造函数，所以是无法被继承的。 2、毕竟通过构造函数创建实例还是SUN公司所提倡的，静态工厂方法跟其他的静态方法区别不大，这样创建的实例谁又知道这个静态方法是创建实例呢？弥补的办法就是：静态工厂方法名字使用valueOf、of、getInstance、newInstance、getType、newType。 NO.2 遇到多个构造器参数时要考虑用构建器当有多个构造方法，一般式参数大于4个的时候，建议使用Builder模式。 NO.3 用私有构造器或者枚举类型强化Singleton属性用单元素的枚举来实现单例模式。 NO.4 通过私有的构造函数强化不可实例化的能力在面向对象程序设计中，假如存在太多只有静态属性和静态方法的类；那么，面向对象的思想可能在这会损失殆尽。 但是，并不能说面向对象的程序中就不应该出现只有静态属性和静态方法的类，相反，有时候我们还必须写这样的类作为工具类。这样的类怎么实现呢？有人可能会把该类定义成抽象类(Abstract class)，的确，抽象类是不可以实例化的，但是别忘了还有继承，继承了抽象类的子类在实例化时候，默认是会先调用父类无参数的构造函数的（super();），这时候，父类不是也被实例化了嘛？ 其实我们可以这样做，把该类的构造函数定义为私有的（private），而类的内部又不调用该构造函数的话，就成功了。这样带来的后果就是该类成了 final的，不可能再被任何类继承了，要被继承，得提供一个公有（public）的或者保护（protect）的构造函数，这样才能被子类调用。 NO.5 避免创建重复的对象如果一个对象是不可变的，那么他总是可以被重用的，如： 1234//不推荐，"test"本来就是一个String实例，如果此方法在一个循环中或者被频繁的调用，将会严重影响性能String s = new String("test");//推荐方式String s = "test"; 对于提供静态方法和构造函数的非可变类，推荐使用静态方法，这样可以避免重复创建对象，如：Boolean.vauleOf(String)方法优于构造函数Boolean(String) 类初始化的顺序： 先初始化父类的静态代码 —&gt; 初始化子类的静态代码 —&gt; 初始化父类的非静态代码 —&gt; 初始化父类构造函数 —&gt; 初始化子类非静态代码 —&gt;初始化子类构造函数。 NO.6 消除过期的对象引用内存泄漏问题：如果一个对象的引用被无意识的保留起来，那么垃圾回收机制是不会去处理这个对象，而且也不会去处理被这个对象引用的其它对象。比如堆栈的弹出栈元素方法。 1234567891011public Object pop()&#123; if(size == 0)&#123; throw new EmptyStackException(); &#125; Object result = elements[--size]; //自减后把原来的引用置为null elements[size] = null; return result;&#125; 内存泄露常出现在： 过期对象 缓存，由于缓存没有及时清除无用的条目而出现，可以使用weakHashMap来避免这种情况 监听器和其他回调 清理过期对象引用的好处是：如果在以后又被使用到该引用，最会抛下NullPointException而不是让程序继续错误的运行下去，尽可能早的监测出程序中的错误总是有好处的。 方法：重新使用这个已经指向一个对象的引用，或结束其生命周期。 对所有对象都通用的方法equals方法(1) equals方法一般用于“值类”的情形，比如Integer,Date目的是为了比较两个指向值对象的引用的时候，希望它们的逻辑是否相等而不是它们是否指向同一个对象。 约定： a 自反性 对任意的对象必须和它自身相等。对值引用x x.equals(x) 一定返回true b 对称性 对任意的值引用x,y,如果x.equals(y) 一定有y.equals(x) c 传递性 对任意的值引用x,y,z，如果x.equals(y),y.equals(z) 一定有x.equals(z) d 一致性 对于任何非null的引用x和y，只要equals的比较操作在对象中所用的信息没有被修改，多次调用x.equals(y)就会一致地返回true 结论： 要想在扩展一个可实例化的类的同时，即要保证增加新的特性，又要保证equals约定，建议复合优于继承原则。若类和类是 a kind of 关系则用继承，若类和类是 a part of 关系则用组合(复合) hashCode相等的对象必须要有相等的散列码，如果违反这个约定可能导致这个类无法与某些散列值得集合结合在一起使用，所以在改写了equals方法的同时一定要重写hashCode方法以保证一致性。 toStringtoString返回值中包含所有信息 cloneComparable类和接口 使类和成员的可访问能力尽量的小 支持非可变性 复合优于继承 接口优于抽象 优先考虑静态成员类 方法 (1) 检查参数的有效性 (2) 需要使用保护性拷贝 (3) 方法设计的一些原则 a、避免长的参数列表，尤其是参数相同的参数列表。 b、对参数类型使用接口，而不是接口的实现类。 c、谨慎使用重载。 d、返回0程度的数组而不是null。 (4) 为所有导出的api方法编写注释 重载(overloaded method) 选择的是静态的。选择的依据是参数类型 重写(oveeridden method) 选择的依据是被调用方法所在对象的运行时的类型。 通用设计方法 (1) 将局部变量的作用域最小化 (2) foreach优于传统的for循环。有三种情况无法使用foreach循环：过滤、转换、平行迭代 (3) 了解和使用类库 (4) 如果要得到精确结果，最好是用BigDecimal 而不使用fload或double (5) 对数量大的字符串连接使用StringBuffer而不是String前者速度快。 (6) 基本类型优先于装箱类型 (7) 当心字符串连接性能 (8) 通过接口引用对象 (9) 接口优先于反射机制 (10) 谨慎使用本地方法 (11) 谨慎进行优化]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase源码分析：HTable put过程]]></title>
    <url>%2F2014%2F06%2F13%2Fhbase-code-about-htable-put%2F</url>
    <content type="text"><![CDATA[HBase版本：0.94.15-cdh4.7.0 在 HBase中，大部分的操作都是在RegionServer完成的，Client端想要插入、删除、查询数据都需要先找到相应的 RegionServer。什么叫相应的RegionServer？就是管理你要操作的那个Region的RegionServer。Client本身并 不知道哪个RegionServer管理哪个Region，那么它是如何找到相应的RegionServer的？本文就是在研究源码的基础上了解这个过程。 首先来看看写过程的序列图： 客户端代码1、put方法 HTable的put有两个方法： 123456789101112131415public void put(final Put put) throws IOException &#123; doPut(put); if (autoFlush) &#123; flushCommits(); &#125;&#125;public void put(final List&lt;Put&gt; puts) throws IOException &#123; for (Put put : puts) &#123; doPut(put); &#125; if (autoFlush) &#123; flushCommits(); &#125;&#125; 从上面代码可以看出：你既可以一次put一行记录也可以一次put多行记录，两个方法内部都会调用doPut方法，最后再来根据autoFlush（默认为true）判断是否需要flushCommits，在autoFlush为false的时候，如果当前容量超过了缓冲区大小（默认值为：2097152=2M），也会调用flushCommits方法。也就是说，在自动提交情况下，你可以手动控制通过一次put多条记录（这时候缓冲区不会满），然后将这些记录flush，以提高写操作tps。 doPut代码如下： 12345678private void doPut(Put put) throws IOException&#123; validatePut(put); //验证Put有效，主要是判断kv的长度 writeBuffer.add(put); //写入缓存 currentWriteBufferSize += put.heapSize(); //计算缓存容量 if (currentWriteBufferSize &gt; writeBufferSize) &#123; flushCommits(); //如果超过缓存容量，则调用flushCommits() &#125;&#125; 2、flushCommits方法如下： 12345678910111213141516171819202122232425262728293031323334public void flushCommits() throws IOException &#123; try &#123; Object[] results = new Object[writeBuffer.size()]; try &#123; //调用HConnection来提交Put this.connection.processBatch(writeBuffer, tableName, pool, results); &#125; catch (InterruptedException e) &#123; throw new IOException(e); &#125; finally &#123; // mutate list so that it is empty for complete success, or contains // only failed records results are returned in the same order as the // requests in list walk the list backwards, so we can remove from list // without impacting the indexes of earlier members for (int i = results.length - 1; i&gt;=0; i--) &#123; if (results[i] instanceof Result) &#123; // successful Puts are removed from the list here. writeBuffer.remove(i); &#125; &#125; &#125; &#125; finally &#123; if (clearBufferOnFail) &#123; writeBuffer.clear(); currentWriteBufferSize = 0; &#125; else &#123; // the write buffer was adjusted by processBatchOfPuts currentWriteBufferSize = 0; //currentWriteBufferSize又重新计算了一遍，看来一批提交不一定会全部提交完 for (Put aPut : writeBuffer) &#123; currentWriteBufferSize += aPut.heapSize(); &#125; &#125; &#125; &#125; 其核心是调用this.connection的processBatch方法，其参数有：writeBuffer、tableName、pool、results writeBuffer，缓冲区，带提交的数据 tableName，表名 pool，ExecutorService类，可以通过HTable构造方法传入一个参数来初始化（例如：HConnectionManager的getTable(byte[] tableName, ExecutorService pool)方法），也可以内部初始化。内部初始化时，其最大线程数由hbase.htable.threads.max设置，keepAliveTime由hbase.htable.threads.keepalivetime设置，默认为60秒 results，保存运行结果 在默认情况下，connection由如下方式初始化： 1this.connection = HConnectionManager.getConnection(conf); //HConnection的实现类为HConnectionImplementation 3、ConnectionImplementation的processBatch方法 12345678910111213public void processBatch(List&lt;? extends Row&gt; list, final byte[] tableName, ExecutorService pool, Object[] results) throws IOException, InterruptedException &#123; // This belongs in HTable!!! Not in here. St.Ack // results must be the same size as list if (results.length != list.size()) &#123; throw new IllegalArgumentException("argument results must be the same size as argument list"); &#125; processBatchCallback(list, tableName, pool, results, null); &#125; 最后是调用的processBatchCallback方法，第五个参数为空，即没有回调方法。 processBatchCallback方法内部可以失败后进行重试，重试次数为hbase.client.retries.number控制，默认为10，每一次重试直接都会休眠一下，每次休眠时间为: 12pause * HConstants.RETRY_BACKOFF[ntries]+(long)(normalPause * RANDOM.nextFloat() * 0.01f);//RETRY_BACKOFF[] = &#123; 1, 1, 1, 2, 2, 4, 4, 8, 16, 32, 64 &#125; pause通过hbase.client.pause设置，默认值为1000，即1秒；ntries为当前重复次数 接下来，第一步，遍历List&lt;? extends Row&gt;，获取每一个行对应HRegion所在位置，并且按regionName对这些待put的行进行分组。 第二步，发送异步请求到服务端。 第三步，接收异步请求的结果，收集成功的和失败的，做好重试准备 第四步，对于失败的，进行重试。 达到重试次数之后，对运行结果判断是否有异常，如果有则抛出RetriesExhaustedWithDetailsException异常。 由以上四步可以看出，重点在于第一、二步。 第一步查找HRegion所在位置过程关键在private HRegionLocation locateRegion(final byte [] tableName,final byte [] row, boolean useCache)方法中，并且为递归方法，过程如下： 调用locateRegionInMeta方法到.META.表中查找tableName的row所对应的HRegion所在位置，先从本地缓存查找，如果没有，则进行下一步； 调用locateRegionInMeta方法到-ROOT-表中查找.META.所对应的HRegion所在位置，先从本地缓存查找，如果没有，则进行下一步 通过rootRegionTracker（即从zk上）获取RootRegionServer地址，即找到-ROOT-表所在的RegionServer地址，然后获取到.META.所在位置，最后在获取.META.表上所有HRegion，并将其加入到本地缓存。 通过示例描述如下： 12345678910111213获取 Table2，RowKey为RK10000的RegionServer=&gt; 获取.META.，RowKey为Table2,RK10000, 99999999999999 的RegionServer =&gt; 获取-ROOT-，RowKey为.META.,Table2,RK10000,99999999999999,99999999999999的RegionServer =&gt; 获取-ROOT-的RegionServer =&gt; 从ZooKeeper得到-ROOT-的RegionServer =&gt; 从-ROOT-表中查到RowKey最接近（小于） .META.,Table2,RK10000,99999999999999,99999999999999 的一条Row，并得到.META.的RegionServer =&gt; 从.META.表中查到RowKey最接近（小于）Table2,RK10000,99999999999999 的一条Row，并得到Table2的K10000的Row对应的HRegionLocation 说明： 当我们创建一个表时，不管是否预建分区，该表创建之后，在.META.上会有一条记录的。 在客户端第一次连接服务端时，会两次查询缓存并没有查到结果，最后在通过-ROOT-–&gt;.META.–&gt;HRegion找到对应的HRegion所在位置。 第二步中，先是创建到RegionServer的连接，后是调用RegionServer上的multi方法，显然这是远程调用的过程。第二步中提交的任务通过下面代码创建： 12345678910111213141516171819202122private &lt;R&gt; Callable&lt;MultiResponse&gt; createCallable(final HRegionLocation loc, final MultiAction&lt;R&gt; multi, final byte [] tableName) &#123; // TODO: This does not belong in here!!! St.Ack HConnections should // not be dealing in Callables; Callables have HConnections, not other // way around. final HConnection connection = this; return new Callable&lt;MultiResponse&gt;() &#123; public MultiResponse call() throws IOException &#123; ServerCallable&lt;MultiResponse&gt; callable = new ServerCallable&lt;MultiResponse&gt;(connection, tableName, null) &#123; public MultiResponse call() throws IOException &#123; return server.multi(multi); &#125; @Override public void connect(boolean reload) throws IOException &#123; server = connection.getHRegionConnection(loc.getHostname(), loc.getPort()); &#125; &#125;; return callable.withoutRetries(); &#125; &#125;;&#125; 从上面代码可以看到，通过connection.getHRegionConnection(loc.getHostname(), loc.getPort())创建一个HRegionInterface的实现类即HRegionServer，方法内使用了代理的方式创建对象。 1234server = HBaseRPC.waitForProxy(this.rpcEngine, serverInterfaceClass, HRegionInterface.VERSION, address, this.conf, this.maxRPCAttempts, this.rpcTimeout, this.rpcTimeout); 服务端上面客户端调用过程分析完毕，继续跟RegionServer服务端的处理。 HRegionServer的multi方法对于客户端写操作，最终会调用HRegionServer的multi方法。 因为传递到RegionServer都是按regionName分组的，故最后的操作实际上都是调用的HRegion对象的方法。 该方法主要就是遍历multi并对actionsForRegion按rowid进行排序，然后分类别对action进行处理，Put和Delete操作会放到一起然后调用batchMutate方法批量提交： 1OperationStatus[] codes =region.batchMutate(mutationsWithLocks.toArray(new Pair[]&#123;&#125;)); 其他的： 对于Get，会调用get方法； 对于Exec，会调用execCoprocessor方法； 对于Increment，会调用increment方法； 对于Append，会调用append方法； 对于RowMutations，会调用mutateRow方法； 对于Put和Delete操作（保存在mutations中），在处理之前，先通过cacheFlusher检查memstore大小吃否超过限定值，如果是，则进行flush。 接下来遍历mutations，为每个Mutation添加一个锁lock，然后再调用region的batchMutate方法。 HRegion的batchMutatebatchMutate方法内部，依次一个个处理： 先检查是否只读 检查当前资源是否支持update操作，会比较memstoreSize和blockingMemStoreSize大小，然后会阻塞线程 调用startRegionOperation，给lock.readLock()加锁 调用doPreMutationHook执行协作器里的一些方法 计算其待添加的大小 计算加入memstore之后的memstore大小 写完之后，释放lock.readLock()锁 判断是否需要flush memstore，如果需要，则调用requestFlush方法，其内部实际是通过RegionServerServices中的FlushRequester（其实现类为MemStoreFlusher）来执行flush操作 MemStoreFlusher flush过程HRegion中的requestFlush方法： 12345678910111213141516private void requestFlush() &#123; if (this.rsServices == null) &#123; return; &#125; synchronized (writestate) &#123; if (this.writestate.isFlushRequested()) &#123; return; &#125; writestate.flushRequested = true; &#125; // Make request outside of synchronize block; HBASE-818. this.rsServices.getFlushRequester().requestFlush(this); if (LOG.isDebugEnabled()) &#123; LOG.debug("Flush requested on " + this); &#125; &#125; 上面this.rsServices.getFlushRequester()其实际上返回的是MemStoreFlusher类。 MemStoreFlusher内部有一个队列和一个Map： 123456//保存待flush的对象private final BlockingQueue&lt;FlushQueueEntry&gt; flushQueue = new DelayQueue&lt;FlushQueueEntry&gt;();//记录队列中存在哪些Regionprivate final Map&lt;HRegion, FlushRegionEntry&gt; regionsInQueue = new HashMap&lt;HRegion, FlushRegionEntry&gt;(); MemStoreFlusher构造方法： 初始化threadWakeFrequency，该值由hbase.server.thread.wakefrequency设置，默认为10 * 1000 初始化globalMemStoreLimit，该值为最大堆内存乘以hbase.regionserver.global.memstore.upperLimit的值，hbase.regionserver.global.memstore.upperLimit参数默认值为0.4 初始化globalMemStoreLimitLowMark，该值为最大堆内存乘以hbase.regionserver.global.memstore.lowerLimit的值，hbase.regionserver.global.memstore.lowerLimit参数默认值为0.35 初始化blockingWaitTime，该值由hbase.hstore.blockingWaitTime设置，默认为90000 MemStoreFlusher实现了Runnable接口，在RegionServer启动过程中会启动一个线程，其run方法逻辑如下： 只要RegionServer一直在运行，该线程就不会停止运行 每隔threadWakeFrequency时间从flushQueue中取出一个对象 如果取出的对象为空或者WakeupFlushThread，则判断：如果当前RegionServer的总大小大于globalMemStoreLimit值，则找到没有太多storefiles（只个数小于hbase.hstore.blockingStoreFiles的，该参数默认值为7）的最大的region和不管有多少storefiles的最大region，比较两个大小找出最大的一个，然后flush该region，并休眠1秒；最后在唤醒flush线程 先flush region上的memstore，这部分代码通过HRegion的internalFlushcache方法来完成，其内部使用了mvcc 判断是否该拆分，如果是则拆分 判断是否该压缩合并，如果是则合并 如果如果取出的对象为FlushRegionEntry，则flush该对象。 如果当前region不是meta region并且当前region的storefiles数大于hbase.hstore.blockingStoreFiles，先判断是否要拆分，然后再判断是否需要合并小文件。这个过程会阻塞blockingWaitTime值定义的时间。 否则， 直接flush该region上的memstore（调用HRegion的internalFlushcache方法），然后再判断是否需要拆分和合并 总结最后总结一下，HRegionServer作用如下： 使得被它管理的一系列HRegion能够被客户端来使用，每个HRegion对应了Table中的一个Region，HRegion中由多个HStore组成。 主要负责响应用户I/O请求，向HDFS文件系统中读写数据。 HRegion定位过程： 1client -&gt; zookeeper -&gt; -ROOT- -&gt; .META -&gt; HRegion地址 -&gt; HRegionServer-&gt; HRegion 在这个过程中客户端先通过zk找到Root表所在的RegionServer（通过zk上的/hbase/root-region-server节点获取），然后找到Meta表对应的HRegion地址，最后在Meta表里找到目标表所在的HRegion地址，这个过程客户端并没有和HMaster进行交互。 Client端并不会每次数据操作都做这整个路由过程，因为HRegion的相关信息会缓存到本地，当有变化时，通过zk监听器能够及时感知。 数据写入过程： client先根据rowkey找到对应的region和regionserver client想regionserver提交写请求 region找到目标region region检查数据是否与scheam一致 如果客户端没有指定版本，则获取当前系统时间作为数据版本 将更新写入wal log 将更新写入memstore 判断memstore是否需要flush为store文件]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase实现简单聚合计算]]></title>
    <url>%2F2014%2F06%2F12%2Fhbase-aggregate-client%2F</url>
    <content type="text"><![CDATA[本文主要记录如何通过打补丁的方式将“hbase中实现简单聚合计算”的特性引入hbase源代码中，并介绍通过命令行和java代码的使用方法。 支持的简单聚合计算，包括： rowcount min max sum std avg median 1、 下载并编译hbase源代码 我这里使用的HBase源代码版本是：cdh4-0.94.6_4.3.0，如果你使用其他版本，有可能patch打不上。 2、 引入patch 基于提交日志add-aggregate-support-in-hbase-shell生成patch文件，然后打patch，或者也可以使用其他方法： 1$ git apply add-aggregate-in-hbase-shell.patch 该patch所做修改包括如下文件： 1234567891011121314src/main/java/org/apache/hadoop/hbase/client/coprocessor/AbstractDoubleColumnInterpreter.javasrc/main/java/org/apache/hadoop/hbase/client/coprocessor/AbstractLongColumnInterpreter.javasrc/main/java/org/apache/hadoop/hbase/client/coprocessor/CompositeDoubleStrColumnInterpreter.javasrc/main/java/org/apache/hadoop/hbase/client/coprocessor/CompositeLongStrColumnInterpreter.javasrc/main/java/org/apache/hadoop/hbase/client/coprocessor/DoubleColumnInterpreter.javasrc/main/java/org/apache/hadoop/hbase/client/coprocessor/DoubleStrColumnInterpreter.javasrc/main/java/org/apache/hadoop/hbase/client/coprocessor/LongColumnInterpreter.javasrc/main/java/org/apache/hadoop/hbase/client/coprocessor/LongStrColumnInterpreter.javasrc/main/ruby/hbase.rbsrc/main/ruby/hbase/coprocessor.rbsrc/main/ruby/hbase/hbase.rbsrc/main/ruby/shell.rbsrc/main/ruby/shell/commands.rbsrc/main/ruby/shell/commands/aggregate.rb 3、 编译源代码 为了使编译花费时间不会太长，请运行如下命令编译代码，你也可以自己修改下面命令： 1$ MAVEN_OPTS=&quot;-Xmx2g&quot; mvn clean install -DskipTests -Prelease,security -Drat.numUnapprovedLicenses=200 -Dhadoop.profile=2.0 4、测试 然后将target目录下生成的jar包拷贝到集群中每个hbase节点的/usr/lib/hbase目录。 修改hbase-site.xml配置文件，添加如下配置： 1234&lt;property&gt; &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&lt;/value&gt;&lt;/property&gt; 重启hbase服务： 12$ /etc/init.d/hbase-master restart$ /etc/init.d/hbase-regionserver restart a）运行hbase shell进行测试 首先创建表并插入几条记录： 123456create 't','f' put 't','1','f:id','1'put 't','2','f:id','2'put 't','2','f:id','3'put 't','3','f:id','4' 在hbase shell命令行中输入agg并按tab键自动提示： 1hbase(main):004:0&gt; aggregate 什么参数不输入，提示如下： 123456789101112131415161718192021222324252627hbase(main):004:0&gt; aggregate ERROR: wrong number of arguments (0 for 2) Here is some help for this command:Execute a Coprocessor aggregation function; pass aggregation function name, table name, column name, column interpreter and optionally a dictionary of aggregation specifications. Aggregation specifications may include STARTROW, STOPROW or FILTER. For a cross-site big table, if no clusters are specified, all clusters will be counted for aggregation.Usage: aggregate 'subcommand','table','column',[&#123;COLUMN_INTERPRETER =&gt; org.apache.hadoop.hbase.client.coprocessor.LongColumnInterpreter.new, STARTROW =&gt; 'abc', STOPROW =&gt; 'def', FILTER =&gt; org.apache.hadoop.hbase.filter.ColumnPaginationFilter.new(1, 0)&#125;]Available subcommands:rowcountminmaxsumstdavgmedianAvailable COLUMN_INTERPRETER:org.apache.hadoop.hbase.client.coprocessor.LongColumnInterpreter.neworg.apache.hadoop.hbase.client.coprocessor.LongStrColumnInterpreter.neworg.apache.hadoop.hbase.client.coprocessor.CompositeLongStrColumnInterpreter.new(",", 0)The default COLUMN_INTERPRETER is org.apache.hadoop.hbase.client.coprocessor.LongStrColumnInterpreter.new. Some examples: hbase&gt; aggregate 'min','t1','f1:c1'hbase&gt; aggregate 'sum','t1','f1:c1','f1:c2'hbase&gt; aggregate 'rowcount','t1','f1:c1' ,&#123;COLUMN_INTERPRETER =&gt; org.apache.hadoop.hbase.client.coprocessor.CompositeLongStrColumnInterpreter.new(",", 0)&#125;hbase&gt; aggregate 'min','t1','f1:c1',&#123;STARTROW =&gt; 'abc', STOPROW =&gt; 'def'&#125; 从上可以看到aggregate的帮助说明。 接下来进行测试，例如求id列的最小值： 1234567hbase(main):005:0&gt; aggregate &apos;min&apos;,&apos;t&apos;,&apos;f:id&apos;The result of min for table t is 10 row(s) in 0.0170 seconds hbase(main):006:0&gt; aggregate &apos;avg&apos;,&apos;t&apos;,&apos;f:id&apos;The result of avg for table t is 2.50 row(s) in 0.0170 seconds 正确输出结果，表明测试成功。 b）通过hbase client测试 创建AggregateTest.java类并添加如下代码： 1234567891011121314151617181920212223242526import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.client.Scan;import org.apache.hadoop.hbase.client.coprocessor.AggregationClient;import org.apache.hadoop.hbase.client.coprocessor.LongStrColumnInterpreter;import org.apache.hadoop.hbase.coprocessor.ColumnInterpreter;import org.apache.hadoop.hbase.util.Bytes;public class AggregateTest &#123; public static void main(String[] args) &#123; Configuration conf = HBaseConfiguration.create(); conf.setInt("hbase.client.retries.number", 1); conf.setInt("ipc.client.connect.max.retries", 1); byte[] table = Bytes.toBytes("t"); Scan scan = new Scan(); scan.addColumn(Bytes.toBytes("f"), Bytes.toBytes("id")); final ColumnInterpreter&lt;Long, Long&gt; columnInterpreter = new LongStrColumnInterpreter(); try &#123; AggregationClient aClient = new AggregationClient(conf); Long rowCount = aClient.min(table, columnInterpreter, scan); System.out.println("The result is " + rowCount); &#125; catch (Throwable e) &#123; e.printStackTrace(); &#125; &#125;&#125; 运行该类并查看输出结果。 以上源代码及所做的修改我已提交到我github仓库上hbase的cdh4-0.94.15_4.7.0分支，见提交日志add-aggregate-support-in-hbase-shell。]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive Over HBase的介绍]]></title>
    <url>%2F2014%2F06%2F12%2Fintro-of-hive-over-hbase%2F</url>
    <content type="text"><![CDATA[Hive Over HBase是基于Hive的HQL查询引擎支持对hbase表提供及时查询的功能，它并不是将hql语句翻译成mapreduce来运行，其响应时间在秒级别。 特性支持的字段类型： boolean, tinyint, smallint, int, bigint, float, double, string, struct(当hbase中的rowkey字段为struct类型，请将子字段定义为string类型，同时指定表的collection items terminated分隔字符以及各字段的长度参数:hbase.rowkey.column.length) 支持的sql语法： where子句 group by，having子句 聚合函数: count, max, min, sum, avg order by with limit(top N) limit 子句 explain 支持的运算 关系操作：&gt;, &gt;=, &lt;=, &lt;, = 算术操作：+, - , * , / , % 逻辑操作：and, or, not 字符串操作函数: substring, concat Distinct : 支持select distinct &lt;col-list&gt; from &lt;tab&gt; where &lt;expr&gt;, select aggr-fun(distinct &lt;col_list&gt;) from &lt;tab&gt; where &lt;expr&gt; Like: 通配符’_’, ’%’ Case when子句 不支持: Sub-query Join Union 原理扩展HBase客户端代码，实现简单聚合计算，基于协作器实现分组计算的功能，并且修改hive的查询引擎，将HQL语句转换成HBase的Task，然后调用HBase中的api实现对HQL语句的解析。 源码暂时不公开。]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hbase</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase客户端实现并行扫描]]></title>
    <url>%2F2014%2F06%2F12%2Fhbase-parallel-client-scanner%2F</url>
    <content type="text"><![CDATA[HBase中有一个类可以实现客户端扫描数据，叫做ClientScanner，该类不是并行的，有没有办法实现一个并行的扫描类，加快扫描速度呢？ 如果是一个Scan，我们可以根据startkey和stopkey将其拆分为多个子Scan，然后让这些Scan并行的去查询数据，然后分别返回执行结果。 实现方式说明：我使用的HBase版本为：cdh4-0.94.15_4.7.0。 在org.apache.hadoop.hbase.client创建ParallelClientScanner类，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304package org.apache.hadoop.hbase.client;import java.io.IOException;import java.util.ArrayList;import java.util.Collections;import java.util.Comparator;import java.util.Iterator;import java.util.LinkedList;import java.util.List;import java.util.Queue;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicBoolean;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HConstants;import org.apache.hadoop.hbase.Stoppable;import org.apache.hadoop.hbase.util.Bytes;public class ParallelClientScanner extends AbstractClientScanner &#123; private static final int SCANNER_WAIT_TIME = 1000; private final HTableInterface table; private final int resultQueueSize; private final int threadCount; private List&lt;Scan&gt; scans; private Iterator&lt;Scan&gt; iter; private ParallelScannerWorkerThread currThread; private Queue&lt;ParallelScannerWorkerThread&gt; nextThreads; private boolean closed; public ParallelClientScanner(Configuration conf, Scan scan, byte[] tableName, List&lt;byte[]&gt; splitKeys) throws IOException &#123; this(new HTable(conf, tableName), scan, splitKeys); &#125; public ParallelClientScanner(HTableInterface table, Scan scan, List&lt;byte[]&gt; splitKeys) throws IOException &#123; this.table = table; this.resultQueueSize = table.getConfiguration().getInt( "hbase.parallel.scanner.queue.size", 1000); this.threadCount = (table.getConfiguration().getInt( "hbase.parallel.scanner.thread.count", 10) - 1); this.scans = new ArrayList&lt;Scan&gt;(); byte[] stopRow = scan.getStopRow(); byte[] lastSplitKey = scan.getStartRow(); Scan subScan = null; for (Iterator&lt;byte[]&gt; it = splitKeys.iterator(); it.hasNext();) &#123; byte[] splitKey = (byte[]) it.next(); if ((Bytes.compareTo(splitKey, lastSplitKey) &lt;= 0) || ((Bytes.compareTo(splitKey, stopRow) &gt;= 0) &amp;&amp; (!Bytes .equals(stopRow, HConstants.EMPTY_END_ROW))) || (Bytes.equals(splitKey, HConstants.EMPTY_END_ROW))) &#123; continue; &#125; subScan = new Scan(scan); subScan.setStartRow(lastSplitKey); subScan.setStopRow(splitKey); subScan.setParallel(false); this.scans.add(subScan); lastSplitKey = splitKey; &#125; subScan = new Scan(scan); subScan.setStartRow(lastSplitKey); subScan.setParallel(false); this.scans.add(subScan); this.nextThreads = new LinkedList&lt;ParallelScannerWorkerThread&gt;(); this.closed = true; initialize(); &#125; public ParallelClientScanner(Configuration conf, List&lt;Scan&gt; scans, byte[] tableName) throws IOException &#123; this(new HTable(conf, tableName), scans); &#125; public ParallelClientScanner(HTableInterface table, List&lt;Scan&gt; scanList) throws IOException &#123; if (null == scanList) &#123; throw new IOException("ScanList cannot be null"); &#125; sort(scanList); this.table = table; this.resultQueueSize = table.getConfiguration().getInt( "hbase.parallel.scanner.queue.size", 1000); this.threadCount = (table.getConfiguration().getInt( "hbase.parallel.scanner.thread.count", 10) - 1); this.scans = new ArrayList&lt;Scan&gt;(); Scan subScan = null; for (int i = 0; i &lt; scanList.size(); i++) &#123; subScan = new Scan((Scan) scanList.get(i)); subScan.setParallel(false); this.scans.add(subScan); &#125; this.nextThreads = new LinkedList&lt;ParallelScannerWorkerThread&gt;(); this.closed = true; initialize(); &#125; protected void initialize() throws IOException &#123; try &#123; this.iter = this.scans.iterator(); if (this.iter.hasNext()) &#123; this.currThread = new ParallelScannerWorkerThread( (Scan) this.iter.next()); this.currThread.start(); &#125; while ((this.iter.hasNext()) &amp;&amp; (this.nextThreads.size() &lt; this.threadCount)) &#123; ParallelScannerWorkerThread worker = new ParallelScannerWorkerThread( (Scan) this.iter.next()); this.nextThreads.offer(worker); worker.start(); &#125; this.closed = false; &#125; catch (IOException e) &#123; close(); throw e; &#125; &#125; public void close() &#123; this.closed = true; if (this.currThread != null) &#123; this.currThread.stop("Closed by user."); &#125; for (ParallelScannerWorkerThread worker : this.nextThreads) worker.stop("Closed by user."); &#125; public Result next() throws IOException &#123; try &#123; return nextInternal(); &#125; catch (IOException e) &#123; close(); throw e; &#125; &#125; private Result nextInternal() throws IOException &#123; if ((this.closed) || (this.currThread == null)) &#123; return null; &#125; Result next = this.currThread.next(); if (next != null) &#123; return next; &#125; if (this.currThread.isError()) &#123; Exception ex = this.currThread.getException(); throw ((ex instanceof IOException) ? (IOException) ex : new IOException(ex)); &#125; while ((next == null) &amp;&amp; (this.currThread != null)) &#123; if (!this.currThread.isStopped()) &#123; this.currThread.stop("Scanner complete."); &#125; this.currThread = ((ParallelScannerWorkerThread) this.nextThreads .poll()); if (this.iter.hasNext()) &#123; ParallelScannerWorkerThread worker = new ParallelScannerWorkerThread( (Scan) this.iter.next()); this.nextThreads.offer(worker); worker.start(); &#125; if (this.currThread != null) &#123; next = this.currThread.next(); &#125; &#125; return next; &#125; public Result[] next(int nbRows) throws IOException &#123; ArrayList&lt;Result&gt; resultSets = new ArrayList&lt;Result&gt;(nbRows); for (int i = 0; i &lt; nbRows; i++) &#123; Result next = next(); if (next == null) break; resultSets.add(next); &#125; return (Result[]) resultSets.toArray(new Result[resultSets.size()]); &#125; private void sort(List&lt;Scan&gt; scanList) throws IOException &#123; Collections.sort(scanList, new ScanComparator()); for (int i = 1; i &lt; scanList.size(); i++) &#123; byte[] currentStartRow = ((Scan) scanList.get(i)).getStartRow(); byte[] lastStopRow = ((Scan) scanList.get(i - 1)).getStopRow(); if ((lastStopRow == null) || (lastStopRow.length == 0)) throw new IOException( "Scan has overlap, last scan's stopRow is null"); if ((currentStartRow == null) || (currentStartRow.length == 0)) &#123; throw new IOException( "Scan has overlap, current scan's startRow is null"); &#125; if (0 &gt;= Bytes.compareTo(lastStopRow, 0, lastStopRow.length, currentStartRow, 0, currentStartRow.length)) continue; throw new IOException( "Scan has overlap, current scan's startRow is smaller than last scan's stop row"); &#125; &#125; private static class ScanComparator implements Comparator&lt;Scan&gt; &#123; public int compare(Scan o1, Scan o2) &#123; if ((o1.getStartRow() == null) || (o1.getStartRow().length == 0)) return -1; if ((o2.getStartRow() == null) || (o2.getStartRow().length == 0)) &#123; return 1; &#125; byte[] o1StartRow = o1.getStartRow(); byte[] o2StartRow = o2.getStartRow(); return Bytes.compareTo(o1StartRow, 0, o1StartRow.length, o2StartRow, 0, o2StartRow.length); &#125; &#125; private class ParallelScannerWorkerThread extends Thread implements Stoppable &#123; private ResultScanner scanner; private BlockingQueue&lt;Result&gt; results; private Object empty; private AtomicBoolean stopped; private Exception exception; protected ParallelScannerWorkerThread(Scan scan) throws IOException &#123; this.scanner = ParallelClientScanner.this.table.getScanner(scan); this.results = new ArrayBlockingQueue&lt;Result&gt;( ParallelClientScanner.this.resultQueueSize); this.empty = new Object(); this.stopped = new AtomicBoolean(false); &#125; public Result next() throws IOException &#123; Result r = (Result) this.results.poll(); while ((!this.stopped.get()) &amp;&amp; (r == null)) &#123; try &#123; synchronized (this.empty) &#123; this.empty.wait(); &#125; r = (Result) this.results.poll(); &#125; catch (InterruptedException e) &#123; throw ((IOException) (IOException) new IOException() .initCause(e)); &#125; &#125; return r; &#125; public void stop(String why) &#123; this.stopped.compareAndSet(false, true); &#125; public boolean isStopped() &#123; return this.stopped.get(); &#125; public boolean isError() &#123; return this.exception != null; &#125; public Exception getException() &#123; return this.exception; &#125; public void run() &#123; try &#123; Result r = this.scanner.next(); while (r != null) &#123; boolean added = false; while (!added) &#123; added = this.results.offer(r, SCANNER_WAIT_TIME, TimeUnit.MILLISECONDS); &#125; synchronized (this.empty) &#123; this.empty.notify(); &#125; r = this.scanner.next(); &#125; &#125; catch (IOException ioe) &#123; this.exception = ioe; &#125; catch (InterruptedException ite) &#123; this.exception = ite; &#125; finally &#123; this.scanner.close(); this.stopped.compareAndSet(false, true); synchronized (this.empty) &#123; this.empty.notify(); &#125; &#125; &#125; &#125;&#125; 然后，需要对Scan做些修改： 作为一个新特性，你需要修改SCAN_VERSION值 增加parallel属性，用于判断是否需要并行扫描 你需要修改序列化和反序列化方法，加上parallel的值 修改HTable中原来的public ResultScanner getScanner(final Scan scan)方法 修改SCAN_VERSION版本值为3，原来值为2： 1private static final byte SCAN_VERSION = (byte)3; 在合适地方添加一个parallel属性和get/set方法： 123456789this.parallel = scan.isParallel();private boolean isParallel() &#123; return parallel;&#125;public void setParallel(boolean parallel) &#123; this.parallel = parallel;&#125; 在public Scan(Scan scan)构造方法中初始化parallel属性： 123456789101112131415161718192021222324252627public Scan(Scan scan) throws IOException &#123; startRow = scan.getStartRow(); stopRow = scan.getStopRow(); maxVersions = scan.getMaxVersions(); batch = scan.getBatch(); caching = scan.getCaching(); cacheBlocks = scan.getCacheBlocks(); filter = scan.getFilter(); // clone? this.parallel = scan.isParallel(); // 初始化parallel属性 TimeRange ctr = scan.getTimeRange(); tr = new TimeRange(ctr.getMin(), ctr.getMax()); Map&lt;byte[], NavigableSet&lt;byte[]&gt;&gt; fams = scan.getFamilyMap(); for (Map.Entry&lt;byte[],NavigableSet&lt;byte[]&gt;&gt; entry : fams.entrySet()) &#123; byte [] fam = entry.getKey(); NavigableSet&lt;byte[]&gt; cols = entry.getValue(); if (cols != null &amp;&amp; cols.size() &gt; 0) &#123; for (byte[] col : cols) &#123; addColumn(fam, col); &#125; &#125; else &#123; addFamily(fam); &#125; &#125; for (Map.Entry&lt;String, byte[]&gt; attr : scan.getAttributesMap().entrySet()) &#123; setAttribute(attr.getKey(), attr.getValue()); &#125; &#125; 在public Map&lt;String, Object&gt; toMap(int maxCols)中添加上parallel属性： 1map.put("parallel", Boolean.valueOf(this.parallel)); 修改readFields方法，在最后添加代码： 123if (version &gt; 2) &#123; this.parallel = in.readBoolean();&#125; 修改write方法，在最后添加代码： 1out.writeBoolean(this.parallel); 修改HTable中原来的public ResultScanner getScanner(final Scan scan)方法如下： 123456public ResultScanner getScanner(final Scan scan) throws IOException &#123; if (scan.getCaching() &lt;= 0) &#123; scan.setCaching(getScannerCaching()); &#125; return scan.getParallel() ? new ParallelClientScanner(this, scan, getStartKeysInRange(scan.getStartRow(), scan.getStopRow())) : new ClientScanner(getConfiguration(), scan, getTableName(), this.connection);&#125; 最后，如果你想再HBase shell中使用该特性，你要做如下修改： 修改src/main/ruby/hbase.rb，在SELECT = &quot;SELECT&quot;下面添加代码一行代码： 1234COLUMN_INTERPRETER="COLUMN_INTERPRETER"KEY = "KEY"SELECT = "SELECT"PARALLEL = "PARALLEL" 修改src/main/ruby/hbase/table.rb，在raw = args[&quot;RAW&quot;] || false下面添加一行代码： 1parallel = args["PARALLEL"] || false 在scan.setRaw(raw)下面添加一行代码： 1234scan.setMaxVersions(versions) if versions &gt; 1scan.setTimeRange(timerange[0], timerange[1]) if timerangescan.setRaw(raw)scan.setParallel(parallel) 修改完之后，编译源代码，就可以进行测试了。 测试 通过hbase shell测试 运行hbase shell进行测试： 1234567hbase(main):003:0&gt; scan 't',&#123;PARALLEL=&gt;true&#125;ROW COLUMN+CELL 1 column=f:id, timestamp=1382528597662, value=1 2 column=f:id, timestamp=1382528594343, value=2 3 column=f:id, timestamp=1382528478893, value=3 4 column=f:id, timestamp=1382528483161, value=44 row(s) in 0.0240 seconds 通过hbase client测试 创建ParallelScannerTest.java类并添加如下代码： 1234567891011121314151617181920212223242526272829303132import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.client.HTable;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.client.ResultScanner;import org.apache.hadoop.hbase.client.Scan;import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;import org.apache.hadoop.hbase.util.Bytes;public class ParallelScannerTest &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = HBaseConfiguration.create(); HTable table = new HTable(conf, "t"); String startKey = "1"; String stopKey = "3"; Scan scan = new Scan(Bytes.toBytes(startKey), Bytes.toBytes(stopKey)); int count = 0; scan.setParallel(true); ResultScanner scanner = table.getScanner(scan); Result r = scanner.next(); while (r != null) &#123; count++; r = scanner.next(); &#125; System.out.println("++ Scanning finished with count : " + count + " ++"); scanner.close(); table.close(); &#125;&#125; 运行该类并查看输出结果。 你可以在hbase-site.xml中修改线程数大小和队列大小，下面两个参数： hbase.parallel.scanner.queue.size hbase.parallel.scanner.thread.count 以上源代码及所做的修改我已提交到我github仓库上hbase的cdh4-0.94.15_4.7.0分支，见提交日志add ParallelClientScanner。]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive中的FetchTask任务]]></title>
    <url>%2F2014%2F06%2F09%2Ffetchtask-in-hive%2F</url>
    <content type="text"><![CDATA[Hive中有各种各样的Task任务，其中FetchTask算是最简单的一种了。FetchTask不同于MapReduce任务，它不会启动mapreduce，而是直接读取文件，输出结果。当你执行简单的select * with limit语句的时候，其不会运行mapreduce任务。 例如，运行下面语句不会出现mapreduce任务（说明：t表有一个字段，id为int类型，该表没有数据）： 123hive&gt; select * from t limit 1; OKTime taken: 2.466 seconds 去掉limit语句，再执行一次，结果如下： 123hive&gt; select * from t ; OKTime taken: 0.097 seconds 从结果来看，这种查询应该是有个默认的limit限制吧。 如果修改查询语句，只查询某一些列呢？ 1234567891011121314151617hive&gt; select id from t ; Total MapReduce jobs = 1Launching Job 1 out of 1Number of reduce tasks is set to 0 since there is no reduce operatorStarting Job = job_1402248601715_0004, Tracking URL = http://cdh1:8088/proxy/application_1402248601715_0004/Kill Command = /usr/lib/hadoop/bin/hadoop job -kill job_1402248601715_0004Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02014-06-09 11:12:54,817 Stage-1 map = 0%, reduce = 0%2014-06-09 11:13:15,790 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 2.96 sec2014-06-09 11:13:16,982 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 2.96 secMapReduce Total cumulative CPU time: 2 seconds 960 msecEnded Job = job_1402248601715_0004MapReduce Jobs Launched: Job 0: Map: 1 Cumulative CPU: 2.96 sec HDFS Read: 257 HDFS Write: 0 SUCCESSTotal MapReduce CPU Time Spent: 2 seconds 960 msecOKTime taken: 51.496 seconds 查看上面运行日志，可以看到该次查询启动了mapreduce任务，mapper数为1，没有reducer任务。有没有一种方法，让上面语句也不允许mapreduce任务呢？ 答案是肯定的！这就要用到 hive.fetch.task.conversion 参数： 12345678910111213&lt;property&gt; &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt; &lt;value&gt;minimal&lt;/value&gt; &lt;description&gt; Some select queries can be converted to single FETCH task minimizing latency.Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incurrs RS), lateral views and joins. 1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only 2. more : SELECT, FILTER, LIMIT only (+TABLESAMPLE, virtual columns) &lt;/description&gt;&lt;/property&gt; 该参数默认值为minimal，表示运行“select * ”并带有limit查询时候，会将其转换为FetchTask；如果参数值为more，则select某一些列并带有limit条件时，也会将其转换为FetchTask任务。当然，还有前天条件：单一数据源，即输入来源一个表或者分区；没有子查询；没有聚合运算和distinct；不能用于视图和join。 测试一下，先讲其参数值设为more，再运行： 1234567hive&gt; set hive.fetch.task.conversion=more;hive&gt; select id from t limit 1; OKTime taken: 0.242 secondshive&gt; select id from t ; OKTime taken: 0.496 seconds 最后，在hive源码中搜索一下hive.fetch.task.conversion，可以找到下面代码(来自SimpleFetchOptimizer类): 1234567891011121314151617181920// returns non-null FetchTask instance when succeeded @SuppressWarnings("unchecked") private FetchTask optimize(ParseContext pctx, String alias, TableScanOperator source) throws HiveException &#123; String mode = HiveConf.getVar( pctx.getConf(), HiveConf.ConfVars.HIVEFETCHTASKCONVERSION); boolean aggressive = "more".equals(mode); FetchData fetch = checkTree(aggressive, pctx, alias, source); if (fetch != null) &#123; int limit = pctx.getQB().getParseInfo().getOuterQueryLimit(); FetchWork fetchWork = fetch.convertToWork(); FetchTask fetchTask = (FetchTask) TaskFactory.get(fetchWork, pctx.getConf()); fetchWork.setSink(fetch.completed(pctx, fetchWork)); fetchWork.setSource(source); fetchWork.setLimit(limit); return fetchTask; &#125; return null; &#125; 从源码中，简单分析可以知道，hive优化器在做FetchTask优化的时候，如果hive.fetch.task.conversion为more，则会做一些优化。]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive中数据的加载和导出]]></title>
    <url>%2F2014%2F06%2F09%2Fhive-data-manipulation-language%2F</url>
    <content type="text"><![CDATA[关于 Hive DML 语法，你可以参考 apache 官方文档的说明:Hive Data Manipulation Language。 apache的hive版本现在应该是 0.13.0，而我使用的 hadoop 版本是 CDH5.0.1，其对应的 hive 版本是 0.12.0。故只能参考apache官方文档来看 cdh5.0.1 实现了哪些特性。 因为 hive 版本会持续升级，故本篇文章不一定会和最新版本保持一致。 1. 准备测试数据首先创建普通表： 1create table test(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE; 创建分区表： 1234567CREATE EXTERNAL TABLE test_p(id int,name string )partitioned by (date STRING)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\,' LINES TERMINATED BY '\n' STORED AS TEXTFILE; 准备数据文件： 12345[/tmp]# cat test.txt 1,a2,b3,c4,d 2.加载数据语法如下： 1LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] 说明： filepath 可能是： 一个相对路径 一个绝对路径，例如：/root/project/data1 一个url地址，可选的可以带上授权信息，例如：hdfs://namenode:9000/user/hive/project/data1 目标可能是一个表或者分区，如果该表是分区，则必须制定分区列。 filepath 可以是一个文件也可以是目录 如果指定了 LOCAL，则： load 命令会在本地查找 filepath。如果 filepath 是相对路径，则相对于当前路径，也可以指定一个 url 或者本地文件，例如：file:///user/hive/project/data1 如果没有指定 LOCAL ，则hive会使用全路径的url，url 中如果没有制定 schema，则默认使用 fs.default.name的值；如果该路径不是绝对路径，则会相对于 /user/&lt;username&gt; 如果使用 OVERWRITE ，则会删除原来的数据，然后导入新的数据，否则，就是追加数据。 需要注意的： filepath 中不能包括子目录 如果没有指定 LOCAL，则 filepath 指向目标表或者分区所在的文件系统。 如果需要压缩，则参考 CompressedStorage 2.1 测试2.1.1 加载本地文件a) 加载到普通表中 1234567hive&gt; load data local inpath '/tmp/test.txt' into table test; Copying data from file:/tmp/test.txtCopying file: file:/tmp/test.txtLoading data to table default.testTable default.test stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 16, raw_data_size: 0]OKTime taken: 0.572 seconds 查看hdfs上的数据： 123$ hadoop fs -ls /user/hive/warehouse/testFound 1 items-rwxrwxrwt 3 hive hadoop 16 2014-06-09 18:36 /user/hive/warehouse/test/test.txt 查看表中数据： 1234567hive&gt; select * from test;OK1 a2 b3 c4 dTime taken: 0.562 seconds, Fetched: 4 row(s) b) 加载文件到分区表 通常是直接使用 load 命令加载： 1LOAD DATA LOCAL INPATH &quot;/tmp/test.txt&quot; INTO TABLE test_p PARTITION (date=20140722) 注意：如果没有加上 overwrite 关键字，则加载相同文件最后会存在多个文件 还有一种方法是：创建分区目录，手动上传文件，最后再添加新的分区，代码如下： 12345hadoop fs -mkdir /user/hive/warehouse/test/date=20140320ALTER TABLE test_p ADD IF NOT EXISTS PARTITION (date=20140320);hive hadoop fs -rm /user/hive/warehouse/test/date=20140320/test.txthadoop fs -put /tmp/test.txt /user/hive/warehouse/test/date=20140320 同样，你也可以查看 hdfs 和表中的数据。 2.1.2 加载hdfs上的文件拷贝 test.txt 为test_1.txt 并将其上传到 /user/hive/warehouse: 12$ cp test.txt test_1.txt$ sudo -u hive hadoop fs -put test_1.txt /user/hive/warehouse 然后将 /user/hive/warehouse/test_1.txt 导入到test表中： 12345hive&gt; load data inpath '/user/hive/warehouse/test_1.txt' into table test; Loading data to table default.testTable default.test stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 16, raw_data_size: 0]OKTime taken: 2.941 seconds 查看hdfs上的数据： 1234$ hadoop fs -ls /user/hive/warehouse/testFound 2 items-rwxr-xr-x 3 hive hadoop 16 2014-06-09 18:48 /user/hive/warehouse/test/test.txt-rwxr-xr-x 3 hive hadoop 16 2014-06-09 18:45 /user/hive/warehouse/test/test_1.txt 查看表中数据： 1234567891011hive&gt; select * from test; OK1 a2 b3 c4 d1 a2 b3 c4 dTime taken: 0.302 seconds, Fetched: 8 row(s) 3. 插入数据标准语法： 123INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...) [IF NOT EXISTS]] select_statement1 FROM from_statement;INSERT INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement; 扩展语法（多个insert）： 123456789FROM from_statementINSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...) [IF NOT EXISTS]] select_statement1[INSERT OVERWRITE TABLE tablename2 [PARTITION ... [IF NOT EXISTS]] select_statement2][INSERT INTO TABLE tablename2 [PARTITION ...] select_statement2] ...;FROM from_statementINSERT INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1[INSERT INTO TABLE tablename2 [PARTITION ...] select_statement2][INSERT OVERWRITE TABLE tablename2 [PARTITION ... [IF NOT EXISTS]] select_statement2] ...; 扩展语法（动态分区insert）： 123INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement;INSERT INTO TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement; 说明： INSERT OVERWRITE 会覆盖存在的数据 输出的格式和序列化类取决于表的元数据 hive 0.13.0之后，select语句可以使用 CTEs 表达式，语法请参考 SELECT syntax，示例见 Common Table Expression Dynamic Partition Inserts dynamic partition inserts在hive 0.6.0中引入。相关的配置参数有： 123456hive.exec.dynamic.partitionhive.exec.dynamic.partition.modehive.exec.max.dynamic.partitions.pernodehive.exec.max.dynamic.partitionshive.exec.max.created.fileshive.error.on.empty.partition 一个示例： 123FROM page_view_stg pvsINSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country) SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip, pvs.cnt 4. 导出数据标准语法： 123INSERT OVERWRITE [LOCAL] DIRECTORY directory1 [ROW FORMAT row_format] [STORED AS file_format] (Note: Only available starting with Hive 0.11.0) SELECT ... FROM ... 扩展语法（多个insert）： 123FROM from_statementINSERT OVERWRITE [LOCAL] DIRECTORY directory1 select_statement1[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2] ... row_format相关语法： 123DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] [NULL DEFINED AS char](Note: Only available starting with Hive 0.13) 说明： Directory 可以是一个全路径的 url。 如果指定 LOCAL，则会将数据写到本地文件系统。 输出的数据序列化为 text 格式，分隔符为 ^A，行于行之间通过换行符连接。如果存在不是基本类型的列，则这些列将被序列化为 JSON 格式。 在 Hive 0.11.0 可以输出字段的分隔符，之前版本的默认为 ^A。 4.1 测试;4.1.1 导出到本地文件系统12345678910111213141516171819hive&gt; insert overwrite local directory &apos;/tmp/test&apos; select * from test;Total MapReduce jobs = 1Launching Job 1 out of 1Number of reduce tasks is set to 0 since there&apos;s no reduce operatorStarting Job = job_1402248601715_0016, Tracking URL = http://cdh1:8088/proxy/application_1402248601715_0016/Kill Command = /usr/lib/hadoop/bin/hadoop job -kill job_1402248601715_0016Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02014-06-09 19:25:12,896 Stage-1 map = 0%, reduce = 0%2014-06-09 19:25:20,380 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 0.99 sec2014-06-09 19:25:21,433 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 0.99 secMapReduce Total cumulative CPU time: 990 msecEnded Job = job_1402248601715_0016Copying data to local directory /tmp/testCopying data to local directory /tmp/testMapReduce Jobs Launched: Job 0: Map: 1 Cumulative CPU: 0.99 sec HDFS Read: 305 HDFS Write: 32 SUCCESSTotal MapReduce CPU Time Spent: 990 msecOKTime taken: 18.438 seconds 导出后的数据预览如下： 123456789[/tmp]# vim test/000000_0 1^Aa2^Ab3^Ac4^Ad1^Aa2^Ab3^Ac4^Ad 可以看到数据中的列与列之间的分隔符是^A(ascii码是\00001)，如果想修改分隔符，可以做如下修改： 1hive&gt; insert overwrite local directory '/tmp/test' row format delimited fields terminated by ',' select * from test; 再来查看数据： 123456789vim test/000000_3 1,a2,b3,c4,d1,a2,b3,c4,d 4.1.2 导出到 HDFS 中1hive&gt; insert overwrite directory &apos;/user/hive/tmp&apos; select * from test; 注意： 和导出文件到本地文件系统的HQL少一个local，数据的存放路径不一样了。 4.1.3 导出到Hive的另一个表中在实际情况中，表的输出结果可能太多，不适于显示在控制台上，这时候，将Hive的查询输出结果直接存在一个新的表中是非常方便的，我们称这种情况为CTAS（ create table .. as select）如下： 1hive&gt; create table test2 as select * from test;]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Scrapy爬取知乎网站]]></title>
    <url>%2F2014%2F06%2F08%2Fusing-scrapy-to-cralw-zhihu%2F</url>
    <content type="text"><![CDATA[本文主要记录使用使用 Scrapy 登录并爬取知乎网站的思路。Scrapy的相关介绍请参考 使用Scrapy抓取数据。 相关代码，见 https://github.com/javachen/scrapy-zhihu-github ，在阅读这部分代码之前，请先了解 Scrapy 的一些基本用法。 使用cookie模拟登陆关于 cookie 的介绍和如何使用 python 实现模拟登陆，请参考python爬虫实践之模拟登录。 从这篇文章你可以学习到如何获取一个网站的 cookie 信息。下面所讲述的方法就是使用 cookie 来模拟登陆知乎网站并爬取用户信息。 一个模拟登陆知乎网站的示例代码如下： 12345678910111213141516171819202122232425262728293031323334353637# -*- coding:utf-8 -*-from scrapy.selector import Selectorfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractorfrom scrapy.contrib.spiders import CrawlSpider, Rulefrom scrapy.http import Request,FormRequestfrom zhihu.settings import *class ZhihuLoginSpider(CrawlSpider): name = 'zhihulogin1' allowed_domains = ['zhihu.com'] start_urls = ['http://www.zhihu.com/lookup/class/'] rules = ( Rule(SgmlLinkExtractor(allow=r'search/')), Rule(SgmlLinkExtractor(allow=r'')), ) def __init__(self): self.headers =HEADER self.cookies =COOKIES def start_requests(self): for i, url in enumerate(self.start_urls): yield FormRequest(url, meta = &#123;'cookiejar': i&#125;, \ headers = self.headers, \ cookies =self.cookies, callback = self.parse_item)#jump to login page def parse_item(self, response): selector = Selector(response) urls = [] for ele in selector.xpath('//ul/li[@class="suggest-item"]/div/a/@href').extract(): urls.append(ele) print urls 上面是一个简单的示例，重写了 start_requests 方法，针对 start_urls 中的每一个url，这里为 http://www.zhihu.com/lookup/class/，重新创建 FormRequest 请求该 url，并设置 headers 和 cookies 两个参数，这样可以通过 cookies 伪造登陆。 FormRequest 请求中有一个回调函数 parse_item 用于解析页面内容。 HEADER 和 COOKIES 在 settings.py 中定义如下： 12345678910111213141516171819202122232425HEADER=&#123; &quot;Host&quot;: &quot;www.zhihu.com&quot;, &quot;Connection&quot;: &quot;keep-alive&quot;, &quot;Cache-Control&quot;: &quot;max-age=0&quot;, &quot;Accept&quot;: &quot;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&quot;, &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.131 Safari/537.36&quot;, &quot;Referer&quot;: &quot;http://www.zhihu.com/people/raymond-wang&quot;, &quot;Accept-Encoding&quot;: &quot;gzip,deflate,sdch&quot;, &quot;Accept-Language&quot;: &quot;zh-CN,zh;q=0.8,en-US;q=0.6,en;q=0.4,zh-TW;q=0.2&quot;, &#125;COOKIES=&#123; &apos;checkcode&apos;:r&apos;&quot;$2a$10$9FVE.1nXJKq/F.nH62OhCevrCqs4skby2bC4IO6VPJITlc7Sh.NZa&quot;&apos;, &apos;c_c&apos;:r&apos;a153f80493f411e3801452540a3121f7&apos;, &apos;_ga&apos;:r&apos;GA1.2.1063404131.1384259893&apos;, &apos;zata&apos;:r&apos;zhihu.com.021715f934634a988abbd3f1f7f31f37.470330&apos;, &apos;q_c1&apos;:r&apos;59c45c60a48d4a5f9a12a52028a9aee7|1400081868000|1400081868000&apos;, &apos;_xsrf&apos;:r&apos;2a7cf7208bf24dbda3f70d953e948135&apos;, &apos;q_c0&apos;:r&apos;&quot;NmE0NzBjZTdmZGI4Yzg3ZWE0NjhkNjkwZGNiZTNiN2F8V2FhRTQ1QklrRjNjNGhMdQ==|1400082425|a801fc83ab07cb92236a75c87de58dcf3fa15cff&quot;&apos;, &apos;__utma&apos;:r&apos;51854390.1063404131.1384259893.1400518549.1400522270.5&apos;, &apos;__utmb&apos;:r&apos;51854390.4.10.1400522270&apos;, &apos;__utmc&apos;:r&apos;51854390&apos;, &apos;__utmz&apos;:r&apos;51854390.1400513283.3.3.utmcsr=zhihu.com|utmccn=(referral)|utmcmd=referral|utmcct=/people/hallson&apos;, &apos;__utmv&apos;:r&apos;51854390.100-1|2=registration_date=20121016=1^3=entry_date=20121016=1&apos;&#125; 这两个参数你都可以通过浏览器的一些开发工具查看到，特别是 COOKIES 中的信息。 通过账号登陆使用账户和密码进行登陆代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# -*- coding:utf-8 -*-from scrapy.selector import Selectorfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractorfrom scrapy.contrib.spiders import CrawlSpider, Rulefrom scrapy.http import Request,FormRequestimport sysreload(sys)sys.setdefaultencoding('utf-8')host='http://www.zhihu.com'class ZhihuUserSpider(CrawlSpider): name = 'zhihu_user' allowed_domains = ['zhihu.com'] start_urls = ["http://www.zhihu.com/lookup/people",] #使用rule时候，不要定义parse方法 rules = ( Rule(SgmlLinkExtractor(allow=("/lookup/class/[^/]+/?$", )), follow=True,callback='parse_item'), Rule(SgmlLinkExtractor(allow=("/lookup/class/$", )), follow=True,callback='parse_item'), Rule(SgmlLinkExtractor(allow=("/lookup/people", )), callback='parse_item'), ) def __init__(self, *a, **kwargs): super(ZhihuLoginSpider, self).__init__(*a, **kwargs) def start_requests(self): return [FormRequest( "http://www.zhihu.com/login", formdata = &#123;'email':'XXXXXX', 'password':'XXXXXX' &#125;, callback = self.after_login )] def after_login(self, response): for url in self.start_urls: yield self.make_requests_from_url(url) def parse_item(self, response): selector = Selector(response) for link in selector.xpath('//div[@id="suggest-list-wrap"]/ul/li/div/a/@href').extract(): #link ===&gt; /people/javachen yield Request(host+link+"/about", callback=self.parse_user) def parse_user(self, response): selector = Selector(response) user = ZhihuUserItem() user['_id']=user['username']=response.url.split('/')[-2] user['url']= response.url user['nickname'] = ''.join(selector.xpath("//div[@class='title-section ellipsis']/a[@class='name']/text()").extract()) user['location'] = ''.join(selector.xpath("//span[@class='location item']/@title").extract()) user['industry'] = ''.join(selector.xpath("//span[@class='business item']/@title").extract()) user['sex'] = ''.join(selector.xpath('//div[@class="item editable-group"]/span/span[@class="item"]/i/@class').extract()).replace("zg-icon gender ","") user['description'] = ''.join(selector.xpath("//span[@class='description unfold-item']/span/text()").extract()).strip().replace("\n",'') user['view_num'] = ''.join(selector.xpath("//span[@class='zg-gray-normal']/strong/text()").extract()) user['update_time'] = str(datetime.now()) #抓取用户信息，此处省略代码 该代码逻辑如下： 重写 start_requests 方法，通过设置 FormRequest 的 formdata 参数，这里是 email 和 password，然后提交请求到 http://www.zhihu.com/login进行登陆，如果登陆成功之后，调用 after_login 回调方法。 在 after_login 方法中，一个个访问 start_urls 中的 url rules 中定义了一些正则匹配的 url 所对应的回调函数 在 parse_user 方法里，你可以通过 xpath 获取到用户的相关信息，也可以去获取关注和粉丝列表的数据。 例如，先获取到用户的关注数 followee_num，就可以通过下面一段代码去获取该用户所有的关注列表。代码如下 12345678910_xsrf = ''.join(selector.xpath('//input[@name="_xsrf"]/@value').extract())hash_id = ''.join(selector.xpath('//div[@class="zm-profile-header-op-btns clearfix"]/button/@data-id').extract())num = int(followee_num) if followee_num else 0page_num = num/20page_num += 1 if num%20 else 0for i in xrange(page_num): params = json.dumps(&#123;"hash_id":hash_id,"order_by":"created","offset":i*20&#125;) payload = &#123;"method":"next", "params": params, "_xsrf":_xsrf&#125; yield Request("http://www.zhihu.com/node/ProfileFolloweesListV2?"+urlencode(payload), callback=self.parse_follow_url) 然后，你需要增加一个处理关注列表的回调方法 parse_follow_url，这部分代码如下： 1234567891011def parse_follow_url(self, response): selector = Selector(response) for link in selector.xpath('//div[@class="zm-list-content-medium"]/h2/a/@href').extract(): #link ===&gt; http://www.zhihu.com/people/peng-leslie-97 username_tmp = link.split('/')[-1] if username_tmp in self.user_names: print 'GET:' + '%s' % username_tmp continue yield Request(link+"/about", callback=self.parse_user) 获取粉丝列表的代码和上面代码类似。 有了用户数据之后，你可以再编写一个爬虫根据用户去爬取问题和答案了，这部分代码略去，详细内容请参考 https://github.com/javachen/scrapy-zhihu-github。其中，还有抓取 github 用户等的相关代码。 其他一些技巧在使用 xpath 过程中，你可以下载浏览器插件 XPath Helper来快速定位元素并获取到 xpath 表达式，关于该插件用法，请自行 google 之。 由于隐私设置的缘故，有些用户可能没有显示一些数据，故针对某些用户 xpath 表达式可能会抛出一些异常，如下面代码获取用户的名称： 1user['nickname'] = selector.xpath("//div[@class='title-section ellipsis']/a[@class='name']/text()").extract()[0] 你可以将上面代码修改如下，以避免出现一个异常： 1user['nickname'] = ''.join(selector.xpath("//div[@class='title-section ellipsis']/a[@class='name']/text()").extract())]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB介绍]]></title>
    <url>%2F2014%2F06%2F06%2Fabout-mongodb%2F</url>
    <content type="text"><![CDATA[MongoDB 是一个开源的，高性能，无模式（或者说是模式自由），使用 C++ 语言编写的面向文档的数据库。正因为 MongoDB 是面向文档的，所以它可以管理类似 JSON 的文档集合。又因为数据可以被嵌套到复杂的体系中并保持可以查询可索引，这样一来，应用程序便可以以一种更加自然的方式来为数据建模。 官方网站：http://www.mongodb.org/ MongoDB介绍所谓“面向集合”（Collenction-Orented），意思是数据被分组存储在数据集中，被称为一个集合（Collenction)。每个集合在数据库中都有一个唯一的标识名，并且可以包含无限数目的文档。集合的概念类似关系型数据库（RDBMS）里的表（table），不同的是它不需要定义任何模式（schema)。 模式自由（schema-free)，意味着对于存储在mongodb数据库中的文件，我们不需要知道它的任何结构定义。如果需要的话，你完全可以把不同结构的文件存储在同一个数据库里。 存储在集合中的文档，被存储为键-值对的形式。键用于唯一标识一个文档，为字符串类型，而值则可以是各中复杂的文件类型。我们称这种存储形式为BSON（Binary Serialized dOcument Format）。 MongoDB 把数据存储在文件中（默认路径为：/data/db），为提高效率使用内存映射文件进行管理。MongoDB的主要目标是在键/值存储方式（提供了高性能和高度伸缩性）以及传统的 RDBMS 系统（丰富的功能）架起一座桥梁，集两者的优势于一身。 根据官方网站的描述，Mongo 适合用于以下场景： 网站数据：Mongo非常适合实时的插入，更新与查询，并具备网站实时数据存储所需的复制及高度伸缩性。 缓存：由于性能很高，Mongo也适合作为信息基础设施的缓存层。在系统重启之后，由Mongo搭建的持久化缓存层可以避免下层的数据源过载。 大尺寸，低价值的数据：使用传统的关系型数据库存储一些数据时可能会比较昂贵，在此之前，很多时候程序员往往会选择传统的文件进行存储。 高伸缩性的场景：Mongo非常适合由数十或数百台服务器组成的数据库。Mongo的路线图中已经包含对MapReduce引擎的内置支持。 用于对象及JSON数据的存储：Mongo的BSON数据格式非常适合文档化格式的存储及查询。 自然，MongoDB 的使用也会有一些限制，例如它不适合： 高度事务性的系统：例如银行或会计系统。传统的关系型数据库目前还是更适用于需要大量原子性复杂事务的应用程序。 传统的商业智能应用：针对特定问题的BI数据库会对产生高度优化的查询方式。对于此类应用，数据仓库可能是更合适的选择。 需要SQL的问题 MongoDB的安装mongodb的官网就有下载，根据系统windows还是linux还是别的下载32位还是64位的，然后解压安装。 如果你使用的时mac系统，你可以通过 brewhome 来安装： 1$ brew install mongodb MongoDB的管理解压完之后进入bin目录，里面都是一些可执行文件，mongo，mongod，mongodump，mongoexport等。 1).启动和停止MongoDB 通过执行mongod来启动MongoDB服务器: 1$ mongod mongod有很多的配置启动选项的，可以通过mongod --help来查看，其中有一些主要的选项： --dbpath：指定数据目录，默认是/data/db/。每个mongod进程都需要独立的目录，启动mongod时就会在数据目录中创建mongod.lock文件，防止其他mongod进程使用该数据目录。 --port：指定服务器监听的端口，默认是27017。 --fork：以守护进程的方式运行MongoDB。 --logpath：指定日志输出路径，如果不指定则会在终端输出。每次启动都会覆盖原来的日志，如果不想覆盖就要用--logappend选项。 --config：指定配置文件，加载命令行未指定的各种选项。我们可以讲我们需要用到的选项写在某个文件中，然后通过该选项来指定这个文件就不必每次启动mongod时都要写。 2).数据备份 a. 数据文件备份 最简单的备份就是数据文件的备份，就是直接赋值data/db这个目录，因为我们前面已经指定了数据目录就是data/db，那么MongoDB多有的数据都在这里，但是有个问题就是最新的数据还在缓存中，没用同步到磁盘，可以先停止shutdownServer()再备份。但是这样会影响MongoDB正常的工作。 b. mongodump和mongorestore bin 中还有 mongodump 和 mongorestore 两个可执行文件，这个是对MongoDB的某个数据库进行备份，可以在MongoDB正在运行时进行备份，比如备份test数据库，然后将备份的数据库文件再倒入别的MongoDB服务器上。这种备份的方式备份的不是最新数据，只是已经写入MongoDB中的数据，可能还有的数据在缓存中没有写入，那么这部分数据就是备份不到的。mongodump和mongorestore也可以通过--help查询所有选项。 123$ mongodump -d text -o test_dbconnected to: 127.0.0.12014-06-06T12:54:59.741+0800 DATABASE: text to test_db/text -d是指定数据库，-o是输出备份文件，上面将 test 数据库备份为 test_db 文件。 1$ mongorestore --port 27017 -d temple --drop test_db/test/ 这里将上面备份出来的 test 数据库现在重新导入到 temple 数据库，--drop代表如果有了 temple 数据库则将其中的所有集合删除，不指定就会和原来 temple 中的集合合并。 c. mongoexport和mongoimport 备份某个数据库中的某个表，同样可以通过--help来查看所有的选项，当然mongoexport也是可以不统计的备份，但是却不一定是最新数据。 12$ mongoexport --port 27017 -d zhihu -c zh_user -o zh_user $ mongoimport --port 9352 -d temple -c user zh_user -c表示 collection 集合，上面将 zhihu 库中的 zh_user 集合备份出来为 zh_user 文件，然后再导入到 temple 库中的user集合。 d. sync和锁 mongodump和mongoexport对都可以不停MongoDB服务器来进行备份，但是却失去了获取实时数据的能力，而fsync命令也能在不停MongoDB的情况下备份实时数据，它的实现其实就是上锁，阻止对数据库的写入操作，然后将缓冲区的数据写入磁盘，备份后再解锁。在这个期间，任何对数据库的写入操作都会阻塞，直到锁被释放。 1234567891011&gt; db.runCommand(&#123;&quot;fsync&quot; : 1, &quot;lock&quot; : 1&#125;)&#123; &quot;info&quot; : &quot;now locked against writes, use db.fsyncUnlock() to unlock&quot;, &quot;seeAlso&quot; : &quot;http://dochub.mongodb.org/core/fsynccommand&quot;, &quot;ok&quot; : 1&#125;&gt;&gt;在这之间进行备份，执行任何insert操作都会阻塞&gt;&gt; db.fsyncUnlock()&#123; &quot;ok&quot; : 1, &quot;info&quot; : &quot;unlock completed&quot; &#125; 数据类型MongoDB的文件存储格式为BSON,同JSON一样支持往其它文档对象和数组中再插入文档对象和数组，同时扩展了JSON的数据类型.与数据库打交道的那些应用。例如，JSON没有日期类型，这会使得处理本来简单的日期问题变得非常繁琐。只有一种数字类型，没法区分浮点数和整数，更不能区分32位和64位数字。也没有办法表示其他常用类型，如正则表达式或函数。 下面是MongoDB的支持的数据类型： null null用于表示空值或者不存在的字段。 {&quot;x&quot;:null} 布尔 布尔类型有两个值’true’和’false1’。 {&quot;X&quot;:true} 32位整数 类型不可用。JavaScript仅支持64位浮点数，所以32位整数会被自动转换。 64位整数 不支持这个类型。shell会使用一个特殊的内嵌文档来显示64位整数。 64位浮点数 shell中的数字都是这种类型。下面的表示都是浮点数： {&quot;X&quot; : 3.1415926} {&quot;X&quot; : 3} 字符串 UTF-8字符串都可表示为字符串类型的数据： {&quot;x&quot; : &quot;foobar&quot;} 符号 不支持这种类型。shell将数据库里的符号类型转换成字符串。 对象id 对象id是文档的12字节的唯一ID：{&quot;_id&quot;:ObjectId() } 日期 日期类型存储的是从标准纪元开始的毫秒数。不存储时区： {&quot;X&quot; : new Date()} 正则表达式 文档中可以包含正则表达式，采用JavaScript的正则表达式语法：{&quot;x&quot;: /foobar/i} 代码 文档中还可以包含JavaScript代码：{&quot;x&quot;: function() { /* …… */ }} 二进制数据 二进制数据可以由任意字节的串组成。不过shell中无法使用。 最大值 BSON包括一个特殊类型，表示可能的最大值。shell中没有这个类型。 最小值 BSON包括一个特殊类型，表示可能的最小值。shell中没有这个类型。 未定义 文档中也可以使用未定义类型：{&quot;x&quot;:undefined} 数组 值的集合或者列表可以表示成数组：{&quot;x&quot; : [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]} 内嵌文档 文档可以包含别的文档，也可以作为值嵌入到父文档中，数据可以组织得更自然些，不用非得存成扁平结构的：{“x” : {“food” ： “noodle”}} JavaScript中只有一种“数字”类型。因为MongoDB中有3种数字类型（32位整数、64位整数和64位浮点数），shell必须绕过JavaScript的限制。默认情况下，shell中的数字都被MongoDB当做是双精度数。这意味着如果你从数据库中获得的是一个32位整数，修改文档后，将文档存回数据库的时候，这个整数也被转换成了浮点数，即便保持这个整数原封不动也会这样的。所以明智的做法是尽量不要在shell下覆盖整个文档。 由于MongoDB中不同文档的同一个key的value数据类型可以不同，当我们根据某个key查询的时候会发生不同数据类型之间的比较。所以MongoDB内部设定了数据类型间的大小，顺序如下： 1最小值&lt;null&lt;数字(32位整数、63位整形、64位浮点数)&lt;字符串&lt;对象/文档&lt;数组&lt;二进制数据&lt;对象ID&lt;布尔型&lt;日期型&lt;时间戳&lt;正则&lt;最大值 参考资料 [1] MongoDB 入门须知 [2] MongoDB入门学习(一)：MongoDB的安装和管理]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>nosql</tag>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不用Cloudera Manager安装Cloudera Search]]></title>
    <url>%2F2014%2F06%2F03%2Finstall_cloudera_search_without_cm%2F</url>
    <content type="text"><![CDATA[Cloudera Search 用来在 hadoop 基础上建立索引和全文检索，本文主要记录如何安装 CLoudera Search 的过程，其中也包括如何安装和启动 Zookeeper、Solr、MapReduce等工具和服务。 Cloudera Search介绍Cloudera Search 核心部件包括 Hadoop 和 Solr，后者建立在 Lucene 之上；而 Hadoop 也正是在06年正式成为 Lucene 的一个子项目而发展起来的。 通过 Tika, Cloudera Search 支持大量的被广泛使用的文件格式；除此之外，Cloudera Search 还支持很多其他在Hadoop应用中常用的数据，譬如 Avro, SequenceFile, 日志文件等。 用来建立索引和全文检索的数据可以是来自于 HDFS，譬如日志文件，Hive 或者 HBase 的表等等（通过集成 NGData 的 Lily 项目，对 HBasae 的支持工作也在进行中）。或者通过结合使用 Flume 采集于外部数据源，通过一个新支持的 Flume Sink 直接写到索引库里；同时还可以充分利用 Flume 来对要建立索引的数据进行各种预处理，譬如转换，提取创建元数据等。 建立的索引存储于 HDFS。这给搜索带来了易于扩展，冗余和容错的好处。此外，我们还可以运行 MapReduce 来对我们所需要检索的数据进行索引，提供给 Solr。 环境 操作系统：CentOs6.5 Hadoop：cdh5.3.0 安装 Hadoop集群这里使用参考 通过Cloudera Manager安装CDH一文搭建的集群，其中也包括了一个三节点的 ZooKeeper 集群。该集群包括三个节点： 123192.168.56.121 cdh1 NameNode、Hive、ResourceManager、HBase192.168.56.122 cdh2 DataNode、SSNameNode、NodeManager、HBase192.168.56.123 cdh3 DataNode、HBase、NodeManager 安装 ZooKeeperZookeeper 的安装过程，请参考 使用yum安装CDH Hadoop集群。 安装 SolrZookeeper 启动之后，需要安装 Solr，关于 Solr 的安装，可以参考 Apache SolrCloud安装。 在三个节点上安装 solr-server： 1$ sudo yum install solr-server solr solr-doc -y 安装 Spark Indexer： 1$ sudo yum install solr-crunch 安装 MapReduce Tools： 1$ sudo yum install solr-mapreduce 安装 Lily HBase Indexer： 1$ sudo yum install hbase-solr-indexer hbase-solr-doc 注意：Lily HBase Indexer和 cdh5 工作的时候，你需要在运行 MapReduce 任务之前运行下面命令：export HADOOP_CLASSPATH=&lt;Path to hbase-protocol-**.jar&gt; 配置 Solr修改 solr 配置文件 /etc/default/solr 中 ZooKeeper 连接地址： 1SOLR_ZK_ENSEMBLE=cdh1:2181,cdh2:2181,cdh3:2181/solr 修改 solr 配置文件 /etc/default/solr 中 HDFS 连接地址： 1SOLR_HDFS_HOME=hdfs://cdh1:8020/solr 设置 HDFS 配置文件目录： 1SOLR_HDFS_CONFIG=/etc/hadoop/conf 如果你配置了 Kerberos，则在 kerberos 服务器上为每个安装 solr 的节点先生成 solr 的凭证： 12345678kadmin: addprinc -randkey solr/cdh1@JAVACHEN.COMkadmin: xst -norandkey -k solr.keytab solr/cdh1kadmin: addprinc -randkey solr/cdh2@JAVACHEN.COMkadmin: xst -norandkey -k solr.keytab solr/cdh2kadmin: addprinc -randkey solr/cdh3@JAVACHEN.COMkadmin: xst -norandkey -k solr.keytab solr/cdh3 然后，将 solr.keytab 拷贝到 /etc/solr/conf： 1234$ sudo mv solr.keytab /etc/solr/conf/$ sudo chown solr:hadoop /etc/solr/conf/solr.keytab$ sudo chmod 400 /etc/solr/conf/solr.keytab 最后，修改每个安装了 solr 节点的 /etc/default/solr，例如在 cdh1节点上修改为： 123SOLR_KERBEROS_ENABLED=trueSOLR_KERBEROS_KEYTAB=/etc/solr/conf/solr.keytabSOLR_KERBEROS_PRINCIPAL=solr/cdh1@JAVACHEN.COM 在 HDFS 中创建 /solr 目录： 12$ sudo -u hdfs hadoop fs -mkdir /solr$ sudo -u hdfs hadoop fs -chown solr /solr 如果开启了 kerberos，则先获取 hdfs 服务的凭证在运行： 1234$ kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/cdh1@JAVACHEN.com$ hadoop fs -mkdir /solr$ hadoop fs -chown solr /solr 集成 Sentry，取消 /etc/default/solr 下面注释： 12# SOLR_AUTHORIZATION_SENTRY_SITE=/etc/solr/conf/sentry-site.xml# SOLR_AUTHORIZATION_SUPERUSER=solr 集成 Hue，取消 /etc/default/solr 下面注释： 123# SOLR_SECURITY_ALLOWED_PROXYUSERS=hue# SOLR_SECURITY_PROXYUSER_hue_HOSTS=*# SOLR_SECURITY_PROXYUSER_hue_GROUPS=* 初始化 ZooKeeper Namespace： 1$ solrctl init 注意：你可以添加 --force 参数强制清空 ZooKeeper 数据，清空之前，请先停止 ZooKeeper 集群。 启动 Solr在每一个安装了 Solr server 的节点上运行： 1$ sudo service solr-server restart 通过下面命令查看 Solr 是否启动成功： 123456789$ jps -lm28053 org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer31710 org.apache.zookeeper.server.quorum.QuorumPeerMain /etc/zookeeper/conf/zoo.cfg14479 org.apache.catalina.startup.Bootstrap start29994 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode29739 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager27298 org.apache.catalina.startup.Bootstrap httpfs start30123 org.apache.hadoop.hdfs.server.namenode.NameNode21761 sun.tools.jps.Jps -lm 上面用到了 solrctl 命令，该命令用来管理 SolrCloud 的部署和配置，其语法如下： 1solrctl [options] command [command-arg] [command [command-arg]] ... 可选参数有： --solr：指定 SolrCloud 的 web API，如果在 SolrCloud 集群之外的节点运行命令，就需要指定该参数。 --zk：指定 zk 集群地址。 --help：打印帮助信息。 --quiet：静默模式运行。 command 命令有： init [--force]：初始化配置。 instancedir：维护实体目录。可选的参数有： --generate path --create name path --update name path --get name path --delete name --list collection：维护 collections。可选的参数有： --create name -s &lt;numShards&gt; [-c &lt;collection.configName&gt;] [-r &lt;replicationFactor&gt;] [-m &lt;maxShardsPerNode&gt;] [-n &lt;createNodeSet&gt;]] --delete name: Deletes a collection. --reload name: Reloads a collection. --stat name: Outputs SolrCloud specific run-time information for a collection. ``–list`: Lists all collections registered in SolrCloud. --deletedocs name: Purges all indexed documents from a collection. core：维护 cores。可选的参数有： --create name [-p name=value]...] --reload name: Reloads a core. --unload name: Unloads a core. --status name: Prints status of a core. cluster：维护集群配置信息。可选的参数有： --get-solrxml file --put-solrxml file 创建 Solr 运行时配置在一个节点上（例如 cdh1）生成配置文件： 1$ solrctl instancedir --generate $HOME/solr_configs 注意：你可以在 /var/lib/solr 创建目录，维护配置文件。 执行完之后，你可以修改 $HOME/solr_configs/conf 目录下的配置文件，其目录下文件如下。 123456789101112131415161718192021$ ll ~/solr_configs/conf/total 348-rw-r--r-- 1 root root 1092 Jun 2 23:10 admin-extra.html-rw-r--r-- 1 root root 953 Jun 2 23:10 admin-extra.menu-bottom.html-rw-r--r-- 1 root root 951 Jun 2 23:10 admin-extra.menu-top.html-rw-r--r-- 1 root root 4041 Jun 2 23:10 currency.xml-rw-r--r-- 1 root root 1386 Jun 2 23:10 elevate.xmldrwxr-xr-x 2 root root 4096 Jun 2 23:10 lang-rw-r--r-- 1 root root 82327 Jun 2 23:10 mapping-FoldToASCII.txt-rw-r--r-- 1 root root 3114 Jun 2 23:10 mapping-ISOLatin1Accent.txt-rw-r--r-- 1 root root 894 Jun 2 23:10 protwords.txt-rw-r--r-- 1 root root 59635 Jun 2 23:10 schema.xml-rw-r--r-- 1 root root 921 Jun 2 23:10 scripts.conf-rw-r--r-- 1 root root 72219 Jun 2 23:10 solrconfig.xml-rw-r--r-- 1 root root 73608 Jun 2 23:10 solrconfig.xml.secure-rw-r--r-- 1 root root 16 Jun 2 23:10 spellings.txt-rw-r--r-- 1 root root 795 Jun 2 23:10 stopwords.txt-rw-r--r-- 1 root root 1148 Jun 2 23:10 synonyms.txt-rw-r--r-- 1 root root 1469 Jun 2 23:10 update-script.jsdrwxr-xr-x 2 root root 4096 Jun 2 23:10 velocitydrwxr-xr-x 2 root root 4096 Jun 2 23:10 xslt 创建 collection1 实例并将配置文件上传到 zookeeper： 1$ solrctl instancedir --create collection1 $HOME/solr_configs 你可以通过下面命令查看上传的 instance： 1$ solrctl instancedir --list 上传到 zookeeper 之后，其他节点就可以从上面下载配置文件。 接下来，还是在 cdh1 节点上创建 collection，因为我的 SolrCloud 集群有三个节点，故这里分片数设为3，并设置副本为1，如果有更多节点，可以将副本设置为更大的一个数： 1$ solrctl collection --create collection1 -s 3 -r 1 运行成功之后，你可以通过 http://cdh1:8983/solr/#/~cloud、http://cdh2:8983/solr/#/~cloud、http://cdh3:8983/solr/#/~cloud 查看创建的分片。从网页上可以看到，已经自动创建了三个分片，并且三个分片分布在3个节点之上。 配置 hbase-solr-indexer1）开启 HBase replication Lily HBase Indexer 的实现依赖于 HBase的replication，故需要开启复制。将 /usr/share/doc/hbase-solr-doc*/demo/hbase-site.xml文件的内容拷贝到 hbase-site.xml，注意：删除掉 replication.replicationsource.implementation 参数配置。 2）将 hbase-solr-indexer 服务指向 hbase 集群 修改 /etc/hbase-solr/conf/hbase-indexer-site.xml，添加如下参数，其值和 hbase-site.xml 中的 hbase.zookeeper.quorum 属性值保持一致（注意添加上端口）： 1234&lt;property&gt; &lt;name&gt;hbaseindexer.zookeeper.connectstring&lt;/name&gt; &lt;value&gt;cdh1:2181,cdh2:2181,cdh3:2181&lt;/value&gt;&lt;/property&gt; 最后再重启服务： 1$ sudo service hbase-solr-indexer restart 总结本文内容主要介绍了如何不使用 Cloudera Manager 来安装 Cloudera Search，下篇文章将介绍如何使用 Cloudera Search。 参考资料 [1] Cloudera Search: 轻松实现Hadoop全文检索 [2] Cloudera-Search-User-Guide]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>solr</tag>
        <tag>solrcloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于CAP理论的一些笔记]]></title>
    <url>%2F2014%2F05%2F30%2Fnote-about-brewers-cap-theorem%2F</url>
    <content type="text"><![CDATA[CAP的概念2000年，Eric Brewer 教授在 ACM 分布式计算年会上指出了著名的 CAP 理论： 分布式系统不可能同时满足一致性(C: Consistency)，可用性(A: Availability)和分区容错性(P: Tolerance of network Partition)这三个需求。大约两年后，Seth Gilbert 和 Nancy lynch 两人证明了CAP理论的正确性。 三者的含义如下： Consistency：一致性，一个服务是一致的完整操作或完全不操作（A service that is consistent operates fully or not at all，精确起见列出原文），也有人将其简称为数据一致性，任何一个读操作总是能读取到之前完成的写操作结果，也就是在分布式环境中，多点的数据是一致的。 Availability：可用性，每一个操作总是能够在确定的时间内返回，也就是系统随时都是可用的。 Tolerance of network Partition：分区容忍性，节点 crash 或者网络分片都不应该导致一个分布式系统停止服务。 关于 CAP 理论的历史和介绍可以参考 Brewer’s CAP Theorem 和 中文翻译。 基本CAP的证明思路CAP 的证明基于 异步网络，异步网络也是反映了真实网络中情况的模型。 真实的网络系统中，节点之间不可能保持同步，即便是时钟也不可能保持同步，所有的节点依靠获得的消息来进行本地计算和通讯。这个概念其实是相当强的，意味着任何超时判断也是不可能的，因为没有共同的时间标准。之后我们会扩展 CAP 的证明到弱一点的异步网络中，这个网络中时钟不完全一致，但是时钟运行的步调是一致的，这种系统是允许节点做超时判断的。 CAP 的证明很简单，假设两个节点集{G1, G2}，由于网络分片导致 G1 和 G2 之间所有的通讯都断开了，如果在 G1 中写，在 G2 中读刚写的数据， G2 中返回的值不可能是 G1 中的写值。由于 A 的要求，G2 一定要返回这次读请求，由于 P 的存在，导致 C一定是不可满足的。 为什么不能完全保证这个三点了，个人觉得主要是因为一旦进行分区了，就说明了必须节点之间必须进行通信，涉及到通信，就无法确保在有限的时间内完成指定的行为，如果要求两个操作之间要完整的进行，肯定会存在某一个时刻只完成一部分的业务操作，在通信完成的这一段时间内，数据就是不一致性的。如果要求保证一致性，那么就必须在通信完成这一段时间内保护数据，使得任何访问这些数据的操作不可用。 如果想保证一致性和可用性，那么数据就不能够分区。一个简单的理解就是所有的数据就必须存放在一个数据库里面，不能进行数据库拆分。这个对于大数据量，高并发的互联网应用来说，是不可接受的。 这里引用 Brewer’s CAP Theorem 中的图和文字来说明。 上图显示了网络中的两个节点 N1，N2，他们共享同一数据 V，其值为 V0。N1 上有一个算法 A，我们可以认为 A 是安全、无 bug、可预测和可靠的。N2 上有一个类似的算法 B。在这个例子中，A 写入 V 的新值而 B 读取 V 的值。 正常情况过程如下： 1) A 写入新的 V 值，我们称作 v1。 2) N1 发送信息给 N2，更新 V 的值。 3) 现在 B 读取的 V 值将会是 V1。 如果网络断开，意味着从 N1 无法发送信息到 N2，那么在第3步的时候，N2 就会包含一个步一致的 V 值。 即使将其扩展到几百个事务中，这也会成为一个大问题。如果 M 是一个异步消息，那么 N1 无法知道 N2 是否收到了消息。即使 M 是保证能发送的，N1 也无法知道是否消息由于分区事件的发生而延迟，或 N2 上的其他故障而延迟。即使将 M 作为同步消息也不能解决问题，因为那将会使得 N1 上 A 的写操作和 N1 到 N2 的更新事件成为一个原子操作，而这将导致同样的等待问题。Gilbert 和Lynch已经证明，使用其他的变种方式，即使是部分同步模型（每个节点上使用安排好的时钟）也无法保证原子性。 因此，CAP 告诉我们，如果想让 A 和 B 是高可用的（例如，以最小的延迟提供服务）并且想让所有的 N1 到 Nn（n的值可以是数百甚至是上千）的节点能够冗余网络的分区（丢失信息，无法传递信息，硬件无法提供服务，处理失败），那么有时我们就得面临这样的情况：某些节点认为 V 的值是 V0 而其他节点会认为 V 的值是 V1。 让我们从事务的角度分析一下。下面的图中 a 是整个过程，要具有一致性的话需要等待 a1 进行 write，然后同步到 a2，然后 a2 再进行 write，只有整个事务完成以后，a2 才能够进行 read。但是这样的话使得整个系统的可用性下降，a2 一直阻塞在那里等待 a1 同步到 a2。这个时候如果对一致性要求不高的话，a2 可以不等待 a1 数据对于 a2 的写同步，直接读取，这样虽然此时的读写不具有一致性，但是在后面可以通过异步的方式使得 a1 和 a2 的数据最终一致，达到 最终一致性。 BASE理论BASE 理论是 CAP 理论结合实际的产物。 BASE(Basically Available, Soft-state,Eventuallyconsistent)英文中有碱的意思，这个正好和 ACID 的酸的意义相对，很有意思。BASE 恰好和 ACID 是相对的，BASE 要求牺牲高一致性，获得可用性或可靠性。 Basically Availble： 基本可用(支持分区失败) Soft-state：软状态/柔性事务(无状态连接，支持异步) Eventual Consistency： 最终一致性(不要求高一致性，只要求最终能够一致) BASE 理论的核心是：牺牲高一致性，获得可用性或可靠性。 CAP选择当处理 CAP 的问题时，你会有几个选择。最明显的是： 放弃 Tolerance of network Partition。如果你想避免分区问题发生，你就必须要阻止其发生。一种做法是将所有的东西（与事务相关的）都放到一台机器或者一个机架上。这样还是有可能部分失败，但你不太可能碰到由分区问题带来的负面效果。当然，这个选择会严重影响系统的扩展。 放弃 Availability。相对于放弃 Tolerance of network Partition 来说，其反面就是放弃 Availability。一旦遇到分区事件，受影响的服务需要等待数据一致，因此在等待期间就无法对外提供服务。在多个节点上控制这一点会相当复杂，而且恢复的节点需要处理逻辑，以便平滑地返回服务状态。 放弃 Consistency。放弃一致性，你的系统可能返回不太精确的数据，但系统将会变得“最终一致”，即使是网络发生分区的时候也是如此。 下面是一个使用 CAP 理论的生态系统的分布图： 任何架构师在设计分布式的系统的时候，都必须在这三者之间进行取舍。首先就是是否选择分区，由于在一个数据分区内，根据数据库的ACID特性，是可以保证一致性的，不会存在可用性和一致性的问题，唯一需要考虑的就是性能问题。对于可用性和一致性，大多数应用就必须保证可用性，毕竟是互联网应用，牺牲了可用性，相当于间接的影响了用户体验，而唯一可以考虑就是一致性了。 牺牲一致性对于牺牲一致性的情况最多的就是缓存和数据库的数据同步问题，我们把缓存看做一个数据分区节点，数据库看作另外一个节点，这两个节点之间的数据在任何时刻都无法保证一致性的。 异常错误检测和补偿还有一种牺牲一致性的方法就是通过一种错误补偿机制来进行 两阶段提交协议第一阶段和第二阶段之间，数据也可不能是一致性的，也可能出现同样的情况导致异常。 CAP的反对声音1，2008年9月CTO of atomikos写了一篇文章“A CAP Solution (Proving Brewer Wrong)”，试图达到CAP都得的效果。 这篇文章的核心内容就是放松Gilbert和Lynch证明中的限制：“系统必须同时达到CAP三个属性”，放松到“系统可以不同时达到CAP，而是分时达到”。 他设计的系统如下： (1)程序如果能够读取数据库的话读取数据库，如果不能的话可以使用缓存代替。 (2)所有的读取操作使用版本号或者其他可以使用乐观锁的机制。 (3)客户端的所有更新操作全部放在队列中顺序处理。更新操作中要包括该更新的读取操作时的版本信息。 (4)当分区数量足够少的时候，可以处理队列中的更新操作。比较简单的方式是建立一个跨越所有分布式副本的事务，对每个副本进行更新操作（其他方式比如quorum等等也可以）。如果该更新的读取操作时的版本信息不是当前数据库中数据的版本信息，则将失败返回给客户端，否则返回成功。 (5)数据库操作结果（确认或者取消）通过异步的方式发送到客户端，可以通过邮件，消息队列或者其他异步方式。 该系统符合CAP如下： 符合C（高一致性）：读取的数据都是基于快照的，而且错误的更新操作不会执行。 符合A（高可用性）：读取和更新都会返回数据。 符合P（高分区容错性）：允许网络或者节点出错。 该设计是符合BASE理论的。 缺点： 1) 读数据可能会不一致，因为之前的写还在排队 2) partition必须在有限的时间内解决 3) update操作必须在所有的节点上保持同样的顺序 2， 2011年11月Twitter的首席工程师Nathan Marz写了一篇文章，描述了他是如何试图打败CAP定理的： How to beat the CAP theorem 本文中，作者还是非常尊重CAP定律，并表示不是要“击败”CAP，而是尝试对数据存储系统进行重新设计，以可控的复杂度来实现CAP。 Marz认为一个分布式系统面临CAP难题的两大问题就是：在数据库中如何使用不断变化的数据，如何使用算法来更新数据库中的数据。Marz提出了几个由于云计算的兴起而改变的传统概念： 1) 数据不存在update，只存在append操作。这样就把对数据的处理由CRUD变为CR 2) 所有的数据的操作就只剩下Create和Read。把Read作为一个Query来处理，而一个Query就是一个对整个数据集执行一个函数操作。 在这样的模型下，我们使用最终一致性的模型来处理数据，可以保证在P的情况下保证A。而所有的不一致性都可以通过重复进行Query去除掉。Martz认为就是因为要不断的更新数据库中的数据，再加上CAP，才导致那些即便使用最终一致性的系统也会变得无比复杂，需要用到向量时钟、读修复这种技术，而如果系统中不存在会改变的数据，所有的更新都作为创建新数据的方式存在，读数据转化为一次请求，这样就可以避免最终一致性的复杂性，转而拥抱CAP。 参考资料 1 Brewer’s CAP Theorem 2 CAP理论 [3] NoSQL学习笔记(二)之CAP理论]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>cap</tag>
        <tag>nosql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Scrapy抓取数据]]></title>
    <url>%2F2014%2F05%2F24%2Fusing-scrapy-to-cralw-data%2F</url>
    <content type="text"><![CDATA[Scrapy是Python开发的一个快速,高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。 官方主页： http://www.scrapy.org/ 中文文档：Scrapy 0.22 文档 GitHub项目主页：https://github.com/scrapy/scrapy Scrapy 使用了 Twisted 异步网络库来处理网络通讯。整体架构大致如下（注：图片来自互联网）： Scrapy主要包括了以下组件： 引擎，用来处理整个系统的数据流处理，触发事务。 调度器，用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回。 下载器，用于下载网页内容，并将网页内容返回给蜘蛛。 蜘蛛，蜘蛛是主要干活的，用它来制订特定域名或网页的解析规则。 项目管道，负责处理有蜘蛛从网页中抽取的项目，他的主要任务是清晰、验证和存储数据。当页面被蜘蛛解析后，将被发送到项目管道，并经过几个特定的次序处理数据。 下载器中间件，位于Scrapy引擎和下载器之间的钩子框架，主要是处理Scrapy引擎与下载器之间的请求及响应。 蜘蛛中间件，介于Scrapy引擎和蜘蛛之间的钩子框架，主要工作是处理蜘蛛的响应输入和请求输出。 调度中间件，介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。 使用Scrapy可以很方便的完成网上数据的采集工作，它为我们完成了大量的工作，而不需要自己费大力气去开发。 1. 安装安装 pythonScrapy 目前最新版本为0.22.2，该版本需要 python 2.7，故需要先安装 python 2.7。这里我使用 centos 服务器来做测试，因为系统自带了 python ，需要先检查 python 版本。 查看python版本： 12$ python -VPython 2.6.6 升级版本到2.7： 12345$ wget http://python.org/ftp/python/2.7.6/Python-2.7.6.tar.xz$ tar xf Python-2.7.6.tar.xz$ cd Python-2.7.6$ ./configure --prefix=/usr/local --enable-unicode=ucs4 --enable-shared LDFLAGS="-Wl,-rpath /usr/local/lib"$ make &amp;&amp; make altinstall 建立软连接，使系统默认的 python指向 python2.7 12$ mv /usr/bin/python /usr/bin/python2.6.6 $ ln -s /usr/local/bin/python2.7 /usr/bin/python 再次查看python版本： 12$ python -VPython 2.7.6 安装这里使用 wget 的方式来安装 setuptools : 1$ wget https://bootstrap.pypa.io/ez_setup.py -O - | python 安装 zope.interface1$ easy_install zope.interface 安装 twistedScrapy 使用了 Twisted 异步网络库来处理网络通讯，故需要安装 twisted。 安装 twisted 前，需要先安装 gcc： 1$ yum install gcc -y 然后，再通过 easy_install 安装 twisted： 1$ easy_install twisted 如果出现下面错误： 1234567891011121314151617$ easy_install twistedSearching for twistedReading https://pypi.python.org/simple/twisted/Best match: Twisted 14.0.0Downloading https://pypi.python.org/packages/source/T/Twisted/Twisted-14.0.0.tar.bz2#md5=9625c094e0a18da77faa4627b98c9815Processing Twisted-14.0.0.tar.bz2Writing /tmp/easy_install-kYHKjn/Twisted-14.0.0/setup.cfgRunning Twisted-14.0.0/setup.py -q bdist_egg --dist-dir /tmp/easy_install-kYHKjn/Twisted-14.0.0/egg-dist-tmp-vu1n6Ytwisted/runner/portmap.c:10:20: error: Python.h: No such file or directorytwisted/runner/portmap.c:14: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘*’ tokentwisted/runner/portmap.c:31: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘*’ tokentwisted/runner/portmap.c:45: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘PortmapMethods’twisted/runner/portmap.c: In function ‘initportmap’:twisted/runner/portmap.c:55: warning: implicit declaration of function ‘Py_InitModule’twisted/runner/portmap.c:55: error: ‘PortmapMethods’ undeclared (first use in this function)twisted/runner/portmap.c:55: error: (Each undeclared identifier is reported only oncetwisted/runner/portmap.c:55: error: for each function it appears in.) 请安装 python-devel 然后再次运行： 12$ yum install python-devel -y$ easy_install twisted 如果出现下面异常： 1error: Not a recognized archive type: /tmp/easy_install-tVwC5O/Twisted-14.0.0.tar.bz2 请手动下载然后安装，下载地址在这里 1234$ wget https://pypi.python.org/packages/source/T/Twisted/Twisted-14.0.0.tar.bz2#md5=9625c094e0a18da77faa4627b98c9815$ tar -vxjf Twisted-14.0.0.tar.bz2$ cd Twisted-14.0.0$ python setup.py install 安装 pyOpenSSL先安装一些依赖： 1$ yum install libffi libffi-devel openssl-devel -y 然后，再通过 easy_install 安装 pyOpenSSL： 1$ easy_install pyOpenSSL 安装 Scrapy先安装一些依赖： 1$ yum install libxml2 libxslt libxslt-devel -y 最后再来安装 Scrapy ： 1$ easy_install scrapy 2. 使用 Scrapy在安装成功之后，你可以了解一些 Scrapy 的基本概念和使用方法，并学习 Scrapy 项目的例子 dirbot 。 Dirbot 项目位于 https://github.com/scrapy/dirbot，该项目包含一个 README 文件，它详细描述了项目的内容。如果你熟悉 git，你可以 checkout 它的源代码。或者你可以通过点击 Downloads 下载 tarball 或 zip 格式的文件。 下面以该例子来描述如何使用 Scrapy 创建一个爬虫项目。 新建工程在抓取之前，你需要新建一个 Scrapy 工程。进入一个你想用来保存代码的目录，然后执行： 1$ scrapy startproject tutorial 这个命令会在当前目录下创建一个新目录 tutorial，它的结构如下： 123456789.├── scrapy.cfg└── tutorial ├── __init__.py ├── items.py ├── pipelines.py ├── settings.py └── spiders └── __init__.py 这些文件主要是： scrapy.cfg: 项目配置文件 tutorial/: 项目python模块, 呆会代码将从这里导入 tutorial/items.py: 项目items文件 tutorial/pipelines.py: 项目管道文件 tutorial/settings.py: 项目配置文件 tutorial/spiders: 放置spider的目录 定义ItemItems是将要装载抓取的数据的容器，它工作方式像 python 里面的字典，但它提供更多的保护，比如对未定义的字段填充以防止拼写错误。 它通过创建一个 scrapy.item.Item 类来声明，定义它的属性为 scrpy.item.Field 对象，就像是一个对象关系映射(ORM).我们通过将需要的item模型化，来控制从 dmoz.org 获得的站点数据，比如我们要获得站点的名字，url 和网站描述，我们定义这三种属性的域。要做到这点，我们编辑在 tutorial 目录下的 items.py 文件，我们的 Item 类将会是这样 12345from scrapy.item import Item, Field class DmozItem(Item): title = Field() link = Field() desc = Field() 刚开始看起来可能会有些困惑，但是定义这些 item 能让你用其他 Scrapy 组件的时候知道你的 items 到底是什么。 编写爬虫(Spider)Spider 是用户编写的类，用于从一个域（或域组）中抓取信息。们定义了用于下载的URL的初步列表，如何跟踪链接，以及如何来解析这些网页的内容用于提取items。 要建立一个 Spider，你可以为 scrapy.spider.BaseSpider 创建一个子类，并确定三个主要的、强制的属性： name：爬虫的识别名，它必须是唯一的，在不同的爬虫中你必须定义不同的名字. start_urls：爬虫开始爬的一个 URL 列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些 URLS 开始。其他子 URL 将会从这些起始 URL 中继承性生成。 parse()：爬虫的方法，调用时候传入从每一个 URL 传回的 Response 对象作为参数，response 将会是 parse 方法的唯一的一个参数, 这个方法负责解析返回的数据、匹配抓取的数据(解析为 item )并跟踪更多的 URL。 在 tutorial/spiders 目录下创建 DmozSpider.py 12345678910111213from scrapy.spider import BaseSpiderclass DmozSpider(BaseSpider): name = "dmoz" allowed_domains = ["dmoz.org"] start_urls = [ "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/", "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/" ] def parse(self, response): filename = response.url.split("/")[-2] open(filename, 'wb').write(response.body) 运行项目1$ scrapy crawl dmoz 该命令从 dmoz.org 域启动爬虫，第三个参数为 DmozSpider.py 中的 name 属性值。 xpath选择器Scrapy 使用一种叫做 XPath selectors 的机制，它基于 XPath 表达式。如果你想了解更多selectors和其他机制你可以查阅资料。 这是一些XPath表达式的例子和他们的含义： /html/head/title: 选择HTML文档 &lt;head&gt; 元素下面的 &lt;title&gt; 标签。 /html/head/title/text(): 选择前面提到的&lt;title&gt; 元素下面的文本内容 //td: 选择所有 &lt;td&gt; 元素 //div[@class=&quot;mine&quot;]: 选择所有包含 class=&quot;mine&quot; 属性的div 标签元素 这只是几个使用 XPath 的简单例子，但是实际上 XPath 非常强大。如果你想了解更多 XPATH 的内容，我们向你推荐这个 XPath 教程 为了方便使用 XPaths，Scrapy 提供 Selector 类， 有三种方法 xpath()：返回selectors列表, 每一个select表示一个xpath参数表达式选择的节点. extract()：返回一个unicode字符串，该字符串为XPath选择器返回的数据 re()： 返回unicode字符串列表，字符串作为参数由正则表达式提取出来 css() 提取数据我们可以通过如下命令选择每个在网站中的 &lt;li&gt; 元素: 1sel.xpath('//ul/li') 然后是网站描述: 1sel.xpath('//ul/li/text()').extract() 网站标题: 1sel.xpath('//ul/li/a/text()').extract() 网站链接: 1sel.xpath('//ul/li/a/@href').extract() 如前所述，每个 xpath() 调用返回一个 selectors 列表，所以我们可以结合 xpath() 去挖掘更深的节点。我们将会用到这些特性，所以: 123456sites = sel.xpath('//ul/li')for site in sites: title = site.xpath('a/text()').extract() link = site.xpath('a/@href').extract() desc = site.xpath('text()').extract() print title, link, desc 使用Itemscrapy.item.Item 的调用接口类似于 python 的 dict ，Item 包含多个 scrapy.item.Field。这跟 django 的 Model 与 Item 通常是在 Spider 的 parse 方法里使用，它用来保存解析到的数据。 最后修改爬虫类，使用 Item 来保存数据，代码如下： 12345678910111213141516171819202122232425262728293031323334from scrapy.spider import Spiderfrom scrapy.selector import Selectorfrom dirbot.items import Websiteclass DmozSpider(Spider): name = "dmoz" allowed_domains = ["dmoz.org"] start_urls = [ "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/", "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/", ] def parse(self, response): """ The lines below is a spider contract. For more info see: http://doc.scrapy.org/en/latest/topics/contracts.html @url http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/ @scrapes name """ sel = Selector(response) sites = sel.xpath('//ul[@class="directory-url"]/li') items = [] for site in sites: item = Website() item['name'] = site.xpath('a/text()').extract() item['url'] = site.xpath('a/@href').extract() item['description'] = site.xpath('text()').re('-\s([^\n]*?)\\n') items.append(item) return items 现在，可以再次运行该项目查看运行结果： 1$ scrapy crawl dmoz 使用Item Pipeline在 settings.py 中设置 ITEM_PIPELINES，其默认为[]，与 django 的 MIDDLEWARE_CLASSES 等相似。从 Spider 的 parse 返回的 Item 数据将依次被 ITEM_PIPELINES 列表中的 Pipeline 类处理。 一个 Item Pipeline 类必须实现以下方法： process_item(item, spider) 为每个 item pipeline 组件调用，并且需要返回一个 scrapy.item.Item 实例对象或者抛出一个 scrapy.exceptions.DropItem 异常。当抛出异常后该 item 将不会被之后的 pipeline 处理。参数: item (Item object) – 由 parse 方法返回的 Item 对象 spider (BaseSpider object) – 抓取到这个 Item 对象对应的爬虫对象 也可额外的实现以下两个方法： open_spider(spider) 当爬虫打开之后被调用。参数: spider (BaseSpider object) – 已经运行的爬虫 close_spider(spider) 当爬虫关闭之后被调用。参数: spider (BaseSpider object) – 已经关闭的爬虫 保存抓取的数据保存信息的最简单的方法是通过 Feed exports，命令如下： 1$ scrapy crawl dmoz -o items.json -t json 除了 json 格式之外，还支持 JSON lines、CSV、XML格式，你也可以通过接口扩展一些格式。 对于小项目用这种方法也足够了。如果是比较复杂的数据的话可能就需要编写一个 Item Pipeline 进行处理了。 所有抓取的 items 将以 JSON 格式被保存在新生成的 items.json 文件中 总结上面描述了如何创建一个爬虫项目的过程，你可以参照上面过程联系一遍。作为学习的例子，你还可以参考这篇文章：scrapy 中文教程（爬cnbeta实例） 。 这篇文章中的爬虫类代码如下： 12345678910111213141516171819202122from scrapy.contrib.spiders import CrawlSpider, Rulefrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractorfrom scrapy.selector import Selector from cnbeta.items import CnbetaItem class CBSpider(CrawlSpider): name = 'cnbeta' allowed_domains = ['cnbeta.com'] start_urls = ['http://www.cnbeta.com'] rules = ( Rule(SgmlLinkExtractor(allow=('/articles/.*\.htm', )), callback='parse_page', follow=True), ) def parse_page(self, response): item = CnbetaItem() sel = Selector(response) item['title'] = sel.xpath('//title/text()').extract() item['url'] = response.url return item 需要说明的是： 该爬虫类继承的是 CrawlSpider 类，并且定义规则，rules指定了含有 /articles/.*\.htm 的链接都会被匹配。 该类并没有实现parse方法，并且规则中定义了回调函数 parse_page，你可以参考更多资料了解 CrawlSpider 的用法 3. 学习资料接触 Scrapy，是因为想爬取一些知乎的数据，最开始的时候搜索了一些相关的资料和别人的实现方式。 Github 上已经有人或多或少的实现了对知乎数据的爬取，我搜索到的有以下几个仓库： https://github.com/KeithYue/Zhihu_Spider 实现先通过用户名和密码登陆再爬取数据，代码见 zhihu_spider.py。 https://github.com/immzz/zhihu-scrapy 使用 selenium 下载和执行 javascript 代码。 https://github.com/tangerinewhite32/zhihu-stat-py https://github.com/Zcc/zhihu 主要是爬指定话题的topanswers，还有用户个人资料，添加了登录代码。 https://github.com/pelick/VerticleSearchEngine 基于爬取的学术资源，提供搜索、推荐、可视化、分享四块。使用了 Scrapy、MongoDB、Apache Lucene/Solr、Apache Tika等技术。 https://github.com/geekan/scrapy-examples scrapy的一些例子，包括获取豆瓣数据、linkedin、腾讯招聘数据等例子。 https://github.com/owengbs/deeplearning 实现分页获取话题。 https://github.com/gnemoug/distribute_crawler 使用scrapy、redis、mongodb、graphite实现的一个分布式网络爬虫,底层存储mongodb集群,分布式使用redis实现,爬虫状态显示使用graphite实现 https://github.com/weizetao/spider-roach 一个分布式定向抓取集群的简单实现。 https://github.com/scrapinghub/portia 这是一个可视化爬虫，基于Scrapy。它提供了可视化操作的Web页面，你只需点击页面上你要抽取的数据就行 https://github.com/binux/pyspider 你如果不喜欢 Scrapy，可以试试 pyspider ，他让你在 WEB 界面编写调试脚本，监控执行状态，查看历史和结果 ，你可以在线试下 demo：Dashboard - pyspider 其他资料： http://www.52ml.net/tags/Scrapy 收集了很多关于 Scrapy 的文章，推荐阅读 用Python Requests抓取知乎用户信息 使用scrapy框架爬取自己的博文 Scrapy 深入一点点 使用python，scrapy写（定制）爬虫的经验，资料，杂。 Scrapy 轻松定制网络爬虫 在scrapy中怎么让Spider自动去抓取豆瓣小组页面 scrapy 和 javascript 交互例子： 用scrapy框架爬取js交互式表格数据 scrapy + selenium 解析javascript 实例 还有一些待整理的知识点： 如何先登陆再爬数据 如何使用规则做过滤 如何递归爬取数据 scrapy的参数设置和优化 如何实现分布式爬取 4. 总结以上就是最近几天学习 Scrapy 的一个笔记和知识整理，参考了一些网上的文章才写成此文，对此表示感谢，也希望这篇文章能够对你有所帮助。如果你有什么想法，欢迎留言；如果喜欢此文，请帮忙分享，谢谢!]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nutch介绍及使用]]></title>
    <url>%2F2014%2F05%2F20%2Fnutch-intro%2F</url>
    <content type="text"><![CDATA[1. Nutch介绍Nutch是一个开源的网络爬虫项目，更具体些是一个爬虫软件，可以直接用于抓取网页内容。 现在Nutch分为两个版本，1.x和2.x。1.x最新版本为1.7，2.x最新版本为2.2.1。两个版本的主要区别在于底层的存储不同。 1.x版本是基于Hadoop架构的，底层存储使用的是HDFS，而2.x通过使用Apache Gora，使得Nutch可以访问HBase、Accumulo、Cassandra、MySQL、DataFileAvroStore、AvroStore等NoSQL。 2. 编译NutchNutch1.x从1.7版本开始不再提供完整的部署文件，只提供源代码文件及相关的build.xml文件,这就要求用户自己编译Nutch，而整个Nutch2.x版本都不提供编译完成的文件，所以想要学习Nutch2.2.1的功能，就必须自己手动编译文件。 2.1 下载解压12$ wget http://archive.apache.org/dist/nutch/2.2.1/apache-nutch-2.2.1-src.tar.gz$ tar zxf apache-nutch-2.2.1-src.tar.gz 2.2 编译12$ cd apache-nutch-2.2.1$ ant 有可能你会得到如下错误： 1234567Trying to override old definition of task javac [taskdef] Could not load definitions from resource org/sonar/ant/antlib.xml. It could not be found.ivy-probe-antlib:ivy-download: [taskdef] Could not load definitions from resource org/sonar/ant/antlib.xml. It could not be found. 解决办法： 下载sonar-ant-task-2.1.jar，将其拷贝到apache-nutch-2.2.1目录下面 修改build.xml，引入上面添加的jar包： 123456&lt;!-- Define the Sonar task if this hasn't been done in a common script --&gt;&lt;taskdef uri="antlib:org.sonar.ant" resource="org/sonar/ant/antlib.xml"&gt; &lt;classpath path="$&#123;ant.library.dir&#125;" /&gt; &lt;classpath path="$&#123;mysql.library.dir&#125;" /&gt; &lt;classpath&gt;&lt;fileset dir="." includes="sonar*.jar" /&gt;&lt;/classpath&gt;&lt;/taskdef&gt; Nutch使用ivy进行构建，故编译需要很长时间，如果编译时间过长，建议修改maven仓库地址，修改方法： 通过用http://mirrors.ibiblio.org/maven2/替换ivy/下ivysettings.xml中的http://repo1.maven.org/maven2/来解决。代码位置为： 1&lt;property name="repo.maven.org" value="http://repo1.maven.org/maven2/" override="false"/&gt; 编译之后的目录如下： 123456789101112131415161718➜ apache-nutch-2.2.1 tree -L 1.├── CHANGES.txt├── LICENSE.txt├── NOTICE.txt├── README.txt├── build├── build.xml├── conf├── default.properties├── docs├── ivy├── lib├── runtime├── sonar-ant-task-2.1.jar└── src7 directories, 7 files 可以看到编译之后多了两个目录：build和runtime 3. 修改配置文件由于Nutch2.x版本存储采用Gora访问Cassandra、HBase、Accumulo、Avro等，需要在该文件中制定Gora属性，比如指定默认的存储方式gora.datastore.default= org.apache.gora.hbase.store.HBaseStore，该属性的值可以在nutch-default.xml中查找storage.data.store.class属性取得，在不做gora.properties文件修改的情况下，存储类为org.apache.gora.memory.store.MemStore，该类将数据存储在内存中，仅用于测试目的。 这里，将其存储方式改为HBase,请参考 http://wiki.apache.org/nutch/Nutch2Tutorial。 修改 conf/nutch-site.xml 12345&lt;property&gt; &lt;name&gt;storage.data.store.class&lt;/name&gt; &lt;value&gt;org.apache.gora.hbase.store.HBaseStore&lt;/value&gt; &lt;description&gt;Default class for storing data&lt;/description&gt;&lt;/property&gt; 修改 ivy/ivy.xml 12&lt;!-- Uncomment this to use HBase as Gora backend. --&gt;&lt;dependency org="org.apache.gora" name="gora-hbase" rev="0.3" conf="*-&gt;default" /&gt; 修改 conf/gora.properties，确保HBaseStore被设置为默认的存储， 1gora.datastore.default=org.apache.gora.hbase.store.HBaseStore 因为这里用到了HBase，故还需要一个HBase环境，你可以使用Standalone模式搭建一个HBase环境，请参考 HBase Quick Start。需要说明的时，目前HBase的版本要求为 hbase-0.90.4。 4. 集成Solr由于建索引的时候需要使用Solr，因此我们需要安装并启动一个Solr服务器。 4.1 下载，解压12$ wget http://mirrors.cnnic.cn/apache/lucene/solr/4.8.0/solr-4.8.0.tgz $ tar -zxf solr-4.8.0.tgz 4.2 运行Solr12$ cd solr-4.8.0/example$ java -jar start.jar 验证是否启动成功 用浏览器打开 http://localhost:8983/solr/admin/，如果能看到页面，说明启动成功。 4.3 修改Solr配置文件将apache-nutch-2.2.1/conf/schema-solr4.xml拷贝到solr-4.8.0/solr/collection1/conf/schema.xml，并在&lt;fields&gt;...&lt;/fields&gt;最后添加一行: 1&lt;field name="_version_" type="long" indexed="true" stored="true" multiValued="false"/&gt; 重启Solr， 12# Ctrl+C to stop Solr$ java -jar start.jar 5. 抓取数据编译后的脚本在 runtime/local/bin 目录下，可以运行命令查看使用方法： crawl命令： 123$ cd runtime/local/bin $ ./crawl Missing seedDir : crawl &lt;seedDir&gt; &lt;crawlID&gt; &lt;solrURL&gt; &lt;numberOfRounds&gt; nutch命令： 1234567891011121314151617181920212223$ ./nutch Usage: nutch COMMANDwhere COMMAND is one of: inject inject new urls into the database hostinject creates or updates an existing host table from a text file generate generate new batches to fetch from crawl db fetch fetch URLs marked during generate parse parse URLs marked during fetch updatedb update web table after parsing updatehostdb update host table after parsing readdb read/dump records from page database readhostdb display entries from the hostDB elasticindex run the elasticsearch indexer solrindex run the solr indexer on parsed batches solrdedup remove duplicates from solr parsechecker check the parser for a given url indexchecker check the indexing filters for a given url plugin load a plugin and run one of its classes main() nutchserver run a (local) Nutch server on a user defined port junit runs the given JUnit test or CLASSNAME run the class named CLASSNAMEMost commands print help when invoked w/o parameters. 接下来可以抓取网页了。 6. 参考文章 Nutch-2.2.1学习 Nutch 快速入门(Nutch 2.2.1)]]></content>
      <categories>
        <category>search-engine</category>
      </categories>
      <tags>
        <tag>solr</tag>
        <tag>nutch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python开发框架Flask]]></title>
    <url>%2F2014%2F05%2F11%2Fflask-intro%2F</url>
    <content type="text"><![CDATA[1. Flask介绍Flask 是一个基于Python的微型的web开发框架。虽然Flask是微框架，不过我们并不需要像别的微框架建议的那样把所有代码都写到单文件中。毕竟微框架真正的含义是简单和短小。 关于Flask值得知道的一些事： Flask由Armin Ronacher于2010年创建。 Flask的灵感来自Sinatra。（Sinatra是一个极力避免小题大作的创建web应用的Ruby框架。） Flask 依赖两个外部库： Jinja2 模板引擎和 Werkzeug WSGI 工具集。 Flask遵循“约定优于配置”以及合理的默认值原则。 默认情况下，Flask 不包含数据库抽象层、表单验证或是任何其它现有库可以胜任的东西。作为替代的是，Flask 支持扩展来给应用添加这些功能，如同是在 Flask 自身中实现。众多的扩展提供了数据库集成、表单验证、上传处理、多种开放认证技术等功能。 Flask 数目众多的配置选项在初始状况下都有一个明智的默认值，并遵循一些惯例。 例如，按照惯例，模板和静态文件存储在应用的 Python 源代码树下的子目录中，名称分别为 templates 和 static 。虽然可以更改这个配置，但你通常不必这么做， 尤其是在刚接触 Flask 的时候。 2. Flask安装你首先需要 Python 2.6 或更高的版本，所以请确认有一个最新的 Python 2.x 安装。 virtualenvvirtualenv 允许多个版本的 Python 同时存在，对应不同的项目。 它实际上并没有安装独立的 Python 副本，但是它确实提供了一种巧妙的方式来让各项目环境保持独立。 如果你在 Mac OS X 或 Linux下，下面两条命令可能会适用: 1$ sudo easy_install virtualenv 或更好的: 1$ sudo pip install virtualenv 上述的命令会在你的系统中安装 virtualenv。它甚至可能会存在于包管理器中，如果你使用 Ubuntu ，可以尝试: 1$ sudo apt-get install python-virtualenv 现在你只需要键入以下的命令来激活 virtualenv 中的 Flask: 1$ pip install Flask 全局安装这样也是可以的，只需要以 root 权限运行 pip: 1$ sudo pip install Flask 3. Flask入门一个最小的 Flask 应用看起来是这样: 123456789from flask import Flaskapp = Flask(__name__)@app.route(&apos;/&apos;)def hello_world(): return &apos;Hello World!&apos;if __name__ == &apos;__main__&apos;: app.run() 把它保存为 hello.py（或是类似的），然后用 Python 解释器来运行。 12$ python hello.py * Running on http://127.0.0.1:5000/ 现在访问http://127.0.0.1:5000/ 我们来解释一下上面的代码吧： 第一行导入了Flask类，以便创建一个Flask应用的实例。 接下来一行我们创建了一个Flask类的实例。这是一个WSGI应用实例。WSGI是”Web服务器网关接口”Web Service Gateway Interface）的缩写，同时也是架设web项目的Python标准。这一行要告诉Flask到哪里去找应用所需的静态资源和模板。在我们的例子中，我们传递了name，让Flask在当前模块内定位资源。 接着我们定义了一些关于/的路由。第一个路由是为根路径/准备的，第二个则对应于类似/shekhar、/abc之类的路径。对于/路由，我们将初始的name设定为Guest。如果用户访问 http://localhost:5000/ ，那么他会看到Hello Guest。如果用户访问 http://localhost:5000/shekhar ，那么他会看到 Hello shekhar。 最后我们用 run() 函数来让应用运行在本地服务器上。 其中 if __name__ == &#39;__main__&#39;: 确保服务器只会在该脚本被 Python 解释器直接执行的时候才会运行，而不是作为模块导入的时候。 如果你禁用了 debug 或信任你所在网络的用户，你可以简单修改调用 run() 的方法使你的服务器公开可用，如下: 1app.run(host=&apos;0.0.0.0&apos;) 这会让操作系统监听所有公开的IP。 有两种途径来启用调试模式。一种是在应用对象上设置: 12app.debug = Trueapp.run() 另一种是作为 run 方法的一个参数传入: 1app.run(debug=True) 4. 总结本文简单介绍了Flask框架的安装和使用，如果你想要深入研究 Flask 的话，可以查看 API。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bower介绍]]></title>
    <url>%2F2014%2F05%2F10%2Fbower-intro%2F</url>
    <content type="text"><![CDATA[1. bower介绍Bower 是用于 web 前端开发的包管理器。对于前端包管理方面的问题，它提供了一套通用、客观的解决方案。它通过一个 API 暴露包之间的依赖模型，这样更利于使用更合适的构建工具。bower 没有系统级的依赖，在不同 app 之间也不互相依赖，依赖树是扁平的。 Bower 运行在 Git 之上，它将所有包都视作一个黑盒子。任何类型的资源文件都可以打包为一个模块，并且可以使用任何规范（例如：AMD、CommonJS 等）。 包管理工具一般有以下的功能： 注册机制：每个包需要确定一个唯一的 ID 使得搜索和下载的时候能够正确匹配，所以包管理工具需要维护注册信息，可以依赖其他平台。 文件存储：确定文件存放的位置，下载的时候可以找到，当然这个地址在网络上是可访问的。 上传下载：这是工具的主要功能，能提高包使用的便利性。比如想用 jquery 只需要 install 一下就可以了，不用到处找下载。上传并不是必备的，根据文件存储的位置而定，但需要有一定的机制保障。 依赖分析：这也是包管理工具主要解决的问题之一，既然包之间是有联系的，那么下载的时候就需要处理他们之间的依赖。下载一个包的时候也需要下载依赖的包。 功能介绍，摘自文章：http://chuo.me/2013/02/twitter-bower.html 2. 安装bower 插件是通过 npm, node.js 包管理器安装和管理的。 npm 是 node 程序包管理器。它是捆绑在 node.js 的安装程序上的，所以一旦你已经安装了 node，npm 也就安装好了。 在 mac上 安装 node.js 方法： 1brew install nodejs 通过 npm 安装 bower 到全局环境中： 1npm install -g bower 3. 使用安装之后，可以通过 bower help 命令可以获取更多帮助信息。了解了这些信息就可以开始了。 安装包及其依赖的包Bower 提供了几种方式用于安装包： 123456# Using the dependencies listed in the current directory's bower.jsonbower install# Using a local or remote packagebower install &lt;package&gt;# Using a specific Git-tagged version from a remote packagebower install &lt;package&gt;#&lt;version&gt; 其中，&lt;package&gt; 可以是以下列出的一种： 注册到 bower 中的一个包名, 例如：jquery。 一个 Git 仓库地址,例如： git://github.com/someone/some-package.git 一个本地的 Git 目录 github 的别名,例如：someone/some-package (defaults to GitHub)。 一个文件 url，包括 zip 和t ar.gz 文件。 举例来看一下来如何使用 bower 安装 jQuery，在你想要安装该包的地方创建一个新的文件夹，键入如下命令： 1$ bower install jquery 上述命令完成以后，你会在你刚才创建的目录下看到一个 bower_components 的文件夹，其中目录如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115$ tree bower_components/bower_components└── jquery ├── MIT-LICENSE.txt ├── bower.json ├── dist │ ├── jquery.js │ ├── jquery.min.js │ └── jquery.min.map └── src ├── ajax │ ├── jsonp.js │ ├── load.js │ ├── parseJSON.js │ ├── parseXML.js │ ├── script.js │ ├── var │ │ ├── nonce.js │ │ └── rquery.js │ └── xhr.js ├── ajax.js ├── attributes │ ├── attr.js │ ├── classes.js │ ├── prop.js │ ├── support.js │ └── val.js ├── attributes.js ├── callbacks.js ├── core │ ├── access.js │ ├── init.js │ ├── parseHTML.js │ ├── ready.js │ └── var │ └── rsingleTag.js ├── core.js ├── css │ ├── addGetHookIf.js │ ├── curCSS.js │ ├── defaultDisplay.js │ ├── hiddenVisibleSelectors.js │ ├── support.js │ ├── swap.js │ └── var │ ├── cssExpand.js │ ├── getStyles.js │ ├── isHidden.js │ ├── rmargin.js │ └── rnumnonpx.js ├── css.js ├── data │ ├── Data.js │ ├── accepts.js │ └── var │ ├── data_priv.js │ └── data_user.js ├── data.js ├── deferred.js ├── deprecated.js ├── dimensions.js ├── effects │ ├── Tween.js │ └── animatedSelector.js ├── effects.js ├── event │ ├── alias.js │ └── support.js ├── event.js ├── exports │ ├── amd.js │ └── global.js ├── intro.js ├── jquery.js ├── manipulation │ ├── _evalUrl.js │ ├── support.js │ └── var │ └── rcheckableType.js ├── manipulation.js ├── offset.js ├── outro.js ├── queue │ └── delay.js ├── queue.js ├── selector-native.js ├── selector-sizzle.js ├── selector.js ├── serialize.js ├── sizzle │ └── dist │ ├── sizzle.js │ ├── sizzle.min.js │ └── sizzle.min.map ├── traversing │ ├── findFilter.js │ └── var │ └── rneedsContext.js ├── traversing.js ├── var │ ├── arr.js │ ├── class2type.js │ ├── concat.js │ ├── hasOwn.js │ ├── indexOf.js │ ├── pnum.js │ ├── push.js │ ├── rnotwhite.js │ ├── slice.js │ ├── strundefined.js │ ├── support.js │ └── toString.js └── wrap.js23 directories, 88 files 包的使用现在就可以在应用程序中使用 jQuery 包了，在 jQuery 里创建一个简单的 html5 文件： 12345678910111213141516171819&lt;!doctype html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Learning bower&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;button&gt;Animate Me!!&lt;/button&gt;&lt;div style="background:red;height:100px;width:100px;position:absolute;"&gt;&lt;/div&gt;&lt;script type="text/javascript" src="bower_components/jquery/jquery.min.js"&gt;&lt;/script&gt;&lt;script type="text/javascript"&gt; $(document).ready(function()&#123; $("button").click(function()&#123; $("div").animate(&#123;left:'250px'&#125;); &#125;); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 正如你所看到的，你刚刚引用 jquery.min.js 文件，现阶段完成。 查看安装的包执行以下命令可以列出所有本地安装的包。 1$ bower list 查找包查找bower注册的包： 1$ bower search [&lt;name&gt;] 只需执行 bower search 命令即可列出所有已经注册的包。 包的信息如果你想看到关于特定的包的信息，可以使用 info 命令来查看该包的所有信息： 12345678910111213141516171819202122232425262728293031323334353637$ bower info jquerybower cached git://github.com/jquery/jquery.git#2.1.1bower validate 2.1.1 against git://github.com/jquery/jquery.git#*&#123; name: 'jquery', version: '2.1.1', main: 'dist/jquery.js', license: 'MIT', ignore: [ '**/.*', 'build', 'speed', 'test', '*.md', 'AUTHORS.txt', 'Gruntfile.js', 'package.json' ], devDependencies: &#123; sizzle: '1.10.19', requirejs: '2.1.10', qunit: '1.14.0', sinon: '1.8.1' &#125;, keywords: [ 'jquery', 'javascript', 'library' ], homepage: 'https://github.com/jquery/jquery'&#125;Available versions: - 2.1.1 - 2.1.1-rc2 - 2.1.1-rc1 注册包可以注册自己的包，这样其他人也可以使用了: 1$ bower register project git://github.com/yourname/project 包的卸载卸载包可以使用 uninstall 命令： 1$ bower uninstall jquery 配置文件每个包应该有一个配置文件，描述包的信息，jquery 的配置文件为 bower.json。 123456789101112131415161718192021222324252627&#123; "name": "jquery", "version": "2.1.1", "main": "dist/jquery.js", "license": "MIT", "ignore": [ "**/.*", "build", "speed", "test", "*.md", "AUTHORS.txt", "Gruntfile.js", "package.json" ], "devDependencies": &#123; "sizzle": "1.10.19", "requirejs": "2.1.10", "qunit": "1.14.0", "sinon": "1.8.1" &#125;, "keywords": [ "jquery", "javascript", "library" ]&#125; name 和 version 描述包的名称和版本，dependencies 描述这个包依赖的其他包。main 指定包中的静态文件，可以为一个数组。license 指定版权协议，ignore 指定忽略哪些文件，devDependencies 指定依赖，keywords 描述该包的关键字。 除了包的配置文件，Bower 有一个项目的配置文件 .bowerrc ，存在于当期项目目录下，和 bower.json 文件同级，该文件内容如下： 1234&#123; "directory": "src/main/resources/static/libs", "json": "bower.json"&#125; 参数说明： directory：配置下载的依赖存放路径，默认为 bower_components json：保存项目依赖的文件名称，默认为 bower.json 更多的配置参数，需要查验官方文档，这里暂时不做补充。 类似的，你可以配置一个全局的配置文件 ~/.bowerrc，位于用户根目录下面。 项目中使用bower.json 文件的使用可以让包的安装更容易，你可以在应用程序的根目录下创建一个名为 bower.json 的文件，并定义它的依赖关系。使用 bower init命令来创建 bower.json 文件： 12345678910111213141516171819202122232425262728293031[?] name: blog[?] version: 0.0.0[?] description: [?] main file: [?] what types of modules does this package expose? [?] keywords: [?] authors: javachen &lt;june.chan@javachen.com&gt;[?] license: MIT[?] homepage: [?] would you like to mark this package as private which prevents it from being accidentally published to the registry? Noaccidentally published to the registry? (y/N) &#123; name: 'blog', version: '0.0.0', authors: [ 'javachen &lt;june.chan@javachen.com&gt;' ], license: 'MIT', ignore: [ '**/.*', 'node_modules', 'bower_components', 'test', 'tests' ], dependencies: &#123; jquery: '~2.1.1' &#125;&#125;[?] Looks good? Yes 注意看，它已经加入了 jQuery 依赖关系。 现在假设也想用 twitter bootstrap，我们可以用下面的命令安装 twitter bootstrap 并更新 bower.json文件： 1$ bower install bootstrap --save 它会自动安装最新版本的 bootstrap 并更新 bower.json 文件： 12345678910111213141516171819&#123; name: 'blog', version: '0.0.0', authors: [ 'javachen &lt;june.chan@javachen.com&gt;' ], license: 'MIT', ignore: [ '**/.*', 'node_modules', 'bower_components', 'test', 'tests' ], dependencies: &#123; jquery: '~2.1.1', bootstrap: '~3.0.0' &#125;&#125; 如果想查看有哪些包和文件，可执行 bower list --path。比如安装了 jquery，可以看到以下信息： 123&#123; "jquery": "bower_components/jquery/dist/jquery.js"&#125; 现在就可以使用了，在当前目录建一个页面，script 嵌入需要的 js。 4. 总结Bower 类似 maven 用于管理 javascript 的版本及其依赖，使用非常简单。 5. 参考文章 用于web前端开发的包管理器 twitter 的包管理工具 - bower Day 1: Bower —— 管理你的客户端依赖关系]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>node.js</tag>
        <tag>bower</tag>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[All Things Markdown]]></title>
    <url>%2F2014%2F04%2F24%2Fall-things-markdown%2F</url>
    <content type="text"><![CDATA[目录 概述 特点 语法 编辑器 浏览器插件 实现版本 参考资料 概述Markdown 是一种轻量级标记语言，创始人为约翰·格鲁伯（John Gruber）和亚伦·斯沃茨（Aaron Swartz）。它允许人们“使用易读易写的纯文本格式编写文档，然后转换成有效的XHTML(或者HTML)文档”。这种语言吸收了很多在电子邮件中已有的纯文本标记的特性。 特点兼容 HTML要在Markdown中输写HTML区块元素，比如&lt;div&gt;、&lt;table&gt;、&lt;pre&gt;、&lt;p&gt; 等标签，必须在前后加上空行与其它内容区隔开，还要求它们的开始标签与结尾标签不能用制表符或空格来缩进。Markdown 的生成器有足够智能，不会在 HTML 区块标签外加上不必要的 &lt;p&gt; 标签。 例子如下，在 Markdown 文件里加上一段 HTML 表格： 1234567891011121314这是一个普通段落。&lt;table&gt; &lt;tr&gt; &lt;td&gt;a&lt;/td&gt; &lt;td&gt;a&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;b&lt;/td&gt; &lt;td&gt;b&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;这是另一个普通段落。 请注意: 在 HTML 区块标签间的 Markdown 格式语法将不会被处理。 HTML 的区段（行内）标签如 &lt;span&gt;、&lt;cite&gt;、&lt;del&gt; 可以在 Markdown 的段落、列表或是标题里随意使用。 特殊字符自动转换Markdown会对一些特殊字符进行转化，如：&amp;、©、&lt;、&gt;,如果&lt;、&gt;用作html标签的界定符，则不会对其转换。 不过需要注意的是，code 范围内，不论是行内还是区块， &lt; 和 &amp; 两个符号都一定会被转换成 HTML 实体。 语法标题Markdown 支持两种标题的语法，类 Setext 和类 atx 形式。 类 Atx 形式则是在行首插入 1 到 6 个 # ，对应到标题 1 到 6 阶，例如： 123# This is an &lt;h1&gt; tag## This is an &lt;h2&gt; tag###### This is an &lt;h6&gt; tag 你可以选择性地「闭合」类 atx 样式的标题，这纯粹只是美观用的，若是觉得这样看起来比较舒适，你就可以在行尾加上 #，而行尾的 # 数量也不用和开头一样（行首的井字符数量决定标题的阶数） 类 Setext 形式是用底线的形式，利用 = （最高阶标题）和 - （第二阶标题），例如： 12345This is an H1=============This is an H2------------- 任何数量的 = 和 - 都可以有效果。 强调Markdown 使用星号（*）和底线（_）作为标记强调字词的符号，被 * 或 _ 包围的字词会被转成用 &lt;em&gt; 标签包围，用两个 * 或 _ 包起来的话，则会被转成 &lt;strong&gt;，例如： 1234567*This text will be italic*_This will also be italic_**This text will be bold**__This will also be bold__*You **can** combine them* 断行如果你真的想在Markdown中插入换行标签&lt;br/&gt;，你可以在行尾输入两个或以上的空格，然后回车。 这样插入换行十分麻烦，但是“每个换行都转换为&lt;br/&gt;”在 Markdown中并不合适，所以只在你确定你需要时手动添加。 列表Markdown 支持有序列表和无序列表。 无序无序列表使用星号、加号或是减号作为列表标记： 123* Red* Green* Blue 有序有序列表则使用数字接着一个英文句点： 1231. Bird2. McHale3. Parish 很重要的一点是，你在列表标记上使用的数字并不会影响输出的 HTML 结果。 如果你的列表标记写成： 1231. Bird1. McHale1. Parish 或甚至是： 1233. Bird1. McHale8. Parish 你都会得到完全相同的 HTML 输出。 注意： 如果列表项目间用空行分开，在输出 HTML 时 Markdown 就会将项目内容用 &lt;p&gt; 标签包起来 列表项目可以包含多个段落，每个项目下的段落都必须缩进 4 个空格或是 1 个制表符 如果要放代码区块的话，该区块就需要缩进两次，也就是 8 个空格或是 2 个制表符 区块引用Markdown 标记区块引用是使用类似 email 中用 &gt; 的引用方式。 1234As Kanye West said:&gt; We&apos;re living the future so&gt; the present is our past. Markdown 也允许你偷懒只在整个段落的第一行最前面加上 &gt; 区块引用可以嵌套（例如：引用内的引用），只要根据层次加上不同数量的 &gt; ： 12345&gt; This is the first level of quoting.&gt;&gt; &gt; This is nested blockquote.&gt;&gt; Back to the first level. 引用的区块内也可以使用其他的 Markdown 语法，包括标题、列表、代码区块等： 12345678&gt; ## 这是一个标题。&gt; &gt; 1. 这是第一行列表项。&gt; 2. 这是第二行列表项。&gt; &gt; 给出一些例子代码：&gt; &gt; return shell_exec(&quot;echo $input | $markdown_script&quot;); 分隔线你可以在一行中用三个以上的星号、减号、底线来建立一个分隔线，行内不能有其他东西。你也可以在星号或是减号中间插入空格。下面每种写法都可以建立分隔线： 12345* * *********- - ---------------------------------------- 代码区块和程序相关的写作或是标签语言原始码通常会有已经排版好的代码区块，通常这些区块我们并不希望它以一般段落文件的方式去排版，而是照原来的样子显示，Markdown 会用 &lt;pre&gt; 和 &lt;code&gt; 标签来把代码区块包起来。 要在 Markdown 中建立代码区块很简单，只要简单地缩进 4 个空格或是 1 个制表符就可以 内联代码1I think you should use an `&lt;addr&gt;` element here instead. 链接Markdown 支持两种形式的链接语法： 行内式和参考式两种形式。 不管是哪一种，链接文字都是用 [方括号] 来标记。 要建立一个行内式的链接，只要在方块括号后面紧接着圆括号并插入网址链接即可，如果你还想要加上链接的 title 文字，只要在网址后面，用双引号把 title 文字包起来即可，例如： 12This is [an example](http://example.com/ &quot;Title&quot;) inline link.[This link](http://example.net/) has no title attribute. 参考式的链接**是在链接文字的括号后面再接上另一个方括号，而在第二个方括号里面要填入用以辨识链接的标记： 1This is [an example][id] reference-style link. 你也可以选择性地在两个方括号中间加上一个空格： 1This is [an example] [id] reference-style link. 接着，在文件的任意处，你可以把这个标记的链接内容定义出来（注意去掉冒号前面空格）： 1[id] : http://example.com/ &quot;Optional Title Here&quot; 引用本地资源，可以使用相对路径：[about me](/about/) 隐式链接标记功能让你可以省略指定链接标记，这种情形下，链接标记会视为等同于链接文字，要用隐式链接标记只要在链接文字后面加上一个空的方括号，如果你要让 “Google” 链接到 google.com，你可以简化成： 1[Google][] 然后定义链接内容（注意去掉冒号前面空格）： 1[Google] : http://google.com/ 由于链接文字可能包含空白，所以这种简化型的标记内也许包含多个单词： 1Visit [Daring Fireball][] for more information. 然后接着定义链接（注意去掉冒号前面空格）： 1[Daring Fireball] : http://daringfireball.net/ 链接的定义可以放在文件中的任何一个地方，我比较偏好直接放在链接出现段落的后面，你也可以把它放在文件最后面，就像是注解一样。 下面是一个参考式链接的范例（注意去掉冒号前面空格）： 123456I get 10 times more traffic from [Google] [1] than from[Yahoo] [2] or [MSN] [3].[1] : http://google.com/ &quot;Google&quot;[2] : http://search.yahoo.com/ &quot;Yahoo Search&quot;[3] : http://search.msn.com/ &quot;MSN Search&quot; 如果改成用链接名称的方式写（注意去掉冒号前面空格）： 123456I get 10 times more traffic from [Google][] than from[Yahoo][] or [MSN][]. [google] : http://google.com/ &quot;Google&quot; [yahoo] : http://search.yahoo.com/ &quot;Yahoo Search&quot; [msn] : http://search.msn.com/ &quot;MSN Search&quot; 图片Markdown 使用一种和链接很相似的语法来标记图片，同样也允许两种样式： 行内式和参考式。 行内式的图片语法看起来像是： 123![Alt text](/path/to/img.jpg)![Alt text](/path/to/img.jpg &quot;Optional title&quot;) 详细叙述如下： 一个惊叹号 ! 接着一个方括号，里面放上图片的替代文字 接着一个普通括号，里面放上图片的网址，最后还可以用引号包住并加上 选择性的 ‘title’ 文字。 参考式的图片语法则长得像这样： 1![Alt text][id] 「id」是图片参考的名称，图片参考的定义方式则和连结参考一样（注意去掉冒号前面空格）： 1[id] : url/to/image &quot;Optional title attribute&quot; 到目前为止， Markdown 还没有办法指定图片的宽高，如果你需要的话，你可以使用普通的 标签。 自动链接Markdown 支持以比较简短的自动链接形式来处理网址和电子邮件信箱，只要是用方括号包起来， Markdown 就会自动把它转成链接。一般网址的链接文字就和链接地址一样，例如 1&lt;http://example.com/&gt; Markdown 会转为： 1&lt;a href=&quot;http://example.com/&quot;&gt;http://example.com/&lt;/a&gt; 邮址的自动链接也很类似，例如： 1&lt;address@example.com&gt; Markdown 会转成： 1&lt;a href=&quot;mailto:address@example.com&quot;&gt;address@example.com&lt;/a&gt; 反斜杠Markdown 可以利用反斜杠来插入一些在语法中有其它意义的符号，例如：如果你想要用星号加在文字旁边的方式来做出强调效果（但不用 &lt;em&gt; 标签），你可以在星号的前面加上反斜杠： 1\*literal asterisks\* Markdown 支持以下这些符号前面加上反斜杠来帮助插入普通的符号： 123456789101112\ 反斜线` 反引号* 星号_ 底线&#123;&#125; 花括号[] 方括号() 括弧# 井字号+ 加号- 减号. 英文句点! 惊叹号 表格使用语法解释引擎 Redcarpet(需要开启tables选项)，则表格如下定义： 1234|head1 head1 head1|head2 head2 head2|head3 head3 head3|head4 head4 head4||---|:---|:---:|---:||row1text1|row1text3|row1text3|row1text4||row2text1|row2text3|row2text3|row2text4| 其中:所在位置表示表格的位置对齐 添加 table thead tobody th tr td 样式后显示的效果是： head1 head1 head1 head2 head2 head2 head3 head3 head3 head4 head4 head4 row1text1 row1text3 row1text3 row1text4 row2text1 row2text3 row2text3 row2text4 Github中定义表格方式如下： 1234First Header | Second Header------------ | -------------Content from cell 1 | Content from cell 2Content in the first column | Content in the second column 显示的效果如下： First Header Second Header Content from cell 1 Content from cell 2 Content in the first column Content in the second column 在线编辑器作为一种小型标记语言，Markdown很容易阅读，也很容易用普通的文本编辑器编辑。另外也有一些编辑器专为Markdown设计，可以直接预览文档的样式。下面有一些编辑器可供参考： Cmd Markdown 支持实时同步预览，区分写作和阅读模式，支持在线存储，分享文稿网址。 Dillinger.io 一个在线Markdown编辑器，提供实时预览以及到 GitHub 和 Dropbox 的拓展连接。 notepag 另一个在线Markdown编辑器，支持实时预览，提供临时网址和和密码，可以分享给其他人。 Mou 一个Mac OS X上的Markdown编辑器。 MarkdownPad a full-featured Markdown editor for Windows. WMD a Javascript WYSIWYM editor for Markdown (from AttackLab) PageDown 一个Javascript WYSIWYM Markdown编辑器 (来自 StackOverflow) IPython Notebook 以ipython为后台，利用浏览器做IDE，支持MarkDown与LaTex公式。 http://mahua.jser.me/ 一个在线编辑markdown文档的编辑器 http://markable.in/editor/ A remarkable online markdown editor OsChina OsChina在线编辑器 Logdown 是台湾一个博客写手和开发者在一个周末和三位朋友在24小时之内做的一个Hackathon 項目。这是一个支持Markdown的博客写作平台。在国际上也引起关注。它的写作界面是单栏宽屏。 &lt;jianshu.io&gt; 这是一个支持Markdown的中文写作社区。 有记 | noteton 有记提供基于云笔记服务的博客发布平台。 浏览器插件 MaDe (Chrome) Markdown Here (Chrome, Firefox, Safari, and Thunderbird) Poe: Markdown Editor (Chrome) MarkDown (Chrome) StackEdit (Chrome) 扩展程序Markdown Preview Plus (Chrome) 实现版本由于Markdown的易读易写，很多人用不同的编程语言实现了多个版本的解析器和生成器。下面是一个按编程语言排序的实现列表。 C Sundown, 一个用C写的Markdown实现。 Discount, 一个Markdown标记语言的C语言实现版本。 peg-markdown, 一个用C写的，使用了PEG (parsing expression grammar)的Markdown实现。 Java MarkdownJ the pure Java port of Markdown. Pegdown, a pure-Java Markdown implementation based on a PEG parser MarkdownPapers, Java implementation based on a JavaCC parser Txtmark, another Markdown implementation written in Java Lua Markdown.lua, a Markdown implementation in Lua Lunamark, a markdown to HTML and LaTeX converter written in Lua, using a PEG grammar PHP PHP Markdown and Markdown Extra Markdown Viewer for PHP, allows the viewing of a Mardown doc via a local PHP server (a wrapper for PHP Markdown) JavaScript Uedit, a Javascript “WYSIWYM”editor for Markdown Strapdown.js - JavaScript客户端解析markdown内容为html Marked - Fast Markdown parser in JavaScript Python Markdown in Python, A Python implementation of Markdown Misaka, a Python binding for Sundown. Ruby BlueCloth, an implementation of Markdown in Ruby Scala Knockoff, a Markdown implementation written in Scala using Parser Combinators Actuarius, another Markdown implementation written in Scala using Parser Combinators GO Blackfriday, another Markdown implementation written in Go 其它 MarkdownSharp, a slightly modified C# implementation of the Markdown markup language. Developed and used by Stack Overflow. Markdownr.com, a simple website to preview markdown in real time Pandoc, a universal document converter written in Haskell ReText, an implementation for Markdown and reStructuredText. 其他资料 Markdown+R写作 参考资料 [1] 维基百科 Markdown [2] Markdown 语法说明 (简体中文版) [3] GitHub Guides:Mastering Markdown [4] Daring Fireball [5] Markdown在线写作速成]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重装Mac系统之后]]></title>
    <url>%2F2014%2F04%2F23%2Fafter-reinstall-mac%2F</url>
    <content type="text"><![CDATA[本文主要记录重装Mac系统之后的一些软件安装和环境变量配置。 系统偏好配置设置主机名： 1$ sudo scutil --set HostName june－mac 设置鼠标滚轮滑动的方向：系统偏好设置－－&gt;鼠标－－&gt;”滚动方向：自然”前面的勾去掉 显示/隐藏Mac隐藏文件： 12defaults write com.apple.finder AppleShowAllFiles -bool true #显示Mac隐藏文件的命令defaults write com.apple.finder AppleShowAllFiles -bool false #隐藏Mac隐藏文件的命令 触控板 光标与点按 &gt; 三指移动 ：这样就可以三指拖动文件了 光标与点按 &gt; 轻拍来点按 ：习惯了轻点完成实际按击 光标与点按 &gt; 跟踪速度 ：默认的指针滑动速度有点慢，设置成刻度7差不多了 键盘 快捷键 &gt; 服务 &gt; 新建位于文件夹位置的终端标签：勾选这设置并设置了快捷键（control+cmt+c），以后在Finder中选择一个目录按下快捷键就可以打开终端并来到当前当前目录，功能很实用啊！注意：在Finder中文件列表使用分栏方式显示时快捷键是无效的。 网络-高级… &gt; DNS ：公共DNS是必须添加的 223.6.6.6 阿里提供的 8.8.4.4 google提供的 114.114.114.114 114服务提供的 Apps VirtualBox Vagrant Unarchiver: 支持多种格式（包括 windows下的格式）的压缩/解压缩工具 OminiFocus ：时间管理工具 Mou：Markdown 编辑器，国人出品 Dash Xmind Shadowsocks WizNote：为知笔记 yEd：画时序图 Iterm2 Moco，一个用来模拟服务器的工具。在服务器端没有开发完成时，可以通过配置来搭建一个模拟服务， 这样可以方便客户端的开发。 HomebrewBrew 是 Mac 下面的包管理工具，通过 Github 托管适合 Mac 的编译配置以及 Patch，可以方便的安装开发工具。 1$ ruby -e "$(curl -fsSL https://raw.github.com/Homebrew/homebrew/go/install)" 通过brew安装软件： 1$ brew install git git-flow curl wget putty tmux ack source-highlight aria2 dos2unix nmap iotop htop ctags tree openvpn 紧接着，我们需要做一件事让通过 Homebrew 安装的程序的启动链接 (在 /usr/local/bin中）可以直接运行，无需将完整路径写出。通过以下命令将 /usr/local/bin 添加至 $PATH 环境变量中: 1$ echo 'export PATH="/usr/local/bin:$PATH"' &gt;&gt; ~/.bash_profile Cmd+T 打开一个新的 terminal 标签页，运行以下命令，确保 brew 运行正常。 1$ brew doctor 使用安装一个包，可以简单的运行： 1$ brew install &lt;package_name&gt; 更新 Homebrew 在服务器端上的包目录： 1$ brew update 查看你的包是否需要更新： 1$ brew outdated 更新包： 1$ brew upgrade &lt;package_name&gt; Homebrew 将会把老版本的包缓存下来，以便当你想回滚至旧版本时使用。但这是比较少使用的情况，当你想清理旧版本的包缓存时，可以运行： 1$ brew cleanup 查看你安装过的包列表（包括版本号）： 1$ brew list --versions CaskBrew cask 是类似 Brew 的管理工具， 直接提供 dmg 级别的二进制包，（Brew 是不带源码，只有对应项目所在的 URL）。我们可以通过 Homebrew Cask 优雅、简单、快速的安装和管理 OS X 图形界面程序，比如 Google Chrome 和 Dropbox。 Brew cask 安装： 12$ brew tap phinze/homebrew-cask$ brew install brew-cask 我通过 Brew cask 安装的软件： 123$ brew cask install google-chrome omnigraffle xtrafinder$ brew update &amp;&amp; brew upgrade brew-cask &amp;&amp; brew cleanup # 更新 相对于 brew cask 的安装方式，本人更倾向于到 App Store 或官方下载 OS X 图形界面程序。主要因为名字不好记忆、偶尔需要手动更新，另外当你使用 Alfred 或 Spotlight ，你将发现将程序安装在 ~/Application 会很方便。 oh-my-zsh使用 Homebrew 完成 zsh 和 zsh completions 的安装 1brew install zsh zsh-completions 把默认 Shell 换为 zsh。 1$ chsh -s /bin/zsh 然后用下面的两句（任选其一）可以自动安装 oh-my-zsh： 1$ curl -L https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh | sh 1$ wget --no-check-certificate https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O - | sh 编辑 ~/.zshrc： 12echo &apos;source ~/.bashrc&apos; &gt;&gt;~/.zshrcecho &apos;source ~/.bash_profile&apos; &gt;&gt;~/.zshrc 用文本编辑器或 vi 打开 .zshrc 进行以下编辑: 12ZSH_THEME=pygmalionplugins=(git colored-man colorize github jira vagrant virtualenv pip python brew osx zsh-syntax-highlighting) 使用 ctrl+r 查找历史命令，在 ~/.zshrc 中添加： 1bindkey &quot;^R&quot; history-incremental-search-backward 使用使用上默认加了很多快捷映射，如： ~: 进入用户根目录，可以少打cd三个字符了 l: 相当于ls -lah ..: 返回上层目录 ...: 返回上上层目录 -: 打开上次所在目录 具体的可以查看其配置文件。 Git安装： 1$ brew install git 好的，现在我们来测试一下 gti 是否安装完好： 1$ git --version 运行 $ which git 将会输出 /usr/local/bin/git. 接着，我们将定义你的 Git 帐号（与你在 GitHub 使用的用户名和邮箱一致） 12$ git config --global user.name "Your Name Here"$ git config --global user.email "your_email@youremail.com" 这些配置信息将会添加进 ~/.gitconfig 文件中. 我们将推荐使用 HTTPS 方法（另一个是 SSH），将你的代码推送到 Github 上的仓库。如果你不想每次都输入用户名和密码的话，可以按照此 描述 说的那样，运行： 1$ git config --global credential.helper osxkeychain 此外，如果你打算使用 SSH方式，可以参考此 链接。 Git Ignore创建一个新文件 ~/.gitignore ，并将以下内容添加进去，这样全部 git 仓库将会忽略以下内容所提及的文件。 1234567891011121314151617181920212223242526272829303132333435# Folder view configuration files.DS_StoreDesktop.ini# Thumbnail cache files._*Thumbs.db# Files that might appear on external disks.Spotlight-V100.Trashes# Compiled Java files.classpath.project.settingsbinbuildtargetdependency-reduced-pom.xml.gradleREADME.html.idea*.iml# Compiled Python files*.pyc# Compiled C++ files*.out# Application specific filesvenvnode_modules.sass-cache 安装Vim插件安装 pathogen： 123$ mkdir -p ~/.vim/autoload ~/.vim/bundle; \$ curl -Sso ~/.vim/autoload/pathogen.vim \ https://raw.github.com/tpope/vim-pathogen/master/autoload/pathogen.vim 安装NERDTree： 12$ cd ~/.vim/bundle$ git clone https://github.com/scrooloose/nerdtree.git 更多请参考：vim配置和插件管理 安装Ruby先安装依赖： 1$ brew install libksba autoconf automake libtool gcc libyaml readline 通过rvm安装ruby，目前需要ruby 2的版本： 1234567$ curl -L get.rvm.io | bash -s stable $ source ~/.bash_profile$ gem sources --remove https://rubygems.org/# 如果http://ruby.taobao.org/有效的话，则添加源$ gem sources -a http://ruby.taobao.org/ $ rvm install 2.2.1$ rvm --default 2.2.1 安装Jekyll1$ sudo gem install jekyll jekyll-paginate 设置环境变量： 12$ echo 'export PATH=$PATH:$HOME/.rvm/bin' &gt;&gt; ~/.bash_profile$ echo '[[ -s "$HOME/.rvm/scripts/rvm" ]] &amp;&amp; . "$HOME/.rvm/scripts/rvm"' &gt;&gt; ~/.bash_profile Java开发环境下载 jdk： jdk6：http://support.apple.com/downloads/DL1572/en_US/JavaForOSX2013-05.dmg jdk7：http://download.oracle.com/otn-pub/java/jdk/7u60-b19/jdk-7u60-macosx-x64.dmg?AuthParam=1403450902_0b8ed262d4128ca82031dcbdc2627aaf 设置 java_home 为 1.7: 1export JAVA_HOME=$(/usr/libexec/java_home -v 1.7) 使用 brew 来安装 ant、maven、ivy、forrest、springboot 等： 1$ brew install https://raw.github.com/Homebrew/homebrew-versions/master/maven30.rb ant ivy apache-forrest springboot 配置 ant、maven 和 ivy 仓库： 1234$ rm -rf ~/.ivy2/cache ~/.m2/repository$ mkdir -p ~/.ivy2 ~/.m2$ ln -s ~/app/repository/cache/ ~/.ivy2/cache$ ln -s ~/app/repository/m2/ ~/.m2/repository 注意，这里我在 ~/app/repository 有两个目录，cache 用于存放 ivy 下载的文件，m2 用于存放 maven 的仓库。 Python开发环境TODO 参考文章 Mac 开发配置手册 MacBook Pro 配置]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu系统编译Bigtop]]></title>
    <url>%2F2014%2F04%2F17%2Fbuilding-bigtop-on-ubuntu%2F</url>
    <content type="text"><![CDATA[1. 安装系统依赖系统更新并安装新的包1234567sudo apt-get updatesudo apt-get install -y cmake git-core git-svn subversion checkinstall build-essential dh-make debhelper ant ant-optional autoconf automake liblzo2-dev libzip-dev sharutils libfuse-dev reprepro libtool libssl-dev asciidoc xmlto ssh curlsudo apt-get install -y devscriptssudo apt-get build-dep pkg-config 安装Sun JDK 6或OpenJDK 7Sun JDK 6: 执行以下脚本： 12345678wget http://archive.cloudera.com/cm4/ubuntu/precise/amd64/cm/pool/contrib/o/oracle-j2sdk1.6/oracle-j2sdk1.6_1.6.0+update31_amd64.debdpkg -i oracle-j2sdk1.6_1.6.0+update31_amd64.debsudo rm /usr/lib/jvm/default-javasudo ln -s /usr/lib/jvm/j2sdk1.6-oracle /usr/lib/jvm/default-javasudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/default-java/bin/java 5sudo update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/default-java/bin/javac 5sudo update-alternatives --set java /usr/lib/jvm/default-java/bin/java OpenJDK 7: OpenJDK 6 fails to build Hadoop because of issue MAPREDUCE-4115 Need to use OpenJDK 7 123456sudo apt-get install openjdk-7-jdksudo rm /usr/lib/jvm/default-javasudo ln -s /usr/lib/jvm/java-7-openjdk-amd64 /usr/lib/jvm/default-javasudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/default-java/bin/java 5sudo update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/default-java/bin/javac 5sudo update-alternatives --set java /usr/lib/jvm/default-java/bin/java 安装Maven 312345wget http://apache.petsads.us/maven/maven-3/3.0.5/binaries/apache-maven-3.0.5-bin.tar.gztar -xzvf apache-maven-3.0.5-bin.tar.gzsudo mkdir /usr/local/maven-3sudo mv apache-maven-3.0.5 /usr/local/maven-3/ 安装Apache Forrest12345678cd $HOMEwget http://archive.apache.org/dist/forrest/0.9/apache-forrest-0.9.tar.gztar -xzvf /home/ubuntu/Downloads/apache-forrest-0.9.tar.gz# modify certain lines in the forrest-validate xml, otherwise build fails. either sed or nano are fine.sed -i 's/property name="forrest.validate.sitemap" value="$&#123;forrest.validate&#125;"/property name="forrest.validate.sitemap" value="false"/g' apache-forrest-0.9/main/targets/validate.xmlsed -i 's/property name="forrest.validate.stylesheets" value="$&#123;forrest.validate&#125;"/property name="forrest.validate.stylesheets" value="false"/g' apache-forrest-0.9/main/targets/validate.xmlsed -i 's/property name="forrest.validate.stylesheets.failonerror" value="$&#123;forrest.validate.failonerror&#125;"/property name="forrest.validate.stylesheets.failonerror" value="false"/g' apache-forrest-0.9/main/targets/validate.xmlsed -i 's/property name="forrest.validate.skins.stylesheets" value="$&#123;forrest.validate.skins&#125;"/property name="forrest.validate.skins.stylesheets" value="false"/g' apache-forrest-0.9/main/targets/validate.xml 安装protobufprotobuf版本至少需要2.4.0,具体版本视hadoop版本而定，例如hadoop-2.4.0即需要依赖protobuf-2.5.0 到 Protocol Buffers 的官网https://code.google.com/p/protobuf/下载2.5.0的安装源文件进行安装： 12345tar -zxf protobuf-2.5.0.tar.gzcd protobuf-2.5.0./configure --prefix=/usr/local/protobufmake checkmake install 安装完成后，执行 protoc --vresion 验证是否安装成功。 2. 设置环境变量创建/etc/profile.d/bigtop.sh并添加如下内容： 123456export JAVA_HOME="/usr/lib/jvm/default-java"export JAVA5_HOME="/usr/lib/jvm/default-java"export JVM_ARGS="-Xmx1024m -XX:MaxPermSize=512m"export MAVEN_HOME="/usr/local/maven-3/apache-maven-3.0.5"export MAVEN_OPTS="-Xmx1024m -XX:MaxPermSize=512m"PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:$MAVEN_HOME/bin" 将FORREST_HOME添加到~/.bashrc: 1export FORREST_HOME="$HOME/apache-forrest-0.9" 3. 下载并编译源代码123git clone git://git.apache.org/bigtop.git # put files under bigtop directorycd bigtop# you can also use a different branch, e.g. git checkout branch-0.7 为了加快编译速度，你可以修改Makefile文件中的APACHE_MIRROR和APACHE_ARCHIVE为国内的速度较快的apache镜像地址，例如：http://mirror.bit.edu.cn/apache 编译源代码： 1234567./check-env.sh # make sure all the required environment variables are setmake realcleanmake bigtop-utils-deb # build this project firstmake bigtop-jsvc-debmake bigtop-tomcat-debmake hadoop-deb # to build just for hadoop firstmake deb # build all the rest 编译之后deb输出在output目录 4. 安装和测试在使用dpkg命令安装之前，先关掉自动启动服务。使用root用欢创建/usr/sbin/policy-rc.d，该文件内容如下： 12#!/bin/shexit 101 添加执行权限： 1sudo chmod +x /usr/sbin/policy-rc.d 安装deb文件： 1234cd output/bigtop-utilssudo dpkg --install *.debcd ..sudo dpkg --install **/**.deb 最后别忘了删除掉policy-rc.d： 1sudo rm /usr/sbin/policy-rc.d 初始化hdfs： 1sudo -u hdfs hadoop namenode -format 启动服务： 1234sudo /etc/init.d/hadoop-hdfs-namenode startsudo /etc/init.d/hadoop-hdfs-datanode start#sudo /etc/init.d/hadoop-xxxx start 接下来可以查看日志和web页面是否正常了。访问http://localhost:50070/，你就可以看到hadoop-2.3.0的小清新的管理界面了。 5. 排错1) bigtop-0.7依赖的是protobuf-2.4.0而不是protobuf-2.5.0，导致编译过程出现protobuf的版本需要2.5.0的提示，请卸载2.4.0版本重新编译protobuf-2.5.0。 2) 运行make deb时出现more change data or trailer的异常(详细异常信息见下面)，请将操作系统的LANG修改为en_US 123456parsechangelog/debian: warning: debian/changelog(l4): badly formatted trailer lineLINE: -- Bigtop &lt;dev@bigtop.apache.org&gt; 四, 17 4月 2014 14:30:17 +0800parsechangelog/debian: warning: debian/changelog(l4): found eof where expected more change data or trailerdpkg-buildpackage: source package zookeeperdpkg-buildpackage: source version 3.4.5-1dpkg-buildpackage: error: unable to determine source changed by 6. 参考文章 [1] Building Bigtop on Ubuntu]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>bigtop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RHEL系统下安装atlassian-jira-5]]></title>
    <url>%2F2014%2F04%2F09%2Finstall-jira5-on-rhel-system%2F</url>
    <content type="text"><![CDATA[部署环境 操作系统：RHEL 6.4 x86_64 Jira版本：atlassian-jira-5.2.11-x64.bin 安装路径:/opt/atlassian/jira/ 数据保存路径：/opt/atlassian/application-data/jira 安装用户：jira 数据库：postgresql JDK：1.6.0_43 jira下载页面：https://www.atlassian.com/software/jira/download 安装步骤运行安装文件1$ . atlassian-jira-5.2.11-x64.bin 在安装过程中会出现选项： 确认安装 123This will install JIRA 6.2.2 on your computer.OK [o, Enter], Cancel [c]o 选择安装类型－1默认安装 －2自定义安装 －3升级 1234Choose the appropriate installation or upgrade option.Please choose one of the following:Express Install (use default settings) [1], Custom Install (recommended for advanced users) [2], Upgrade an existing JIRA installation [3, Enter]3 你可以选择2自定义安装路径、启动端口等等。 接下来选择确认直到安装成功。 初始化数据库这里我选择PostgreSql数据库，先安装数据库，然后创建用户(jira)和数据库(jira)。 123456789101112$ su - postgres-bash-4.1$ cd-bash-4.1$ cd bin-bash-4.1$ ./psql -U postgrespsql (9.0.3)Type &quot;help&quot; for help.Cannot read termcap database;using dumb terminal settings.postgres=# CREATE USER jira WITH PASSWORD &apos;redhat&apos;;postgres=# CREATE DATABASE jira owner=jira;postgres=# GRANT ALL privileges ON DATABASE jira TO jira; 然后打开浏览器范围jira页面：http://ip:8080/，在该页面选择数据库类型，并填写数据库连接信息，测试是否可以ping通，接着运行下一步,在jira官网上注册一个帐号。 破解破解文件： atlassian-extras-2.2.2.jar atlassian-extras-2.2.2.crack 执行以下命令覆盖原来文件： 12$ cp atlassian-extras-2.2.2.crack /opt/atlassian/jira/atlassian-jira/WEB-INF/classes/$ \cp atlassian-extras-2.2.2.jar /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/ 检查是否可以创建issue，并查看jira版本和过期时间。 汉化下载JIRA汉化包：JIRA-5.0-language-pack-zh_CN.jar,并在jira管理页面将其上传，然后在个人设置页面，可以设置语言为中文。 JIRA使用启动： 12$ cd /opt/atlassian/jira/bin$ sh start-jira.sh 停止： 12$ cd /opt/atlassian/jira/bin$ sh stop-jira.sh 查看日志： 12$ cd /opt/atlassian/jira/bin$ tailf catalina.out 修改JVM参数12$ vi /opt/atlassian/jira/bin/setenv.shJAVA_OPTS="-Xms1024m -Xmx2048m -XX:MaxPermSize=256m $JAVA_OPTS -Djava.awt.headless=true "]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>jira</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java笔记：Java内存模型]]></title>
    <url>%2F2014%2F04%2F09%2Fnote-about-jvm-memery-model%2F</url>
    <content type="text"><![CDATA[1. 基本概念《深入理解Java内存模型》详细讲解了java的内存模型，这里对其中的一些基本概念做个简单的笔记。以下内容摘自 《深入理解Java内存模型》读书总结 并发定义：即，并发(同时)发生。在操作系统中，是指一个时间段中有几个程序都处于已启动运行到运行完毕之间，且这几个程序都是在同一个处理机上运行，但任一个时刻点上只有一个程序在处理机上运行。 并发需要处理两个关键问题：线程之间如何通信及线程之间如何同步。 通信：是指线程之间如何交换信息。在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。 同步：是指程序用于控制不同线程之间操作发生相对顺序的机制。在Java中，可以通过volatile、synchronized、锁等方式实现同步。 主内存和本地内存主内存：即 main memory。在java中，实例域、静态域和数组元素是线程之间共享的数据，它们存储在主内存中。 本地内存：即 local memory。 局部变量，方法定义参数 和 异常处理器参数是不会在线程之间共享的，它们存储在线程的本地内存中。 重排序定义：重排序是指“编译器和处理器”为了提高性能，而在程序执行时会对程序进行的重排序。 说明：重排序分为“编译器”和“处理器”两个方面，而“处理器”重排序又包括“指令级重排序”和“内存的重排序”。 关于重排序，我们需要理解它的思想：为了提高程序的并发度，从而提高性能！但是对于多线程程序，重排序可能会导致程序执行的结果不是我们需要的结果！因此，就需要我们通过volatile、synchronize、锁等方式实现同步。 内存屏障定义：包括LoadLoad, LoadStore, StoreLoad, StoreStore共4种内存屏障。内存屏障是与相应的内存重排序相对应的。 作用：通过内存屏障可以禁止特定类型处理器的重排序，从而让程序按我们预想的流程去执行。 happens-before定义：JDK5(JSR-133)提供的概念，用于描述多线程操作之间的内存可见性。如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须存在 happens-before 关系。 作用：描述多线程操作之间的内存可见性。 数据依赖性定义：如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。 作用：编译器和处理器不会对“存在数据依赖关系的两个操作”执行重排序。 as-if-serial定义：不管怎么重排序，程序的执行结果不能被改变。 顺序一致性内存模型定义：它是理想化的内存模型。有以下规则： 一个线程中的所有操作必须按照程序的顺序来执行。 所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。 Java内存模型定义：Java Memory Mode，它是Java线程之间通信的控制机制。 说明：JMM 对 Java 程序作出保证，如果程序是正确同步的，程序的执行将具有顺序一致性。即，程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同。 可见性可见性一般用于指不同线程之间的数据是否可见。 在 java 中， 实例域、静态域和数组元素这些数据是线程之间共享的数据，它们存储在主内存中；主内存中的所有数据对该内存中的线程都是可见的。而局部变量，方法定义参数和异常处理器参数这些数据是不会在线程之间共享的，它们存储在线程的本地内存中；它们对其它线程是不可见的。 此外，对于主内存中的数据，在本地内存中会对应的创建该数据的副本(相当于缓冲)；这些副本对于其它线程也是不可见的。 原子性是指一个操作是按原子的方式执行的。要么该操作不被执行；要么以原子方式执行，即执行过程中不会被其它线程中断。 2. JVM内存模型虽然平时我们用的大多是 Sun JDK 提供的 JVM，但是 JVM 本身是一个 规范，所以可以有多种实现，除了 Hotspot 外，还有诸如 Oracle 的 JRockit、IBM 的 J9也都是非常有名的 JVM。 Java 虚拟机在执行 Java 程序的过程中会把它所管理的内存划分为若干个不同的数据区域，这些区域都有各自的用途，以及创建和销毁的时间。有的区域随着虚拟机进程的启动就存在了， 有的区域则是依赖用户线程。根据《Java虚拟机规范（第二版）》，Java 虚拟机所管理的内存包含如下图的几个区域。 由上图可以看出 JVM 组成如下： 运行时数据区（内存空间） 方法区 堆 虚拟机栈 程序计数器 本地方法栈 直接内存 执行引擎 本地库接口 从上图中还可以看出，在内存空间中方法区和堆是所有Java线程共享的，称之为线程共享数据区，而虚拟机栈、程序计数器、本地方法栈则由每个线程私有，称之为线程隔离数据区。 关于本地方法： 众所周知，Java 语言具有跨平台的特性，这也是由 JVM 来实现的。更准确地说，是 Sun 利用 JVM 在不同平台上的实现帮我们把平台相关性的问题给解决了，这就好比是 HTML 语言可以在不同厂商的浏览器上呈现元素（虽然某些浏览器在对W3C标准的支持上还有一些问题）。同时，Java 语言支持通过 JNI（Java Native Interface）来实现本地方法的调用，但是需要注意到，如果你在 Java 程序用调用了本地方法，那么你的程序就很可能不再具有跨平台性，即本地方法会破坏平台无关性。 下面分别就线程共享数据区和线程共享数据区进行说明。 2.1 线程共享数据区所谓线程共享数据区，是指在多线程环境下，该部分区域数据可以被所有线程所共享，主要有方法区和堆。 方法区方法区用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等等。方法区中对于每个类存储了以下数据： 类及其父类的全限定名（java.lang.Object没有父类） 类的类型（Class or Interface） 访问修饰符（public, abstract, final） 实现的接口的全限定名的列表 常量池 字段信息 方法信息 静态变量 ClassLoader 引用 Class 引用 可见类的所有信息都存储在方法区中。由于方法区是所有线程共享的，所以必须保证线程安全，举例来说：如果两个类同时要加载一个尚未被加载的类，那么一个类会请求它的 ClassLoader 去加载需要的类，另一个类只能等待而不会重复加载。 注意事项： 在 HotSpot 虚拟机中，很多人都把方法区成为永久代，默认最小值为16MB，最大值为64MB。其实只在 HotSpot 才存在方法区，在其他的虚拟机没有方法区这一个说法的。本文是采用 Hotspot，所以把方法区介绍了。 如果方法区无法满足内存分配需求时候就会抛出 OutOfMemoryError 异常。 堆堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例及数组内容，几乎所有的对象实例都在这里分配内存。堆中有指向类数据的指针，该指针指向了方法区中对应的类型信息，堆中还可能存放了指向方法表的指针。堆是所有线程共享的，所以在进行实例化对象等操作时，需要解决同步问题。此外，堆中的实例数据中还包含了对象锁，并且针对不同的垃圾收集策略，可能存放了引用计数或清扫标记等数据。 在 Java 中，堆被划分成两个不同的区域：新生代 ( Young )、老年代 ( Old )。新生代 ( Young ) 又被划分为三个区域：Eden、From Survivor、To Survivor。 ​从图中可以看出： 堆大小 = 新生代 + 老年代，其中，堆的大小可以通过参数 -Xms、-Xmx 来指定。本人使用的是 JDK1.6，以下涉及的 JVM 默认值均以该版本为准。 默认的，Young : Old = 1 : m ，该比例值 m 可以通过参数 -XX:NewRatio 来指定，默认值为2，即新生代 ( Young ) = 1/3 的堆空间大小，老年代 ( Old ) = 2/3 的堆空间大小。 默认的，Edem : from : to = n : 1 : 1 ，该比例值 n 可以参数 -XX:SurvivorRatio 来设定，默认值为8 ，即 Eden = 8/10 的新生代空间大小，from = to = 1/10 的新生代空间大小。 JVM 每次只会使用 Eden 和其中的一块 Survivor 区域来为对象服务，所以无论什么时候，总是有一块 Survivor 区域是空闲着的，因此，新生代实际可用的内存空间为 9/10 ( 即90% )的新生代空间。 根据 Java 虚拟机规范的规定，Java 堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘空间一样。在实现时，既可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的。如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出 OutOfMemoryError 异常。 2.2 线程隔离数据区所谓线程隔离数据区是指在多线程环境下，每个线程所独享的数据区域。主要有程序计数器、Java虚拟机栈、本地方法栈三个数据区。 程序计数器程序计数器 ，计算机处理器中的寄存器，它包含当前正在执行的指令的地址（位置）。当每个指令被获取，程序计数器的存储地址加一。在每个指令被获取之后，程序计数器指向顺序中的下一个指令。当计算机重启或复位时，程序计数器通常恢复到零。 在Java中程序计数器是一块较小的内存空间，充当当前线程所执行的字节码的行号指示器的角色。 在多线程环境下，当某个线程失去处理器执行权时，需要记录该线程被切换出去时所执行的程序位置。从而方便该线程被切换回来(重新被处理器处理)时能恢复到当初的执行位置，因此每个线程都需要有一个独立的程序计数器。各个线程的程序计数器互不影响，并且独立存储。 当线程正在执行一个 java 方法时，这个程序计数器记录的时正在执行的虚拟机字节码指令的地址。 当线程执行的是 Native方法，这个计数器值为空。 此内存区域是唯一一个在 java 虚拟机规范中没有规定任何 OutOfMemoryError 情况的区域。 Java 虚拟机栈与程序计数器一样，Java 虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。Java 虚拟机栈描述的是 Java 方法执行的内存模型，每个方法在执行的同时都会创建一个栈帧用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每个方法从调用直至执行完成的过程，对应着一个栈帧在虚拟机中入栈到进栈的过程。 在 Hot Spot 虚拟机中，可以使用 -Xss 参数来设置栈的大小。栈的大小直接决定了函数调用的深度。 某个线程正在执行的方法被称为该线程的当前方法，当前方法使用的栈帧成为当前帧，当前方法所属的类成为当前类，当前类的常量池成为当前常量池。在线程执行一个方法时，它会跟踪当前类和当前常量池。此外，当虚拟机遇到栈内操作指令时，它对当前帧内数据执行操作。 它分为三部分：局部变量区、操作数栈、帧数据区。 1、局部变量区 局部变量区是以字长为单位的数组，在这里，byte、short、char 类型会被转换成 int 类型存储，除了 long 和 double 类型占两个字长以外，其余类型都只占用一个字长。特别地，boolean 类型在编译时会被转换成 int 或 byte 类型，boolean 数组会被当做 byte 类型数组来处理。局部变量区也会包含对象的引用，包括类引用、接口引用以及数组引用。 局部变量区包含了方法参数和局部变量，此外，实例方法隐含第一个局部变量 this，它指向调用该方法的对象引用。对于对象，局部变量区中永远只有指向堆的引用。 注意： 局部变量表中的字可能会影响 GC 回收。如果这个字没有被后续代码复用，那么它所引用的对象不会被 GC 释放，手工对要释放的变量赋值为 null，是一种有效的做法。 2、操作数栈 操作数栈也是以字长为单位的数组，但是正如其名，它只能进行入栈出栈的基本操作。在进行计算时，操作数被弹出栈，计算完毕后再入栈。 每当线程调用一个Java方法时，虚拟机都会在该线程的Java栈中压入一个新帧。而这个新帧自然就成为了当前帧。在执行这个方法时，它使用这个帧来存储参数、局部变量、中间运算结果等等数据。 Java 方法可以以两种方式完成。一种通过 return 返回的，称为正常返回；一种是通过抛出异常而异常中止的。不管以哪种方式返回，虚拟机都会将当前帧弹出Java栈然后释放掉，这样上一个方法的帧就成为当前帧了。 Java 栈上的所有数据都是此线程私有的。任何线程都不能访问另一个线程的栈数据，因此我们不需要考虑多线程情况下栈数据的访问同步问题。当一个线程调用一个方法时，方法的局部变量保存在调用线程 Java 栈的帧中。只有一个线程总是访问哪些局部变量，即调用方法的线程。 3、帧数据区 帧数据区的任务主要有： a.记录指向类的常量池的指针，以便于解析。 b.帮助方法的正常返回，包括恢复调用该方法的栈帧，设置PC寄存器指向调用方法对应的下一条指令，把返回值压入调用栈帧的操作数栈中。 c.记录异常表，发生异常时将控制权交由对应异常的catch子句，如果没有找到对应的catch子句，会恢复调用方法的栈帧并重新抛出异常。 局部变量区和操作数栈的大小依照具体方法在编译时就已经确定。调用方法时会从方法区中找到对应类的类型信息，从中得到具体方法的局部变量区和操作数栈的大小，依此分配栈帧内存，压入Java栈。 在 Java 虚拟机规范中，对这个区域规定了两种异常状况： 如果线程请求的栈深度大于虚拟机所允许的深度，将抛出 StackOverflowError 异常； 如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），当扩展时无法申请到足够的内存时会抛出 OutOfMemoryError 异常。 本地方法栈本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行 Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的 Native 方法服务。虚拟机规范中对本地方法栈中的方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如 Sun HotSpot 虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出 StackOverflowError 和 OutOfMemoryError 异常。 2.3 直接内存直接内存并不是虚拟机运行时数据区的一部分，也不是 Java 虚拟机规范中定义的内存区域。 JDK1.4 中出现了 NIO，其引入了一种基于通道与缓冲区的 I/O 方式，它可以使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中得 DirectoryByteBuffer 对象作为这块内存的引用进行操作。这样可以避免 Java 堆和 Native 堆之间的来回复制数据。 当机器直接内存去除 JVM 内存之后的内存不能满足直接内存大小要求其，将会抛出 OutOfMemoryError 异常。 3. 垃圾回收过程 JVM 采用一种分代回收 (generational collection) 的策略，用较高的频率对年轻的对象进行扫描和回收，这种叫做 minor collection ，而对老对象的检查回收频率要低很多，称为 major collection。这样就不需要每次 GC 都将内存中所有对象都检查一遍。 新生代被划分为三部分，Eden 区和两个大小严格相同的 Survivor 区，其中 Survivor 区间，某一时刻只有其中一个是被使用的，另外一个留做垃圾收集时复制对象用，在 Young 区间变满的时候，minor GC 就会将存活的对象移到空闲的 Survivor 区间中，根据 JVM 的策略，在经过几次垃圾收集后，仍然存活于 Survivor 的对象将被移动到老年代。 老年代主要保存生命周期长的对象，一般是一些老的对象，当一些对象在 Young 复制转移一定的次数以后，对象就会被转移到老年区，一般如果系统中用了 application 级别的缓存，缓存中的对象往往会被转移到这一区间。 Minor collection 的过程就是将 eden 和在用survivor space中的活对象 copy 到空闲survivor space中。所谓 survivor，也就是大部分对象在 eden 出生后，根本活不过一次 GC。对象在新生代里经历了一定次数的 minor collection 后，年纪大了，就会被移到老年代中，称为 tenuring。 剩余内存空间不足会触发 GC，如 eden 空间不够了就要进行 minor collection，老年代空间不够要进行 major collection，永久代(Permanent Space)空间不足会引发full GC。 举例：当一个 URL 被访问时，内存申请过程如下： A. JVM 会试图为相关 Java 对象在 Eden 中初始化一块内存区域 B. 当 Eden 空间足够时，内存申请结束。否则到下一步 C. JVM 试图释放在 Eden 中所有不活跃的对象，释放后若 Eden 空间仍然不足以放入新对象，则试图将部分 Eden 中活跃对象放入 Survivor 区 D. Survivor 区被用来作为 Eden 及 Old 的中间交换区域，当 Old 区空间足够时，Survivor 区的对象会被移到 Old 区，否则会被保留在 Survivor区 E. 当 Old 区空间不够时，JVM 会在 Old 区进行完全的垃圾收集 F. 完全垃圾收集后，若 Survivor 及 Old 区仍然无法存放从 Eden 复制过来的部分对象，导致 JVM 无法在 Eden 区为新对象创建内存区域，则出现 out of memory 错误 HotSpot jvm 都给我们提供了下面参数来对内存进行配置： 配置总内存 -Xms ：指定了 JVM 初始启动以后初始化内存 -Xmx：指定 JVM 堆得最大内存，在JVM启动以后，会分配 -Xmx 参数指定大小的内存给 JVM，但是不一定全部使用，JVM 会根据 -Xms 参数来调节真正用于JVM的内存，-Xmx-Xms 之差就是三个 Virtual 空间的大小 配置新生代 -Xmn: 参数设置了年轻代的大小 -XX:SurvivorRatio: 表示 eden 和一个 surivivor 的比例，缺省值为8 -XX:NewSize 和 -XX:MaxNewSize：直接指定了年轻代的缺省大小和最大大小 配置老年代 -XX:NewRatio: 表示年老年代和新生代内存的比例，缺省值为2 配置持久代 -XX:MaxPermSize：表示持久代的最大值 -XX:PermSize：设置最小分配空间 配置虚拟机栈 -Xss：参数来设置栈的大小，默认值为128 kb。栈的大小直接决定了函数调用的深度 4. 常见的垃圾收集策略垃圾收集提供了内存管理的机制，使得应用程序不需要在关注内存如何释放，内存用完后，垃圾收集会进行收集，这样就减轻了因为人为的管理内存而造成的错误，比如在 C++ 语言里，出现内存泄露时很常见的。Java 语言是目前使用最多的依赖于垃圾收集器的语言，但是垃圾收集器策略从20世纪60年代就已经流行起来了，比如 Smalltalk,Eiffel 等编程语言也集成了垃圾收集器的机制。 所有的垃圾收集算法都面临同一个问题，那就是找出应用程序不可到达的内存块，将其释放，这里面得不可到达主要是指应用程序已经没有内存块的引用了，而在 JAVA中，某个对象对应用程序是可到达的是指：这个对象被根（根主要是指类的静态变量，常量或者活跃在所有线程栈的对象的引用）引用或者对象被另一个可到达的对象引用。 下面我们介绍一下几种常见的垃圾收集策略： 4.1 Reference Counting(引用计数）引用计数是最简单直接的一种方式，这种方式在每一个对象中增加一个引用的计数，这个计数代表当前程序有多少个引用引用了此对象，如果此对象的引用计数变为0，那么此对象就可以作为垃圾收集器的目标对象来收集。 优点：简单，直接，不需要暂停整个应用 缺点：需要编译器的配合，编译器要生成特殊的指令来进行引用计数的操作，比如每次将对象赋值给新的引用，或者者对象的引用超出了作用域等。不能处理循环引用的问题 4.2 跟踪收集器跟踪收集器首先要暂停整个应用程序，然后开始从根对象扫描整个堆，判断扫描的对象是否有对象引用。 如果每次扫描整个堆，那么势必让 GC 的时间变长，从而影响了应用本身的执行。因此在 JVM 里面采用了分代收集，在新生代收集的时候 minor gc 只需要扫描新生代，而不需要扫描老生代。minor gc 怎么判断是否有老生代的对象引用了新生代的对象，JVM 采用了卡片标记的策略，卡片标记将老生代分成了一块一块的，划分以后的每一个块就叫做一个卡片，JVM 采用卡表维护了每一个块的状态，当 JAVA 程序运行的时候，如果发现老生代对象引用或者释放了新生代对象的引用，那么就 JVM 就将卡表的状态设置为脏状态，这样每次 minor gc 的时候就会只扫描被标记为脏状态的卡片，而不需要扫描整个堆。 上面说了 Jvm 需要判断对象是否有引用存在，而 Java 中的引用又分为了如下几种，不同种类的引用对垃圾收集有不同的影响，下面我们分开描述一下： 1）Strong Reference(强引用) 强引用是 JAVA 中默认采用的一种方式，我们平时创建的引用都属于强引用。如果一个对象没有强引用，那么对象就会被回收。 1234567public void testStrongReference()&#123; Object referent = new Object(); Object strongReference = referent; referent = null; System.gc(); assertNotNull(strongReference);&#125; 2）Soft Reference(软引用) 软引用的对象在 GC 的时候不会被回收，只有当内存不够用的时候才会真正的回收，因此软引用适合缓存的场合，这样使得缓存中的对象可以尽量的再内存中待长久一点。 1234567Public void testSoftReference()&#123; String str = "test"; SoftReference&lt;String&gt; softreference = new SoftReference&lt;String&gt;(str); str=null; System.gc(); assertNotNull(softreference.get());&#125; 3）Weak Reference(弱引用) 弱引用有利于对象更快的被回收，假如一个对象没有强引用只有弱引用，那么在 GC 后，这个对象肯定会被回收。 1234567Public void testWeakReference()&#123; String str = "test"; WeakReference&lt;String&gt; weakReference = new WeakReference&lt;String&gt;(str); str=null; System.gc(); assertNull(weakReference.get());&#125; 4）Phantom reference(幽灵引用) 幽灵引用说是引用，但是你不能通过幽灵引用来获取对象实例，它主要目的是为了当设置了幽灵引用的对象在被回收的时候可以收到通知。 跟踪收集器常见的有如下几种： 4.2.1 Mark-Sweep Collector(标记-清除收集器）标记清除收集器最早由Lisp的发明人于1960年提出，标记清除收集器停止所有的工作，从根扫描每个活跃的对象，然后标记扫描过的对象，标记完成以后，清除那些没有被标记的对象。 优点： 解决循环引用的问题 不需要编译器的配合，从而就不执行额外的指令 缺点： 每个活跃的对象都要进行扫描，收集暂停的时间比较长。 4.2.2 Copying Collector(复制收集器）复制收集器将内存分为两块一样大小空间，某一个时刻，只有一个空间处于活跃的状态，当活跃的空间满的时候，GC就会将活跃的对象复制到未使用的空间中去，原来不活跃的空间就变为了活跃的空间。 优点： 只扫描可以到达的对象，不需要扫描所有的对象，从而减少了应用暂停的时间 缺点： 需要额外的空间消耗，某一个时刻，总是有一块内存处于未使用状态 复制对象需要一定的开销 4.2.3 Mark-Compact Collector(标记-整理收集器）标记整理收集器汲取了标记清除和复制收集器的优点，它分两个阶段执行，在第一个阶段，首先扫描所有活跃的对象，并标记所有活跃的对象，第二个阶段首先清除未标记的对象，然后将活跃的的对象复制到堆得底部。 Mark-compact 策略极大的减少了内存碎片，并且不需要像 Copy Collector 一样需要两倍的空间。 5. HotSpot JVM 垃圾收集策略GC 的执行时要耗费一定的 CPU 资源和时间的，因此在 JDK1.2 以后，JVM 引入了分代收集的策略，其中对新生代采用 ”Mark-Compact” 策略，而对老生代采用了 “Mark-Sweep” 的策略。其中新生代的垃圾收集器命名为 “minor gc”，老生代的 GC 命名为 ”Full Gc 或者Major GC”。其中用 System.gc() 强制执行的是 Full GC。 HotSpot JVM 的垃圾收集器按照并发性可以分为如下三种类型： 5.1 串行收集器（Serial Collector）Serial Collector 是指任何时刻都只有一个线程进行垃圾收集，这种策略有一个名字 stop the whole world，它需要停止整个应用的执行。这种类型的收集器适合于单CPU的机器。 Serial Collector 有如下两个： 1）Serial Copying Collector 此种 GC 用 -XX:UseSerialGC 选项配置，它只用于新生代对象的收集。 JDK 1.5.0 以后 -XX:MaxTenuringThreshold 用来设置对象复制的次数。当 eden 空间不够的时候，GC 会将 eden 的活跃对象和一个名叫 From survivor 空间中尚不够资格放入 Old 代的对象复制到另外一个名字叫 To Survivor 的空间。而此参数就是用来说明到底 From survivor 中的哪些对象不够资格，假如这个参数设置为31，那么也就是说只有对象复制31次以后才算是有资格的对象。 这里需要注意几个个问题： From Survivor 和 To survivor的角色是不断的变化的，同一时间只有一块空间处于使用状态，这个空间就叫做 From Survivor 区，当复制一次后角色就发生了变化。 如果复制的过程中发现 To survivor 空间已经满了，那么就直接复制到 old generation。 比较大的对象也会直接复制到Old generation，在开发中，我们应该尽量避免这种情况的发生。 2）Serial Mark-Compact Collector 串行的标记-整理收集器是 JDK5 update6 之前默认的老生代的垃圾收集器，此收集使得内存碎片最少化，但是它需要暂停的时间比较长 5.2 并行收集器（Parallel Collector）Parallel Collector 主要是为了应对多 CPU，大数据量的环境。Parallel Collector又可以分为以下三种： 1）Parallel Copying Collector 此种 GC 用 -XX:UseParNewGC 参数配置，它主要用于新生代的收集，此 GC 可以配合CMS一起使用，适用于1.4.1以后。 2）Parallel Mark-Compact Collector 此种 GC 用 -XX:UseParallelOldGC 参数配置，此 GC 主要用于老生代对象的收集。适用于1.6.0以后。 3）Parallel scavenging Collector 此种 GC 用 -XX:UseParallelGC 参数配置，它是对新生代对象的垃圾收集器，但是它不能和CMS配合使用，它适合于比较大新生代的情况，此收集器起始于 jdk 1.4.0。它比较适合于对吞吐量高于暂停时间的场合。 5.3 并发收集器 (Concurrent Collector)Concurrent Collector 通过并行的方式进行垃圾收集，这样就减少了垃圾收集器收集一次的时间，在 HotSpot JVM 中，我们称之为 CMS GC，这种 GC 在实时性要求高于吞吐量的时候比较有用。此种 GC 可以用参数 -XX:UseConcMarkSweepGC 配置，此 GC 主要用于老生代和 Perm 代的收集。 CMS GC有可能出现并发模型失败： CMS GC 在运行的时候，用户线程也在运行，当 GC 的速度比新增对象的速度慢的时候，或者说当正在 GC 的时候，老年代的空间不能满足用户线程内存分配的需求的时候，就会出现并发模型失败，出现并发模型失败的时候，JVM 会触发一次 stop-the-world 的 Full GC 这将导致暂停时间过长。不过 CMS GC 提供了一个参数 -XX:CMSInitiatingOccupancyFraction 来指定当老年代的空间超过某个值的时候即触发 GC，因此如果此参数设置的过高，可能会导致更多的并发模型失败。 并发和并行收集器区别： 并发收集器是指垃圾收集器线程和应用线程可以并发的执行，也就是清除的时候不需要 stop the world，但是并行收集器指的的是可以多个线程并行的进行垃圾收集，并行收集器还是要暂停应用的 6. HotSpot Jvm 垃圾收集器的配置策略下面我们分两种情况来分别描述一下不同情况下的垃圾收集配置策略。 6.1 吞吐量优先吞吐量是指 GC 的时间与运行总时间的比值，比如系统运行了100 分钟，而 GC 占用了一分钟，那么吞吐量就是 99%，吞吐量优先一般运用于对响应性要求不高的场合，比如 web 应用，因为网络传输本来就有延迟的问题，GC 造成的短暂的暂停使得用户以为是网络阻塞所致。 吞吐量优先可以通过 -XX:GCTimeRatio 来指定。当通过 -XX:GCTimeRatio 不能满足系统的要求以后，我们可以更加细致的来对 JVM 进行调优。 首先因为要求高吞吐量，这样就需要一个较大的 Young generation，此时就需要引入 Parallel scavenging Collector ，可以通过参数：-XX:UseParallelGC来配置。 1java -server -Xms3072m -Xmx3072m -XX:NewSize=2560m -XX:MaxNewSize=2560 -XX:SurvivorRatio=2 -XX:+UseParallelGC 当年轻代使用了 Parallel scavenge collector 后，老生代就不能使用 CMS GC 了，在 JDK1.6 之前，此时老生代只能采用串行收集，而 JDK1.6 引入了并行版本的老生代收集器，可以用参数 -XX:UseParallelOldGC 来配置。 1.控制并行的线程数 缺省情况下，Parallel scavenging Collector 会开启与 cpu 数量相同的线程进行并行的收集，但是也可以调节并行的线程数。假如你想用4个并行的线程去收集 Young generation 的话，那么就可以配置 -XX:ParallelGCThreads=4，此时JVM的配置参数如下： 1java -server -Xms3072m -Xmx3072m -XX:NewSize=2560m -XX:MaxNewSize=2560 -XX:SurvivorRatio=2 -XX:+UseParallelGC -XX:ParallelGCThreads=4 2.自动调节新生代 在采用了 Parallel scavenge collector 后，此 GC 会根据运行时的情况自动调节 survivor ratio 来使得性能最优，因此 Parallel scavenge collector 应该总是开启 -XX:+UseAdaptiveSizePolicy 参数。此时JVM的参数配置如下： 1java -server -Xms3072m -Xmx3072m -XX:+UseParallelGC -XX:ParallelGCThreads=4 -XX:+UseAdaptiveSizePolicy 6.2 响应时间优先响应时间优先是指 GC 每次运行的时间不能太久，这种情况一般使用与对及时性要求很高的系统，比如股票系统等。 响应时间优先可以通过参数 -XX:MaxGCPauseMillis来配置，配置以后 JVM 将会自动调节年轻代，老生代的内存分配来满足参数设置。 在一般情况下，JVM 的默认配置就可以满足要求，只有默认配置不能满足系统的要求时候，才会根据具体的情况来对 JVM 进行性能调优。如果采用默认的配置不能满足系统的要求，那么此时就可以自己动手来调节。此时 Young generation 可以采用 Parallel copying collector，而 Old generation 则可以采用 Concurrent Collector。 举个例子来说，以下参数设置了新生代用 Parallel Copying Collector，老生代采用 CMS 收集器。 1java -server -Xms512m -Xmx512m -XX:NewSize=64m -XX:MaxNewSize=64m -XX:SurvivorRatio=2 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC 此时需要注意两个问题： 1.如果没有指定 -XX:+UseParNewGC，则采用默认的非并行版本的 copy collector 2.如果在一个单 CPU 的系统上设置了 -XX:+UseParNewGC，则默认还是采用缺省的copy collector 1.控制并行的线程数 默认情况下，Parallel copy collector 启动和 CPU 数量一样的线程，也可以通过参数 -XX:ParallelGCThreads 来指定，比如你想用 4 个线程去进行并发的复制收集，那么可以改变上述参数如下： 1java -server -Xms512m -Xmx512m -XX:NewSize=64m -XX:MaxNewSize=64m -XX:SurvivorRatio=2 -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC 2.控制并发收集的临界值 默认情况下，CMS GC在 old generation 空间占用率高于 68% 的时候，就会进行垃圾收集，而如果想控制收集的临界值，可以通过参数：-XX:CMSInitiatingOccupancyFraction 来控制，比如改变上述的JVM配置如下： 1java -server -Xms512m -Xmx512m -XX:NewSize=64m -XX:MaxNewSize=64m -XX:SurvivorRatio=2 -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:CMSInitiatingOccupancyFraction=35 此外顺便说一个参数：-XX:+PrintCommandLineFlags 通过此参数可以知道在没有显示指定内存配置和垃圾收集算法的情况下，JVM 采用的默认配置。 比如我在自己的机器上面通过如下命令 java -XX:+PrintCommandLineFlags -version 得到的结果如下所示： 12345-XX:InitialHeapSize=1055308032 -XX:MaxHeapSize=16884928512 -XX:ParallelGCThreads=8 -XX:+PrintCommandLineFlags -XX:+UseCompressedOops -XX:+UseParallelGCjava version &quot;1.6.0_45&quot;Java(TM) SE Runtime Environment (build 1.6.0_45-b06)Java HotSpot(TM) 64-Bit Server VM (build 20.45-b01, mixed mode)You have new mail in /var/spool/mail/root 从输出可以清楚的看到JVM通过自己检测硬件配置而给出的缺省配置。 参考资料 Jvm内存模型以及垃圾收集策略解析系列(一) Jvm内存模型以及垃圾收集策略解析系列(二) Java theory and practice: A brief history of garbage collection Java theory and practice: Garbage collection in the HotSpot JVM Understanding CMS GC Logs Java HotSpot VM Options Server-Class Machine Detection]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PostgreSQL测试工具PGbench]]></title>
    <url>%2F2014%2F04%2F08%2Fa-benchmark-tool-on-postgresql%2F</url>
    <content type="text"><![CDATA[pgbench 是一个简单的给 PostgreSQL 做性能测试的程序。它反复运行同样的 SQL 命令序列，可能是在多个并发数据库会话上头，然后检查平均的事务速度（每秒的事务数 tps）。缺省的时候，pgbench 测试一个（松散的）接近 TPC-B 的情况，每个事务包括五个 SELECT，UPDATE，和 INSERT命令。不过，我们可以很轻松地使用自己的事务脚本文件来实现其它情况。 典型的输出看上去会是这样： 1234567transaction type: TPC-B (sort of)scaling factor: 10number of clients: 10number of transactions per client: 1000number of transactions actually processed: 10000/10000tps = 85.184871 (including connections establishing)tps = 85.296346 (excluding connections establishing) 头四行只是报告一些最重要的参数设置。跟着的一行报告完成的事务数和期望完成的事务数（后者只是客户端数乘以事务数）；这两个会相等，除非在完成之前运行就失败了。最后两行报告 TPS 速率，分别有计算启动数据库会话时间和不计算启动会话时间的。 使用环境： 在比较新的9.1，9.2，9.3数据库的发行版本中,pgbench是在安装contrib包时直接编译的,可以在postgres的bin目录下找到该命令，如果没有发现该命令可以在安装contrib的目录下找到pgbench的源码文件包，编译一下就可以使用。 1. pgbench测试库初始化123postgres$ pgbench --help # 和postgres其他命令的使用方式一样，--help获取命令使用方式的简单介绍postgres$ createdb pgbench # 创建测试库postgres$ pgbench -i pgbench # 初始化测试库 默认会在测试库中建4张表pgbench_accounts，pgbench_branches，pgbench_history，pgbench_tellers 。当然也可以自己建表，自己写测试脚本，这四张表只是默认的测试脚本会用到。 pgbench在建默认库时 -s 参数设定测设表的大小，默认参数是1 。pgbench_accounts 总行数是10W,-s后面接具体数值如100，则pgbench_accounts中的测试数据将达到1千万行 2. 默认的测试脚本介绍默认的测试脚本可以在官方文档中找到（新的版本中不指定模板就会使用默认模板） 123456789101112131415$ cat test.sql\set nbranches 1 * :scale\set ntellers 10 * :scale\set naccounts 100000 * :scale\setrandom aid 1 :naccounts\setrandom bid 1 :nbranches\setrandom tid 1 :ntellers\setrandom delta -5000 5000BEGIN;UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid;SELECT abalance FROM pgbench_accounts WHERE aid = :aid;UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid;UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid;INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP);END; 脚本说明： 可以看到脚本中的一个事物包含了update,select,insert操作，不同的操作起到不同的测试目的 （1）UPDATE pgbench_accounts:作为最大的表，起到促发磁盘I/O的作用。 （2）SELECT abalance:由于上一条UPDATE语句更新一些信息，存在于缓存内用于回应这个查询。 （3）UPDATE pgbench_tellers:职员的数量比账号的数量要少得多，所以这个表也很小，并且极有可能存在于内存中。 （4）UPDATE pgbench_branches:作为更小的表，内容被缓存，如果用户的环境是数量较小的数据库和多个客户端时，对其锁操作可能会成为性能的瓶颈。 （5）INSERT INTO pgbench_history:history表是个附加表，后续并不会进行更新或查询操作，而且也没有任何索引。相对于UPDATE语句，对其的插入操作对磁盘的写入成本也很小。 3. 测试结果说明1234567891011121314151617181920212223242526postgres$ pgbench -c 15 -t 300 pgbench -r -f test.sql #执行命令starting vacuum...end.transaction type: Custom queryscaling factor: 1query mode: simple number of clients: 15 #-c参数控制并发量number of threads: 1 number of transactions per client: 300 #每个客户端执行事务的数量number of transactions actually processed: 4500/4500 #总执行量tps = 453.309203 (including connections establishing) #tps每秒钟处理的事务数包含网络开销 tps = 457.358998 (excluding connections establishing) #不包含网络开销statement latencies in milliseconds: #带-r的效果，每个客户端事务具体的执行时间，单位是毫秒0.005198 \set nbranches 1 * :scale 0.001144 \set ntellers 10 * :scale0.001088 \set naccounts 100000 * :scale 0.001400 \setrandom aid 1 :naccounts0.000814 \setrandom bid 1 :nbranches0.000929 \setrandom tid 1 :ntellers0.000981 \setrandom delta -5000 50000.613757 BEGIN;1.027969 UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid;0.754162 SELECT abalance FROM pgbench_accounts WHERE aid = :aid;14.167980 UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid;13.587156 UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid;0.582075 INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP);1.628262 END; 默认的基准测试给出了一个指标TPS，同样的测试参数，tps的值越高，相对来说服务器的性能越好。上面的测试由于数据量的问题，表的内容全部缓存进了内存,磁盘io对上面的结果影响较小。 4. 自定义测试环境在实际的应用中测试可以自己定义测试环境，模拟生产需求。 测试举例 1234567pgbench=# create table pg_test (a1 serial,a2 int,a3 varchar(20),a4 timestamp); #创建测试表postgres$cat pg_test.sqlpgbench=# insert into pg_test(a2,a3,a4) select (random()*(2*10^5)),substr('abcdefghijklmnopqrstuvwxyz',1, (random()*26)::integer),now(); #每个事务插入一条数据 postgres$pgbench -c 90 -T 10 pgbench -r -f pg_test.sql #90个并发测试每秒插入的数据量~~~ 测试结果截取： number of transactions actually processed: 20196 #10秒钟90并发用户共插入20196条数据，每条数据插入费时42ms，平均每秒插入2000条数据tps = 1997.514876 (including connections establishing)tps = 2119.279239 (excluding connections establishing)statement latencies in milliseconds:42.217948 pgbench=# select count(*) from pg_test; count 20196 12345678910# 5. pgbench在参数调节上的辅助使用简单举例：`work_mem`~~~bashpostgres=# show work_mem ; #数据库当前的work_memwork_mem ----------1MB 查询样本： 123postgres$cat select.sqlSELECT customerid FROM customers ORDER BY zip; #orders表是一张postgres样例表，样例库全名dellstore2postgres$pgbench -c 90 -T 5 pgbench -r -f select.sql #多用户并发做单表排序操作单个事务执行的时间可能会很大，但是平均事务执行时间和单个用户的执行时间差距没那么明显。 执行结果截取 12345678number of clients: 90number of threads: 1duration: 5 snumber of transactions actually processed: 150tps = 26.593887 (including connections establishing)tps = 27.972988 (excluding connections establishing)statement latencies in milliseconds:3115.754673 SELECT customerid FROM customers ORDER BY zip; 测试环境相同调节work_mem参数为2M试试 123456789101112131415161718192021222324number of clients: 90number of threads: 1duration: 5 snumber of transactions actually processed: 243tps = 44.553026 (including connections establishing)tps = 47.027276 (excluding connections establishing)statement latencies in milliseconds:1865.636761 SELECT customerid FROM customers ORDER BY zip; #5s内事务执行的总量明显增加一共做了243次单表排序~~~ 原因分析，由于排序操作会关系到`work_mem`，排序操作能全在缓存中进行当然速度会明显加快，查看执行计划~~~sqlpostgres=# explain analyze SELECT customerid FROM customers ORDER BY zip;QUERY PLAN --------------------------------------------------------------------------------------------Sort (cost=2116.77..2166.77 rows=20000 width=8) (actual time=42.536..46.117 rows=20000 loops=1)Sort Key: zipSort Method: external sort Disk: 352kB-&gt; Seq Scan on customers (cost=0.00..688.00 rows=20000 width=8) (actual time=0.013..8.942 rows=20000 loops=1)Total runtime: 48.858 ms 由上面的执行计划可以看出在work_mem大小为1M的时候排序一共需要1.352M空间做排序,所以加大work_mem参数排序速度明显增加。 这只是个简单的例子，work_mem的大小调节还有很多其他方面要考虑的，比如在高并发的情况下，需要为每个用户分配同样大小的排序空间，会占用大量的内存空间。参数调节在任何时候保持一个均衡才是应该考虑的。 参考文章 [1] PGbench]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>postgresql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RHEL系统安装PostgreSQL]]></title>
    <url>%2F2014%2F04%2F07%2Finstall-postgresql-on-rhel-system%2F</url>
    <content type="text"><![CDATA[环境说明 OS：RHEL6.4（x86_64） postgresql版本：PostgreSQL9.2.8 安装步骤1. 下载所需的PostgreSQL rpm包基础安装： postgresql92-libs-9.2.8-1PGDG.rhel6.x86_64.rpm postgresql92-9.2.8-1PGDG.rhel6.x86_64.rpm postgresql92-server-9.2.8-1PGDG.rhel6.x86_64.rpm 扩展安装： postgresql92-contrib-9.2.8-1PGDG.rhel6.x86_64.rpm postgresql92-devel-9.2.8-1PGDG.rhel6.x86_64.rpm 下载地址：http://yum.postgresql.org/9.2/redhat/rhel-6.4-x86_64/ 2. 安装基础的rpm包在命令行执行如下命令进行安装： 123$ rpm -ivh postgresql92-libs-9.2.8-1PGDG.rhel6.x86_64.rpm$ rpm -ivh postgresql92-9.2.8-1PGDG.rhel6.x86_64.rpm$ rpm -ivh postgresql92-server-9.2.8-1PGDG.rhel6.x86_64.rpm 按照上面的顺序安装rpm时，会报与系统的libcrypto.so.10和libssl.so.10依赖错误，错误信息如下： 12345$ rpm -ivh postgresql92-libs-9.2.8-1PGDG.rhel6.x86_64.rpm warning: postgresql92-libs-9.2.8-1PGDG.rhel6.x86_64.rpm: Header V4 DSA/SHA1 Signature, key ID 442df0f8: NOKEYerror: Failed dependencies: libcrypto.so.10(libcrypto.so.10)(64bit) is needed by postgresql92-libs-9.2.8-1PGDG.rhel6.x86_64 libssl.so.10(libssl.so.10)(64bit) is needed by postgresql92-libs-9.2.8-1PGDG.rhel6.x86_64 因此，我们需要对系统的openssl进行升级。 升级步骤 首先，使用下面的命令卸载系统的openssl： 1$ rpm --nodeps -e openssl 然后，下载PostgreSQL9.2.8依赖的的openssl10并安装。 下载地址： ftp://ftp.pbone.net/mirror/dl.iuscommunity.org/pub/ius/stable/Redhat/6/x86_64/openssl10-libs-1.0.1e-1.ius.el6.x86_64.rpm 最后，重新安装PostgreSQL9.2.8的rpm包。 3. 初始化数据到自定义目录创建自定义目录/opt/pg/data 1$ mkdir /opt/pg_data 更改目录所有者 1$ chown postgres:postgres /opt/pg_data 使用postgres用户初始化数据目录（每次启动数据库的时加-D参数指定路径，或者修改postgres用户下的$PGDATA变量为当前数据目录） 1/usr/pgsql-9.1/bin/initdb -D /opt/pg_data 初始化数据后，会显示启动数据库的命令。]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>postgresql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PostgreSQL监控指标]]></title>
    <url>%2F2014%2F04%2F07%2Fsome-metrics-in-postgresql%2F</url>
    <content type="text"><![CDATA[数据库版本：9.3.1（不同版本数据库相关表列名可能略有不同） 数据库状态信息数据库状态信息主要体现数据库的当前状态 1.目前客户端的连接数 1postgres=# SELECT count(*) FROM pg_stat_activity WHERE NOT pid=pg_backend_pid(); 2.连接状态 12345postgres=# SELECT pid,waiting,current_timestamp - least(query_start,xact_start) AS runtime,substr(query,1,25) AS current_query FROM pg_stat_activity WHERE NOT pid=pg_backend_pid(); pid | waiting | runtime | current_query ------+---------+-----------------+-----------------------9381 | f | 00:01:24.425487 | select * from orders; 可以查看每个连接的pid，执行的操作，是否发生等待，根据查询,或者事务统计开始时间等等。有多少个链接查询就有多少行 所以可以用order by,limit限制查询行数 3.数据库占用空间 1postgres=# select pg_size_pretty(pg_database_size('postgres')); 一个数据库数据文件对应存储在以这个数据库PID命名的文件中,通过统计所有以PID命名文件的总大小，也可以得出这个数据库占用的空间。 表占用的空间使用pg_relation_size()查询，对应的系统中的文件名和pg_class中的filenode相同，一个热点的表最好一天一统计大小，获得表的一个增长状况，来预测数据库未来可能的增长状况。根据需求提前准备空间应付数据库的增长。 4.等待事件 12345postgres# SELECT bl.pid AS blocked_pid, a.usename AS blocked_user, kl.pid AS blocking_pid, ka.usename AS blocking_user, a.query AS blocked_statement FROM pg_locks bl JOIN pg_stat_activity a ON a.pid = bl.pid JOIN pg_locks kl ON kl.transactionid = bl.transactionid AND kl.pid != bl.pid JOIN pg_stat_activity ka ON ka.pid = kl.pid WHERE NOT bl.granted; 根据阻塞的语句的会话PID做相应处理 数据库统计信息1.sql语句统计 实现上述统计需要安装一个pg的extension:pg_stat_statements–1.1.sql，调整postgres.conf:shared_preload_libraries = &#39;pg_stat_statements&#39;,就可以使用了 123456789postgres=# SELECT calls, total_time, rows, 100.0 * shared_blks_hit /nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent,substr(query,1,25)FROM pg_stat_statements ORDER BY total_time DESC LIMIT 5;calls | total_time | rows | hit_percent | substr -------+------------+------+----------------------+---------------------------1 | 23.104 | 17 | 99.4974874371859296 | SELECT n.nspname as Sche1 | 21.808 | 2 | | select * from pg_stat_sta2 | 5.391 | 2 | | SELECT name FROM (SELECT3 | 3.307 | 57 | 100.0000000000000000 | SELECT pg_catalog.quote_i4 | 1.318 | 19 | 100.0000000000000000 | SELECT calls, total_time, 上述查询是按照查询的执行时间来排序的，可以定位执行比较慢的sql,也可以用来查出常用sql，以及sql共享内存的命中率等信息 2.表的共享内存的利用情况统计 实现上述统计需要安装一个pg的extension:pg_buffercache–1.0.sql 12345678910postgres=# SELECT c.relname, count(*) AS buffers FROM pg_buffercache b INNER JOIN pg_class c ON b.relfilenode = pg_relation_filenode(c.oid) AND b.reldatabase IN (0, (SELECT oid FROM pg_database WHERE datname = current_database())) GROUP BY c.relname ORDER BY 2 DESC LIMIT 5;relname | buffers --------------------------------+---------pg_proc | 28pg_attribute | 23pg_operator | 14pg_proc_proname_args_nsp_index | 10pg_class | 9 表在共享内存中占用的块数，用来查看表是不是在内存中，buffers的单位是数据块，默认8K，如果计算大小等于表的大小，说明全表的数据都在缓存中，这时的查询速度是很快的 3.对一个表执行操作的统计 实现统计对一个表操作的偏重，insert,update,delete的比率 1234567891011postgres=# update products set price=11.00 where prod_id=30;UPDATE 1postgres=# delete from products where prod_id=30;DELETE 1postgres=# SELECT relname,cast(n_tup_ins AS numeric) / (n_tup_ins + n_tup_upd + n_tup_del) AS ins_pct,cast(n_tup_upd AS numeric) / (n_tup_ins + n_tup_upd + n_tup_del) AS upd_pct, cast(n_tup_del AS numeric) / (n_tup_ins + n_tup_upd + n_tup_del) AS del_pct FROM pg_stat_user_tables where relname='products';relname | ins_pct | upd_pct | del_pct ----------+------------------------+------------------------+------------------------products | 0.00000000000000000000 | 0.50000000000000000000 | 0.50000000000000000000 4.表的IO和索引的IO 表的IO主要涉及查询的时候查询的逻辑块和物理块，归到底也是命中率的问题，当然最简单的思维方式，一张表存在内存中的内容越多，相应的查询速度越快。相关视图：pg_stat_user_tables 相对于表的大小来说，索引占用的空间要小的多，所以常用的表，可以让其索引一直存在内存中，很多时候保持索引的一个高命中率是非常必要的。相关视图: pg_stat_user_indexes 123postgres# SELECT indexrelname,cast(idx_blks_hit as numeric) / (idx_blks_hit + idx_blks_read) AS hit_pct,idx_blks_hit,idx_blks_read FROM pg_statio_user_indexes WHERE (idx_blks_hit + idx_blks_read)&gt;0 ORDER BY hit_pct; 5.buffer background 和 checkpoint 涉及检查点写和后台写的比率问题，检查点的集中数据写入会对数据库IO的性能有很大的提升，但相应的需要部分空间存储脏数据，而且一旦数据库崩溃，内存中未被写入磁盘的脏数据越多，数据库恢复时间也就越长，这是一个数据库的平衡问题，相关问题在调优文档中做深入研究。 相关视图：pg_stat_bgwriter 1234567postgres=# SELECT(100 * checkpoints_req) / (checkpoints_timed + checkpoints_req) AS checkpoints_req_pct,pg_size_pretty(buffers_checkpoint * block_size / (checkpoints_timed + checkpoints_req)) AS avg_checkpoint_write,pg_size_pretty(block_size * (buffers_checkpoint + buffers_clean + buffers_backend)) AS total_written,100 * buffers_checkpoint / (buffers_checkpoint + buffers_clean + buffers_backend) AS checkpoint_write_pct,100 * buffers_backend / (buffers_checkpoint + buffers_clean + buffers_backend) AS backend_write_pct,*FROM pg_stat_bgwriter,(SELECT cast(current_setting('block_size') AS integer) AS block_size) AS bs; 系统监控信息介绍一些关于数据库需要查看的系统状态信息 1.数据库基本状态信息 123# ps -ef | grep postgres# netstat -altunp | grep 5432# pg_controdata pg_controdata命令和psql同在postgres的bin目录下,系统命令下查询到最全面的数据库快照信息 2.top动态信息配合其他命令使用(截取相关行) 123456789# top -u postgresCpu(s): 1.7%us, 1.0%sy, 0.0%ni, 97.3%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stMem: 2051164k total, 1476536k used, 574628k free, 239624k buffersSwap: 2097144k total, 0k used, 2097144k free, 768676k cachedPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 11172 postgres 20 0 709m 34m 33m S 0.0 1.7 0:00.79 postgres 9380 postgres 20 0 167m 5284 2272 S 0.0 0.3 0:00.05 psql 11178 postgres 20 0 709m 5168 4408 S 0.0 0.3 0:00.01 postgres 11179 postgres 20 0 709m 4656 3920 S 0.0 0.2 0:00.01 postgres 12345# freetotal used free shared buffers cachedMem: 2051164 1476032 575132 0 239624 768676-/+ buffers/cache: 467732 1583432Swap: 2097144 0 2097144 12345678# iotop -u postgresTotal DISK READ: 0.00 B/s | Total DISK WRITE: 0.00 B/s11175 be/4 postgres 0.00 B/s 0.00 B/s 0.00 % 0.00 % postgres: logger process11181 be/4 postgres 0.00 B/s 0.00 B/s 0.00 % 0.00 % postgres: autovacuum launcher process11178 be/4 postgres 0.00 B/s 0.00 B/s 0.00 % 0.00 % postgres: checkpointer process11180 be/4 postgres 0.00 B/s 0.00 B/s 0.00 % 0.00 % postgres: wal writer process11182 be/4 postgres 0.00 B/s 0.00 B/s 0.00 % 0.00 % postgres: stats collector process11179 be/4 postgres 0.00 B/s 0.00 B/s 0.00 % 0.00 % postgres: writer process 简单分析下top命令,用top可以分析出系统的当前总体的负载状况，如上例，总体负载率很低，在io等待率高的时候可以使用iotop来定位io具体的进程，top中的VIRT RES 可以看出进程希望获取的内存，和占用系统内存的数量，可以根据来判定系统内存的分配情况，内存的值也关联到一些参数的设定，如postgres在发生检查点之前checkpointer process进程会消耗比较大的物理内存，直到检查点发生后，占用的内存才会被释放掉，所以在设置检查点时间的时候也要将内存的占用考虑进去。 free总体的表现内存的使用情况，buffers和cached在应用申请内存的时候会被系统释放掉，所以应用实际可以使用的内存是free的值加上buffers和cached的。 3.sar做辅助分析 sar类似于快照的方式来保存系统过去的信息 12345# sar03:40:01 PM CPU %user %nice %system %iowait %steal %idle03:50:01 PM all 1.56 0.00 0.68 0.10 0.00 97.6704:00:02 PM all 1.91 0.00 0.63 0.05 0.00 97.41Average: all 1.07 0.04 1.95 2.65 0.00 94.29 123456# sar -r12:40:01 PM kbmemfree kbmemused %memused kbbuffers kbcached kbcommit %commit12:50:01 PM 567124 1484040 72.35 237596 755720 1426740 34.3901:10:01 PM 567256 1483908 72.34 237600 755720 1426740 34.3901:20:01 PM 567132 1484032 72.35 237600 755724 1426740 34.39Average: 742177 1308987 63.82 195809 669444 1390037 33.51 这些统计信息可以对照性能问题，查看当时的系统状态。]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>postgresql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RHEL系统安装MySql]]></title>
    <url>%2F2014%2F04%2F06%2Finstall-mysql-on-rhel-system%2F</url>
    <content type="text"><![CDATA[环境说明 操作系统:linux6.4 MySql版本：percona 5.6.14 rpm包下载地址：http://www.percona.com/downloads/Percona-Server-5.6/LATEST/RPM/rhel6/x86_64 安装步骤1. 安装所需要的rpm包123rpm -ivh Percona-Server-shared-56-5.6.14-rel62.0.483.rhel6.x86_64.rpmrpm -ivh Percona-Server-client-56-5.6.14-rel62.0.483.rhel6.x86_64.rpmrpm -ivh Percona-Server-server-56-5.6.14-rel62.0.483.rhel6.x86_64.rpm 注意: 第三个包依赖前2个包，第三个包安装时可能会报错，需要将系统中原先的mysql-libs卸载（yum remove mysql-libs） 没有yum使用rpm -e --nodeps的方式卸载安装包，可以使用rpm -qa | grep mysql的方式查看安装的包 2. 启动mysql1#service mysql start 停止：stop 重启：restart，查看状态：status 3. 设置远程登录123#mysqlmysql&gt; grant all PRIVILEGES on test.* to test@&apos;%&apos; identified by &apos;test&apos;;mysql&gt;flush privileges; 将test数据库的权限授予test用户，登录密码是test，%代表所有ip。 4. 配置文件rpm包默认配置启动文件模板/usr/share/doc/Percona-Server-server-56-5.6.14/my-default.cnf，可以将他拷贝到/etc/my.cnf作为配置文件使用。 配置文件修改举例： 123456789101112$ cp /usr/share/doc/Percona-Server-server-56-5.6.14/my-default.cnf /etc/my.cnf$ vim /etc/my.cnf #将需要修改的参数做如下填写[mysqld]# These are commonly set, remove the # and set as required.# basedir = .....# datadir = .....# port = .....# server_id = .....# socket = .....innodb_file_format=barracudainnodb_file_per_table=trueinnodb_large_prefix=on 上面参数作用，可以解决建索引时Specified key was too long; max key length is 767 bytes的报错，拓展支持的最大索引长度，如使用上述功能建表时加ROW_FORMAT=DYNAMIC 数据目录默认：/var/lib/mysql/]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RHEL系统安装MySQL主备环境]]></title>
    <url>%2F2014%2F04%2F06%2Fmysql-config-for-master-slave-replication%2F</url>
    <content type="text"><![CDATA[环境准备 操作系统： rhel6.4 数据库： percona 5.6.14 使用3306端口保证端口未被占用，selinux关闭状态 原理说明mysql的复制（Replication)是一个异步的复制，从一个mysql instance(称之为master)复制到另一个mysql instance(称之为slave).实现整个复制操作主要由三个进程完成的，其中俩个进程在slave(sql进程和io进程），另外一个进程在master（IO进程）上。 要实施复制，首先要打开master端的binary log(bin-log)功能，否则无法实现。因为整个复制过程实际上就是slave从master端获取该日志然后在自己升上完全顺序的执行日志中所记录的各种操作。 配置说明1. 配置master并启动拷贝配置文件： 1cp /usr/share/doc/Percona-Server-server-56-5.6.14/my-default.cnf /etc/my.cnf 编辑/etc/my.cnf： 12345[mysqld]explicit_defaults_for_timestamp=true ##开启查询缓存# log_binlog_bin = mysql-binserver_id = 36 启动mysql服务： 1service mysql start 2. 配置slave并启动拷贝配置文件： 1cp /usr/share/doc/Percona-Server-server-56-5.6.14/my-default.cnf /etc/my.cnf 编辑/etc/my.cnf： 1234567[mysqld]explicit_defaults_for_timestamp=true log_bin = mysql-binserver_id = 37relay_log=/var/lib/mysql/mysql-relay-bin ##传送过来的日志存放目录log_slave_updates=1read_only=1 ##这个参数只对普通用户生效，超级用户和复制用户无效 启动mysql服务： 1service mysql start 3. 主从分别授权用户在master,slave分别执行以下命令： 123#mysqlmysql-&gt;grant replication slave,replication client on *.* to repl@&apos;%&apos; identified by &apos;123456&apos;;mysql-&gt;flush priviledges; 4. 主库数据备份到从库master上运行： 1# mysqldump -A &gt;all.sql slave上运行： 1# mysql &lt;all.sql 5. 根据主状态启动从库(1) 查询主库状态 12345678mysql-&gt;show master status \G *************************** 1. row *************************** File: mysql-bin.000001 Position: 697 Binlog_Do_DB: Binlog_Ignore_DB: Executed_Gtid_Set: 1 row in set (0.00 sec) (2) 从库启动复制 1234#mysqlmysql-&gt;change master to -&gt;master_host=&quot;192.168.0.114&quot;,master_port=3306,master_user=&quot;repl&quot;,master_password=&quot;123456&quot;,master_log_file=&quot;mysql-bin.000001&quot;,master_log_pos=697;mysql-&gt;start slave; (3) 从库状态 1234567891011121314mysql&gt; show slave status \G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.0.114 Master_User: repl Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 697 Relay_Log_File: mysql-relay-bin.000002 Relay_Log_Pos: 563 Relay_Master_Log_File: mysql-bin.000001 ....Exec_Master_Log_Pos: 697 .... 6. 主从用于复制的进程在master上查看进程： 1mysql-&gt;show processlist 1个如下状态的进程: 1Master has sent all binlog to slave; waiting for binlog to be updated 在slave上查看进程： 1mysql-&gt;show processlist 2个如下状态的进程 12Waiting for master to send eventSlave has read all relay log; waiting for the slave I/O thread to update it 7. 主从状态监控常规做法是slave上show slave status\G中的两个变量的差值（Read_Master_Log_Pos，Exec_Master_Log_Pos),也可以使用percona提供的工具包pt-heartbeat。 8. Percona-tookit 安装及pg-heartbeat使用工具集中包含pt-heartbeat, 安装依赖perl-DBD-MySQL， perl-IO-Socket-SSL: 1% rpm -ivh percona-toolkit-2.2.6-1.noarch.rpm pg-heartbeat功能介绍： 监控复制延迟 测量复制落后时间 实现机制： 第一步，pt-heartbeat的–update线程会在指定的时间间隔更新一个时间戳。 第二步，pt-heartbeat的–monitor线程或者–check线程连接到从上检查前面更新的时间戳，和主当地时间做比较，得出时间差。 使用例子： 1）初始化环境，创建一个后台进程定期更新主上的test库heartbeat表,默认是一秒可以–interval指定，执行后会生成一个heartbeat表，test为需要监控的同步库 1pt-heartbeat -D test --update -u repl -p 123456 -h 192.168.0.108 --create-table --daemonize 2）监控并输出slave落后时间 1234pt-heartbeat -D test --monitor -u repl -p 123456 -h 192.168.0.1150.00s [ 0.00s, 0.00s, 0.00s ] ###瞬时延迟 [一分钟平均延迟，五分钟平均延迟，十五分钟平均延迟]0.00s [ 0.00s, 0.00s, 0.00s ]0.00s [ 0.00s, 0.00s, 0.00s ] 结果如下 会一直输出,断开一下连接得到如下结果，最后同步 120.00s [ 0.34s, 0.07s, 0.02s ]0.00s [ 0.00s, 0.07s, 0.02s ] 3）只输出瞬时的差值 12#pt-heartbeat -D test --test -u repl -p 123456 -h 192.168.0.1150.00 ###只代表瞬时延迟 9. mysql主从互换(1) 停止从库复制,从新设置状态 123mysql-&gt;stop slave;mysql-&gt;reset master;mysql-&gt;reset slave; (2) 如配置文件相同的情况下，配置文件无需更改。否者主备的配置文件交换。 (3) 原先的主变为备 12mysql-&gt;reset master;mysql-&gt;reset slave; 从新配置change master to参数 (4) 服务器重启 如原先的主中有bin日志在从上为实现同步，可以认为读取bin日志的内容，在新的主中执行]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Lua和OpenResty搭建验证码服务器]]></title>
    <url>%2F2014%2F04%2F01%2Fdeploy-a-captcha-server-using-lua-and-openresty%2F</url>
    <content type="text"><![CDATA[Lua下有个Lua-GD图形库，通过简单的Lua语句就能控制、生成图片。 环境说明： 操作系统：RHEL6.4 RHEL系统默认已安装RPM包的Lua-5.1.4，但其只具有Lua基本功能，不提供 lua.h 等，但 Lua-GD 编译需要用到 lua.h，故 Lua 需要编译安装。 Lua-GD 版本号格式为X.Y.XrW，其中X.Y.Z代表gd版本，W代表效力版本，所以 lua-gd 版本：lua-gd-2.0.33r2 相对应 gd 版本为：gd-2.0.33，须注意保持一致。 因生成gif的lua脚本中用到md5加密，故需编译安装md5。 因为生成图片需要唯一命名，故依赖 UUID 另外： 以下操作均以root用户运行，并且以下脚本的当前目录为/opt，即所有的下载的文件都会保存在/opt目录下。 需要安装的软件如下： OpenResty：WEB应用服务器，部署lua代码，提供URL供用户调用和访问 LuaJIT：LUA代码解释器，使用OpenResty中集成的版本 GD库：C图形库 Lua-GD库：Lua绑定的C图形库，使得lua可调用gd Lua-Resty-UUID库：用于生成UUID，保证图片命名唯一性 LuaSocket：lua 的 socket 库 安装lua安装编译所需软件包: 1$ yum install -y make gcc 下载并编译安装 lua-5.1： 123456$ yum install -y readline-devel$ wget http://www.lua.org/ftp/lua-5.1.4.tar.gz$ tar lua-5.1.4.tar.gz$ cd lua-5.1.4$ make linux$ make linux install 安装 gdGD版本：gd-2.0.33 下载地址: http://www.boutell.com/gd/http/gd-2.0.33.tar.gz 1234567$ yum install -y libjpeg-devel libpng-devel freetype-devel fontconfig-devel libXpm-devel$ wget http://www.boutell.com/gd/http/gd-2.0.33.tar.gz$ tar zvxf gd-2.0.33.tar.gz$ cd gd-2.0.33$ ./configure$ make &amp;&amp; make install 安装 Lua-gd 库Lua-GD版本：lua-gd-2.0.33r2 下载地址: http://jaist.dl.sourceforge.net/project/lua-gd/lua-gd/lua-gd-2.0.33r2%20%28for%20Lua%205.1%29/lua-gd-2.0.33r2.tar.gz 开发手册可参考: http://ittner.github.io/lua-gd/manual.html 说明： 须先完成gd的安装，且版本号必须为gd-2.0.33调用Lua-GD库的lua代码须由OpenResty中集成的LuaJIT解释执行 123$ wget http://sourceforge.net/projects/lua-gd/files/lua-gd/lua-gd-2.0.33r2%20(for%20Lua%205.1)/lua-gd-2.0.33r2.tar.gz/download?use_mirror=jaist$ tar zvxf lua-gd-2.0.33r2.tar.gz$ cd lua-gd-2.0.33r2 接写来修改Makefile文件： 注释第36～42行 打开第48～52行注释，并做如下修改 1234567OUTFILE=gd.soCFLAGS=-Wall `gdlib-config --cflags` -I/usr/local/include/lua -O3 //第49行，修改 lua 的 C 库头文件所在路径GDFEATURES=`gdlib-config --features |sed -e &quot;s/GD_/-DGD_/g&quot;`LFLAGS=-shared `gdlib-config --ldflags` `gdlib-config --libs` -llua -lgd //第51行，取消lua库版本号51INSTALL_PATH=/usr/local/lib/lua/5.1 //第52行，设置 gd.so 的安装路径$(CC) -fPIC -o ... //第70行，gcc 编译，添加 -fPIC 参数 然后编译： 1$ make &amp;&amp; make install 安装 md5123456$ yum install unzip$ wget https://github.com/keplerproject/md5/archive/master.zip -O md5-master.zip$ unzip md5-master.zip$ cd md5-master$ make &amp;&amp; make install 安装 Lua-resty-UUID 库调用系统的UUID模块生成的由32位16进制（0-f）数组成的的串，本模块进一步压缩为62进制。正如你所想，生成的UUID越长，理论冲突率就越小，请根据业务需要自行斟酌。 基本思想为把系统生成的16字节（128bit）的UUID转换为62进制（a-zA-Z0-9），同时根据业务需要进行截断。 下载地址: https://github.com/dcshi/lua-resty-UUID/archive/master.zip 12345$ yum -y install libuuid-devel$ wget https://github.com/dcshi/lua-resty-UUID/archive/master.zip -O lua-resty-UUID-master.zip$ unzip lua-resty-UUID-master.zip$ cd lua-resty-UUID-master/clib$ make 下载nginx sysguard模块 如果nginx被攻击或者访问量突然变大，nginx会因为负载变高或者内存不够用导致服务器宕机，最终导致站点无法访问。今天要谈到的解决方法来自淘宝开发的模块nginx-http-sysguard，主要用于当负载和内存达到一定的阀值之时，会执行相应的动作，比如直接返回503,504或者其他的。一直等到内存或者负载回到阀值的范围内，站点恢复可用。简单的说，这几个模块是让nginx有个缓冲时间，缓缓。 12$ wget https://github.com/alibaba/nginx-http-sysguard/archive/master.zip -O nginx-http-sysguard-master.zip$ unzip nginx-http-sysguard-master.zip 安装 OpenResty OpenResty（也称为 ngx_openresty）是一个全功能的 Web 应用服务器。它打包了标准的 Nginx 核心，很多的常用的第三方模块，以及它们的大多数依赖项。OpenResty 中的 LuaJIT 组件默认未激活，需使用 --with-luajit 选项在编译 OpenResty 时激活,使用--add-module，添加上sysguard模块 安装的版本：1.2.7.6 下载地址： http://openresty.org/#Download http://openresty.org/download/ngx_openresty-1.2.7.6.tar.gz 先安装依赖软件，然后在编译代码，编译时使用--perfix选项指定 OpenResty 的安装目录，--with-luajit 选项激活 LuaJIT 组件。 1234567$ yum -y install gcc make gmake openssl-devel pcre-devel readline-devel zlib-devel$ wget http://openresty.org/download/ngx_openresty-1.2.7.6.tar.gz$ tar zvxf ngx_openresty-1.2.7.6.tar.gz$ cd ngx_openresty-1.2.7.6$ ./configure --with-luajit --with-http_stub_status_module --add-module=/opt/nginx-http-sysguard-master/$ gmake &amp;&amp; gmake install 创建软连接： 1$ ln -s /usr/local/openresty/nginx/sbin/nginx /usr/sbin/nginx 安装 Redis Server Lua 脚本功能是 Reids 2.6 版本的最大亮点， 通过内嵌对 Lua 环境的支持， Redis 解决了长久以来不能高效地处理 CAS （check-and-set）命令的缺点， 并且可以通过组合使用多个命令， 轻松实现以前很难实现或者不能高效实现的模式。 1234567891011121314151617181920212223$ wget http://redis.googlecode.com/files/redis-2.6.14.tar.gz$ tar zvxf redis-2.6.14.tar.gz$ cd redis-2.6.14$ make &amp;&amp; make install$ mkdir -p /usr/local/redis/conf$ cp redis.conf /usr/local/redis/conf/~~~ # 安装 LuaSocket 库&gt; LuaSocket是一个Lua扩展库，它能很方便地提供SMTP、HTTP、FTP等网络议访问操作。LuaSocket版本：luasocket-2.0-beta2下载地址: &lt;http://files.luaforge.net/releases/luasocket/luasocket/luasocket-2.0-beta2/luasocket-2.0-beta2.tar.gz&gt;~~~bash$ wget http://files.luaforge.net/releases/luasocket/luasocket/luasocket-2.0.2/luasocket-2.0.2.tar.gz$ tar zvxf luasocket-2.0.2.tar.gz$ cd luasocket-2.0.2$ make -f makefile.Linux 安装 redis-lua 库Redis-Lua版本：2.0 下载地址: https://github.com/nrk/redis-lua/archive/version-2.0.zip 123$ wget https://github.com/nrk/redis-lua/archive/version-2.0.zip$ unzip redis-lua-version-2.0.zip$ cd redis-lua-version-2.0 然后，拷贝redis.lua至所需目录。 lua调用方式如下： 1local redis = require(“redis”) 安装 zklua zklua 仅依赖 zookeeper c API 实现，一般存在于 zookeeper-X.Y.Z/src/c， 因此你需要首先安装 zookeeper c API。 zookeeper c API 安装: 12345$ wget http://mirrors.cnnic.cn/apache/zookeeper/zookeeper-3.4.5/$ tar zvxf zookeeper-3.4.5$ cd zookeeper-3.4.5/src/c$ ./configure$ make &amp;&amp; make install 然后安装zklua： 1234$ wget https://github.com/forhappy/zklua/archive/master.zip -O zklua-master.zip$ unzip zklua-master.zip$ cd zklua-master$ make &amp;&amp; make install 修改配置文件配置openrestyopenresty安装在/usr/local/openresty目录，在其目录下创建lualib，用于存放上面安装的一些动态连接库 1234567891011mkdir -p /usr/local/openresty/lualib/captchacp lua-resty-UUID-master/clib/libuuidx.so /usr/local/openresty/lualib/captcha/ #拷贝uuid的库文件cp -r lua-resty-UUID-master/lib/* /usr/local/openresty/lualib/captcha/cp luasocket-2.0.2/luasocket.so.2.0 /usr/local/openresty/lualib/captcha/ #拷贝luasocket的库文件到/usr/local/openresty/lualib/captcha/ln -s /usr/local/openresty/lualib/captcha/luasocket.so.2.0 /usr/local/openresty/lualib/captcha/socket.socp redis-lua-version-2.0/src/redis.lua /usr/local/openresty/lualib/captcha/ #拷贝reis.lua到/usr/local/openresty/lualib/captcha/mkdir -p /usr/local/openresty/lualib/zklua #拷贝zklua文件到/usr/local/openresty/lualib/captcha/cp cd zklua-master/zklua.so /usr/local/openresty/lualib/zklua/ 配置nginx创建www用户： 1useradd -M -s /sbin/nologin www 编辑ngnix.conf，内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113 user www; worker_processes 31; error_log logs/error.log; pid logs/nginx.pid; worker_rlimit_nofile 65535; events &#123; worker_connections 1024; use epoll; &#125; http &#123; include mime.types; default_type application/octet-stream; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log logs/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; gzip on; gzip_min_length 1K; gzip_buffers 4 8k; gzip_comp_level 2; gzip_types text/plain image/gif image/png image/jpg application/x-javascript text/css application/xml text/javascript; gzip_vary on; upstream redis-pool&#123; server 127.0.0.1:10005; keepalive 1024; &#125; server &#123; sysguard on; sysguard_load load=90 action=/50x.html; server_tokens off; listen 10002; server_name localhost; charset utf-8; location / &#123; root html; index index.html index.htm; &#125; #----------------------------------------------------------------------------------------- # 验证码生成 location /captcha &#123; set $percent 0; set $modecount 1; content_by_lua_file /usr/local/openresty/nginx/luascripts/luajit/captcha.lua; &#125; #----------------------------------------------------------------------------------------- # 验证码校验 location /captcha-check &#123; content_by_lua_file /usr/local/openresty/nginx/luascripts/luajit/captcha-check.lua; &#125; # 验证码删除 location /captcha-delete &#123; content_by_lua_file /usr/local/openresty/nginx/luascripts/luajit/captcha-delete.lua; &#125; #----------------------------------------------------------------------------------------- # 样式1-静态图片 location /mode1 &#123; content_by_lua_file /usr/local/openresty/nginx/luascripts/luajit/mode/mode1.lua; &#125; #----------------------------------------------------------------------------------------- # redis中添加key-value location /redisSetQueue &#123; internal; set_unescape_uri $key $arg_key; set_unescape_uri $val $arg_val; redis2_query rpush $key $val; redis2_pass redis-pool; &#125; # redis中获取captcha-string location /redisGetStr &#123; internal; set_unescape_uri $key $arg_key; redis2_query lindex $key 0; redis2_pass redis-pool; &#125; # redis中获取captcha-image location /redisGetImg &#123; internal; set_unescape_uri $key $arg_key; redis2_query lindex $key 1; redis2_pass redis-pool; &#125; #----------------------------------------------------------------------------------------- location ~.*.(gif|jpg|png)$ &#123; expires 10s; &#125; error_page 404 /404.html; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; 上面将 ngnix 的端口修改为10002。 /usr/local/openresty/nginx/luascripts/luajit/captcha.lua 是用于生成验证码，内容如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061--中控脚本----部分应用预先生成--部分应用实时生成，并且随机选择生成样式------------------------------------------------------------------------------------------------package.path = "/usr/local/openresty/lualib/?.lua;/usr/local/openresty/lualib/captcha/?.lua;"package.cpath = "/usr/local/openresty/lualib/?.so;/usr/local/openresty/lualib/captcha/?.so;"------------------------------------------------------------------------------------------------设置随机种子local resty_uuid=require("resty.uuid")math.randomseed(tonumber(resty_uuid.gennum20()))---------------------------------------------------------------------------------------------[[ 预先生成 ]]--if math.random(1,99)&lt;tonumber(ngx.var.percent) then --在redis的预先生成key中随机选择keyid local kid=math.random(1,ngx.var.pregencount) local res = ngx.location.capture('/redisGetImg',&#123; args = &#123; key = kid &#125; &#125;) if res.status==200 then local parser=require("redis.parser") local pic=parser.parse_reply(res.body) ngx.header.content_type="application/octet-stream" --在header中返回用于去redis中查找记录的key ngx.header.picgid=kid --在body中返回captcha ngx.say(pic) ngx.exit(200) endend---------------------------------------------------------------------------------------------[[ 实时生成 ]]----随机选择captcha模式Xlocal mode=math.random(1,ngx.var.modecount)--调用modeX.lua，生成captchalocal res = ngx.location.capture("/mode"..mode)if res.status==200 then ngx.header.content_type="application/octet-stream" --在header中返回用于去redis中查找记录的key ngx.header.picgid=res.header.picgid --在body中返回captcha ngx.say(res.body) ngx.exit(200)end /usr/local/openresty/nginx/luascripts/luajit/captcha-check.lua 用于校验验证码： 1234567891011121314151617181920212223242526272829303132--[[captcha check]]----------------------------------------------------------------------------------------------package.path = "/usr/local/openresty/lualib/?.lua;/usr/local/openresty/lualib/captcha/?.lua;"package.cpath = "/usr/local/openresty/lualib/?.so;/usr/local/openresty/lualib/captcha/?.so;"------------------------------------------------------------------------------------------------获取请求中参数local uriargs = ngx.req.get_uri_args()local picgid = uriargs["image"]local ustr=string.lower(uriargs["str"])--查找redis中key为picgid的记录local res = ngx.location.capture('/redisGetStr',&#123; args = &#123; key = picgid &#125; &#125;)if res.status==200 then local parser=require("redis.parser") local reply=parser.parse_reply(res.body) local rstr=string.lower(reply) --匹配用户输入字符串与redis中记录的字符串，一致返回True，否则返回False ngx.header.content_type="text/plain" if ustr == rstr then ngx.say("True") else ngx.say("False") end --匹配操作后删除redis中该key记录 local redis = require('redis') local client = redis.connect('127.0.0.1', 10005) client:del(picgid)end /usr/local/openresty/nginx/luascripts/luajit/mode/mode1.lua 是生成静态验证码图片： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384--静态图片------------------------------------------------------------------------------------------------package.path = "/usr/local/openresty/lualib/?.lua;/usr/local/openresty/lualib/captcha/?.lua;"package.cpath = "/usr/local/openresty/lualib/?.so;/usr/local/openresty/lualib/captcha/?.so;"--------------------------------------------------------------------------------------------------Redis中插入记录方法function setRedis(skey, sval) local res = ngx.location.capture('/redisSetQueue', &#123;args= &#123;key=skey,val=sval&#125;&#125;) if res.status == 200 then return true else return false endend--设置随机种子local resty_uuid=require("resty.uuid")math.randomseed(tonumber(resty_uuid.gennum20()))--在32个备选字符中随机筛选4个作为captcha字符串local dict=&#123;'A','B','C','D','E','F','G','H','J','K','L','M','N','P','Q','R','S','T','U','V','W','X','Y','Z','2','3','4','5','6','7','8','9'&#125;local stringmark=""for i=1,4 do stringmark=stringmark..dict[math.random(1,32)]end--图片基本info--picgidlocal filename= "1"..resty_uuid.gen20()..".png"--图片78x26local xsize = 78local ysize = 26--字体大小local wsize = 17.5--干扰线(yes/no)local line = "yes"--加载模块local gd=require('gd')--创建面板local im = gd.createTrueColor(xsize, ysize)--定义颜色local black = im:colorAllocate(0, 0, 0)local grey = im:colorAllocate(202,202,202)local color=&#123;&#125;for c=1,100 do color[c] = im:colorAllocate(math.random(100),math.random(100),math.random(100))end--画背景x, y = im:sizeXY()im:filledRectangle(0, 0, x, y, grey)--画字符gd.useFontConfig(true)for i=1,4 do k=(i-1)*16+3 im:stringFT(color[math.random(100)],"Arial:bold",wsize,math.rad(math.random(-10,10)),k,22,string.sub(stringmark,i,i))end--干扰线点if line=="yes" then for j=1,math.random(3) do im:line(math.random(xsize),math.random(ysize),math.random(xsize),math.random(ysize),color[math.random(100)]) end for p=1,20 do im:setPixel(math.random(xsize),math.random(ysize),color[math.random(100)]) endend--流输出local fp=im:pngStr(75)--redis中添加picgid为key,string为value的记录setRedis(filename,stringmark)--response header中传参picgidngx.header.content_type="text/plain"ngx.header.picgid=filename--页面返回picngx.say(fp)--nginx退出ngx.exit(200) 配置redis在/usr/local/openresty/redis/conf/创建redis-10005.conf文件，内容如下： 12345678910111213141516171819202122232425262728293031323334353637383940daemonize yespidfile /usr/local/openresty/redis/redis-10005.pidport 10005timeout 300tcp-keepalive 10loglevel noticelogfile /usr/local/openresty/redis/redis-10005.logdatabases 16save 900 1save 300 10save 60 10000stop-writes-on-bgsave-error yesrdbcompression yesrdbchecksum yesdbfilename dump-10005.rdbdir /usr/local/openresty/redisslave-serve-stale-data yesslave-read-only yesrepl-disable-tcp-nodelay noslave-priority 100appendonly noappendfsync everysecno-appendfsync-on-rewrite noauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mblua-time-limit 5000slowlog-log-slower-than 10000slowlog-max-len 128hash-max-ziplist-entries 512hash-max-ziplist-value 64list-max-ziplist-entries 512list-max-ziplist-value 64set-max-intset-entries 512zset-max-ziplist-entries 128zset-max-ziplist-value 64activerehashing yesclient-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60hz 10 配置验证码服务器在/etc/ld.so.conf.d/目录创建captcha.conf，内容如下： 123456$ vim /etc/ld.so.conf.d/captcha.conf/usr/local/lib/usr/local/openresty/lualib/usr/local/openresty/lualib/captcha/usr/local/openresty/lualib/zklua/usr/local/openresty/luajit/lib 测试生成验证码URL：http://IP:10002/captcha 然后从响应Header中获取图片的picgid=XXXXX 验证码校验URL：http://IP:10002/captcha-check?image=XXXXX&amp;str=ABCDhttp://IP:10002/captcha-check?image=XXXXX&amp;str=ABCD&amp;delete=true或 http://IP:10002/captcha-check?image=XXXXX&amp;str=ABCD&amp;delete=false 参数说明如下： 参数image：要校验的验证码图片的picgid。 参数str：用户输入的验证码字符串。 参数delete：当且仅当传该参数且参数值为false时，校验完成之后该验证码记录不被删除，验证码未过期之前可多次校验，用于异步校验应用中；否则，若不传该参数或者其值为true，校验完成之后该验证码记录删除。 验证码删除URL：http://IP:10002/captcha-delete?image=XXXXX 其中image为要删除的验证码图片的picgid。]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>lua</tag>
        <tag>nginx</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用SaltStack安装JDK1.6]]></title>
    <url>%2F2014%2F04%2F01%2Finstall-jdk-with-saltstack%2F</url>
    <content type="text"><![CDATA[创建states文件在/srv/salt目录下创建jdk目录，并在jdk目录创建init.sls文件，init.sls文件内容如下： 1234567891011121314151617181920212223jdk-file: file.managed: - source: salt://jdk/files/jdk1.6.0_39.tar.gz - name: /usr/java/jdk1.6.0_39.tar.gz - include_empty: True jdk-install: cmd.run: - name: '/bin/tar -zxf jdk1.6.0_39.tar.gz &amp;&amp; /bin/ln -s jdk1.6.0_39 latest ' - cwd: /usr/java - unless: 'test -e jdk1.6.0_39' - require: - file: jdk-file jdk-rmzip: file.absent: - name: /usr/java/jdk1.6.0_39.tar.gz /root/.bashrc: file.append: - text: - export JAVA_HOME=/usr/java/latest - export PATH=$JAVA_HOME/bin:$PATH 上面sls文件需要引用jdk1.6.0_39.tar.gz文件，故需要下载jdk1.6.0_39.bin安装之后将其打包成jdk1.6.0_39.tar.gz拷贝到/srv/salt/jdk/files目录。 init.sls文件执行过程包括以下几个步骤： jdk-file，将salt://jdk/files/jdk1.6.0_39.tar.gz文件拷贝到/usr/java jdk-install，解压文件 jdk-rmzip，删除压缩包 /root/.bashrc，设置JAVA_HOME 修改top.sls文件（该步骤为可选），添加jdk.init: 123base: '*': - jdk.init 测试运行在master上运行下面命令，并观察运行结果： 12345678910111213141516171819202122232425262728293031323334353637383940414243[root@sk1 vagrant]# salt '*' state.sls jdksk2:---------- ID: jdk-file Function: file.managed Name: /usr/java/jdk1.6.0_39.tar.gz Result: True Comment: File /usr/java/jdk1.6.0_39.tar.gz updated Changes: ---------- diff: New file mode: 0644---------- ID: jdk-install Function: cmd.run Name: /bin/tar -zxf jdk1.6.0_39.tar.gz &amp;&amp; /bin/ln -s jdk1.6.0_39 latest Result: True Comment: unless execution succeeded Changes: ---------- ID: jdk-rmzip Function: file.absent Name: /usr/java/jdk1.6.0_39.tar.gz Result: True Comment: Removed file /usr/java/jdk1.6.0_39.tar.gz Changes: ---------- removed: /usr/java/jdk1.6.0_39.tar.gz---------- ID: /root/.bashrc Function: file.append Result: True Comment: Appended 0 lines Changes: Summary------------Succeeded: 4Failed: 0------------Total: 4 从上可以看出成功了4个，失败为0。 安装了jdk之后，需要重启minion(还需要修改minion启动脚本，让minion加载上系统环境变量，详细说明，见安装SaltStack和halite)才能通过下面脚本运行java相关的命令，如java、jps等等： 1salt '*' cmd.run 'jps' 否则，你需要通过下面脚本来执行： 1salt '*' cmd.run 'source /root/.bashrc ;jps' 设置pillar将上面的jdk/init.sls文件修改为通过pillar引用变量 a.首先在/srv/pillar目录创建jdk目录，并在jdk目录下创建init.sls文件，内容如下： 1234jdk: name: jdk1.6.0_39 srvpath: salt://jdk/files homepath: /usr/java b.在/srv/pillar/top.sls中添加jdk.init 123base: '*': - jdk.init c.修改/srv/salt/jdk/init.sls文件为从pillar引入变量，内容如下： undefined d.参考上面，再次测试一遍]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>saltstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python模拟新浪微博登录]]></title>
    <url>%2F2014%2F03%2F18%2Fsimulate-weibo-login-in-python%2F</url>
    <content type="text"><![CDATA[看到一篇Python模拟新浪微博登录的文章，想熟悉一下其中实现方式，并且顺便掌握python相关知识点。 代码下面的代码是来自上面这篇文章，并稍作修改添加了一些注释。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778# -*- coding: utf-8 -*import urllib2import urllibimport cookielib import lxml.html as HTML class Fetcher(object): def __init__(self, username=None, pwd=None, cookie_filename=None): #获取一个保存cookie的对象 self.cj = cookielib.LWPCookieJar() if cookie_filename is not None: self.cj.load(cookie_filename) #将一个保存cookie对象，和一个HTTP的cookie的处理器绑定 self.cookie_processor = urllib2.HTTPCookieProcessor(self.cj) #创建一个opener，将保存了cookie的http处理器，还有设置一个handler用于处理http的URL的打开 self.opener = urllib2.build_opener(self.cookie_processor, urllib2.HTTPHandler) #将包含了cookie、http处理器、http的handler的资源和urllib2对象绑定在一起 urllib2.install_opener(self.opener) self.username = username self.pwd = pwd self.headers = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; rv:14.0) Gecko/20100101 Firefox/14.0.1', 'Referer':'','Content-Type':'application/x-www-form-urlencoded'&#125; def get_rand(self, url): headers = &#123;'User-Agent':'Mozilla/5.0 (Windows;U;Windows NT 5.1;zh-CN;rv:1.9.2.9)Gecko/20100824 Firefox/3.6.9', 'Referer':''&#125; req = urllib2.Request(url ,"", headers) login_page = urllib2.urlopen(req).read() rand = HTML.fromstring(login_page).xpath("//form/@action")[0] passwd = HTML.fromstring(login_page).xpath("//input[@type='password']/@name")[0] vk = HTML.fromstring(login_page).xpath("//input[@name='vk']/@value")[0] return rand, passwd, vk def login(self, username=None, pwd=None, cookie_filename=None): if self.username is None or self.pwd is None: self.username = username self.pwd = pwd assert self.username is not None and self.pwd is not None url = 'http://3g.sina.com.cn/prog/wapsite/sso/login.php?ns=1&amp;revalid=2&amp;backURL=http%3A%2F%2Fweibo.cn%2F&amp;backTitle=%D0%C2%C0%CB%CE%A2%B2%A9&amp;vt=' # 获取随机数rand、password的name和vk rand, passwd, vk = self.get_rand(url) data = urllib.urlencode(&#123;'mobile': self.username, passwd: self.pwd, 'remember': 'on', 'backURL': 'http://weibo.cn/', 'backTitle': '新浪微博', 'vk': vk, 'submit': '登录', 'encoding': 'utf-8'&#125;) url = 'http://3g.sina.com.cn/prog/wapsite/sso/' + rand # 模拟提交登陆 page =self.fetch(url,data) link = HTML.fromstring(page).xpath("//a/@href")[0] if not link.startswith('http://'): link = 'http://weibo.cn/%s' % link # 手动跳转到微薄页面 self.fetch(link,"") # 保存cookie if cookie_filename is not None: self.cj.save(filename=cookie_filename) elif self.cj.filename is not None: self.cj.save() print 'login success!',data def fetch(self, url,data): print 'fetch url: ', url req = urllib2.Request(url,data, headers=self.headers) return urllib2.urlopen(req).read()# 开始运行fet=Fetcher();fet.login("huaiyu2006","XXXXXX") 以上代码引入了一些python的模块，然后创建了一个class封装了login方法。 以上代码的登录逻辑： 1、进入到登陆页面，获取一些关键参数，包括随机数rand、password的name和vk。 2、模拟提交登陆，登陆之后跳到微薄页面。 3、手动跳转到微薄页面。 总结： 以上代码是模拟手机版微博的登陆，如果你想模拟登陆网页版的微博，你可以参考下面两个项目中的代码： cola WeiboMsgBackupGUI Python模块Python urllib模块Python urllib模块提供了一个从指定的URL地址获取网页数据，然后对其进行分析处理，获取想要的数据。 1、urllib模块提供的urlopen函数 123456789101112➜ py-test pythonPython 2.7.5+ (default, Feb 27 2014, 19:37:08) [GCC 4.8.1] on linux2Type "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import urllib&gt;&gt;&gt; help(urllib.urlopen)Help on function urlopen in module urllib:urlopen(url, data=None, proxies=None) Create a file-like object for the specified URL to read from.(END) urllib.urlopen创建一个类文件对象为指定的url来读取： 参数url表示远程数据的路径，一般是http或者ftp路径。 参数data表示以get或者post方式提交到url的数据。 参数proxies表示用于代理的设置。 示例： 123import urllibprint urllib.urlopen('http://www.baidu.com').read() urlopen返回一个类文件对象，它提供了如下方法： 1）read() , readline() , readlines()，fileno()和close()： 这些方法的使用与文件对象完全一样。 2）info()：返回一个httplib.HTTPMessage 对象，表示远程服务器返回的头信息。 3）getcode()：返回Http状态码，如果是http请求，200表示请求成功完成;404表示网址未找到。 4）geturl()：返回请求的url地址。 2.urllibe模块提供的urlretrieve函数。 urlretrieve方法直接将远程数据下载到本地。 参数finename指定了保存本地路径（如果参数未指定，urllib会生成一个临时文件保存数据。） 参数reporthook是一个回调函数，当连接上服务器、以及相应的数据块传输完毕时会触发该回调，我们可以利用这个回调函数来显示当前的下载进度。 参数data指post到服务器的数据，该方法返回一个包含两个元素的(filename, headers)元组，filename表示保存到本地的路径，header表示服务器的响应头。 示例：urlretrieve方法下载文件实例，可以显示下载进度。 123456789101112131415161718#!/usr/bin/python#encoding:utf-8import urllibimport osdef Schedule(a,b,c): ''''' a:已经下载的数据块 b:数据块的大小 c:远程文件的大小 ''' per = 100.0 * a * b / c if per &gt; 100 : per = 100 print '%.2f%%' % perurl = 'http://www.python.org/ftp/python/2.7.5/Python-2.7.5.tar.bz2'#local = url.split('/')[-1]local = os.path.join('/data/software','Python-2.7.5.tar.bz2')urllib.urlretrieve(url,local,Schedule) 3.辅助方法 urllib中还提供了一些辅助方法，用于对url进行编码、解码。 urllib.quote(string[, safe])：对字符串进行编码。参数safe指定了不需要编码的字符; urllib.unquote(string) ：对字符串进行解码； urllib.quote_plus(string [ , safe ] ) ：与urllib.quote类似，但这个方法用’+’来替换’ ‘，而quote用’%20’来代替’ ‘ urllib.unquote_plus(string ) ：对字符串进行解码； urllib.urlencode(query[, doseq])：将dict或者包含两个元素的元组列表转换成url参数。例如 字典{‘name’: ‘dark-bull’, ‘age’: 200}将被转换为”name=dark-bull&amp;age=200” urllib.pathname2url(path)：将本地路径转换成url路径； urllib.url2pathname(path)：将url路径转换成本地路径； 通过上面的练习可以知道，urlopen可以轻松获取远端html页面信息，然后通过python正则对所需要的数据进行分析，匹配出想要用的数据，在利用urlretrieve将数据下载到本地。对于访问受限或者对连接数有限制的远程url地址可以采用proxies（代理的方式）连接，如果远程数据量过大，单线程下载太慢的话可以采用多线程下载，这个就是传说中的爬虫。 Python urllib2模块客户端与服务器端通过request与response来沟通，客户端先向服务端发送request，然后接收服务端返回的response urllib2提供了request的类，可以让用户在发送请求前先构造一个request的对象，然后通过urllib2.urlopen方法来发送请求 更详细的说明请参考：http://zhuoqiang.me/python-urllib2-usage.html Python cookielib模块cookielib模块的主要作用是提供可存储cookie的对象，以便于与urllib2模块配合使用来访问Internet资源。例如可以利用本模块的CookieJar类的对象来捕获cookie并在后续连接请求时重新发送。coiokielib模块用到的对象主要有下面几个：CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。 CookieJar管理HTTP cookie值、存储HTTP请求生成的cookie、向传出的HTTP请求添加cookie的对象。整个cookie都存储在内存中，对CookieJar实例进行垃圾回收后cookie也将丢失。 FileCookieJar检索cookie信息并将cookie存储到文件中。filename是存储cookie的文件名。delayload为True时支持延迟访问访问文件，即只有在需要时才读取文件或在文件中存储数据。 MozillaCookieJar创建与Mozilla浏览器cookies.txt兼容的FileCookieJar实例。 LWPCookieJar创建与libwww-perl的Set-Cookie3文件格式兼容的FileCookieJar实例。 cookielib模块一般与urllib2模块配合使用，主要用在urllib2.build_oper()函数中作为urllib2.HTTPCookieProcessor()的参数。 使用方法如下面登录人人网的代码: 12345678910111213#! /usr/bin/env python#coding=utf-8import urllib2import urllibimport cookielibdata=&#123;"email":"用户名","password":"密码"&#125; #登陆用户名和密码post_data=urllib.urlencode(data)cj=cookielib.CookieJar()opener=urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))headers =&#123;"User-agent":"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1"&#125;req=urllib2.Request("http://www.renren.com/PLogin.do",post_data,headers)content=opener.open(req)print content2.read().decode("utf-8").encode("gbk") Python lxml模块具体用法可以参考 使用由 Python 编写的 lxml 实现高性能 XML 解析 上面python脚本主要是使用了lxml的xpath语法进行快速查找。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDH HBase中实现的一些特性]]></title>
    <url>%2F2014%2F03%2F15%2Fnew-features-in-idh-hbase%2F</url>
    <content type="text"><![CDATA[IDH为Intel’s Distribution of Hadoop的简称，中文为英特尔Hadoop发行版，目前应该没有人在维护该产品了。这里简单介绍一下IDH HBase中实现的一些特性。 以下部分内容摘自IDH官方的一些文档，部分内容来自我的整理： 1、 单调数据的加盐处理 对于写入的rowkey是基本单调的（例如时序数据），IDH引入了一个新的接口：SaltedTableInterface 提高近乎透明的“加盐”，方便使用 封装了get、scan、put、delete等操作 2、提供了Rolling Scanner应对HFile数量大量增加情况下的get、scan性能 3、提供了ParallelClientScanner加速大范围查询性能 具体实现，请参考HBase客户端实现并行扫描 4、使得协作器实用化，从而可使用协作器来进行计算 相关说明，可以参考HBase实现简单聚合计算 5、提供基于lucene的全文检索 6、提供大对象的高效存储 类似Oracle的BLOB存储 对用户透明 2x以上的写入性能，还有些进步空间 2x的随机访问性能 1.3x的scan性能 接近直接写入hdfs性能 7、引入交互式的hive over hbase 完全的hive支持，常用功能（select、group by、top n等等）用hbase协作器实现，其余功能（大表关联等等）用mapreduce无缝对接 去除mapreduce的overhead，大大地减少了数据传输 性能有3x-10x提升 具体介绍，请参考Hive Over HBase的介绍 8、支持跨数据中心的大表 9、HBase中支持对某列族设置副本数 10、可以通过定时任务设置文件压缩合并频率]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Solr的schema.xml]]></title>
    <url>%2F2014%2F03%2F15%2Fschema-in-solr%2F</url>
    <content type="text"><![CDATA[schema.xml是Solr一个配置文件，它包含了你的文档所有的字段，以及当文档被加入索引或查询字段时，这些字段是如何被处理的。这个文件被存储在Solr主文件夹下的conf目录下，默认的路径./solr/conf/schema.xml，也可以是Solr webapp的类加载器所能确定的路径。在下载的Solr包里，有一个schema的样例文件，用户可以从那个文件出发，来观察如何编写自己的Schema.xml。 type节点先来看下type节点，这里面定义FieldType子节点，包括name、class、positionIncrementGap等一些参数。必选参数： name：就是这个FieldType的名称。 class：指向org.apache.solr.analysis包里面对应的class名称，用来定义这个类型的行为。 其他可选的属性： sortMissingLast，sortMissingFirst两个属性是用在可以内在使用String排序的类型上，默认false，适用于字段类型：string、boolean、sint、slong、sfloat、sdouble、pdate。 sortMissingLast=”true”，没有该field的数据排在有该field的数据之后，而不管请求时的排序规则，在Java中对应的意思就是，该字段为NULL，排在后面。 sortMissingFirst=”true”，排序规则与sortMissingLast相反。 positionIncrementGap：可选属性，定义在同一个文档中此类型数据的空白间隔，避免短语匹配错误。 在配置中，string类型的class是solr.StrField，而这个字段是不会被分析存储的，也就是说不会被分词。 而对于文章或者长文本来说，我们必须对其进行分词才能保证搜索某些字段时能够给出正确的结果。这时我们就可以用到另外一个class，solr.TextField。它允许用户通过分析器来定制索引和查询，分析器包括一个分词器（tokenizer）和多个过滤器（filter） 。 一个标准的分词： 1234567891011&lt;fieldType name="text_general" class="solr.TextField" positionIncrementGap="100"&gt; &lt;analyzer type="index"&gt; &lt;tokenizer class="solr.StandardTokenizerFactory" /&gt; &lt;filter class="solr.LowerCaseFilterFactory" /&gt; &lt;/analyzer&gt; &lt;analyzer type="query"&gt; &lt;tokenizer class="solr.StandardTokenizerFactory" /&gt; &lt;filter class="solr.LowerCaseFilterFactory" /&gt; &lt;filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" enablePositionIncrements="true" /&gt; &lt;/analyzer&gt;&lt;/fieldType&gt; 分词用的依旧是fieldType，为的是在下面的field中能够用到。有两个analyzer，一个是index，一个是query，index是针对于所有，query是针对于搜索。 tokenizer节点当然就是对应分析链中的起点Tokenizer。接下来串联了2个filter，分别是solr.StopFilterFactory，solr.LowerCaseFilterFactory。stop word filter就是把那些the、 of、 on之类的词从token中去除掉，由于这类词在文档中出现的频率非常高，而对文档的特征又没什么影响，所以这类词对查询没什么意义。Lower case filter的作用是将所有的token转换成小写，也就是在最终的index中保存的都是小写 你也可以定义一个analyzer，例如使用mmseg4j进行中文分词： 12345&lt;fieldType name="text_zh" class="solr.TextField" positionIncrementGap="100"&gt; &lt;analyzer&gt; &lt;tokenizer class="com.chenlb.mmseg4j.solr.MMSegTokenizerFactory" mode="complex" /&gt; &lt;/analyzer&gt;&lt;/fieldType&gt; filed节点filed节点用于定义数据源字段所使用的搜索类型与相关设置。含有以下属性 name：数据源字段名，搜索使用到。 type：搜索类型名例如中文ika搜索名text_ika，对应于fieldType中的name。不需要分词的字符串类型，string即可，如果需要分词，用上面配置好的分词type。 indexed：是否被索引，只有设置为true的字段才能进行搜索排序分片(earchable、 sortable、 facetable)。 stored：是否存储内容，如果不需要存储字段值，尽量设置为false以提高效率。 multiValued：是否为多值类型，SOLR允许配置多个数据源字段存储到一个搜索字段中。多个值必须为true，否则有可能抛出异常。 omitNorms：是否忽略掉Norm，可以节省内存空间，只有全文本field和need an index-time boost的field需要norm。（具体没看懂，注释里有矛盾） termVectors：当设置true，会存储 term vector。当使用MoreLikeThis，用来作为相似词的field应该存储起来。 termPositions：存储 term vector中的地址信息，会消耗存储开销。 termOffsets：存储 term vector 的偏移量，会消耗存储开销。 default：如果没有属性需要修改，就可以用这个标识下。 docValues：Solr 4.2中加入了该属性 docValuesFormat：可选的值为Disk或者Memory 举例： 1&lt;field name="manu_exact" type="string" indexed="false" stored="false" docValues="true" /&gt; copyField节点如果我们的搜索需要搜索多个字段该怎么办呢？这时候，我们就可以使用copyField。代码如下： 12&lt;copyField source="name" dest="all" maxChars="30000"/&gt;&lt;copyField source="address" dest="all" maxChars="30000"/&gt; 作用： 将多个field的数据放在一起同时搜索，提供速度 将一个field的数据拷贝到另一个，可以用2种不同的方式来建立索引 我们将所有的中文分词字段全部拷贝至all中，当我们进行全文检索是，只用搜索all字段就OK了。 其包含属性： source：源field字段 dest：目标field字段 maxChars：最多拷贝多少字符 注意，这里的目标字段必须支持多值，最好不要存储，因为他只是做搜索。indexed为true，stored为false。 copyField节点和field节点都在fields节点之内。 dynamicField节点动态字段，没有具体名称的字段，用dynamicField字段 如：name为*_i，定义它的type为int，那么在使用这个字段的时候，任务以_i结果的字段都被认为符合这个定义。如name_i、 school_i 123&lt;dynamicField name="*_i" type="int" indexed="true" stored="true"/&gt; &lt;dynamicField name="*_s" type="string" indexed="true" stored="true"/&gt;&lt;dynamicField name="*_l" type="long" indexed="true" stored="true"/&gt; uniqueKey节点solr必须设置一个唯一字段，常设置为id，此唯一一段有uniqueKey节点指定。 例如： 1&lt;uniqueKey&gt;id&lt;/uniqueKey&gt; defaultSearchField节点默认搜索的字段，我们已经将需要搜索的字段拷贝至all字段了，在这里设为all即可。 1&lt;defaultSearchField&gt;all&lt;/defaultSearchField&gt; solrQueryParser节点默认搜索操作符参数，及搜索短语间的逻辑，用AND增加准确率，用OR增加覆盖面，建议用AND，也可在搜索语句中定义。例如搜索“手机 苹果”，使用AND默认搜索为“手机AND苹果“。 1&lt;solrQueryParser defaultOperator="OR"/&gt; similarity节点Similarity式lucene中的一个类，用来在搜索过程中对一个文档进行评分。该类可以做些修改以支持自定义的排序。在Solr4中，你可以为每一个field配置一个不同的similarity，你也可以在schema.xml中使用DefaultSimilarityFactory类配置一个全局的similarity。 你可以使用默认的工厂类来创建一个实例，例如： 1&lt;similarity class="solr.DefaultSimilarityFactory"/&gt; 你也可以使用其他的工厂类，然后设置一些可选的初始化参数： 123456&lt;similarity class="solr.DFRSimilarityFactory"&gt; &lt;str name="basicModel"&gt;P&lt;/str&gt; &lt;str name="afterEffect"&gt;L&lt;/str&gt; &lt;str name="normalization"&gt;H2&lt;/str&gt; &lt;float name="c"&gt;7&lt;/float&gt;&lt;/similarity&gt; 在Solr 4中，你可以为每一个field配置： 12345678&lt;fieldType name="text_ib"&gt; &lt;analyzer/&gt; &lt;similarity class="solr.IBSimilarityFactory"&gt; &lt;str name="distribution"&gt;SPL&lt;/str&gt; &lt;str name="lambda"&gt;DF&lt;/str&gt; &lt;str name="normalization"&gt;H2&lt;/str&gt; &lt;/similarity&gt;&lt;/fieldType&gt; 上面例子中，使用了DFRSimilarityFactory和IBSimilarityFactory，这里还有一些其他的实现类。在Solr 4.2中加入了SweetSpotSimilarityFactory。其他还有：BM25SimilarityFactory、SchemaSimilarityFactory等。 参考文章 [1] Solr配置，schema.xml的配置，以及中文分词 [2] Other Schema Elements]]></content>
      <categories>
        <category>search-engine</category>
      </categories>
      <tags>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BroadLeaf项目集成SolrCloud]]></title>
    <url>%2F2014%2F03%2F14%2Fbroadleaf-project-with-solrcloud%2F</url>
    <content type="text"><![CDATA[《BroadLeaf项目搜索功能改进》一文中介绍了 BroadLeaf 项目中如何改进搜索引擎这一块的代码，其中使用的是单节点的 solr 服务器，这篇文章主要介绍 BroadLeaf 项目如何集成 SolrCloud 集群。 1、SolrCloud环境搭建参考 《Apache SolrCloud安装》，搭建Solr集群环境，将 Demosite 所用的 Solr 配置文件 solrconfig.xml 和 schema.xml 上传到 zookeeper 集群中，保证成功启动 Solr 集群。 2、扩展SearcheService类扩展SearchService类的步骤与单节点集成一致，此处不再叙述。 3、修改Solr相关配置文件a) 删除site模块中的site/src/main/webapp/WEB-INF/applicationContext.xml中的以下代码： 1234567&lt;bean id="solrEmbedded" class="java.lang.String"&gt; &lt;constructor-arg value="solrhome"/&gt;&lt;/bean&gt;&lt;bean id="blSearchService" class="org.broadleafcommerce.core.search.service.solr.SolrSearchServiceImpl"&gt; &lt;constructor-arg name="solrServer" ref="$&#123;solr.source&#125;" /&gt; &lt;constructor-arg name="reindexServer" ref="$&#123;solr.source.reindex&#125;" /&gt;&lt;/bean&gt; b)删除site模块的site/src/main/resources/runtime-properties/common.properties中以下代码： 12solr.source=solrEmbeddedsolr.source.reindex=solrEmbedded c)在core模块中core/src/main/resources/applicationContext.xml添加如下代码： 12345678910111213141516&lt;bean id="solrServer" class="org.apache.solr.client.solrj.impl.CloudSolrServer"&gt; &lt;constructor-arg value="$&#123;solr.url&#125;"/&gt; &lt;property name="defaultCollection" value="product" /&gt; &lt;property name="zkClientTimeout" value="20000" /&gt; &lt;property name="zkConnectTimeout" value="1000" /&gt;&lt;/bean&gt;&lt;bean id="solrReindexServer" class="org.apache.solr.client.solrj.impl.CloudSolrServer"&gt; &lt;constructor-arg value="$&#123;solr.url.reindex&#125;" /&gt; &lt;property name="defaultCollection" value="product" /&gt; &lt;property name="zkClientTimeout" value="20000" /&gt; &lt;property name="zkConnectTimeout" value="1000" /&gt;&lt;/bean&gt;&lt;bean id="blSearchService" class="org.broadleafcommerce.core.search.service.solr.ExtSolrSearchServiceImpl"&gt; &lt;constructor-arg name="solrServer" ref="$&#123;solr.source&#125;" /&gt; &lt;constructor-arg name="reindexServer" ref="$&#123;solr.source.reindex&#125;"/&gt;&lt;/bean&gt; 注：上述配置中的defaultCollection的值product对应solr集群的collection名字，根据实际情况修改此处的值。 d) 在 core模块中core/src/main/resources/runtime-properties/common-shared.properties添加如下代码： 1234solr.url=192.168.56.121\:2181,192.168.56.122\:2181,1192.168.56.123\:2181solr.url.reindex=192.168.56.121\:2181,192.168.56.122\:2181,1192.168.56.123\:2181solr.source=solrServersolr.source.reindex=solrReindexServer 4、重写rebuildIndex方法在core模块的org.broadleafcommerce.core.search.service.solr包下添加LLSolrIndexServiceImpl，重写源码broadleaf-framework/SolrIndexServiceImpl中的rebuildIndex()方法，屏蔽如下代码： 1234567891011121314151617// // Swap the active and the reindex cores// shs.swapActiveCores();// // If we are not in single core mode, we delete the documents for the unused core after swapping// if (!SolrContext.isSingleCoreMode()) &#123;// deleteAllDocuments();// &#125;~~~ # 5、修改定时任务 web系统启动时候，会查询数据库中商品，然后重建索引。该功能在applicationContext.xml中已经定义了定时任务，修改rebuildIndexJobDetail中的targetObject，对应rebuildIndex所在的服务类，如下： ~~~xml&lt;bean id="rebuildIndexJobDetail" class="org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean"&gt; &lt;property name="targetObject" ref="llSolrIndexService" /&gt; &lt;property name="targetMethod" value="rebuildIndex" /&gt; &lt;/bean&gt; 如果需要手动创建索引，则需要取消applicationContext.xml中定义的定时任务，步骤如下： a）去掉如下代码： 12345678910111213141516&lt;bean id="rebuildIndexJobDetail" class="org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean"&gt; &lt;property name="targetObject" ref="blSearchService" /&gt; &lt;property name="targetMethod" value="rebuildIndex" /&gt;&lt;/bean&gt;&lt;bean id="rebuildIndexTrigger" class="org.springframework.scheduling.quartz.SimpleTriggerBean"&gt; &lt;property name="jobDetail" ref="rebuildIndexJobDetail" /&gt; &lt;property name="startDelay" value="$&#123;solr.index.start.delay&#125;" /&gt; &lt;property name="repeatInterval" value="$&#123;solr.index.repeat.interval&#125;" /&gt;&lt;/bean&gt;&lt;bean class="org.springframework.scheduling.quartz.SchedulerFactoryBean"&gt; &lt;property name="triggers"&gt; &lt;list&gt; &lt;ref bean="rebuildIndexTrigger" /&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; b）编写main方法，打成jar包，然后编写shell脚本，用于手动重建索引或者设置定时任务。该类需要获取一个名称为blSearchService的bean，然后调用该bean的rebuildIndex方法，主要代码如下： 12345@Resource(name = "blSearchService")private SearchService extSolrSe earchService;public void doRebuild()&#123; extSolrSearchService.rebuildIndex();&#125; 6、扩展CatalogService 添加如下代码： 12@Resource(name = "blSearchService")private ExtSolrSearchService extSolrSearchService; 修改该类的saveProduct方法如下： 123456789101112131415@Override@Transactional("blTransactionManager")public Product saveProduct(Product product) &#123; Product dbProduct = catalogService.saveProduct(product); try &#123; extSolrSearchService.addProductIndex(dbProduct); &#125; catch (ServiceException e) &#123; e.printStackTrace(); throw new RuntimeException(e); &#125; catch (IOException e) &#123; e.printStackTrace(); throw new RuntimeException(e); &#125; return dbProduct;&#125;]]></content>
      <categories>
        <category>search-engine</category>
      </categories>
      <tags>
        <tag>solr</tag>
        <tag>solrcloud</tag>
        <tag>broadleaf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Solr中使用中文分词]]></title>
    <url>%2F2014%2F03%2F14%2Fsplit-chinese-in-solr%2F</url>
    <content type="text"><![CDATA[使用全文检索，中文分词是离不开的，这里我采用的是 mmseg4j 分词器。mmseg4j分词器内置了对solr的支持，最新版本可支持4.X版本的sorl，使用起来很是方便。 下载mmseg4jGoogleCode地址：http://code.google.com/p/mmseg4j/ 请下载最新版本：mmseg4j-1.9.1，然后将mmseg4j-1.9.1/dist下的jar包拷贝至solr.war的lib目录，例如：apache-tomcat-6.0.36/webapps/solr/WEB-INF/lib/ 配置schema.xml使用mmseg4j中文分词器，首先需要在schema.xml文件中配置一个fieldType节点： 12345&lt;fieldType name="text_zh" class="solr.TextField" positionIncrementGap="100"&gt; &lt;analyzer&gt; &lt;tokenizer class="com.chenlb.mmseg4j.solr.MMSegTokenizerFactory" mode="complex" /&gt; &lt;/analyzer&gt;&lt;/fieldType&gt; 然后就可以在field节点中引用该filedType了，假设你有个字段叫content需要支持中文分词，则需要定义示例filed节点如下： 1&lt;field name="content" type="text_zh" indexed="true" stored="false" multiValued="true"/&gt; 接下来，重启solr服务器。 测试我这里使用的是broadleaf项目(broadleaf是什么，请参考：BroadLeaf项目搜索功能改进)中的schema.xml，需要修改成如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;schema name="example" version="1.5"&gt; &lt;fields&gt; &lt;field name="namespace" type="string" indexed="true" stored="true" /&gt; &lt;field name="id" type="string" indexed="true" stored="true" /&gt; &lt;field name="productId" type="long" indexed="true" stored="true" /&gt; &lt;field name="category" type="long" indexed="true" stored="true" multiValued="true" /&gt; &lt;field name="explicitCategory" type="long" indexed="true" stored="true" multiValued="true" /&gt; &lt;field name="searchable" type="text_zh" indexed="true" stored="false" /&gt; &lt;field name="_version_" type="long" indexed="true" stored="true" multiValued="false"/&gt; &lt;dynamicField name="*_searchable" type="text_zh" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_i" type="int" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_is" type="int" indexed="true" stored="true" multiValued="true" /&gt; &lt;dynamicField name="*_s" type="text_zh" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_ss" type="text_zh" indexed="true" stored="true" multiValued="true" /&gt; &lt;dynamicField name="*_l" type="long" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_ls" type="long" indexed="true" stored="true" multiValued="true" /&gt; &lt;dynamicField name="*_t" type="text_zh" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_txt" type="text_zh" indexed="true" stored="true" multiValued="true" /&gt; &lt;dynamicField name="*_b" type="boolean" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_bs" type="boolean" indexed="true" stored="true" multiValued="true" /&gt; &lt;dynamicField name="*_d" type="double" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_ds" type="double" indexed="true" stored="true" multiValued="true" /&gt; &lt;dynamicField name="*_p" type="double" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_dt" type="date" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_dts" type="date" indexed="true" stored="true" multiValued="true" /&gt; &lt;!-- some trie-coded dynamic fields for faster range queries --&gt; &lt;dynamicField name="*_ti" type="tint" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_tl" type="tlong" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_td" type="tdouble" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_tdt" type="tdate" indexed="true" stored="true" /&gt; &lt;/fields&gt; &lt;uniqueKey&gt;id&lt;/uniqueKey&gt; &lt;types&gt; &lt;fieldType name="text_zh" class="solr.TextField" positionIncrementGap="100"&gt; &lt;analyzer&gt; &lt;tokenizer class="com.chenlb.mmseg4j.solr.MMSegTokenizerFactory" mode="complex" /&gt; &lt;/analyzer&gt; &lt;/fieldType&gt; &lt;fieldType name="string" class="solr.StrField" sortMissingLast="true" /&gt; &lt;fieldType name="boolean" class="solr.BoolField" sortMissingLast="true" /&gt; &lt;fieldType name="int" class="solr.TrieIntField" precisionStep="0" positionIncrementGap="0" /&gt; &lt;fieldType name="long" class="solr.TrieLongField" precisionStep="0" positionIncrementGap="0" /&gt; &lt;fieldType name="double" class="solr.TrieDoubleField" precisionStep="0" positionIncrementGap="0" /&gt; &lt;fieldType name="tint" class="solr.TrieIntField" precisionStep="8" positionIncrementGap="0" /&gt; &lt;fieldType name="tlong" class="solr.TrieLongField" precisionStep="8" positionIncrementGap="0" /&gt; &lt;fieldType name="tdouble" class="solr.TrieDoubleField" precisionStep="8" positionIncrementGap="0" /&gt; &lt;fieldType name="date" class="solr.TrieDateField" precisionStep="0" positionIncrementGap="0" /&gt; &lt;fieldType name="tdate" class="solr.TrieDateField" precisionStep="6" positionIncrementGap="0" /&gt; &lt;fieldType name="text_general" class="solr.TextField" positionIncrementGap="100"&gt; &lt;analyzer type="index"&gt; &lt;tokenizer class="solr.StandardTokenizerFactory" /&gt; &lt;filter class="solr.LowerCaseFilterFactory" /&gt; &lt;/analyzer&gt; &lt;analyzer type="query"&gt; &lt;tokenizer class="solr.StandardTokenizerFactory" /&gt; &lt;filter class="solr.LowerCaseFilterFactory" /&gt; &lt;/analyzer&gt; &lt;/fieldType&gt; &lt;/types&gt;&lt;/schema&gt; 接下来，在浏览器中进行测试,输入下面url： 1http://192.168.56.123:8080/solr/primary_shard2_replica1/select?q=*%3A*&amp;wt=json&amp;indent=true&amp;rows=6&amp;start=0&amp;fq=category%3A2002&amp;fq=namespace%3Ad&amp;fq=%7B%21tag%3Da%7D%28en_US_name_s%3A大理%29 以上搜索的是category=2002,namespace=d，en_US_name_s=大理的记录，查询结果为： 123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;responseHeader&quot;:&#123; &quot;status&quot;:0, &quot;QTime&quot;:20&#125;, &quot;response&quot;:&#123;&quot;numFound&quot;:1,&quot;start&quot;:0,&quot;maxScore&quot;:1.0,&quot;docs&quot;:[ &#123; &quot;namespace&quot;:&quot;d&quot;, &quot;id&quot;:&quot;5&quot;, &quot;productId&quot;:5, &quot;explicitCategory&quot;:[2002], &quot;category_2002_sort_i&quot;:4, &quot;category&quot;:[2002, 1, 2], &quot;price_p&quot;:480.0, &quot;en_US_name_t&quot;:&quot;大理风情&quot;, &quot;en_name_t&quot;:&quot;大理风情&quot;, &quot;en_US_name_s&quot;:&quot;大理风情&quot;, &quot;en_name_s&quot;:&quot;大理风情&quot;, &quot;en_US_desc_t&quot;:&quot;体验不一样的风景&quot;, &quot;en_desc_t&quot;:&quot;体验不一样的风景&quot;, &quot;en_US_ldesc_t&quot;:&quot;大理风情养老基地坐落在美丽的洱海边，这里依山傍水，鲜花遍地，适合老年人居住、旅游。&quot;, &quot;en_ldesc_t&quot;:&quot;大理风情养老基地坐落在美丽的洱海边，这里依山傍水，鲜花遍地，适合老年人居住、旅游。&quot;, &quot;en_US_city_t&quot;:&quot;5329&quot;, &quot;en_city_t&quot;:&quot;5329&quot;, &quot;en_US_city_i&quot;:5329, &quot;en_city_i&quot;:5329, &quot;en_US_hotelType_t&quot;:&quot;A&quot;, &quot;en_hotelType_t&quot;:&quot;A&quot;, &quot;en_US_hotelType_s&quot;:&quot;A&quot;, &quot;en_hotelType_s&quot;:&quot;A&quot;, &quot;en_US_county_t&quot;:&quot;532901&quot;, &quot;en_county_t&quot;:&quot;532901&quot;, &quot;en_US_county_i&quot;:532901, &quot;en_county_i&quot;:532901, &quot;en_US_estatePrice_p&quot;:480.0, &quot;en_estatePrice_p&quot;:480.0, &quot;_version_&quot;:1462514915941023744&#125;] &#125;&#125; 通过查询结果，可以知道：只搜索”大理”关键字，可以查询出en_US_name_s为”大理风情”的记录。 参考文章 [1] Solr4.4的安装与配置]]></content>
      <categories>
        <category>search-engine</category>
      </categories>
      <tags>
        <tag>solr</tag>
        <tag>solrcloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache SolrCloud安装]]></title>
    <url>%2F2014%2F03%2F10%2Fhow-to-install-solrcloud%2F</url>
    <content type="text"><![CDATA[SolrCloud 通过 ZooKeeper 集群来进行协调，使一个索引进行分片，各个分片可以分布在不同的物理节点上，多个物理分片组成一个完成的索引 Collection。SolrCloud 自动支持 Solr Replication，可以同时对分片进行复制，冗余存储。下面，我们基于 Solr 最新的 4.4.0 版本进行安装配置 SolrCloud 集群。 1. 安装环境我使用的安装程序各版本如下： Solr： Apache Solr-4.4.0 Tomcat： Apache Tomcat 6.0.36 ZooKeeper： Apache ZooKeeper 3.4.5 各个目录说明： 所有的程序安装在 /opt 目录下，你可以依照你的实际情况下修改安装目录。 ZooKeeper的数据目录在： /data/zookeeper/data solrhome设置在： /usr/local/solrhome 2. 规划SolrCloud 单一SolrCloud数据集合： primary ZooKeeper集群： 3台 SolrCloud实例： 3节点 索引分片： 3 复制因子： 2 手动将3个索引分片(Shard)的复本(Replica)分布在3个 SolrCloud 节点上 三个节点： 192.168.56.121 192.168.56.122 192.168.56.123 3. 安装ZooKeeper集群由于需要用到 ZooKeeper，故我们先安装好 ZooKeeper 集群。 安装 ZooKeeper 集群之前，请确保每台机器上配置 /etc/hosts文件，使每个节点都能通过机器名访问。 首先，在第一个节点上将 zookeeper-3.4.5.tar.gz 解压到 /opt 目录： 1$ tar zxvf zookeeper-3.4.5.tar.gz -C /opt/ 创建 ZooKeeper 配置文件 zookeeper-3.4.5/conf/zoo.cfg，内容如下： 12345678tickTime=2000initLimit=10syncLimit=5dataDir=/data/zookeeper/dataclientPort=2181server.1=192.168.56.121:2888:3888server.2=192.168.56.122:2888:3888server.3=192.168.56.123:2888:3888 ZooKeeper 的数据目录指定在 /data/zookeeper/data ，你也可以使用其他目录，通过下面命令进行创建该目录： 1$ mkdir /data/zookeeper/data -p 然后，初始化 myid ，三个节点编号依次为 1，2，3 ，在其余节点上分别执行命令（注意修改编号）。 1$ echo "1" &gt;/data/zookeeper/data/myid 然后，在第二个和第三个节点上依次重复上面的操作。这样第一个节点中 myid 内容为1，第二个节点为2，第三个节点为3。 最后，启动 ZooKeeper 集群，在每个节点上分别启动 ZooKeeper 服务： 12$ cd /opt$ sh zookeeper-3.4.5/bin/zkServer.sh start 可以查看 ZooKeeper 集群的状态，保证集群启动没有问题： 1234[root@192.168.56.121 opt]# sh zookeeper-3.4.5/bin/zkServer.sh statusJMX enabled by defaultUsing config: /opt/zookeeper-3.4.5/bin/../conf/zoo.cfgMode: follower 4. 安装Solr你可以参考《Apache Solr介绍及安装》 简单来说，执行以下命令： 123456789$ unzip apache-tomcat-6.0.36.zip -d /opt$ unzip solr-4.4.0.zip -d /opt$ cd /opt$ chmod +x apache-tomcat-6.0.36/bin/*.sh$ cp solr-4.4.0/example/webapps/solr.war apache-tomcat-6.0.36/webapps/$ cp solr-4.4.0/example/lib/ext/* apache-tomcat-6.0.36/webapps/solr/WEB-INF/lib/$ cp solr-4.4.0/example/resources/log4j.properties apache-tomcat-6.0.36/lib/ 在其他节点上重复以上操作完成所有节点的 solr 的安装。 5. 设置 SolrCloud 配置文件1、 创建一个 SolrCloud 目录，并将 solr 的 lib 文件拷贝到这个目录： 12$ mkdir -p /usr/local/solrcloud/solr-lib/$ cp apache-tomcat-6.0.36/webapps/solr/WEB-INF/lib/* /usr/local/solrcloud/solr-lib/ 2、 通过 bootstrap 设置 solrhome ： 这里设置 solrhome 为 /usr/local/solrhome，创建该目录： 1mkdir -p /usr/local/solrhome 然后，运行下面命令将 solrhome 下面的配置上传到 zookeeper： 1$ java -classpath .:/usr/local/solrcloud/solr-lib/* org.apache.solr.cloud.ZkCLI -zkhost 192.168.56.121:2181,192.168.56.122:2181,192.168.56.123:2181 -cmd bootstrap -solrhome SolrCloud 集群的所有的配置存储在 ZooKeeper。 一旦 SolrCloud 节点启动时配置了 -Dbootstrap_confdir 参数， 该节点的配置信息将发送到 ZooKeeper 上存储。基它节点启动时会应用 ZooKeeper 上的配置信息，这样当我们改动配置时就不用一个个机子去更改了。 3、SolrCloud 是通过 ZooKeeper 集群来保证配置文件的变更及时同步到各个节点上，所以，需要将我们自己的配置文件（在 /usr/local/solrcloud/conf/primary/conf 目录下）上传到 ZooKeeper 集群中，配置名称设为 primaryconf： 1$ java -classpath .:/usr/local/solrcloud/solr-lib/* org.apache.solr.cloud.ZkCLI -zkhost 192.168.56.121:2181,192.168.56.122:2181,192.168.56.123:2181 -cmd upconfig -confdir /usr/local/solrcloud/conf/primary/conf -confname primaryconf 说明： zkhost 指定 ZooKeeper 地址，逗号分割 /usr/local/solrcloud/conf/ 目录下存在名称为 primary 的目录，该目录下的配置是后面需要用到的。 primaryconf 为在 ZooKeeper 上的配置文件名称。 /usr/local/solrcloud/conf 结构如下： 123456789$ tree /usr/local/solrcloud/conf/usr/local/solrcloud/conf├── primary│ └── conf│ ├── schema.xml│ └── solrconfig.xml└── solr.xml2 directories, 3 files schema.xml 内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;schema name="example" version="1.5"&gt; &lt;fields&gt; &lt;field name="namespace" type="string" indexed="true" stored="true" /&gt; &lt;field name="id" type="string" indexed="true" stored="true" /&gt; &lt;field name="productId" type="long" indexed="true" stored="true" /&gt; &lt;field name="category" type="long" indexed="true" stored="true" multiValued="true" /&gt; &lt;field name="explicitCategory" type="long" indexed="true" stored="true" multiValued="true" /&gt; &lt;field name="searchable" type="text_general" indexed="true" stored="false" /&gt; &lt;field name="_version_" type="long" indexed="true" stored="true" multiValued="false"/&gt; &lt;dynamicField name="*_searchable" type="text_general" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_i" type="int" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_is" type="int" indexed="true" stored="true" multiValued="true" /&gt; &lt;dynamicField name="*_s" type="string" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_ss" type="string" indexed="true" stored="true" multiValued="true" /&gt; &lt;dynamicField name="*_l" type="long" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_ls" type="long" indexed="true" stored="true" multiValued="true" /&gt; &lt;dynamicField name="*_t" type="text_general" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_txt" type="text_general" indexed="true" stored="true" multiValued="true" /&gt; &lt;dynamicField name="*_b" type="boolean" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_bs" type="boolean" indexed="true" stored="true" multiValued="true" /&gt; &lt;dynamicField name="*_d" type="double" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_ds" type="double" indexed="true" stored="true" multiValued="true" /&gt; &lt;dynamicField name="*_p" type="double" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_dt" type="date" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_dts" type="date" indexed="true" stored="true" multiValued="true" /&gt; &lt;!-- some trie-coded dynamic fields for faster range queries --&gt; &lt;dynamicField name="*_ti" type="tint" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_tl" type="tlong" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_td" type="tdouble" indexed="true" stored="true" /&gt; &lt;dynamicField name="*_tdt" type="tdate" indexed="true" stored="true" /&gt; &lt;/fields&gt; &lt;uniqueKey&gt;id&lt;/uniqueKey&gt; &lt;types&gt; &lt;fieldType name="string" class="solr.StrField" sortMissingLast="true" /&gt; &lt;fieldType name="boolean" class="solr.BoolField" sortMissingLast="true" /&gt; &lt;fieldType name="int" class="solr.TrieIntField" precisionStep="0" positionIncrementGap="0" /&gt; &lt;fieldType name="long" class="solr.TrieLongField" precisionStep="0" positionIncrementGap="0" /&gt; &lt;fieldType name="double" class="solr.TrieDoubleField" precisionStep="0" positionIncrementGap="0" /&gt; &lt;fieldType name="tint" class="solr.TrieIntField" precisionStep="8" positionIncrementGap="0" /&gt; &lt;fieldType name="tlong" class="solr.TrieLongField" precisionStep="8" positionIncrementGap="0" /&gt; &lt;fieldType name="tdouble" class="solr.TrieDoubleField" precisionStep="8" positionIncrementGap="0" /&gt; &lt;fieldType name="date" class="solr.TrieDateField" precisionStep="0" positionIncrementGap="0" /&gt; &lt;!-- A Trie based date field for faster date range queries and date faceting. --&gt; &lt;fieldType name="tdate" class="solr.TrieDateField" precisionStep="6" positionIncrementGap="0" /&gt; &lt;fieldType name="text_general" class="solr.TextField" positionIncrementGap="100"&gt; &lt;analyzer type="index"&gt; &lt;tokenizer class="solr.StandardTokenizerFactory" /&gt; &lt;filter class="solr.LowerCaseFilterFactory" /&gt; &lt;/analyzer&gt; &lt;analyzer type="query"&gt; &lt;tokenizer class="solr.StandardTokenizerFactory" /&gt; &lt;filter class="solr.LowerCaseFilterFactory" /&gt; &lt;/analyzer&gt; &lt;/fieldType&gt; &lt;/types&gt;&lt;/schema&gt; solrconfig.xml 内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;config&gt; &lt;luceneMatchVersion&gt;4.4&lt;/luceneMatchVersion&gt; &lt;directoryFactory name="DirectoryFactory" class="$&#123;solr.directoryFactory:solr.StandardDirectoryFactory&#125;"/&gt; &lt;schemaFactory class="ClassicIndexSchemaFactory"/&gt; &lt;updateHandler class="solr.DirectUpdateHandler2"&gt; &lt;updateLog&gt; &lt;str name="dir"&gt;$&#123;solr.data.dir:&#125;&lt;/str&gt; &lt;/updateLog&gt; &lt;/updateHandler&gt; &lt;requestHandler name="/get" class="solr.RealTimeGetHandler"&gt; &lt;lst name="defaults"&gt; &lt;str name="omitHeader"&gt;true&lt;/str&gt; &lt;/lst&gt; &lt;/requestHandler&gt; &lt;requestHandler name="/replication" class="solr.ReplicationHandler" startup="lazy" /&gt; &lt;requestDispatcher handleSelect="true" &gt; &lt;requestParsers enableRemoteStreaming="false" multipartUploadLimitInKB="2048" formdataUploadLimitInKB="2048" /&gt; &lt;httpCaching never304="true" /&gt; &lt;/requestDispatcher&gt; &lt;requestHandler name="standard" class="solr.StandardRequestHandler" default="true" /&gt; &lt;requestHandler name="/analysis/field" startup="lazy" class="solr.FieldAnalysisRequestHandler" /&gt; &lt;requestHandler name="/update" class="solr.UpdateRequestHandler" /&gt; &lt;requestHandler name="/update/json" class="solr.JsonUpdateRequestHandler" startup="lazy" /&gt; &lt;requestHandler name="/admin/" class="org.apache.solr.handler.admin.AdminHandlers" /&gt; &lt;requestHandler name="/admin/ping" class="solr.PingRequestHandler"&gt; &lt;lst name="invariants"&gt; &lt;str name="q"&gt;solrpingquery&lt;/str&gt; &lt;/lst&gt; &lt;lst name="defaults"&gt; &lt;str name="echoParams"&gt;all&lt;/str&gt; &lt;str name="df"&gt;id&lt;/str&gt; &lt;/lst&gt; &lt;/requestHandler&gt; &lt;queryResponseWriter name="json" class="solr.JSONResponseWriter"&gt; &lt;str name="content-type"&gt;text/plain; charset=UTF-8&lt;/str&gt; &lt;/queryResponseWriter&gt; &lt;updateRequestProcessorChain name="sample"&gt; &lt;processor class="solr.LogUpdateProcessorFactory" /&gt; &lt;processor class="solr.DistributedUpdateProcessorFactory"/&gt; &lt;processor class="solr.RunUpdateProcessorFactory" /&gt; &lt;/updateRequestProcessorChain&gt; &lt;!-- config for the admin interface --&gt; &lt;admin&gt; &lt;defaultQuery&gt;solr&lt;/defaultQuery&gt; &lt;pingQuery&gt;q=solr&amp;amp;version=2.0&amp;amp;start=0&amp;amp;rows=0&lt;/pingQuery&gt; &lt;healthcheck type="file"&gt;server-enabled&lt;/healthcheck&gt; &lt;/admin&gt;&lt;/config&gt; 4、创建 collection 并和配置文件关联： 1$ java -classpath .:/usr/local/solrcloud/solr-lib/* org.apache.solr.cloud.ZkCLI -zkhost 192.168.56.121:2181,192.168.56.122:2181,192.168.56.123:2181 -cmd linkconfig -collection primary -confname primaryconf 说明： 创建的 collection 叫做 primary，并指定和 primaryconf 连接 5、查看 ZooKeeper 上状态 在任意一个节点的 /opt 目录下执行如下命令： 12345678910$ zookeeper-3.4.5/bin/zkCli.sh [zk: localhost:2181(CONNECTED) 0] ls /[configs,zookeeper,clusterstate.json,aliases.json,live_nodes,overseer,collections,overseer_elect][zk: localhost:2181(CONNECTED) 1] ls /configs[primaryconf,][zk: localhost:2181(CONNECTED) 1] ls /collections[primary] 查看 /configs 和 /collections 目录均有值，说明配置文件已经上传到 ZooKeeper 上了，接下来启动 solr。 6. Tomcat 配置与启动1、修改每个节点上的 tomcat 配置文件，在环境变量中添加 zkHost 变量 编辑 apache-tomcat-6.0.36/bin/catalina.sh ，添加如下代码： 1JAVA_OPTS='-Djetty.port=8080 -Dsolr.solr.home=/usr/local/solrhome -DzkHost=192.168.56.122:2181,192.168.56.122:2181,192.168.56.123:2181' 在 /usr/local/solrhome/ 目录创建 solr.xml ： 1234&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;solr persistent="true" sharedLib="lib"&gt; &lt;cores adminPath="/admin/cores" zkClientTimeout="$&#123;zkClientTimeout:15000&#125;" hostPort="$&#123;jetty.port:8080&#125;" hostContext="$&#123;hostContext:solr&#125;"&gt;&lt;/cores&gt;&lt;/solr&gt; 说明： -Djetty.port：配置 solr 使用的端口，默认为 8983，这里我们使用的是 tomcat，端口为 8080 -Dsolr.solr.home：配置 solr/home -zkHost: 配置 zookeeper 集群地址，多个地址逗号分隔 最后，在 /opt 目录下启动 tomcat： 1$ sh apache-tomcat-6.0.36/bin/startup.sh 通过 http://192.168.56.121:8080/solr/ 进行访问，界面如图提示 There are no SolrCores running.，这是因为配置文件尚未配置 solrcore。 7. 创建 Collection、Shard 和 Replication手动创建 Collection 及初始 Shard直接通过 REST 接口来创建 Collection，你也可以通过浏览器访问下面地址，如下所示： 1$ curl 'http://192.168.56.121:8080/solr/admin/collections?action=CREATE&amp;name=primary&amp;numShards=3&amp;replicationFactor=1' 如果成功，会输出如下响应内容： 1234567891011121314151617181920212223242526272829303132&lt;response&gt;&lt;lst name="responseHeader"&gt; &lt;int name="status"&gt;0&lt;/int&gt; &lt;int name="QTime"&gt;2649&lt;/int&gt;&lt;/lst&gt;&lt;lst name="success"&gt; &lt;lst&gt; &lt;lst name="responseHeader"&gt; &lt;int name="status"&gt;0&lt;/int&gt; &lt;int name="QTime"&gt;2521&lt;/int&gt; &lt;/lst&gt; &lt;str name="core"&gt;primary_shard2_replica1&lt;/str&gt; &lt;str name="saved"&gt;/usr/local/solrhome/solr.xml&lt;/str&gt; &lt;/lst&gt; &lt;lst&gt; &lt;lst name="responseHeader"&gt; &lt;int name="status"&gt;0&lt;/int&gt; &lt;int name="QTime"&gt;2561&lt;/int&gt; &lt;/lst&gt; &lt;str name="core"&gt;primary_shard3_replica1&lt;/str&gt; &lt;str name="saved"&gt;/usr/local/solrhome/solr.xml&lt;/str&gt; &lt;/lst&gt; &lt;lst&gt; &lt;lst name="responseHeader"&gt; &lt;int name="status"&gt;0&lt;/int&gt; &lt;int name="QTime"&gt;2607&lt;/int&gt; &lt;/lst&gt; &lt;str name="core"&gt;primary_shard1_replica1&lt;/str&gt; &lt;str name="saved"&gt;/usr/local/solrhome/solr.xml&lt;/str&gt; &lt;/lst&gt;&lt;/lst&gt;&lt;/response&gt; 上面链接中的几个参数的含义，说明如下： name： 待创建Collection的名称 numShards： 分片的数量 replicationFactor： 复制副本的数量 可以通过 Web 管理页面，访问 http://192.168.56.121:8080/solr/#/~cloud，查看 SolrCloud 集群的分片信息，如图所示: 实际上，我们从192.168.56.121节点可以看到，SOLR 的配置文件内容，已经发生了变化，如下所示： 123456&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;solr persistent="true" sharedLib="lib"&gt; &lt;cores adminPath="/admin/cores" zkClientTimeout="20000" hostPort="$&#123;jetty.port:8080&#125;" hostContext="$&#123;hostContext:solr&#125;"&gt; &lt;core shard="shard2" instanceDir="primary_shard2_replica1/" name="primary_shard2_replica1" collection="primary"/&gt; &lt;/cores&gt;&lt;/solr&gt; 同时，你还可以看另外两个节点上的 solr.xml 文件的变化。 手动创建 Replication下面对已经创建的初始分片进行复制。 shard1 已经在 192.168.56.123 上，我们复制分片到 192.168.56.121 和 192.168.56.122 上，执行如下命令： 123$ curl 'http://192.168.56.121:8080/solr/admin/cores?action=CREATE&amp;collection=primary&amp;name=primary_shard1_replica_2&amp;shard=shard1'$ curl 'http://192.168.56.122:8080/solr/admin/cores?action=CREATE&amp;collection=primary&amp;name=primary_shard1_replica_3&amp;shard=shard1' 最后的结果是，192.168.56.123 上的 shard1，在 192.168.56.121 节点上有1个副本，名称为 primary_shard1_replica_2 ，在 192.168.56.122 节点上有一个副本，名称为 primary_shard1_replica_3 。也可以通过查看 192.168.56.121 和 192.168.56.122 上的目录变化，如下所示： 12345$ ll /usr/local/solrhome/total 16drwxr-xr-x 3 root root 4096 Mar 10 17:11 primary_shard1_replica2drwxr-xr-x 3 root root 4096 Mar 10 17:02 primary_shard2_replica1-rw-r--r-- 1 root root 444 Mar 10 17:16 solr.xml 你还可以对 shard2 和 shard3 添加副本。shard2 已经在 192.168.56.121 上，我们复制分片到 192.168.56.122 和 192.168.56.123 上，执行如下命令： 12$ curl 'http://192.168.56.122:8080/solr/admin/cores?action=CREATE&amp;collection=primary&amp;name=primary_shard2_replica_2&amp;shard=shard2'$ curl 'http://192.168.56.123:8080/solr/admin/cores?action=CREATE&amp;collection=primary&amp;name=primary_shard2_replica_3&amp;shard=shard2' shard3 已经在 192.168.56.122 上，我们复制分片到 192.168.56.121 和 192.168.56.123 上，执行如下命令： 12$ curl 'http://192.168.56.121:8080/solr/admin/cores?action=CREATE&amp;collection=primary&amp;name=primary_shard3_replica_2&amp;shard=shard3'$ curl 'http://192.168.56.123:8080/solr/admin/cores?action=CREATE&amp;collection=primary&amp;name=primary_shard3_replica_3&amp;shard=shard3' 我们再次从 192.168.56.121 节点可以看到，SOLR 的配置文件内容，又发生了变化，如下所示： 12345678&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;solr persistent="true" sharedLib="lib"&gt; &lt;cores adminPath="/admin/cores" zkClientTimeout="20000" hostPort="$&#123;jetty.port:8080&#125;" hostContext="$&#123;hostContext:solr&#125;"&gt; &lt;core shard="shard1" instanceDir="primary_shard1_replica2/" name="primary_shard1_replica_2" collection="primary"/&gt; &lt;core shard="shard2" instanceDir="primary_shard2_replica1/" name="primary_shard2_replica_1" collection="primary"/&gt; &lt;core shard="shard3" instanceDir="primary_shard2_replica2/" name="primary_shard2_replica_2" collection="primary"/&gt; &lt;/cores&gt;&lt;/solr&gt; 到此为止，我们已经基于3个节点，配置完成了 SolrCloud 集群。最后效果如下： 8. 其他说明8.1 SolrCloud 的一些必要配置schema.xml必须定义 _version_ 字段： 1&lt;field name="_version_" type="long" indexed="true" stored="true" multiValued="false"/&gt; solrconfig.xmlupdateHandler 节点下需要定义 updateLog： 12345678910&lt;!-- Enables a transaction log， currently used for real-time get. "dir" - the target directory for transaction logs， defaults to the solr data directory. --&gt;&lt;updateLog&gt; &lt;str name="dir"&gt;$&#123;solr.data.dir:&#125;&lt;/str&gt; &lt;!-- if you want to take control of the synchronization you may specify the syncLevel as one of the following where ''flush'' is the default. fsync will reduce throughput. &lt;str name="syncLevel"&gt;flush|fsync|none&lt;/str&gt; --&gt;&lt;/updateLog&gt; 需要定义一个 replication handler，名称为 /replication ： 1&lt;requestHandler name="/replication" class="solr.ReplicationHandler" startup="lazy" /&gt; 需要定义一个 realtime get handler，名称为/get: 12345&lt;requestHandler name="/get" class="solr.RealTimeGetHandler"&gt; &lt;lst name="defaults"&gt; &lt;str name="omitHeader"&gt;true&lt;/str&gt; &lt;/lst&gt; &lt;/requestHandler&gt; 需要定义 admin handlers： 1&lt;requestHandler name=&quot;/admin/&quot; class=&quot;solr.admin.AdminHandlers&quot; /&gt; 需要定义 updateRequestProcessorChain： 12345&lt;updateRequestProcessorChain name="sample"&gt; &lt;processor class="solr.LogUpdateProcessorFactory" /&gt; &lt;processor class="solr.DistributedUpdateProcessorFactory"/&gt; &lt;processor class="solr.RunUpdateProcessorFactory" /&gt; &lt;/updateRequestProcessorChain&gt; solr.xmlcores 节点需要定义 adminPath 属性： 1&lt;cores adminPath="/admin/cores" &gt; 8.2 SolrCloud 分布式检索时忽略宕机的 Shard1234&lt;lst name=”error”&gt; &lt;str name=”msg”&gt;no servers hosting shard:&lt;/str&gt; &lt;int name=”code”&gt;503&lt;/int&gt;&lt;/lst&gt; 加入下面参数，只从存活的 shards 获取数据： 1shards.tolerant=true 如：http://192.168.56.121:8080/solr/primary_shard2_replica1/select?q=*%3A*&amp;wt=xml&amp;indent=true&amp;shards.tolerant=true 没有打此参数，如果集群内有挂掉的 shard，将显示： 1no servers hosting shard 8.3 自动创建 Collection 及初始 Shard自动创建 Collection 及初始 Shard，不需要通过 zookeeper 手动上传配置文件并关联 collection。 1、在第一个节点修改 tomcat 启动参数 1JAVA_OPTS=&apos;-Djetty.port=8080 -Dsolr.solr.home=/usr/local/solrhome -DzkHost=192.168.56.122:2181,192.168.56.122:2181,192.168.56.123:2181 -DnumShards=3 -Dbootstrap_confdir=/usr/local/solrhome/primary/conf -Dcollection.configName=primaryconf &apos; 然后启动 tomcat。这个步骤上传了集群的相关配置信息(/usr/local/solrhome/primary/conf)到 ZooKeeper 中去，所以启动下一个节点时不用再指定配置文件了。 2、在第二个和第三个节点修改 tomcat 启动参数 1JAVA_OPTS=&apos;-Djetty.port=8080 -Dsolr.solr.home=/usr/local/solrhome -DzkHost=192.168.56.122:2181,192.168.56.122:2181,192.168.56.123:2181 -DnumShards=3&apos; 然后启动 tomcat。 这样就会创建3个 shard 分别分布在三个节点上，如果你在增加一个节点，这节点会附加到一个 shard 上成为一个 replica，而不会创建新的 shard。 9. 总结本文记录了如何 zookeeper、SolrCloud 的安装和配置过程，solrcore 是通过 restapi 进行手动创建，然后又对自动创建 Collection 及初始 Shard 进行了说明。 10. 参考文章 [1] SolrCloud 4.3.1+Tomcat 7安装配置实践 [2] SolrCloud Wiki [3] SolrCloud使用教程、原理介绍]]></content>
      <categories>
        <category>search-engine</category>
      </categories>
      <tags>
        <tag>solr</tag>
        <tag>solrcloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase源码：HMaster启动过程]]></title>
    <url>%2F2014%2F03%2F09%2Fhbase-note-about-hmaster-startup%2F</url>
    <content type="text"><![CDATA[版本：HBase 0.94.15-cdh4.7.0 调试HMaster 说明： 这部分参考和使用了https://github.com/codefollower/HBase-Research上的代码（注意：原仓库已经被作者删除了），包括该作者自己写的一些测试类和文档。 首先，在IDE里启动HMaster和HRegionServer： 运行/hbase/src/test/java/my/test/start/HMasterStarter.java，当看到提示Waiting for region servers count to settle时，再打开同目录中的HRegionServerStarter，统一运行该类。 此时会有两个Console，在HMasterStarter这个Console最后出现Master has completed initialization，这样的信息时就表示它启动成功了，而HRegionServerStarter这个Console最后出现Done with post open deploy task这样的信息时说明它启动成功了。 main方法运行HMasterStarter类启动HMaster： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package my.test.start;import java.io.File;import my.test.TestBase;import org.apache.hadoop.hbase.HConstants;import org.apache.hadoop.hbase.master.HMaster;import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;public class HMasterStarter &#123; public static void deleteRecursive(File[] files) &#123; if (files == null) return; for (File f : files) &#123; if (f.isDirectory()) &#123; deleteRecursive(f.listFiles()); &#125; f.delete(); &#125; &#125; public static void main(String[] args) throws Exception &#123; File f = TestBase.getTestDir(); //删除临时测试目录 deleteRecursive(f.listFiles()); new ZookeeperThread().start(); Thread.sleep(1000); HMaster.main(new String[] &#123; "start" &#125;); &#125; public static class ZookeeperThread extends Thread &#123; public void run() &#123; MiniZooKeeperCluster zooKeeperCluster = new MiniZooKeeperCluster(); File zkDataPath = new File(TestBase.sharedConf.get(HConstants.ZOOKEEPER_DATA_DIR)); int zkClientPort = TestBase.sharedConf.getInt(HConstants.ZOOKEEPER_CLIENT_PORT, 2181); zooKeeperCluster.setDefaultClientPort(zkClientPort); try &#123; zooKeeperCluster.startup(zkDataPath); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; HMaster的入口是main方法，main方法需要传递一个参数，start或者stop。 main方法内首先打印hbase版本信息，然后在调用HMasterCommandLine的doMain方法。HMasterCommandLine继承自ServerCommandLine类并且ServerCommandLine类实现了Tool接口。 1234567public void doMain(String args[]) throws Exception &#123; int ret = ToolRunner.run( HBaseConfiguration.create(), this, args); if (ret != 0) &#123; System.exit(ret); &#125; &#125; doMain方法内会调用ToolRunner的run方法，查看ToolRunner类可以知道，实际上最后会调用HMasterCommandLine的run方法。 接下来会解析参数，根据参数值判断是执行startMaster方法还是stopMaster方法。 startMaster方法中分两种情况：本地模式和分布式模式。如果是分布式模式，通过反射调用HMaster的构造方法，并调用其start和join方法。 HMaster继承自HasThread类，而HasThread类实现了Runnable接口，故HMaster也是一个线程。 HMaster类图HMaster类继承关系如下图： HMaster的构造方法1、构造方法总体过程 创建Configuration并设置和获取一些参数。包括： 在master上禁止block cache 设置服务端重试次数 获取主机名称和master绑定的ip和端口号，端口号默认为60000 设置regionserver的coprocessorhandler线程数为0 创建rpcServer（见下文分析） 初始化serverName，其值为：192.168.1.129,60000,1404117936154 zk授权登录和hbase授权 设置当前线程名称：master + &quot;-&quot; + this.serverName.toString() 判断是否开启复制：Replication.decorateMasterConfiguration(this.conf); 设置mapred.task.id，如果其为空，则其值为：&quot;hb_m_&quot; + this.serverName.toString() 创建ZooKeeperWatcher监听器（见下文分析），并在zookeeper上创建一些节点 启动rpcServer中的线程 创建一个MasterMetrics 判断是否进行健康检测：HealthCheckChore 另外还初始化两个参数：shouldSplitMetaSeparately、waitingOnLogSplitting 涉及到的参数有： 1234567891011121314151617hfile.block.cache.sizehbase.master.dns.interfacehbase.master.dns.nameserverhbase.master.porthbase.master.ipc.addresshbase.master.handler.counthbase.regionserver.handler.counthbase.master.buffer.for.rs.fatalshbase.zookeeper.client.keytab.filehbase.zookeeper.client.kerberos.principalhbase.master.keytab.filehbase.master.kerberos.principalhbase.master.logcleaner.pluginsmapred.task.idhbase.node.health.script.frequencyhbase.regionserver.separate.hlog.for.metahbase.master.wait.for.log.splitting 2、创建rpcServer并启动其中的线程： 这部分涉及到RPC的使用，包括的知识点有动态代理、Java NIO等。 通过反射创建RpcEngine的实现类，实现类可以在配置文件中配置（hbase.rpc.engine），默认实现为WritableRpcEngine。调用getServer方法，其实也就是new一个HBaseServer类。 构造方法中： 启动一个Listener线程，功能是监听client的请求，将请求放入nio请求队列，逻辑如下： –&gt;创建n个selector，和一个n个线程的readpool，n由ipc.server.read.threadpool.size决定，默认为10 –&gt;读取每个请求的头和内容，将内容放入priorityQueue中 启动一个Responder线程，功能是将响应队列里的数据写给各个client的connection通道，逻辑如下： –&gt;创建nio selector –&gt;默认超时时间为15 mins –&gt;依次将responseQueue中的内容写回各通道，并关闭连接，buffer=8k –&gt;如果该请求的返回没有写完，则放回队列头，推迟再发送 –&gt;对于超时未完成的响应，丢弃并关闭相应连接 启动N（n默认为10）个Handler线程，功能是处理请求队列，并将结果写到响应队列 –&gt;读取priorityQueue中的call，调用对应的call方法获得value，写回out并调用doRespond方法，处理该请求，并唤醒writable selector –&gt;启动M(m默认为0)个Handler线程以处理priority 3、创建ZooKeeperWatcher 构造函数中生成如下持久节点： 123456789101112/hbase/hbase/root-region-server/hbase/rs/table/draining/hbase/master/hbase/backup-masters/hbase/shutdown/hbase/unassigned/hbase/table94/hbase/table/hbase/hbaseid/hbase/splitlog run方法接下来看HMaster的run方法做了哪些事情。 1、总体过程 创建MonitoredTask，并把HMaster的状态设置为Master startup 启动info server，即Jetty服务器，端口默认为60010，其对外提供两个接口：/master-status和/dump 调用becomeActiveMaster方法（见下文分析），阻塞等待直至当前master成为active master 当成为了master之后并且当前master进程正在运行，则调用finishInitialization方法（见下文分析），并且调用loop方法循环等待，一直到stop发生 当HMaster停止运行时候，会做以下事情： 清理startupStatus 停止balancerChore和catalogJanitorChore 让RegionServers shutdown 停止服务线程：rpcServer、logCleaner、hfileCleaner、infoServer、executorService、healthCheckChore 停止以下线程：activeMasterManager、catalogTracker、serverManager、assignmentManager、fileSystemManager、snapshotManager、zooKeeper 2、becomeActiveMaster方法： 创建ActiveMasterManager ZooKeeperWatcher注册activeMasterManager监听器 调用stallIfBackupMaster： –&gt;先检查配置项 “hbase.master.backup”，自己是否backup机器，如果是则直接block直至检查到系统中的active master挂掉（zookeeper.session.timeout，默认每3分钟检查一次） 创建clusterStatusTracker并启动 调用activeMasterManager的blockUntilBecomingActiveMaster方法。 创建短暂的”/hbase/master”，此节点值为version+ServerName，如果创建成功，则删除备份节点；否则，创建备份节点 获得”/hbase/master”节点上的数据，如果不为null，则获得ServerName，并判断是否是在当前节点上创建了”/hbase/master”，如果是则删除该节点，这是因为该节点已经是备份节点了。 3、finishInitialization方法： 创建MasterFileSystem对象，封装了master常用的一些文件系统操作，包括splitlog file、删除region目录、删除table目录、删除cf目录、检查文件系统状态等. 创建FSTableDescriptors对象 设置集群id 如果不是备份master： 创建ExecutorService，维护一个ExecutorMap,一种Event对应一个Executor(线程池).可以提交EventHandler来执行异步事件； 创建serverManager，管理regionserver信息,维护着onlineregion server 和deadregion server列表，处理regionserver的startups、shutdowns、 deaths，同时也维护着每个regionserver rpc stub. 调用initializeZKBasedSystemTrackers，初始化zk文件系统 创建CatalogTracker, 它包含RootRegionTracker和MetaNodeTracker，对应”/hbase/root-region-server”和/“hbase/unassigned/1028785192”这两个结点(1028785192是.META.的分区名)。如果之前从未启动过hbase，那么在start CatalogTracker时这两个结点不存在。”/hbase/root-region-server”是一个持久结点，在RootLocationEditor中建立 创建 LoadBalancer，负责region在regionserver之间的移动，关于balancer的策略，可以通过hbase.regions.slop来设置load区间 创建 AssignmentManager，负责管理和分配region，同时它也会接受zk上关于region的event，根据event来完成region的上下线、关闭打开等工作。 创建 RegionServerTracker: 监控”/hbase/rs”结点，通过ZK的Event来跟踪onlineregion servers， 如果有rs下线，删除ServerManager中对应的onlineregions. 创建 DrainingServerTracker: 监控”/hbase/draining”结点 创建 ClusterStatusTracker，监控”/hbase/shutdown”结点维护集群状态 创建SnapshotManager 如果不是备份master，初始化MasterCoprocessorHost并执行startServiceThreads()。说明：info server的启动移到构造函数了去了，这样可以早点通过Jetty服务器查看HMaster启动状态。 创建一些executorService 创建logCleaner并启动 创建hfileCleaner并启动 启动healthCheckChore 打开rpcServer 等待RegionServer注册。满足以下这些条件后返回当前所有region server上的region数后继续： a 至少等待4.5s，”hbase.master.wait.on.regionservers.timeout” b 成功启动regionserver节点数&gt;=1，”hbase.master.wait.on.regionservers.mintostart” c 1.5s内没有regionsever死掉或重新启动，hbase.master.wait.on.regionservers.interval) serverManager注册新的在线region server 如果不是备份master，启动assignmentManager 获取下线的Region server，然后拆分HLog –&gt;依次检查每一个hlog目录，查看它所属的region server是否online，如果是则不需要做任何动作，region server自己会恢复数据，如果不是，则需要将它分配给其它的region server –&gt;split是加锁操作: –&gt; 创建一个新的hlogsplitter,遍历每一个server目录下的所有hlog文件，依次做如下操作。（如果遇到文件损坏等无法跳过的错误，配 置 hbase.hlog.split.skip.errors=true 以忽略之） –&gt;启动hbase.regionserver.hlog.splitlog.writer.threads（默认为3）个线程，共使用128MB内存，启动这些写线程 –&gt;先通过lease机制检查文件是否能够append，如果不能则死循环等待 –&gt;把hlog中的内容全部加载到内存中（内存同时被几个写线程消费)） –&gt;把有损坏并且跳过的文件移到/hbase/.corrupt/目录中 –&gt; 把其余己经处理过的文件移到/hbase/.oldlogs中，然后删除原有的server目录 –&gt; 等待写线程结束，返回新写的所有路径 –&gt;解锁 写线程逻辑： –&gt;从内存中读出每一行数据的key和value，然后查询相应的region路径。如果该region路径不存在，说明该region很可能己经被split了，则不处理这部分数据,因为此时忽略它们是安全的。 –&gt;如果上一步能查到相应的路径，则到对应路径下创建”recovered.edits”文件夹(如果该文件夹存在则删除后覆盖之)，然后将数据写入该文件夹 调用assignRoot方法，检查是否分配了-ROOT-表，如果没有，则通过assignmentManager.assignRoot()来分配root表，并激活该表 运行this.serverManager.enableSSHForRoot()方法 拆分.META. server上的HLog 分配.META.表 enableServerShutdownHandler 处理dead的server assignmentManager.joinCluster(); 设置balancer fixupDaughters(status) 如果不是备份master 启动balancerChore线程，运行LoadBalancer 启动startCatalogJanitorChore，周期性扫描.META.表上未使用的region并回收 registerMBean serverManager.clearDeadServersWithSameHostNameAndPortOfOnlineServer，清理dead的server 如果不是备份master，cpHost.postStartMaster MasterFileSystem构造方法在HMaster.finishInitialization方法中触发了MasterFileSystem的构造方法，该类在HMaster类中会被以下类使用： LogCleaner HFileCleaner 另外该类可以完成拆分log的工作： 1234567/** * Override to change master's splitLogAfterStartup. Used testing * @param mfs */protected void splitLogAfterStartup(final MasterFileSystem mfs)&#123; mfs.splitLogAfterStartup();&#125; 这里主要是关心创建了哪些目录，其他用途暂不分析。 1、接下来，看其构造方法运行过程： 获取rootdir：由参数hbase.rootdir配置 获取tempdir：${hbase.rootdir}/.tmp 获取文件系统的uri，并设置到fs.default.name和fs.defaultFS 判断是否进行分布式文件拆分，参数：hbase.master.distributed.log.splitting，如果需要，则创建SplitLogManager 创建oldLogDir，调用createInitialFileSystemLayout方法 checkRootDir 等待fs退出安全模式(默认10秒钟轮循一次，可通过参数hbase.server.thread.wakefrequency调整 如果hbase.rootdir目录不存在则创建它，然后在此目录中创建名为”hbase.version”的文件，内容是文件系统版本号，当前为7；如果hbase.rootdir目录已存在，则读出”hbase.version”文件的内容与当前的版本号相比，如果不相等，则打印错误信息(提示版本不对)，抛出异常FileSystemVersionException 检查${hbase.rootdir}目录下是否有名为”hbase.id”的文件，如果没有则创建它，内容是随机生成的UUID(总长度36位，由5部份组成，用”-“分隔)，如：6c43f934-37a2-4cae-9d49-3f5abfdc113d 读出”hbase.id”的文件的内容存到clusterId字段 判断hbase.rootdir目录中是否有”-ROOT-/70236052”目录，没有的话说明是第一次启动hbase，进入bootstrap方法 createRootTableInfo 建立”-ROOT-“表的描述文件，判断hbase.rootdir/-ROOT-目录中是否存在tableinfo开头的文件，另外还创建了.tmp目录 checkTempDir 如果oldLogDir（${hbase.rootdir}/.oldlogs）不存在，则创建 2、bootstrap方法运行过程： 调用HRegion.createHRegion建立”-ROOT-“分区和”.META.”分区 把”.META.”分区信息加到”-ROOT-“表，并关闭分区和hlog 总结经过上面分析之后，来看看zookeeper创建的一些目录分布式由哪个类来监控的： /hbase /hbase/root-region-server：RootRegionTracker，监控root所在的regionserver /hbase/rs：RegionServerTracker，监控regionserver的上线和下线 /table/draining：DrainingServerTracker，监听regionserver列表的变化 /hbase/master：在HMaster中建立，并且是一个短暂结点，结点的值是HMaster的ServerName：hostname,port,当前毫秒 /hbase/backup-masters /hbase/shutdown：ClusterStatusTracker，当HMaster启动之后，会将当前时间（Bytes.toBytes(new java.util.Date().toString())）存到该节点 /hbase/unassigned：MetaNodeTracker /hbase/table94 /hbase/table /hbase/hbaseid：在HMaster.finishInitialization方法中调用ClusterId.setClusterId建立，结点值是UUID /hbase/splitlog 在HMaster启动之后，${hbase.rootdir}目录如下: 1234567891011121314151617181920212223242526272829.├── -ROOT- //&quot;-ROOT-&quot;表名│ ├── ..tableinfo.0000000001.crc //crc校验文件│ ├── .tableinfo.0000000001│ ├── .tmp│ └── 70236052 //&quot;-ROOT-&quot;分区名│ ├── ..regioninfo.crc│ ├── .oldlogs //存放hlog文件│ │ ├── .hlog.1402551641526.crc│ │ └── hlog.1402551641526│ ├── .regioninfo //&quot;-ROOT-&quot;分区描述表件│ ├── .tmp│ └── info //列族名│ ├── .5037e69a0c244bd78945aaa333d0230a.crc│ └── 5037e69a0c244bd78945aaa333d0230a //存放&quot;.META.&quot;分区信息的StoreFile├── .META.│ └── 1028785192│ ├── ..regioninfo.crc│ ├── .oldlogs│ │ ├── .hlog.1402551641701.crc│ │ └── hlog.1402551641701│ ├── .regioninfo│ └── info├── .hbase.id.crc├── .hbase.version.crc├── .oldlogs├── .tmp├── hbase.id //集群uuid└── hbase.version //hbase版本 简单总结一下HMaster启动过程做了哪些事情： 创建rpcServer，及HBaseServer 创建ZooKeeperWatcher监听器 阻塞等待成为activeMaster 创建master的一些文件目录 初始化一些基于zk的跟踪器 创建LoadBalancer 创建SnapshotManager 如果不是备份master 创建logCleaner并启动 创建hfileCleaner并启动 创建jetty的infoServer并启动 启动健康检查 打开rpcServer 等待RegionServer注册 从hlog中恢复数据 分配root和meta表 分配region 运行负载均衡线程 周期性扫描.META.表上未使用的region并回收]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase源码：HRegionServer启动过程]]></title>
    <url>%2F2014%2F03%2F09%2Fhbase-note-about-hregionserver-startup%2F</url>
    <content type="text"><![CDATA[版本：HBase 0.94.15-cdh4.7.0 关于HMaster启动过程，请参考HBase源码：HMaster启动过程。先启动了HMaster之后，再启动HRegionServer。 运行HRegionServerStarter类启动HRegionServer： 12345678910111213package my.test.start;import org.apache.hadoop.hbase.regionserver.HRegionServer;public class HRegionServerStarter &#123; public static void main(String[] args) throws Exception &#123; //new HMasterStarter.ZookeeperThread().start(); HRegionServer.main(new String[] &#123; "start" &#125;); &#125;&#125; 同样参考HBase源码：HMaster启动过程，运行HRegionServer.main方法，会通过反射创建一个HRegionServer实例，然后调用其run方法。 HRegionServer类继承关系如下： 构造方法主要包括： 设置服务端HConnection重试次数 检查压缩编码，通过hbase.regionserver.codecs可以配置编码类，一一检测，判断是否支持其压缩算法。 获取useHBaseChecksum值，是否开启hbase checksum校验 获取hbase.regionserver.separate.hlog.for.meta参数值 获取客户端重复次数 获取threadWakeFrequency值 获取hbase.regionserver.msginterval值 创建Sleeper对象，用于周期性休眠线程 获取最大扫描结果集大小，hbase.client.scanner.max.result.size，默认无穷大 获取hbase.regionserver.numregionstoreport值 获取rpctimeout值，hbase.rpc.timeout，默认60000 获取主机名和绑定的ip和端口，端口默认为60020 创建rpcServer zk授权登录和hbase授权 创建RegionServerAccounting 创建CacheConfig run方法 preRegistrationInitialization initializeZooKeeper，此方法不会创建任何节点- 创建ZooKeeperWatcher - 创建MasterAddressTracker 并等到&quot;/hbase/master&quot;节点有数据为止 - 创建ClusterStatusTracker 并等到&quot;/hbase/shutdown&quot;节点有数据为止 - 创建CatalogTracker 不做任何等待 - 创建RegionServerSnapshotManager 设置集群id 初始化线程：initializeThreads 创建 cacheFlusher 创建 compactSplitThread 创建 compactionChecker 创建 periodicFlusher 创建 healthCheckChore 创建 Leases 判断是否启动 HRegionThriftServer 参数hbase.regionserver.nbreservationblocks默认为4，默认会预留20M(每个5M,20M = 4*5M)的内存防止OOM 初始化rpcEngine = HBaseRPC.getProtocolEngine(conf) reportForDuty，轮询，向汇报master自己已经启动 getMaster()，取出”/hbase/master”节点中的数据，构造一个master的ServerName，然后基于此生成一个HMasterRegionInterface接口的代理，此代理用于调用master的方法 regionServerStartup 当轮询结果不为空时，调用handleReportForDutyResponse- regionServerStartup会返回来一个MapWritable，这个MapWritable有三个值，这三个key的值会覆盖rs原有的conf: - &quot;hbase.regionserver.hostname.seen.by.master&quot; = master为rs重新定义的hostname(通常跟rs的InetSocketAddress.getHostName一样)rs会用它重新得到serverNameFromMasterPOV - &quot;fs.default.name&quot; = &quot;file:///&quot; - &quot;hbase.rootdir&quot; = &quot;file:///E:/hbase/tmp&quot; - 查看conf中是否有&quot;mapred.task.id&quot;，没有就自动设一个(格式: &quot;hb_rs_&quot;+serverNameFromMasterPOV)，例如: hb_rs_localhost,60050,1323525314060 - createMyEphemeralNode：在zk中建立 短暂节点&quot;/hbase/rs/localhost,60050,1323525314060&quot;，也就是把当前rs的serverNameFromMasterPOV(为null的话用rs的InetSocketAddress、port、startcode构建新的ServerName)放到/hbase/rs节点下，&quot;/hbase/rs/localhost,60050,1323525314060&quot;节点没有数据 - 设置fs.defaultFS值为hbase.rootdir的值 - 生成一个只读的FSTableDescriptors - 调用setupWALAndReplication - 初始化 hlog、metrics、dynamicMetrics、rsHost - 调用startServiceThreads启动服务线程 - 启动一些ExecutorService - 启动hlogRoller - 启动cacheFlusher - 启动compactionChecker - 启动healthCheckChore - 启动periodicFlusher - leases.start() - 启动jetty的infoServer，默认端口为60030 - 启动复制相关打的一些handler：replicationSourceHandler、replicationSourceHandler、replicationSinkHandler - rpcServer启动 - 创建并启动SplitLogWorker registerMBean snapshotManager启动快照服务 在master上注册之后，进入运行模式，周期性(msgInterval默认3妙)调用doMetrics，tryRegionServerReport isHealthy健康检查，只要Leases、MemStoreFlusher、LogRoller、periodicFlusher、CompactionChecker有一个线程退出，rs就停止 doMetrics tryRegionServerReport向master汇报rs的负载HServerLoad shutdown之后的一些操作 unregisterMBean 停掉thriftServer、leases、rpcServer、splitLogWorker、infoServer、cacheConfig 中断一些线程：cacheFlusher、compactSplitThread、hlogRoller、metaHLogRoller、compactionChecker、healthCheckChore 停掉napshotManager 停掉 catalogTracker、compactSplitThread 等待所有region关闭 关闭wal 删除zk上的一些临时节点，zooKeeper关闭 总结一下，HRegionServer主要干以下事情： 在zk上注册自己，表明自己上线了 跟master汇报 设置wal和复制 注册协作器RegionServerCoprocessorHost 启动hlogRoller 定期刷新memstore 定期检测是否需要压缩合并 启动租约 启动jetty的infoserver 创建SplitLogWorker，用于拆分HLog 快照管理 总结HRegionServer类中创建了一些对象： HBaseServer：处理客户端请求 Leases：租约 InfoServer：Jetty服务器 RegionServerMetrics： RegionServerDynamicMetrics： CompactSplitThread：合并文件线程 MemStoreFlusher：刷新memstore线程 两个Chore：compactionChecker、periodicFlusher 两个LogRoller：hlogRoller、metaHLogRoller MasterAddressTracker：跟踪master地址 CatalogTracker：跟踪-ROOT-和.META.表 ClusterStatusTracker：跟踪集群状态 SplitLogWorker：拆分log Sleeper： ExecutorService： ReplicationSourceService和ReplicationSinkService：复制服务 RegionServerAccounting： CacheConfig：缓存配置和block RegionServerCoprocessorHost：RegionServer协作器 HealthCheckChore：健康检查]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2013年度年终总结]]></title>
    <url>%2F2014%2F03%2F06%2Fsummary-of-the-work-in-2013%2F</url>
    <content type="text"><![CDATA[回首2011年和2012年的年终总结，发现公司在2012年提到的一些不足仍然出现在2013年，不知道每年的总结是否有被认真阅读过、重视过。故虽谏且议，使人不得而知焉。 2013年，通过了RHCE考试，掌握了shell编程，初识Python； 2013年，不再负责、管理具体的项目，可还不是逃不过事后填坑的无奈； 2013年，Cassandra不再，迎来Hadoop，满腔热血的学习Hadoop的安装、部署、原理、开发甚至还做了一些入门普及培训，但真的只是一个人在战斗； 2013年，开始是一个人带着几个同事在探索和研究hadoop，慢慢地失去了自己的自主权，更多的时间是被花在了具体的项目上，自己的工作全是被计划安排着； 2013年，开始做EAC，界面原型、客户需求、技术选型，花费了大量的时间去讨论、修改、论证，却错过了快速迭代出一个产品的最佳时机；有时候不是我们脚步太慢，而是我们想的太复杂； 2013年，开发BMP，架构文档不断地被否定，被几个人反复修改，最后也还是没有捣鼓出一个无懈可击的文档出来；项目经理职责不明确，没有发挥应有的协调作用； 2013年，做了一些测试的工作，从测试方法、过程和结果上来看，在测试方面还是不够专业。 关于招聘。来来往往，来了不少人，也走了不少人，基本上没有来了后花些时间就能自己独立干活的。招三个人干四个人的事情发五个人的工资，想做到这个，不做一些改变，那只能是一厢情愿的事情了。 关于团队建设。新人入职，无人指导，想仅靠wiki上的文章就能无师自通，这可是没那么容易的吧？跟优秀的人一起工作，才能变得更优秀。员工内部的分享与交流，至今都未形成；员工的归属感和存在感，不知道又遗失了多少。公司不可能只靠几个人单打独斗，而需要大家一起齐心协力同奋进。 关于管理。项目的进度和质量管控不严，表现在任务的时间分配不合理，任务完成进度没跟踪和把控，代码质量和应用健壮性没审查和测试，代码开发不规范等等。另外，管理者的角色不明确以及执行力不够。 还有其他的一些问题，所有这些问题主要都是不够【职业化】的问题。公司未来，应该朝职业化发展。 博学而笃志，切问而近思，仁在其中矣。吾尝终日不食，终夜不寝，以思，无益，不如学也。每次都是年终的时候才总结一年的得失成败，费尽脑汁挤出的几条总结，能有多大的时效性、客观性和借鉴性呢？ 一样的月光，一样的照着新店溪；一样的冬天，一样的下着冰冷的雨；一样的尘埃，一样的在风中堆积，一样的笑容，一样的泪水，一样的日子，一样的我和你，什么时候蓝天白云都成了记忆，什么时候梦想变得如此的拥挤。谁能告诉我，是我们改变了公司，还是公司改变了我和你。 2014年，等风来。]]></content>
      <categories>
        <category>work</category>
      </categories>
      <tags>
        <tag>work</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Solr查询语法]]></title>
    <url>%2F2014%2F03%2F03%2Fsolr-query-syntax%2F</url>
    <content type="text"><![CDATA[查询参数常用： q - 查询字符串，必须的。 fl - 指定返回那些字段内容，用逗号或空格分隔多个。 start - 返回第一条记录在完整找到结果中的偏移位置，0开始，一般分页用。 rows - 指定返回结果最多有多少条记录，配合start来实现分页。 sort - 排序，格式：sort=&lt;field name&gt;+&lt;desc|asc&gt;[,&lt;field name&gt;+&lt;desc|asc&gt;]。示例：（inStock desc, price asc）表示先 “inStock” 降序, 再 “price” 升序，默认是相关性降序。 wt - (writer type)指定输出格式，可以有 xml, json, php, phps。 fq - （filter query）过虑查询，作用：在q查询符合结果中同时是fq查询符合的，例如：q=mm&amp;fq=date_time:[20081001 TO 20091031]，找关键字mm，并且date_time是20081001到20091031之间的 不常用： defType： q.op - 覆盖schema.xml的defaultOperator（有空格时用”AND”还是用”OR”操作逻辑），一般默认指定 df - 默认的查询字段，一般默认指定 qt - （query type）指定那个类型来处理查询请求，一般不用指定，默认是standard。 其它： indent - 返回的结果是否缩进，默认关闭，用 indent=true|on 开启，一般调试json,php,phps,ruby输出才有必要用这个参数。 version- 查询语法的版本，建议不使用它，由服务器指定默认值。 检索运算符 : 指定字段查指定值，如返回所有值: ? 表示单个任意字符的通配 * 表示多个任意字符的通配（不能在检索的项开始使用*或者?符号） ~ 表示模糊检索，如检索拼写类似于”roam”的项这样写：roam将找到形如foam和roams的单词；roam0.8，检索返回相似度在0.8以上的记录。 邻近检索，如检索相隔10个单词的”apache”和”jakarta”，”jakarta apache”~10 ^ 控制相关度检索，如检索jakarta apache，同时希望去让”jakarta”的相关度更加好，那么在其后加上”^”符号和增量值，即jakarta^4 apache 布尔操作符AND、|| 布尔操作符OR、&amp;&amp; 布尔操作符NOT、!、-（排除操作符不能单独与项使用构成查询） + 存在操作符，要求符号”+”后的项必须在文档相应的域中存在 () 用于构成子查询 [] 包含范围检索，如检索某时间段记录，包含头尾，date:[200707 TO 200710] {}不包含范围检索，如检索某时间段记录，不包含头尾，date:{200707 TO 200710} &quot; 转义操作符，特殊字符包括+ - &amp;&amp; || ! ( ) { } [ ] ^ “ ~ * ? : “ 示例 查询所有 1http://localhost:8080/solr/primary/select?q=*:* 限定返回字段 1http://localhost:8080/solr/primary/select?q=*:*&amp;fl=productId 表示：查询所有记录，只返回productId字段 分页 1http://localhost:8080/solr/primary/select?q=*:*&amp;fl=productId&amp;rows=6&amp;start=0 表示：查询前六条记录，只返回productId字段 增加限定条件 1http://localhost:8080/solr/primary/select?q=*:*&amp;fl=productId&amp;rows=6&amp;start=0&amp;fq=category:2002&amp;fq=namespace:d&amp;fl=productId+category&amp;fq=en_US_city_i:1101 表示：查询category=2002、en_US_city_i=110以及namespace=d的前六条记录，只返回productId和category字段 添加排序 1http://localhost:8080/solr/primary/select?q=*:*&amp;fl=productId&amp;rows=6&amp;start=0&amp;fq=category:2002&amp;fq=namespace:d&amp;sort=category_2002_sort_i+asc 表示：查询category=2002以及namespace=d并按category_2002_sort_i升序排序的前六条记录，只返回productId字段 facet查询 现实分组统计结果 123http://localhost:8080/solr/primary/select?q=*:*&amp;fl=productId&amp;fq=category:2002&amp;facet=true&amp;facet.field=en_US_county_i&amp;facet.field=en_US_hotelType_s&amp;facet.field=price_p&amp;facet.field=heatRange_ihttp://localhost:8080/solr/primary/select?q=*:*&amp;fl=productId&amp;fq=category:2002&amp;facet=true&amp;facet.field=en_US_county_i&amp;facet.field=en_US_hotelType_s&amp;facet.field=price_p&amp;facet.field=heatRange_i&amp;facet.query=price_p:[300.00000+TO+*] 高亮hl-highlight，h1=true，表示采用高亮。可以用h1.fl=field1,field2 来设定高亮显示的字段。 hl.fl:用空格或逗号隔开的字段列表。要启用某个字段的highlight功能，就得保证该字段在schema中是stored。如果该参数未被给出，那么就会高 亮默认字段 standard handler会用df参数，dismax字段用qf参数。你可以使用星号去方便的高亮所有字段。如果你使用了通配符，那么要考虑启用 。 hl.requireFieldMatch:如果置为true，除非该字段的查询结果不为空才会被高亮。它的默认值是false，意味 着它可能匹配某个字段却高亮一个不同的字段。如果hl.fl使用了通配符，那么就要启用该参数。尽管如此，如果你的查询是all字段（可能是使用 copy-field 指令），那么还是把它设为false，这样搜索结果能表明哪个字段的查询文本未被找到 hl.usePhraseHighlighter:如果一个查询中含有短语（引号框起来的）那么会保证一定要完全匹配短语的才会被高亮。 hl.highlightMultiTerm如果使用通配符和模糊搜索，那么会确保与通配符匹配的term会高亮。默认为false，同时hl.usePhraseHighlighter要为true。 hl.snippets： 这是highlighted片段的最大数。默认值为1，也几乎不会修改。如果某个特定的字段的该值被置为0（如f.allText.hl.snippets=0），这就表明该字段被禁用高亮了。你可能在hl.fl=*时会这么用。 hl.fragsize: 每个snippet返回的最大字符数。默认是100.如果为0，那么该字段不会被fragmented且整个字段的值会被返回。大字段时不会这么做。 hl.mergeContiguous: 如果被置为true，当snippet重叠时会merge起来。 hl.maxAnalyzedChars: 会搜索高亮的最大字符，默认值为51200，如果你想禁用，设为-1 hl.alternateField: 如果没有生成snippet（没有terms 匹配），那么使用另一个字段值作为返回。 hl.maxAlternateFieldLength: 如果hl.alternateField启用，则有时需要制定alternateField的最大字符长度，默认0是即没有限制。所以合理的值是应该为hl.snippets * hl.fragsize这样返回结果的大小就能保持一致。 hl.formatter:一个提供可替换的formatting算法的扩展点。默认值是simple，这是目前仅有的选项。显然这不够用，你可以看看org.apache.solr.highlight.HtmlFormatter.java 和 solrconfig.xml 中highlighting元素是如何配置的。注意在不论原文中被高亮了什么值的情况下，如预先已存在的em tags，也不会被转义，所以在有时会导致假的高亮。 hl.fragmenter:这个是solr制定fragment算法的扩展点。gap是默认值。regex是另一种选项，这种选项指明highlight的边界由一个正则表达式确定。这是一种非典型 的高级选项。为了知道默认设置和fragmenters (and formatters)是如何配置的，可以看看 solrconfig.xml 中的highlight段。 hl.regex.pattern:正则表达式的pattern hl.regex.slop:这是hl.fragsize能变化以适应正则表达式的因子。默认值是0.6，意思是如果 hlfragsize=100 那么fragment的大小会从40-160.]]></content>
      <categories>
        <category>search-engine</category>
      </categories>
      <tags>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Solr介绍及安装]]></title>
    <url>%2F2014%2F02%2F26%2Fhow-to-install-solr%2F</url>
    <content type="text"><![CDATA[Solr是什么Solr是一个基于Lucene java库的企业级搜索服务器，包含XML/HTTP，JSON API，高亮查询结果，缓存，复制，还有一个WEB管理界面。Solr运行在Servlet容器中，其架构如下： 主要功能包括全文检索，高亮命中，分面搜索(faceted search)，近实时索引，动态集群，数据库集成，富文本索引，空间搜索；通过提供分布式索引，复制，负载均衡查询，自动故障转移和恢复，集中配置等功能实现高可用，可伸缩和可容错。 Solr和Lucene的本质区别有以下三点：搜索服务器、企业级和管理。Lucene本质上是搜索库，不是独立的应用程序，而Solr是。Lucene专注于搜索底层的建设，而Solr专注于企业应用。Lucene不负责支撑搜索服务所必须的管理，而Solr负责。所以说Solr是Lucene面向企业搜索应用的扩展。 Solr目前有很多用户了，比较著名的用户有 AOL、 Disney、 Apple等，国内的有淘宝，淘宝的终搜就是基于Solr改造的，终搜用于淘宝的SNS、淘女郎等处的搜索。 安装和部署1. 下载官方网址：http://lucene.apache.org/solr/ 下载地址：http://archive.apache.org/dist/lucene/solr/ 2. 安装与配置以solr-4.4.0为例，解压之后的目录如下： 1234567891011121314➜ solr-4.4.0 tree -L 1.├── CHANGES.txt├── contrib├── dist├── docs├── example├── licenses├── LICENSE.txt├── NOTICE.txt├── README.txt└── SYSTEM_REQUIREMENTS.txt5 directories, 5 files solr提供一个war包可以运行web界面，该文件位于exmaple/webapps目录下，发布该war包之前需要配置solr home，solr home是索引和配置文件所在的目录。 solr home的设置有好几种方式： 1、 基于环境变量solr.solr.home 直接修改JAVA全局环境变量 1export JAVA_OPTS=&quot;$JAVA_OPTS -Dsolr.solr.home=/tmp/solrhome&quot; 你也可以修改TOMCAT_HOME/bin/catalina.sh，在文件开头添加： 1JAVA_OPTS=&apos;-Dsolr.solr.home=/tmp/solrhome&apos; 或者，在启动时进行设置。start.jar在源码包中可以找到，内部包含jetty容器。 1$ java -Dsolr.solr.home=/tmp/solrhome -jar start.jar 2、 基于JNDI配置 修改war中的web.xml文件，取消下面对下面的注视，并修改env-entry-value的值。 12345&lt;env-entry&gt; &lt;env-entry-name&gt;solr/home&lt;/env-entry-name&gt; &lt;env-entry-value&gt;/tmp/solrhome&lt;/env-entry-value&gt; &lt;env-entry-type&gt;java.lang.String&lt;/env-entry-type&gt;&lt;/env-entry&gt; 或者，创建solr.xml文件放于TOMCAT_HOME/conf/Catalina/localhost，内容如下： 1234&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;Context docBase="TOMCAT_HOME/webapps/solr.war" debug="0" crossContext="true"&gt; &lt;Environment name="solr/home" type="java.lang.String" value="/tmp/solrhomehome" override="true"/&gt;&lt;/Context&gt; 3、 基于当前路径的方式 这种情况需要在example目录下去启动tomcat，Solr查找./solr，因此在启动时候需要切换到example目录下面 3. 在Jetty上运行Solr在example目录下，运行下面命令即可启动一个内置的jetty容器： 1$ java -Dsolr.solr.home=/tmp/solrhome -jar start.jar 通过http://localhost:8983/solr即可访问。 4. 在tomcat中运行Solr将example/webapps/solr.war拷贝到tomcat的webapps目录下，然后参照上面的说明设置solr home值。tomcat版本可以使用tomcat-6.0.36。 其次，将example/lib/ext目录中的jar包拷贝到tomcat-6.0.36/webapps/solr/WEB-INF/lib目录下。 然后，将example/resources/log4j.properties也拷到classpath，或者在tomcat-6.0.36/webapps/solr/目录下新建了一个classes目录，将log4j.properties放进去。 这时候启动tomcat后访问http://localhost:8080/solr会提示错误，这是因为solr home目录下没有solr的配置文件和一些目录。请将solr-4.4.0/example/solr/目录下的文件拷贝到solr home目录下，例如： 1$ cp -r solr-4.4.0/example/solr/ /tmp/solrhome/ 最后，启动tomcat，然后通过浏览器访问。 5. 其他关于中文支持关于中文，solr内核支持UTF-8编码，所以在tomcat里的server.xml需要进行配置 1&lt;Connector port="8080" maxHttpHeaderSize="8192" URIEncoding="UTF-8" /&gt; 另外，向solr Post请求的时候需要转为utf-8编码。对solr 返回的查询结果也需要进行一次utf-8的转码。检索数据时对查询的关键字也需要转码，然后用“+”连接。 1234String[] array = StringUtils.split(query, null, 0);for (String str : array) &#123; result = result + URLEncoder.encode(str, "UTF-8") + "+";&#125;]]></content>
      <categories>
        <category>search-engine</category>
      </categories>
      <tags>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Vagrant创建虚拟机]]></title>
    <url>%2F2014%2F02%2F23%2Fcreate-virtualbox-by-vagrant%2F</url>
    <content type="text"><![CDATA[安装VirtualBox下载地址：https://www.virtualbox.org/wiki/Downloads/ 安装Vagrant下载安装包：http://downloads.vagrantup.com/，然后安装。 下载box下载适合你的box，地址：http://www.vagrantbox.es/。 例如下载 CentOS7.2： 1$ wget https://github.com/CommanderK5/packer-centos-template/releases/download/0.7.2/vagrant-centos-7.2.box 添加box首先查看已经添加的box： 1$ vagrant box list 添加新的box，可以是远程地址也可以是本地文件，建议先下载到本地再进行添加： 1$ vagrant box add centos7.2 ./vagrant-centos-7.2.box 其语法如下： 1vagrant box add &#123;title&#125; &#123;url&#125; box 被安装在 ~/.vagrant.d/boxes 目录下面。 创建虚拟机先创建一个目录： 1$ mkdir -p ~/workspace/vagrant/cdh 初始化，使用 centos7.2 box： 12$ cd ~/workspace/vagrant/cdh$ vagrant init centos7.2 输出如下日志： 1234A `Vagrantfile` has been placed in this directory. You are nowready to `vagrant up` your first virtual environment! Please readthe comments in the Vagrantfile as well as documentation on`vagrantup.com` for more information on using Vagrant. 在当前目录生成了 Vagrantfile 文件。 修改Vagrantfile修改Vagrantfile文件，创建三个虚拟机，如下： 123456789101112131415161718192021# -*- mode: ruby -*-# vi: set ft=ruby :# Vagrantfile API/syntax version. Don't touch unless you know what you're doing!VAGRANTFILE_API_VERSION = "2"Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| (1..3).each do |i| config.vm.define vm_name = "cdh#&#123;i&#125;" do |config| config.vm.provider "virtualbox" do |v| v.customize ["modifyvm", :id, "--name", vm_name, "--memory", "2048",'--cpus', 1] end config.vm.box = "centos7.2" config.vm.hostname ="#&#123;vm_name&#125;.example.com" config.ssh.username = "vagrant" config.vm.network :private_network, ip: "192.168.56.12#&#123;i&#125;" config.vm.provision :shell, :path =&gt; "bootstrap.sh" end endend 这里设置hostname为域名格式，如：cdh1.example.com。 网络使用的是 host-only 网络。 在启动成功之后，会运行 bootstrap.sh 脚本进行初始化工作。该脚本内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101#!/usr/bin/env bash# The output of all these installation steps is noisy. With this utility# the progress report is nice and concise.function install &#123; echo Install $1 shift yum -y install "$@" &gt;/dev/null 2&gt;&amp;1&#125;echo "Begin to run bootstrap: $(date)"echo "Remove unused logs"rm -rf /root/*.logecho "Setup yum repos"rm -rf /etc/yum.repos.d/*yum clean all &gt;/dev/null 2&gt;&amp;1wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repoyum install wget vim -yecho "Disable iptables"setenforce 0 &gt;/dev/null 2&gt;&amp;1 &amp;&amp; iptables -Fecho "Disable IPv6"cat &gt; /etc/sysctl.conf &lt;&lt;EOFnet.ipv6.conf.all.disable_ipv6=1net.ipv6.conf.default.disable_ipv6=1net.ipv6.conf.lo.disable_ipv6=1EOFsysctl -pcat /proc/sys/net/ipv6/conf/all/disable_ipv6echo "Update /etc/hosts"cat &gt; /etc/hosts &lt;&lt;EOF127.0.0.1 localhost192.168.56.121 cdh1 cdh1.example.com192.168.56.122 cdh2 cdh2.example.com192.168.56.123 cdh3 cdh3.example.comEOFecho "Set hostname"echo "hostname:$&#123;hostname&#125;"hostnamectl set-hostname $(hostname)cat &gt; /etc/sysconfig/network&lt;&lt;EOFHOSTNAME=$(hostname)EOFecho "Check hostname"cat /etc/sysconfig/networkuname -ayum install net-tools -y &amp;&amp; ifconfig |grep -B1 broadcastyum install bind-utils -y &amp;&amp; host -v -t A `hostname`echo "Set locale and timezone"#yum groupinstall "fonts" -ycat &gt; /etc/locale.conf &lt;&lt;EOFLANG="zh_CN.UTF-8"LC_CTYPE=zh_CN.UTF-8LC_ALL=zh_CN.UTF-8EOFsource /etc/locale.confecho "Check locale"cat /etc/locale.confcp /usr/share/zoneinfo/Asia/Shanghai -f -n /etc/localtimeecho "Setup root paassword"# Setup sudo to allow no-password sudo for "admin". Additionally,# make "admin" an exempt group so that the PATH is inherited.cp /etc/sudoers /etc/sudoers.origecho "root ALL=(ALL) NOPASSWD: ALL" &gt;&gt; /etc/sudoersecho 'redhat'|passwd root --stdin &gt;/dev/null 2&gt;&amp;1echo "Setup nameservers"# http://ithelpblog.com/os/linux/redhat/centos-redhat/howto-fix-couldnt-resolve-host-on-centos-redhat-rhel-fedora/# http://stackoverflow.com/a/850731/1486325echo "nameserver 8.8.8.8" | tee -a /etc/resolv.confecho "nameserver 8.8.4.4" | tee -a /etc/resolv.confecho "Generate root ssh"[ ! -d ~/.ssh ] &amp;&amp; ( mkdir /root/.ssh ) &amp;&amp; ( chmod 600 ~/.ssh ) &amp;&amp; yes|ssh-keygen -f ~/.ssh/id_rsa -t rsa -N ""echo "Set npt service"yum install ntp -ycat &gt; /etc/ntp.conf &lt;&lt;EOFrestrict default ignore //默认不允许修改或者查询ntp,并且不接收特殊封包restrict 127.0.0.1 //给于本机所有权限restrict 192.168.56.0 mask 255.255.255.0 notrap nomodify //给于局域网机的机器有同步时间的权限server 192.168.56.121 # local clockdriftfile /var/lib/ntp/driftfudge 127.127.1.0 stratum 10EOFchkconfig ntpd on &amp;&amp; service ntpd restartecho "Set swappiness"sysctl -w vm.swappiness=0echo vm.swappiness = 0 &gt;&gt; /etc/sysctl.confecho "Install jdk"yum install java-1.7.0-openjdk java-1.7.0-openjdk-devel -yecho "Finish to run bootstrap: $(date)" 以上脚本主要做了以下几件事： 1、删除无用的日志 2、设置yum源 3、禁用iptables 4、禁用IPv6 5、设置hosts文件 6、设置hostname 7、设置时区 8、修改root帐号密码为redhat 9、设置命名服务 10、生成ssh公要文件 11、安装并启动ntp服务 12、设置swappiness 13、安装jdk 以上所有配置可以在 这里找 找到。 创建并启动虚拟机执行以下命令会依次启动三个虚拟机： 1$ vagrant up 启动成功之后，就可以通过 ssh 登陆到虚拟机： 1$ vagrant ssh cdh1]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>vagrant</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础入门]]></title>
    <url>%2F2014%2F02%2F22%2Fpython-introduction-of-basics%2F</url>
    <content type="text"><![CDATA[1. Python介绍Python是一种解释性的面向对象的语言。Python使用C语言编写，不需要事先声明变量的类型（动态类型），但是一旦变量有了值，那么这个变脸是有一个类型的，不同类型的变量之间赋值需要类型转换（强类型）。 1.1 安装 Python现在的操作系统都自带安装了 Python，要测试你是否安装了Python，你可以打开一个shell程序（就像konsole或gnome-terminal），然后输入如下所示的命令python -V 12$ python -VPython 2.7.4 如果你看见向上面所示的那样一些版本信息，那么你已经安装了Python了。 2. Python 基础2.1 注释无论是行注释还是段注释，均以 # 加一个空格来注释。 如果需要在代码中使用中文注释，必须在 python 文件的最前面加上如下注释说明： 1# -* - coding: UTF-8 -* - 如下注释用于指定解释器： 1#! /usr/bin/python 2.2 数据类型和变量 python的数字类型分为整型、长整型、浮点型、布尔型、复数类型、None类型。 python没有字符类型 python内部没有普通类型，任何类型都是对象。 如果需要查看变量的类型，可以使用type类，该类可以返回变量的类型或创建一个新的类型。python有3种表示字符串类型的方式，即单引号、双引号、三引号。单引号和双引号的作用是相同的。python程序员更喜欢用单引号，C/Java程序员则习惯使用双引号表示字符串。三引号中可以输入单引号、双引号或换行等字符。 变量命名规则： 第一个字符必须是字母表中的字母（大写或小写）或者一个下划线（‘ _ ’） 其他部分可以由字母（大写或小写）、下划线（‘ _ ’）或数字（0-9）组成 对大小写敏感 python中的变量不需要声明，变量的赋值操作即使变量声明和定义的过程。 1&gt;&gt;&gt;a = 10 那么你的内存里就有了一个变量a， 它的值是10，它的类型是integer (整数)。 在此之前你不需要做什么特别的声明，而数据类型是Python自动决定的。 12&gt;&gt;&gt;print a&gt;&gt;&gt;print type(a) 那么会有如下输出： 1210&lt;type &apos;int&apos;&gt; 这里，我们学到一个内置函数type(), 用以查询变量的类型。 如果你想让a存储不同的数据，你不需要删除原有变量就可以直接赋值。 12&gt;&gt;&gt;a = 1.3&gt;&gt;&gt;print a,type(a) 会有如下输出： 11.3 &lt;type &apos;float&apos;&gt; python 中一次新的赋值，将创建一个新的变量。即使变量的名称相同，变量的标识并不相同。用id()函数可以获取变量标识： 1234x = 1print id(x)x = 2print id(x) 如果变量没有赋值，则python认为该变量不存在。 在函数之外定义的变量都可以称为全局变量。全局变量可以被文件内部的任何函数和外部文件访问。 全局变量建议在文件的开头定义，也可以把全局变量放到一个专门的文件中，然后通过import来引用 2.3 序列sequence(序列)是一组有顺序的元素的集合。 序列包括字符串、列表、元组。 序列可以包含一个或多个元素，也可以没有任何元素。 使用索引来访问序列，索引从0开始。 可以使用分片操作来访问一定范围内的元素。 序列有两种：tuple（定值表； 也有翻译为元组） 和 list (表)。tuple 和 list 的主要区别在于，一旦建立，tuple 的各个元素不可再变更，而 list 的各个元素可以再变更。 1234&gt;&gt;&gt;s1 = (2, 1.3, 'love', 5.6, 9, 12, False) # s1是一个tuple&gt;&gt;&gt;s2 = [True, 5, 'smile'] # s2是一个list&gt;&gt;&gt;print s1,type(s1)&gt;&gt;&gt;print s2,type(s2) 序列可以进行相加和乘法等操作： 12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; [1,2]+[3,4] [1,2,3,4]#列表和字符串是无法连接在一起的&gt;&gt;&gt; "hello,"+"world!" "hello,world!"&gt;&gt;&gt; "a" * 5 "aaaaa"&gt;&gt;&gt; [2] * 5 [2,2,2,2,2]#初始化一个长度为3的空序列&gt;&gt;&gt; seq=[None] * 3 [None,None,Noe]#判断一个元素是否存在于序列中&gt;&gt;&gt; permissions='rw'&gt;&gt;&gt; 'w' in permissions True&gt;&gt;&gt; users=['a','b','c']&gt;&gt;&gt; 'a' in users True&gt;&gt;&gt; database=[ ['a','1234'], ['b','2344']]&gt;&gt;&gt; ['c','1234'] in database False 2.4. 列表列表类似于c语言中的数组，用于存储顺序结构。例如：[1,2,3,4,5]。列表中的各个元素可以是任意类型，元素之间用逗号分隔。列表的下标从0开始，和c语言类似，但是增加了负下标的使用。 对列表的操作： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&gt;&gt;&gt; names=['zhangsan','lisi','lili','wangwu']#删除&gt;&gt;&gt; del names[0]#分片赋值&gt;&gt;&gt; names[1:]=list('ab') #list函数用于将字符串转换为列表&gt;&gt;&gt; names ['lisi','a','b']#在不需要替换任何原有元素的情况下插入新元素&gt;&gt;&gt; names[1:1]=['c','d']&gt;&gt;&gt; names ['lisi','c','d','a','b']&gt;&gt;&gt; names[1:4]=[]&gt;&gt;&gt; names ['lisi','b']#添加&gt;&gt;&gt; names.append('bb')&gt;&gt;&gt; names.count('b')#extend效率高于连接操作&gt;&gt;&gt; names.extend(['c','e','d'])&gt;&gt;&gt; names ['lisi','b','bb','c','e','d']&gt;&gt;&gt; names.index('lisi')&gt;&gt;&gt; names.insert(1,'a')&gt;&gt;&gt; names.pop() 'e'&gt;&gt;&gt; names.remove('lisi')&gt;&gt;&gt; names ['a','b','bb','c','e','d']&gt;&gt;&gt; names.reverse() ['d','e','c','bb','b','a']#排序&gt;&gt;&gt; names.sort() ['a','b','bb','c','d','e']&gt;&gt;&gt; sorted(names) ['a','b','bb','c','d','e']#高级排序&gt;&gt;&gt; names.sort(cmp)&gt;&gt;&gt; names.sort(key=len)&gt;&gt;&gt; names.sort(reverse=True) 2.5 元组元组 tuple 是常量 list。tuple 不能 pop,remove,insert 等方法。 tuple 用 () 表示，如 a=(1,2,3,4),括号可以省略。 tuple 可以用下标返回元素或者子 tuple tuple 可以用于多个变量的赋值。例如：a,b=(1,2) 表示只含有一个元素的 tuple 的方法是：(1,),后面有个逗号，用来和单独的变量相区分。 字符串是一种特殊的元素，因此可以执行元组的相关操作。 tuple 比 list 性能好，也就是不用提供动态内存管理的功能。 123456789101112131415161718192021222324252627#定义一个元组&gt;&gt;&gt; 1,2,3 (1,2,3)#空的元组&gt;&gt;&gt; ()&gt;&gt;&gt; 42 42&gt;&gt;&gt; 42, (42,)&gt;&gt;&gt; (42,) (42,)#tuple函数&gt;&gt;&gt; tuple([1,2,3]) (1,2,3)&gt;&gt;&gt; tuple('abc') ('a','b','c')&gt;&gt;&gt; tuple((1,2,3)) (1,2,3)&gt;&gt;&gt; x=1,2,3&gt;&gt;&gt; x[1] 2&gt;&gt;&gt; x[0:2] (1,2) 2.6 字典字典是一个无序存储结构。每一个元素是一个 pair，包括 key 和 value 两个不服。key 的类型是 integer 或者 string 或者任何同时含有 __hash__和__cmp__ 的对象。字典中没有重复的 key，其每一个元素是一个元组。 创建和使用字典： 1phonebook = &#123;'zhangsan':'1234',"lisi":'1231'&#125; dict函数： 12345678&gt;&gt;&gt; items = [('name','aa'),('age',18)]&gt;&gt;&gt; d = dict(items)&gt;&gt;&gt; d &#123;'age':18,'name':'aa'&#125;&gt;&gt;&gt; d = dict(name='aa',age=18)&gt;&gt;&gt; d &#123;'age':18,'name':'aa'&#125; 字典的基本操作： 2.7 setset和dict类似，也是一组key的集合，但不存储value。由于key不能重复，所以，在set中，没有重复的key。 要创建一个set，需要提供一个list作为输入集合： 123&gt;&gt;&gt; s = set([1, 2, 3])&gt;&gt;&gt; sset([1, 2, 3]) 注意，传入的参数[1, 2, 3]是一个list，而显示的set([1, 2, 3])只是告诉你这个set内部有1，2，3这3个元素，显示的[]不表示这是一个list。 重复元素在set中自动被过滤： 12345678910111213&gt;&gt;&gt; s = set([1, 1, 2, 2, 3, 3])&gt;&gt;&gt; sset([1, 2, 3])通过add(key)方法可以添加元素到set中，可以重复添加，但不会有效果：~~~python&gt;&gt;&gt; s.add(4)&gt;&gt;&gt; sset([1, 2, 3, 4])&gt;&gt;&gt; s.add(4)&gt;&gt;&gt; sset([1, 2, 3, 4]) 通过remove(key)方法可以删除元素： 123&gt;&gt;&gt; s.remove(4)&gt;&gt;&gt; sset([1, 2, 3]) set可以看成数学意义上的无序和无重复元素的集合，因此，两个set可以做数学意义上的交集、并集等操作： 123456&gt;&gt;&gt; s1 = set([1, 2, 3])&gt;&gt;&gt; s2 = set([2, 3, 4])&gt;&gt;&gt; s1 &amp; s2set([2, 3])&gt;&gt;&gt; s1 | s2set([1, 2, 3, 4]) set和dict的唯一区别仅在于没有存储对应的value，但是，set的原理和dict一样，所以，同样不可以放入可变对象，因为无法判断两个可变对象是否相等，也就无法保证set内部“不会有重复元素”。试试把list放入set，看看是否会报错。 2.8 字符串字符串普通字符串使用双引号或者单引号或者 &quot;&quot;&quot; 来表示。例如： 123456789101112print "hello,world !"print "hello,\world !"print ''' hello,world!'''print """ hello,world!""" 自然字符串如果你想要指示某些不需要如转义符那样的特别处理的字符串，那么你需要指定一个自然字符串。自然字符串通过给字符串加上前缀 r 或 R 来指定。例如 1r"Newlines are indicated by \n". 注意：自然字符串结尾不能输入反斜线 Unicode字符串Unicode 是书写国际文本的标准方法。如果你想要用你的母语如北印度语或阿拉伯语写文本，那么你需要有一个支持 Unicode 的编辑器。类似地，Python 允许你处理 Unicode 文本——你只需要在字符串前加上前缀 u 或 U。例如： 1u"This is a Unicode string.". Python 中的普通字符串在内部是以8位的 ascii 码形式存储的，而 Unicode 字符串则存储为16位 Unicode 字符。 记住，在你处理文本文件的时候使用 Unicode 字符串，特别是当你知道这个文件含有用非英语的语言写的文本。 3. 函数 python程序由包(package)、模块(module)和函数组成。包是由一系列模块组成的集合。模块是处理某一类问题的函数和类的集合。 包就是一个完成特定任务的工具箱。 包必须含有一个__init__.py文件，它用于标识当前文件夹是一个包。 python的程序是由一个个模块组成的。模块把一组相关的函数或代码组织到一个文件中，一个文件即是一个模块。模块由代码、函数和类组成。导入模块使用import语句。 包的作用是实现程序的重用。 函数是一段可以重复多次调用的代码。 函数返回值可以用return来控制。 3.1 定义函数函数定义示例如下： 123456789101112131415161718192021def function_arithmetic(x,y,operator): ''' usage: function for arithmetic ''' result = &#123; "+":x+y, "-":x-y, "*":x*y, "/":x/y &#125; return result# 函数名称func_name = function_arithmetic.__name__name = '%s' % func_name.replace('_', '-').strip('-')# 函数doc 文档help_ = function_arithmetic.__doc__.strip()print func_nameprint nameprint help_ 3.2 函数的参数默认参数最有用的形式是为一个或更多参数指定默认值。这样创建的函式调用时可以不用给足参数.。例如: 12def ask_ok(prompt, retries=4, complaint='Yes or no, please!'): pass 关键字参数函式也可以通过 keyword = value 形式的关键字参数来调用。例如: 1ask_ok('ok?',complaint='Yes or no, please!') 可变参数在Python函数中，还可以定义可变参数。顾名思义，可变参数就是传入的参数个数是可变的，可以是1个、2个到任意个，还可以是0个。 1234567891011121314def calc(*numbers): sum = 0 for n in numbers: sum = sum + n * n return sum&gt;&gt;&gt; calc(1, 2)5&gt;&gt;&gt; calc()0&gt;&gt;&gt; nums = [1, 2, 3]&gt;&gt;&gt; calc(*nums)14 关键字参数可变参数允许你传入0个或任意个参数，这些可变参数在函数调用时自动组装为一个tuple。而关键字参数允许你传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict。请看示例： 12def person(name, age, **kw): print 'name:', name, 'age:', age, 'other:', kw 函数person除了必选参数name和age外，还接受关键字参数kw。在调用该函数时，可以只传入必选参数： 12&gt;&gt;&gt; person('Michael', 30)name: Michael age: 30 other: &#123;&#125; 也可以传入任意个数的关键字参数： 1234&gt;&gt;&gt; person('Bob', 35, city='Beijing')name: Bob age: 35 other: &#123;'city': 'Beijing'&#125;&gt;&gt;&gt; person('Adam', 45, gender='M', job='Engineer')name: Adam age: 45 other: &#123;'gender': 'M', 'job': 'Engineer'&#125; 参数组合在Python中定义函数，可以用必选参数、默认参数、可变参数和关键字参数，这4种参数都可以一起使用，或者只用其中某些，但是请注意，参数定义的顺序必须是：必选参数、默认参数、可变参数和关键字参数。 比如定义一个函数，包含上述4种参数： 12def func(a, b, c=0, *args, **kw): print 'a =', a, 'b =', b, 'c =', c, 'args =', args, 'kw =', kw 在函数调用的时候，Python解释器自动按照参数位置和参数名把对应的参数传进去。 12345678&gt;&gt;&gt; func(1, 2)a = 1 b = 2 c = 0 args = () kw = &#123;&#125;&gt;&gt;&gt; func(1, 2, c=3)a = 1 b = 2 c = 3 args = () kw = &#123;&#125;&gt;&gt;&gt; func(1, 2, 3, 'a', 'b')a = 1 b = 2 c = 3 args = ('a', 'b') kw = &#123;&#125;&gt;&gt;&gt; func(1, 2, 3, 'a', 'b', x=99)a = 1 b = 2 c = 3 args = ('a', 'b') kw = &#123;'x': 99&#125; 最神奇的是通过一个tuple和dict，你也可以调用该函数： 1234&gt;&gt;&gt; args = (1, 2, 3, 4)&gt;&gt;&gt; kw = &#123;'x': 99&#125;&gt;&gt;&gt; func(*args, **kw)a = 1 b = 2 c = 3 args = (4,) kw = &#123;'x': 99&#125; 所以，对于任意函数，都可以通过类似func(*args, **kw)的形式调用它，无论它的参数是如何定义的。 参数列表解包也存在相反的情形: 当参数存在于一个既存的列表或者元组之中, 但却需要解包以若干位置参数的形式被函数调用. 例如, 内建的 range() 函数期望接收分别的开始和结束的位置参数. 如果它们不是分别可用 (而是同时存在于一个列表或者元组中), 下面是一个利用 * 操作符解从列表或者元组中解包参数以供函数调用的例子: 123456789101112131415&gt;&gt;&gt; x,y,z=1,2,3&gt;&gt;&gt; print x,y,z 1 2 3#交换值&gt;&gt;&gt; x,y=y,x&gt;&gt;&gt; phonebook = &#123;'zhangsan':'1234',"lisi":'1231'&#125;&gt;&gt;&gt; key,value=phonebook.popitem()&gt;&gt;&gt; list(range(3, 6)) # 使用分离的参数正常调用[3, 4, 5]&gt;&gt;&gt; args = [3, 6]&gt;&gt;&gt; list(range(*args)) # 通过解包列表参数调用[3, 4, 5] 同样的, 字典可以通过 ** 操作符来解包参数: 12345678&gt;&gt;&gt; def parrot(voltage, state='a stiff', action='voom'): print("-- This parrot wouldn't", action, end=' ') print("if you put", voltage, "volts through it.", end=' ') print("E's", state, "!")&gt;&gt;&gt; d = &#123;"voltage": "four million", "state": "bleedin' demised", "action": "VOOM"&#125;&gt;&gt;&gt; parrot(**d) This parrot wouldn't VOOM if you put four million volts through it. E's bleedin' demised 4. 流程控制4.1 if 语句1234567891011&gt;&gt;&gt; x = int(input("Please enter an integer: "))Please enter an integer: 42&gt;&gt;&gt; if x &lt; 0: x = 0 print('Negative changed to zero') elif x == 0: print('Zero') elif x == 1: print('Single') else: print('More') 4.2 for 语句Python中的 for 语句与你在 C 或是 Pascal 中使用的略有不同. 不同于在 Pascal 中总是依据一个等差的数值序列迭代, 也不同于在 C 中允许用户同时定义迭代步骤和终止条件, Python中的 for 语句在任意序列 (列表或者字符串) 中迭代时, 总是按照元素在序列中的出现顺序依次迭代. 123 a = ['cat', 'window', 'defenestrate']&gt;&gt;&gt; for x in a: print(x, len(x)) 在循环过程中修改被迭代的对象是不安全的 (这只可能发生在可变序列类型上,如列表)。 若想在循环体内修改你正迭代的序列 (例如复制序列中选定的项), 最好是先制作一个副本。 而切片则让这种操作十分方便: 12&gt;&gt;&gt; for x in a[:]: # 制造整个列表的切片复本 if len(x) &gt; 6: a.insert(0, x) 4.4 pass 语句pass 语句什么都不做. 当语法上需要一个语句, 但程序不要动作时, 就可以使用它. 例如: 12&gt;&gt;&gt; while True: pass # 忙等待键盘中断 (Ctrl+C) 一般也可以用于创建最小类: 12&gt;&gt;&gt; class MyEmptyClass: pass 另一个使用 pass 的地方是，作为函式或条件体的占位符，当你在新代码工作时，它让你能保持在更抽象的级别思考。 pass 会被默默地被忽略： 12&gt;&gt;&gt; def initlog(*args): pass # 记得实现这里! 5. 高级特性5.1 切片下面这部分内容来自 廖雪峰的Python教程-切片 取一个list或tuple的部分元素是非常常见的操作。比如，一个list如下： 1&gt;&gt;&gt; L = ['Michael', 'Sarah', 'Tracy', 'Bob', 'Jack'] 取前3个元素，用一行代码就可以完成切片： 12&gt;&gt;&gt; L[0:3]['Michael', 'Sarah', 'Tracy'] L[0:3] 表示，从索引0开始取，直到索引3为止，但不包括索引3。即索引0，1，2，正好是3个元素。 如果第一个索引是0，还可以省略： 12&gt;&gt;&gt; L[:3]['Michael', 'Sarah', 'Tracy'] 也可以从索引1开始，取出2个元素出来： 12&gt;&gt;&gt; L[1:3]['Sarah', 'Tracy'] 类似的，既然Python支持 L[-1] 取倒数第一个元素，那么它同样支持倒数切片，试试： 1234&gt;&gt;&gt; L[-2:]['Bob', 'Jack']&gt;&gt;&gt; L[-2:-1]['Bob'] 记住倒数第一个元素的索引是 -1。 切片操作十分有用。我们先创建一个0-99的数列： 123&gt;&gt;&gt; L = range(100)&gt;&gt;&gt; L[0, 1, 2, 3, ..., 99] 可以通过切片轻松取出某一段数列。比如前10个数： 12&gt;&gt;&gt; L[:10][0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 后10个数： 12&gt;&gt;&gt; L[-10:][90, 91, 92, 93, 94, 95, 96, 97, 98, 99] 前11-20个数： 12&gt;&gt;&gt; L[10:20][10, 11, 12, 13, 14, 15, 16, 17, 18, 19] 前10个数，每两个取一个： 12&gt;&gt;&gt; L[:10:2][0, 2, 4, 6, 8] 所有数，每5个取一个： 12&gt;&gt;&gt; L[::5][0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95] 甚至什么都不写，只写 [:] 就可以原样复制一个list： 12&gt;&gt;&gt; L[:][0, 1, 2, 3, ..., 99] tuple 也是一种 list，唯一区别是 tuple 不可变。因此，tuple 也可以用切片操作，只是操作的结果仍是 tuple： 12&gt;&gt;&gt; (0, 1, 2, 3, 4, 5)[:3](0, 1, 2) 字符串 ‘xxx’ 或 Unicode 字符串 u’xxx’ 也可以看成是一种 list，每个元素就是一个字符。因此，字符串也可以用切片操作，只是操作结果仍是字符串： 12345&gt;&gt;&gt; 'ABCDEFG'[:3]'ABC'&gt;&gt;&gt; 'ABCDEFG'[::2]'ACEG'Try 5.2 迭代使用 for 关键字来迭代序列、字段或者可迭代的对象。 1234567891011121314151617d = &#123;'a': 1, 'b': 2, 'c': 3&#125;# 迭代字段的 keyfor key in d: print key# 迭代字段的 valuefor value in d.itervalues(): print print# 迭代字符串for ch in 'ABC': print ch# Python内置的enumerate函数可以把一个list变成索引-元素对，这样就可以在for循环中同时迭代索引和元素本身for i, value in enumerate(['A', 'B', 'C']): print i, value 判断一个对象是否支持迭代： 1234567&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance('abc', Iterable) # str是否可迭代True&gt;&gt;&gt; isinstance([1,2,3], Iterable) # list是否可迭代True&gt;&gt;&gt; isinstance(123, Iterable) # 整数是否可迭代False 5.3 列表推导式12345678&gt;&gt;&gt; [x*x for x in range(7)] [0,1,4,9,16,25,36]&gt;&gt;&gt; [x*x for x in range(7) if x%3 ==0 ] [0,9,36]&gt;&gt;&gt; [(x,y) for x in range(3) for y in range(2)] [(0,0),(0,1),(1,0),(1,1),(2,0),(2,1)] 另一个使用列表推导式的代码示例： 1234567891011121314151617num = [1, 4, -5, 10, -7, 2, 3, -1]def square_generator(optional_parameter): return (x ** 2 for x in num if x &gt; optional_parameter)print square_generator(0)# &lt;generator object &lt;genexpr&gt; at 0x004E6418&gt;# Option Ifor k in square_generator(0): print k# 1, 16, 100, 4, 9# Option IIg = list(square_generator(0))print g# [1, 16, 100, 4, 9] 5.4 生成器除非特殊的原因，应该经常在代码中使用生成器表达式。但除非是面对非常大的列表，否则是不会看出明显区别的。 使用生成器得到当前目录及其子目录中的所有文件的代码，下面代码来自 伯乐在线-python高级编程技巧： 123456789import osdef tree(top): #path,folder list,file list for path, names, fnames in os.walk(top): for fname in fnames: yield os.path.join(path, fname)for name in tree(os.getcwd()): print name 参考文章 Python教程 Python Basics]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Confluence 5.4.2安装]]></title>
    <url>%2F2014%2F02%2F21%2Finstall-confluence5-4-2%2F</url>
    <content type="text"><![CDATA[Confluence是Atlassian公司出品的团队协同与知识管理工具。 Confluence是一个专业的企业知识管理与协同软件，也可以用于构建企业wiki。通过它可以实现团队成员之间的协作和知识共享。 1、下载下载指定版本Confluence 123$ mkdir -p /data/confluence$ cd /data/confluence$ wget www.atlassian.com/software/confluence/downloads/binary/atlassian-confluence-5.4.2.tar.gz 2、安装解压： 1$ tar -zxvf atlassian-confluence-5.4.2.tar.gz 进入目录： 1$ cd atlassian-confluence-5.4.2 配置confluence安装目录： 1$ vim confluence/WEB-INF/classes/confluence-init.properties 在属性文件中添加安装目录路径： 12345############################ Note for Unix Users ############################# - For example:confluence.home=/data/confluence/confluence-data 创建目录： 1$ mkdir confluence-data 创建完成后，运行启动： 1$ sh bin/start-confluence.sh 访问ip+portnum，默认端口为8090，如果出现破解界面，以上步骤即为成功 3、启停服务使用压缩包形式的Confluence无法直接使用服务启停，需要配合目录的sh文件 启动服务： 1$ sh bin/start-confluence.sh 停止服务： 1$ sh bin/stop-confluence.sh 4、修改默认端口配置文件位置：atlassian-confluence-5.4.2/conf/server.xml 打开文件， 1&lt;Connector className="org.apache.coyote.tomcat4.CoyoteConnector" port="8090" minProcessors="5" 修改 port 为需要的端口。 5、更换默认的数据库在完成之前的步骤后，数据库使用的是一个 confluence 默认的 hsql 数据库，此数据库缺陷较多，例如：不支持事务管理。因此需要将数据库迁移为指定的数据库类型。 进入 confluence-data 目录，修改 confluence-cfg.xml 文件中数据库相关的连接信息。 6、安装中文字体：默认情况下 Confluence 导出 PDF 不支持中文，需要修改如下： 管理员登录 “Confluence Admin”，选择左边菜单 “CONFIGURATION”-“PDF Export Language Support”，选择安装中文字体，例如：simsun.ttc 7、破解请参考：http://582033.vicp.net/?p=1085]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>confluence</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Backbone中的模型]]></title>
    <url>%2F2014%2F02%2F16%2Fbackbone-model%2F</url>
    <content type="text"><![CDATA[创建model模型是所有Javascript应用程序的核心，包括交互数据及相关的大量逻辑： 转换、验证、计算属性和访问控制。你可以用特定的方法扩展Backbone.Model，模型也提供了一组基本的管理变化的功能。 1234567Person = Backbone.Model.extend(&#123; initialize: function()&#123; alert("Welcome to this world"); &#125;&#125;);var person = new Person; new一个model的实例后就会触发initialize()函数。 设置属性现在我们想设置一些属性，有两种方式，可以在创建model实例时进行传参，也可以在实例生成后通过model.set(obj)来进行设置。 12345678910Person = Backbone.Model.extend(&#123; initialize: function()&#123; alert("Welcome to this world"); &#125;&#125;);var person = new Person(&#123; name: "Thomas", age: 67&#125;);// or we can set afterwards, these operations are equivelentvar person = new Person();person.set(&#123; name: "Thomas", age: 67&#125;); 获取属性1234567891011Person = Backbone.Model.extend(&#123; initialize: function()&#123; alert("Welcome to this world"); &#125;&#125;);var person = new Person(&#123; name: "Thomas", age: 67, child: 'Ryan'&#125;);var age = person.get("age"); // 67var name = person.get("name"); // "Thomas"var child = person.get("child"); // 'Ryan' 设置model默认属性有的时候你可能会想让model有默认属性值，只要在进行model声明的时候设置个defaults就行了。 12345678910111213141516Person = Backbone.Model.extend(&#123; defaults: &#123; name: 'Fetus', age: 0, child: '' &#125;, initialize: function()&#123; alert("Welcome to this world"); &#125;&#125;);var person = new Person(&#123; name: "Thomas", age: 67, child: 'Ryan'&#125;);var age = person.get("age"); // 67var name = person.get("name"); // "Thomas"var child = person.get("child"); // 'Ryan' 监听model的属性改变我们可以通过model.bind(event,callback)方法来绑定change事件来监听属性改变。下面的这个例子就是在initialize方法中绑定了一个name属性改变的事件监听。如果person的name属性改变了，就会弹出个对话框显示新值。 12345678910111213141516Person = Backbone.Model.extend(&#123; defaults: &#123; name: 'Fetus', age: 0 &#125;, initialize: function()&#123; alert("Welcome to this world"); this.on("change:name", function(model)&#123; var name = model.get("name"); // 'Stewie Griffin' alert("Changed my name to " + name ); &#125;); &#125;&#125;);var person = new Person(&#123; name: "Thomas", age: 67&#125;);person.set(&#123;name: 'Stewie Griffin'&#125;); // This triggers a change and will alert() 和服务端交互服务端实现一个RESTful的url例如/user，可以允许我们通过他与后台交互。 1234567var UserModel = Backbone.Model.extend(&#123; urlRoot: '/user', defaults: &#123; name: '', email: '' &#125;&#125;); model.urlRoot:如果使用的集合外部的模型，通过指定 urlRoot 来设置生成基于模型 id 的 URLs 的默认 url 函数。 “/[urlRoot]/id” 创建一个新model如果id为null，则会提交一个POST请求到/user。 123456789101112131415161718192021var UserModel = Backbone.Model.extend(&#123; urlRoot: '/user', defaults: &#123; name: '', email: '' &#125;&#125;);var user = new Usermodel();// Notice that we haven't set an `id`var userDetails = &#123; name: 'Thomas', email: 'thomasalwyndavis@gmail.com'&#125;;// Because we have not set a `id` the server will call// POST /user with a payload of &#123;name:'Thomas', email: 'thomasalwyndavis@gmail.com'&#125;// The server should save the data and return a response containing the new `id`user.save(userDetails, &#123; success: function (user) &#123; alert(user.toJSON()); &#125;&#125;) model.save([attributes], [options]): 通过委托Backbone.sync保存模型到数据库（或可替代的持久层）。 attributes 散列表 (在 set) 应当包含想要改变的属性，不涉及的键不会被修改。 如果模型含有validate方法，并且验证失败，模型不会保存。 如果模型isNew, 保存将采用 “create” (HTTP POST) 方法, 如果模型已经在服务器存在，保存将采用 “update” (HTTP PUT) 方法. 获取一个model初始化一个model实例并设置其id属性，并调用fetch方法，这样会请求urlRoot + &#39;/id&#39;地址到后台。 12345678910// Here we have set the `id` of the modelvar user = new Usermodel(&#123;id: 1&#125;);// The fetch below will perform GET /user/1// The server should return the id, name and email from the databaseuser.fetch(&#123; success: function (user) &#123; alert(user.toJSON()); &#125;&#125;) model.fetch([options]): 从服务器重置模型状态。这对模型尚未填充数据，或者服务器端已有最新状态的情况很有用处。 如果服务器端状态与当前属性不同，则触发change事件。 选项的散列表参数接受success和error回调函数， 回调函数中可以传入(model,response)作为参数。 1234// 每隔 10 秒从服务器拉取数据以保持模型是最新的setInterval(function() &#123; user.fetch();&#125;, 10000); 更新一个model当保存的model对象的id不为空时，则会提交一个PUT请求到urlRoot。 123456789101112131415// Here we have set the `id` of the modelvar user = new Usermodel(&#123; id: 1, name: 'Thomas', email: 'thomasalwyndavis@gmail.com'&#125;);// Let's change the name and update the server// Because there is `id` present, Backbone.js will fire// PUT /user/1 with a payload of `&#123;name: 'Davis', email: 'thomasalwyndavis@gmail.com'&#125;`user.save(&#123;name: 'Davis'&#125;, &#123; success: function (model) &#123; alert(user.toJSON()); &#125;&#125;); 删除一个model调用model的destroy方法时，则会提交请求到urlRoot+”/id” 1234567891011121314// Here we have set the `id` of the modelvar user = new Usermodel(&#123; id: 1, name: 'Thomas', email: 'thomasalwyndavis@gmail.com'&#125;);// Because there is `id` present, Backbone.js will fire// DELETE /user/1 user.destroy(&#123; success: function () &#123; alert('Destroyed'); &#125;&#125;); model.destroy([options]):通过委托HTTP DELETE请求到Backbone.sync销毁服务器上的模型. 接受success和error回调函数作为选项散列表参数。将在模型上触发destroy事件，该事件可以通过任意包含它的集合向上冒泡。 其他方法model还有一些其他的方法，可以参考api：Backbone.js API中文文档 123456var person = new Person(&#123; name: "Thomas", age: 67&#125;);var attributes = person.toJSON(); // &#123; name: "Thomas", age: 67&#125;/* This simply returns a copy of the current attributes. */var attributes = person.attributes;/* The line above gives a direct reference to the attributes and you should be careful when playing with it. Best practise would suggest that you use .set() to edit attributes of a model to take advantage of backbone listeners. */ 参考文章 what-is-a-model Backbone.js API中文文档]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>backbone</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在CentOs6系统上安装Ganglia]]></title>
    <url>%2F2014%2F01%2F25%2Fhow-to-install-ganglia-on-centos6%2F</url>
    <content type="text"><![CDATA[Ganglia是UC Berkeley发起的一个开源集群监视项目，设计用于测量数以千计的节点。Ganglia的核心包含gmond、gmetad以及一个Web前端。主要是用来监控系统性能，由RRDTool工具处理数据，并生成相应的的图形显示，以Web方式直观的提供给客户端。如：cpu 、mem、硬盘利用率， I/O负载、网络流量情况等，通过曲线很容易见到每个节点的工作状态，对合理调整、分配系统资源，提高系统整体性能起到重要作用。 配置yum源首先配置好CentOs系统的yum源，然后需要包含有ganglia的yum源。 在/etc/yum.repos.d下创建ganglia.repo，内容如下： 12345[ganglia]name= gangliabaseurl = http://vuksan.com/centos/RPMS/enabled = 1gpgcheck = 0 为了方便离线使用，你可以下载该yum源内容： 12$ cd /opt$ reposync -r ganglia 然后在/opt/ganglia下执行如下的命令： 1$ createrepo . 这样你就可以将ganglia.repo修改为本地yum的方式。 管理机上安装gmetad执行如下命令： 1$ yum -y install ganglia-gmetad 安装时遇到如下的错误： 12Error: Package: rrdtool-1.4.5-1.x86_64 (ganglia) Requires: dejavu-lgc-fonts rrdtool依赖dejavu-lgc-fonts，但是系统源并不包含这个，你可以从网上下载，然后安装： 1$ rpm -Uvh http://mirror.steadfast.net/centos/5/os/x86_64//CentOS/dejavu-lgc-fonts-2.10-1.noarch.rpm 管理机上安装ganglia-web先安装apache和php等依赖： 1$ yum install php* httpd 然后下载ganglia-web: 12345$ wget http://sourceforge.net/projects/ganglia/files/ganglia-web/3.5.12/ganglia-web-3.5.12.tar.gz/download$ tar zxvf ganglia-web-3.5.12.tar.gz$ cd ganglia-web-3.5.12$ make install 将ganglia-web拷贝或者添加软链接到apache的目录下去，以下是拷贝： 12$ mkdir /var/www/html/ganglia$ cp -a /usr/share/ganglia/ /var/www/html/ganglia 在httpd的conf.d目录下添加ganglia.conf，命令： 1$ vim /etc/httpd/conf.d/ganglia.conf 内容如下： 12345678&lt;Location /ganglia&gt; Order deny,allow Deny from all ALLOW from all# Allow from 127.0.0.1# Allow from ::1# Allow from .example.com&lt;/Location&gt; 客户端机器上安装gmond执行如下命令： 1$ yum install gmond 启动服务在管理机上启动gmetad 1$ /etc/init.d/gmetad start 在客户端机器上启动gmond 1$ /etc/init.d/gmond start 在管理机上启动httpd 1$ /etc/init.d/httpd start 然后通过web界面（http://manager-ip/ganglia）访问ganglia-web 参考文章 [1] ganglia监控的搭建部署(从源码安装)]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>ganglia</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在RHEL系统上安装Nagios]]></title>
    <url>%2F2014%2F01%2F24%2Fhow-to-install-nagios-on-rhel6%2F</url>
    <content type="text"><![CDATA[在管理机上安装rpm包12345$ rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm$ rpm -Uvh http://rpms.famillecollet.com/enterprise/remi-release-6.rpm$ yum -y install nagios nagios-plugins-all nagios-plugins-nrpe nrpe php httpd$ chkconfig httpd on &amp;&amp; chkconfig nagios on$ service httpd start &amp;&amp; service nagios start 设置管理界面密码1$ htpasswd -c /etc/nagios/passwd nagiosadmin 密码和用户名保持一致（都设置为nagiosadmin），否则你需要修改/etc/nagios/cgi.cfg 访问Nagios打开http://ip/nagios，输入用户名和密码即可访问 在客户端上安装NRPE1234$ rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm$ rpm -Uvh http://rpms.famillecollet.com/enterprise/remi-release-6.rpm$ yum -y install nagios nagios-plugins-all nrpe$ chkconfig nrpe on 修改配置/etc/nagios/nrpe.cfg，例如： 1234567891011121314151617log_facility=daemonpid_file=/var/run/nrpe/nrpe.pidserver_port=5666nrpe_user=nrpenrpe_group=nrpeallowed_hosts=198.211.117.251dont_blame_nrpe=1debug=0command_timeout=60connection_timeout=300include_dir=/etc/nrpe.d/command[check_users]=/usr/lib64/nagios/plugins/check_users -w 5 -c 10command[check_load]=/usr/lib64/nagios/plugins/check_load -w 15,10,5 -c 30,25,20command[check_disk]=/usr/lib64/nagios/plugins/check_disk -w 20% -c 10% -p /dev/vdacommand[check_zombie_procs]=/usr/lib64/nagios/plugins/check_procs -w 5 -c 10 -s Zcommand[check_total_procs]=/usr/lib64/nagios/plugins/check_procs -w 150 -c 200command[check_procs]=/usr/lib64/nagios/plugins/check_procs -w $ARG1$ -c $ARG2$ -s $ARG3$ 请注意修改allowed_hosts值为你的nagios监控机ip 设置iptables： 12345$ iptables -N NRPE$ iptables -I INPUT -s 0/0 -p tcp --dport 5666 -j NRPE$ iptables -I NRPE -s 198.211.117.251 -j ACCEPT$ iptables -A NRPE -s 0/0 -j DROP$ /etc/init.d/iptables save 或者，关闭iptables： 1$ /etc/init.d/iptables stop 启动NRPE： 1$ service nrpe start 在管理机上添加配置文件1234$ echo &quot;cfg_dir=/etc/nagios/servers&quot; &gt;&gt; /etc/nagios/nagios.cfg$ cd /etc/nagios/servers$ touch hadoop.tk.cfg$ touch hbase.tk.cfg 然后修改每一个配置文件： 1$ vim /etc/nagios/servers/hadoop.tk.cfg 添加内容如下，你也可以稍作修改： 12345678910111213141516171819202122232425262728define host &#123; use linux-server host_name cloudmail.tk alias cloudmail.tk address 192.168.56.122 &#125;define service &#123; use generic-service host_name cloudmail.tk service_description PING check_command check_ping!100.0,20%!500.0,60% &#125;define service &#123; use generic-service host_name cloudmail.tk service_description SSH check_command check_ssh notifications_enabled 0 &#125;define service &#123; use generic-service host_name cloudmail.tk service_description Current Load check_command check_local_load!5.0,4.0,3.0!10.0,6.0,4.0 &#125; 重启nagios： 12$ chown -R nagios. /etc/nagios$ service nagios restart 其他资源nagios客户端： nagioschecker Firefox extension made as the statusbar indicator of the events from the network monitoring system Nagios. nagstamon Nagios status monitor Nagstamon is a Nagios status monitor which resides in systray or desktop (GNOME, KDE, Windows) as floating statusbar to inform you in realtime about the status of your hosts and services. Nagios Monitor for Android NagMonDroid retrieves the current problems from your Nagios install and displays them. It has a variable update frequency and can be set to vibrate on new update. 资料： CentOS下nagios报警飞信部署四步走]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>nagios</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[All Things OpenTSDB]]></title>
    <url>%2F2014%2F01%2F22%2Fall-things-opentsdb%2F</url>
    <content type="text"><![CDATA[1. OpenTSDB介绍OpenTSDB用HBase存储所有的时序（无须采样）来构建一个分布式、可伸缩的时间序列数据库。它支持秒级数据采集所有metrics，支持永久存储，可以做容量规划，并很容易的接入到现有的报警系统里。OpenTSDB可以从大规模的集群（包括集群中的网络设备、操作系统、应用程序）中获取相应的metrics并进行存储、索引以及服务，从而使得这些数据更容易让人理解，如web化、图形化等。 对于运维工程师而言，OpenTSDB可以获取基础设施和服务的实时状态信息，展示集群的各种软硬件错误，性能变化以及性能瓶颈。对于管理者而言，OpenTSDB可以衡量系统的SLA，理解复杂系统间的相互作用，展示资源消耗情况。集群的整体作业情况，可以用以辅助预算和集群资源协调。对于开发者而言，OpenTSDB可以展示集群的主要性能瓶颈，经常出现的错误，从而可以着力重点解决重要问题。 OpenTSDB使用LGPLv2.1+开源协议,目前版本为2.X。 官网地址：http://opentsdb.net/ 源代码：https://github.com/OpenTSDB/opentsdb/ 2. 安装OpenTSDB2.1 依赖OpenTSDB依赖jdk和Gnuplot，Gnuplot需要提前安装，版本要求为最小4.2,最大4.4,执行以下命令安装即可： 12yum install gnuplot autoconfapt-get install gnuplot OpenTSDB是用java编写的，但是项目构建不是用的java的方式而是使用的C、C++程序员构建项目的方式。运行时依赖： JDK 1.6 asynchbase 1.3.0 (BSD) Guava 12.0 (ASLv2) logback 1.0 (LGPLv2.1 / EPL) Netty 3.4 (ASLv2) SLF4J 1.6 (MIT) with Log4J and JCL adapters suasync 1.2 (BSD) ZooKeeper 3.3 (ASLv2) 可选的编译时依赖： GWT 2.4 (ASLv2) 可选的单元测试依赖： Javassist 3.15 (MPL / LGPL) JUnit 4.10 (CPL) Mockito 1.9 (MIT) PowerMock 1.4 (ASLv2) 2.2 下载并编译源代码首先安装必要依赖： 1yum install gnuplot automake autoconf git -y 下载源代码，可以指定最新版本或者手动checkout 123git clone git://github.com/OpenTSDB/opentsdb.gitcd opentsdb./build.sh 2.3 安装 首先安装一个单节点或者多节点集群的hbase环境，hbase版本要求为0.94 设置环境变量并创建opentsdb使用的表，需要设置的环境变量为COMPRESSION和HBASE_HOME，前者设置是否启用压缩，或者设置hbase home目录。如果使用压缩，则还需要安装lzo 执行建表语句src/create_table.sh 启动TSD 123tsdtmp=$&#123;TMPDIR-&apos;/tmp&apos;&#125;/tsd # For best performance, make suremkdir -p &quot;$tsdtmp&quot; # your temporary directory uses tmpfs./build/tsdb tsd --port=4242 --staticroot=build/staticroot --cachedir=&quot;$tsdtmp&quot; --auto-metric 如果你使用的是hbase集群，则你还需要设置--zkquorum，--cachedir对应的目录会产生一些临时文件，你可以设置cron定时任务进行删除。添加--auto-metric，则当新的数据被搜集时自动创建指标。 你可以将这些参数编写到配置文件中，然后通过--config指定该文件所在路径。 启动成功之后，你可以通过127.0.0.1:4242进行访问。 从源代码安装gnuplot、autoconf、opentsdb以及tcollector，可以参考：OpenTSDB &amp; tcollector 安装部署（Installation and Deployment） 3. 使用向导3.1 配置OpenTSDB的配置参数可以在命令行指定，也可以在配置文件中指定。配置文件使用的是java的properties文件，文件中key为小写，支持逗号连接字符串但是不能有空格。所有的OpenTSDB属性都以tsdb开头，例如： 12# List of Zookeeper hosts that manage the HBase clustertsd.storage.hbase.zk_quorum = 192.168.1.100 配置参数优先级： 命令行参数 &gt; 配置文件 &gt; 默认值 你可以在命令行中通过--config指定配置文件所在路径，如果没有指定，OpenTSDB会从以下路径寻找配置文件： ./opentsdb.conf /etc/opentsdb.conf /etc/opentsdb/opentsdb.conf /opt/opentsdb/opentsdb.conf 如果一个合法的配置文件没有找到并且一些必须参数没有设置，TSD进程将不会启动。 配置文件中可配置的属性请参考：Properties 3.2 基本概念在深入理解OpenTSDB之前，需要了解一些基本概念。 Cardinality。基数，在数学中定义为一个集合中的一些元素，在数据库中定义为一个索引的一些唯一元素，在OpenTSDB定义为： 一个给定指标的一些唯一时间序列 和一个标签名称相关联的一些唯一标签值 在OpenTSDB中拥有高基数的指标在查询过程中返回的值要多于低基数的指标，这样花费的时间也就越多。 Compaction。在OpenTSDB中，会将多列合并到一列之中以减少磁盘占用空间，这和hbase中的Compaction不一样。这个过程会在TSD写数据或者查询过程中不定期的发生。 Data Point。每一个指标可以被记录为某一个时间点的一个数值。Data Point包括以下部分： 一个指标：metric 一个数值 这个数值被记录的时间戳 多个标签 Metric。一个可测量的单位的标称。metric不包括一个数值或一个时间，其仅仅是一个标签，包含数值和时间的叫datapoints，metric是用逗号连接的不允许有空格，例如： hours.worked webserver.downloads accumulation.snow Tags。一个metric应该描述什么东西被测量，在OpenTSDB中，其不应该定义的太简单。通常，更好的做法是用Tags来描述具有相同维度的metric。Tags由tagk和tagv组成，前者表示一个分组，后者表示一个特定的项。 Time Series。一个metric的带有多个tag的data point集合。 Timestamp。一个绝对时间，用来描述一个数值或者一个给定的metric是在什么时候定义的。 Value。一个Value表示一个metric的实际数值。 UID。在OpenTSDB中，每一个metric、tagk或者tagv在创建的时候被分配一个唯一标识叫做UID，他们组合在一起可以创建一个序列的UID或者TSUID。在OpenTSDB的存储中，对于每一个metric、tagk或者tagv都存在从0开始的计数器，每来一个新的metric、tagk或者tagv，对应的计数器就会加1。当data point写到TSD时，UID是自动分配的。你也可以手动分配UID，前提是auto metric被设置为true。默认地，UID被编码为3Bytes，每一种UID类型最多可以有16,777,215个UID。你也可以修改源代码改为4Bytes。UID的展示有几种方式，最常见的方式是通过http api访问时，3 bytes的UID被编码为16进制的字符串。例如，UID为1的写为二进制的形式为000000000000000000000001，最为一个无符号的byte数组，其可以表示为[0,0,1]，编码为16进制字符串为000001,其中每一位左边都被补上0,如果其不足两位。故，UID为255的会显示为[0,0,255]和0000FF。 关于为什么使用UID而不使用hashes，可以参考：why-uids TSUID。当一个data point被写到OpenTSDB时，其row key格式为：&lt;metric_UID&gt;&lt;timestamp&gt;&lt;tagk1_UID&gt;&lt;tagv1_UID&gt;[...&lt;tagkN_UID&gt;&lt;tagvN_UID&gt;]，不考虑时间戳的话，将其余部分都转换为UID，然后拼在一起，就可以组成为TSUID。 Metadata。主要用于记录data point的一些附加的信息，方便搜索和跟踪，分为UIDMeta和TSMeta。 每一个UID都有一个metadata记录保存在tsdb-uid表中，每一个UID包括一些不可变的字段，如uid、type、name和created字段表示什么时候被创建，还可以有一些额外字段，如description、notes、displayName和一些custom key/value对，详细信息，可以查看 /api/uid/uidmeta 同样，每一个TSUID可以对应一个TSMeta，记录在tsdb-uid中，其包括的字段有tsuid、metric、tags、lastReceived和created，可选的字段有description, notes，详细信息，可以查看/api/uid/tsmeta 开启Metadata有以下几个参数： tsd.core.meta.enable_realtime_uid tsd.core.meta.enable_tsuid_tracking tsd.core.meta.enable_tsuid_incrementing tsd.core.meta.enable_realtime_ts metadata的另外一个形式是Annotations，详细说明，请参考annotations Tree 3.3 数据存储方式OpenTSDB使用HBase作为后端存储，在安装OpenTSDB之前，需要先启动一个hbase节点或者集群，然后再执行建表语句src/create_table.sh创建hbase表。建表语句如下： 123456789101112create &apos;$UID_TABLE&apos;, &#123;NAME =&gt; &apos;id&apos;, COMPRESSION =&gt; &apos;$COMPRESSION&apos;, BLOOMFILTER =&gt; &apos;$BLOOMFILTER&apos;&#125;, &#123;NAME =&gt; &apos;name&apos;, COMPRESSION =&gt; &apos;$COMPRESSION&apos;, BLOOMFILTER =&gt; &apos;$BLOOMFILTER&apos;&#125;create &apos;$TSDB_TABLE&apos;, &#123;NAME =&gt; &apos;t&apos;, VERSIONS =&gt; 1, COMPRESSION =&gt; &apos;$COMPRESSION&apos;, BLOOMFILTER =&gt; &apos;$BLOOMFILTER&apos;&#125; create &apos;$TREE_TABLE&apos;, &#123;NAME =&gt; &apos;t&apos;, VERSIONS =&gt; 1, COMPRESSION =&gt; &apos;$COMPRESSION&apos;, BLOOMFILTER =&gt; &apos;$BLOOMFILTER&apos;&#125; create &apos;$META_TABLE&apos;, &#123;NAME =&gt; &apos;name&apos;, COMPRESSION =&gt; &apos;$COMPRESSION&apos;, BLOOMFILTER =&gt; &apos;$BLOOMFILTER&apos;&#125; 从上面可以看出一共创建了4张表，并且可以设置是否压缩、是否启用布隆过滤、保存版本号等等，如果追求hbase读写性能，还可以预建分区。 3.3.1 Data Table Schema在OpenTSDB中，所有数据存储在一张叫做tsdb的表中，这是为了充分利用hbase有序和region分布式的特点。所有的值都保存在列族t中。 rowkey为&lt;metric_uid&gt;&lt;timestamp&gt;&lt;tagk1&gt;&lt;tagv1&gt;[...&lt;tagkN&gt;&lt;tagvN&gt;]，UID默认编码为3 Bytes，而时间戳会编码为4 Bytes OpenTSDB的tsdb启动之后，会监控指定的socket端口（默认为4242），接收到监控数据，包括指标、时间戳、数据、tag标签，tag标签包括tag名称ID和tag值ID。例如： 1myservice.latency.avg 1292148123 42 reqtype=foo host=web42 对于指标myservice.latency.avg的ID为：[0, 0, -69],reqtype标签名称的ID为：[0, 0, 1], foo标签值的ID为：[0, 1, 11], 标签名称的ID为：[0, 0, 2] web42标签值的ID为：[0, -7, 42]，他们组成rowkey： 12345[0, 0, -69, 77, 4, -99, 32, 0, 0, 1, 0, 1, 11, 0, 0, 2, 0, -7, 42] `-------&apos; `------------&apos; `-----&apos; `------&apos; `-----&apos; `-------&apos; metric ID base timestamp name ID value ID name ID value ID `---------------&apos; `---------------&apos; first tag second tag row表示格式为： 每个数字对应1 byte [0, 0, -69] metric ID [77, 4, -99, 32] base timestamp = 1292148000. timestamps in the row key are rounded down to a 60 minute boundary。也就是说对于同一个小时的metric + tags相同的数据都会存放在一个row下面 [0, 0, 1] “reqtype” index [0, 1, 11] “foo” index [0, 0, 2] “host” index [0, -7, 42] “web42” index NOTE：可以看到，对于metric + tags相同的数据都会连续存放，且metic相同的数据也会连续存放，这样对于scan以及做aggregation都非常有帮助 column qualifier 占用2 bytes或者4 bytes，占用2 bytes时表示以秒为单位的偏移，格式为： 12 bits:相对row表示的小时的delta, 最多2^ 12 = 4096 &gt; 3600因此没有问题 4 bits:format flags 1 bit: an integer or floating point 3 bits: 标明数据的长度，其长度必须是1、2、4、8。000表示1个byte,010表示2byte，011表示4byte，100表示8byte 占用4 bytes时表示以毫秒为单位的偏移，格式为： 4 bits：十六进制的1或者F 22 bits:毫秒偏移 2 bit:保留 4 bits: format flags 1 bit: an integer or floating point，0表示整数,1表示浮点数 3 bits: 标明数据的长度，其长度必须是1、2、4、8。000表示1个byte,010表示2byte，011表示4byte，100表示8byte 举例： 对于时间戳为1292148123的数据点来说，其转换为以小时为单位的基准时间(去掉小时后的秒）为129214800,偏移为123,转换为二进制为1111011，因为该值为整数且长度为8位（对应为2byte，故最后3bit为100）,故其对应的列族名为：0000011110110100，将其转换为十六进制为07B4 value 使用8bytes存储，既可以存储long,也可以存储double。 总结一下，tsdb表结构如下： 3.3.2 UID Table Schema一个单独的较小的表叫做tsdb-uid用来存储UID映射，包括正向的和反向的。存在两列族，一列族叫做name用来将一个UID映射到一个字符串，另一个列族叫做id，用来将字符串映射到UID。列族的每一行都至少有以下三列中的一个： metrics 将metric的名称映射到UID tagk 将tag名称映射到UID tagv 将tag的值映射到UID 如果配置了metadata，则name列族还可以包括额外的metatata列。 id 列族 Row Key - 将会是一个分配到UID的字符串，例如，对于一个指标可能有一个值为sys.cpu.user或者对于一个标签其值可能为42 Column Qualifiers - 上面三种列类型中一种。 Column Value - 一个无符号的整数，默认被编码为3个byte，其值为UID。 例如以下几行数据是从tsdb-uid表中查询出来的数据，第一个列为row key，第二列为”列族:列名”，第三列为值，对应为UID 123proc.stat.cpu id:metrics \x00\x00\x01host id:tagk \x00\x00\x01cdh1 id:tagv \x00\x00\x01 name 列族 Row Key - 为UID Column Qualifiers - 上面三种列类型中一种或者为metrics_meta、tagk_meta、tagv_meta Column Value - 与UID对应的字符串，对于一个*_meta列，其值将会是一个UTF-8编码的JSON格式字符串。不要在OpenTSDB外部去修改该值，其中的字段顺序会影响CAS调用。 例如,以下几行数据是从tsdb-uid表中查询出来的数据，第一个列为row key，第二列为”列族:列名”，第三列为值，对应为UID 123456\x00\x00\x01 name:metrics proc.stat.cpu\x00\x00\x01 name:tagk host\x00\x00\x01 name:tagv cdh1\x00\x00\x01 name:tagk_meta &#123;&quot;uid&quot;:&quot;000001&quot;,&quot;type&quot;:&quot;TAGK&quot;,&quot;name&quot;:&quot;host&quot;,&quot;description&quot;:&quot;&quot;,&quot;notes&quot;:&quot;&quot;,&quot;created&quot;:1395213193,&quot;custom&quot;:null,&quot;displayName&quot;:&quot;&quot;&#125;\x00\x00\x01 name:tagv_meta &#123;&quot;uid&quot;:&quot;000001&quot;,&quot;type&quot;:&quot;TAGV&quot;,&quot;name&quot;:&quot;cdh1&quot;,&quot;description&quot;:&quot;&quot;,&quot;notes&quot;:&quot;&quot;,&quot;created&quot;:1395213193,&quot;custom&quot;:null,&quot;displayName&quot;:&quot;&quot;&#125;\x00\x00\x01 name:metric_meta &#123;&quot;uid&quot;:&quot;000001&quot;,&quot;type&quot;:&quot;METRIC&quot;,&quot;name&quot;:&quot;metrics proc.stat.cpu&quot;,&quot;description&quot;:&quot;&quot;,&quot;notes&quot;:&quot;&quot;,&quot;created&quot;:1395213193,&quot;custom&quot;:null,&quot;displayName&quot;:&quot;&quot;&#125; 总结一下，tsdb-uid表结构如下： 上图对应的一个datapoint如下： 1proc.stat.cpu 1292148123 80 host=cdh1 从上图可以看出tsdb-uid的表结构以及数据存储方式，对于一个data point来说，其被保存到opentsdb之前，会对metrics、tagk、tagv、metric_meta、tagk_meta、tagv_meta生成一个UID（如上图中的000001）,然后将其插入hbase表中，rowkey为UID，同时会存储多行记录，分别保存metrics、tagk、tagv、metric_meta、tagk_meta、tagv_meta到UID的映射。 3.3.3 Meta Table Schema这个表是OpenTSDB中不同时间序列的一个索引，可以用来存储一些额外的信息。这个表名称叫做tsdb-meta，该表只有一个列族name，两个列，分别为ts_meta、ts_ctr，该表中数据如下： 12345\x00\x00\x01\x00\x00\x01\x00\x00\x01 name:ts_ctr \x00\x00\x00\x00\x00\x00\x00p\x00\x00\x01\x00\x00\x01\x00\x00\x01 name:ts_meta &#123;&quot;tsuid&quot;:&quot;000001000001000001&quot;,&quot;displayName&quot;:&quot;&quot;,&quot;description&quot;:&quot;&quot;,&quot;notes&quot;:&quot;&quot;,&quot;created&quot;:1395213196,&quot;custom&quot;:null,&quot;units&quot;:&quot;&quot;,&quot;dataType&quot;:&quot;&quot;,&quot;retention&quot;:0,&quot;max&quot;:&quot;NaN&quot;,&quot;min&quot;:&quot;NaN&quot;&#125;\x00\x00\x02\x00\x00\x01\x00\x00\x01 name:ts_ctr \x00\x00\x00\x00\x00\x00\x00p\x00\x00\x02\x00\x00\x01\x00\x00\x01 name:ts_meta &#123;&quot;tsuid&quot;:&quot;000002000001000001&quot;,&quot;displayName&quot;:&quot;&quot;,&quot;description&quot;:&quot;&quot;,&quot;notes&quot;:&quot;&quot;,&quot;created&quot;:1395213196,&quot;custom&quot;:null,&quot;units&quot;:&quot;&quot;,&quot;dataType&quot;:&quot;&quot;,&quot;retention&quot;:0,&quot;max&quot;:&quot;NaN&quot;,&quot;min&quot;:&quot;NaN&quot;&#125; Row Key 和tsdb表一样，其中不包含时间戳，&lt;metric_uid&gt;&lt;tagk1&gt;&lt;tagv1&gt;[...&lt;tagkN&gt;&lt;tagvN&gt;] TSMeta Column 和UIDMeta相似，其为UTF-8编码的JSON格式字符串 ts_ctr Column 计数器，用来记录一个时间序列中存储的数据个数，其列名为ts_ctr，为8位有符号的整数。 3.3.4 Tree Table Schema索引表，用于展示树状结构的，类似于文件系统，以方便其他系统使用，例如：Graphite 3.4 如何写数据3.5 如何查询数据3.6 CLI Toolstsdb支持以下参数： 123[root@cdh1 build]# ./tsdb usage: tsdb &lt;command&gt; [args]Valid commands: fsck, import, mkmetric, query, tsd, scan, uid 通过以下命令创建指标： 1./tsdb mkmetric mysql.bytes_received mysql.bytes_sent 执行上述命令的结果如下： 12metrics mysql.bytes_received: [0, 0, -93]metrics mysql.bytes_sent: [0, 0, -92] 3.11 Utilities3.12 Logging4. HTTP API5. 谁在用OpenTSDB StumbleUpon StumbleUpon is the easiest way to find cool new websites, videos, photos and images from across the Web box Box simplifies online file storage, replaces FTP and connects teams in online workspaces. tumblr 一个轻量级博客，用户可以跟进其他的会员并在自己的页面上看到跟进会员发表的文章，还可以转发他人在Tumblr上的文章 6. KairosDB KairosDB是一个快速可靠的分布式时间序列数据库，主要用于Cassandra当然也可以适用与HBase。KairosDB是在OpenTSDB基础上重写的，他不仅可以在HBase上存储数据还支持Cassandra。 KairosDB主页：https://code.google.com/p/kairosdb/ 7. 参考资料 tlog数据存储 OpenTSDB源码分析系列文章 OpenTSDB的设计之道 opentsdb]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>hbase</tag>
        <tag>opentsdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[All Things Jekyll]]></title>
    <url>%2F2014%2F01%2F21%2Fall-things-about-jekyll%2F</url>
    <content type="text"><![CDATA[Jekyll是一个简洁的、特别针对博客平台的静态网站生成器。它使用一个模板目录作为网站布局的基础框架，并在其上运行Textile、Markdown或Liquid标记语言的转换器，最终生成一个完整的静态Web站点，可以被放置在Apache或者你喜欢的其他任何Web服务器上。它同时也是GitHub Pages、一个由GitHub提供的用于托管项目主页或博客的服务，在后台所运行的引擎。 1. 安装Jekyll使用动态脚本语言Ruby写成。请首先下载并安装Ruby，目前需要的ruby版本为1.9.1。 在使用Jekyll之前，你可能想要对Ruby语言有一些初步了解（非必需）。 安装Jekyll的最好方式是通过RubyGems： 1gem install jekyll Jekyll依赖以下的gems模块：liquid、fast-stemmer、classifier、directory_watcher、syntax、maruku、kramdown、posix-spawn和albino。它们会被gem install命令自动安装。 2. 模板引擎2.1 RDiscount如果你想用 RDiscount 取代 Maruku 作为你的Markdown标记语言转换引擎，只需确认安装： 1gem install rdiscount 在你站点下的_config.yml文件中加入以下配置： 1markdown: rdiscount 2.2 RedCloth若要使用Textile标记语言，需要安装相应的转换引擎RedCloth。 1gem install RedCloth 2.3 RedcarpetRedcarpet是由GitHub自己人开发的，一直以来它被用于在GitHub上渲染Markdown格式文本（也就是GitHub Flavored Markdown）。由于API兼容性的原因，Jekyll以前的版本并不支持Redcarpet。现在，Jekyll 0.12.0终于增加了对Redcarpet 2引擎的支持，只需安装： 1$ gem install redcarpet 把_config.yml中的Markdown引擎设置从： 1markdown: rdiscount 改为： 1markdown: redcarpet 就可以迁移到Redcarpet上了。 Redcarpet所支持的GitHub Flavored Markdown比起标准的Markdown语法来增加了不少便利之处，诸如围栏式代码块（Fenced code blocks）。 redcarpet 有很多很多的options可以设置，见：https://github.com/vmg/redcarpet edcarpet 只和 markdown parser有关，如果需要设置高亮，可以使用pygments 你可能会用到的标记语言和模板引擎： Textile 可读性好的轻量级标记语言，可以被转换成XHTML格式。 Textile Home Page A Textile Reference RedCloth Ruby的Textile实现引擎。 Markdown 另一种Jekyll所支持的轻量级标记语言。 Markdown Home Page BlueCloth Ruby的Markdown实现引擎。 Maruku Ruby的另一个Markdown实现引擎，效率较高。 RDiscount Ruby的另一个Markdown实现引擎，效率比Maruku更高。 Liquid Ruby的模板渲染引擎。它也是Jekyll所使用的模板引擎。 Liquid for Designers Liquid for Programmers 3. 基本结构Jekyll从核心上来说是一个文本转换引擎。该系统内部的工作原理是：你输入一些用自己喜爱的标记语言格式书写的文本，可以是Markdown、Textile或纯粹的HTML，它将这些文本混合后放入一个或一整套页面布局当中。在整个过程中，你可以自行决定你的站点URL的模式、以及哪些数据将被显示在页面中，等等。这一切都将通过严格的文本编辑完成，而生成的Web界面则是最终的产品。 一个典型的Jekyll站点通常具有如下结构： 123456789.|-- _config.yml|-- _includes|-- _layouts| |-- default.html| `-- post.html|-- _posts|-- _site`-- index.html 以下是每部分功能的简述： _config.yml。保存Jekyll配置的文件。虽然绝大部分选项可以通过命令行参数指定，但将它们写入配置文件可以使你在每次执行时不必记住它们。 _includes。该目录存放可以与_layouts和_posts混合、匹配并重用的文件。Liquid标签{ % include % }可以用于嵌入文件_includes/file.ext。 _layouts。该目录存放用来插入帖子的网页布局模板。页面布局基于类似博客平台的一个帖子接一个帖子的原则，通过YAML前置数据定义。Liquid标签用于在页面上插入帖子的文本内容。 _posts。该目录下存放的可以说成是你的”动态内容”。这些文件的格式很重要，它们的命名模式必须遵循 YEAR-MONTH-DATE-title.MARKUP 。每一个帖子的固定链接URL可以作弹性的调整，但帖子的发布日期和转换所使用的标记语言会根据且仅根据文件名中的相应部分来识别。 _site。这里是Jekyll用以存放最终生成站点的根路径位置。也许把它加到你的 .gitignore 列表中会是个不错的主意。 4. 运行、部署通常直接在命令行下使用可执行的Ruby脚本 jekyll ，它可以从gem安装。如果要启动一个临时的Web服务器并测试你的Jekyll站点，执行： 1jekyll --server 然后在浏览器中访问 http://localhost:4000 或 http://0.0.0.0:4000 。当然这里还有其他许多参数选项可以使用。 由于Jekyll所做的仅仅是生成一个包含HTML等静态网站文件的目录（_site），它可以通过简单的拷贝（scp）、远程同步（rsync）、ftp上传或git等方式部署到任何Web服务器上，例如github、gitcafe、qiniu。 5. 一些技巧使用表格使用redcarpet模板引擎，通过gem安装redcarpet并修改_config.yml 123markdown: redcarpetredcarpet: extensions: [&quot;tables&quot;] 在 Markdwon 文件中可以依据以下语法进行书写 1234|head1 head1 head1|head2 head2 head2|head3 head3 head3|head4 head4 head4||---|:---|:---:|---:||row1text1|row1text3|row1text3|row1text4||row2text1|row2text3|row2text3|row2text4| 然后添加如下样式： 12345678910111213141516171819202122232425table &#123;margin-bottom: 20px;max-width: 100%;border-collapse: collapse;transition: all 0.3s;border: 1px solid rgba(0,0,0,0.167);&#125;table &#123;border-collapse: collapse;&#125;table thead&gt;tr &#123;background-color: rgba(249,249,249,0.9);&#125;table thead tr th &#123;border: 1px solid rgba(0,0,0,0.167);border-top: 0px none;border-bottom-width: 2px;vertical-align: bottom;padding: 8px;&#125;table tbody tr td &#123;border: 1px solid rgba(0,0,0,0.167);vertical-align: top;padding: 8px;&#125; 6. 其他静态网站生成器如果想要尝试一些其他的静态网页生成器，这里是一个简略的列表： Ruby Jekyll Bonsai 一个非常简单（但实用）的小脚本 Webgen 一个较复杂的生成器 Python Hyde Jekyll的Python语言实现版本 Cyrax 使用Jinja2模板引擎的生成器 PHP Phrozn PHP语言实现的静态网站生成器 nodejs hexo一个台湾人实现的生成器 papery 纯nodejs编写 DocPad static web sites generator using Node.js 更详细的列表和介绍请参见： Static Blog Generators 32 Static Website Generators For Your Site, Blog Or Wiki 7. 资源 jekyll官网 Jekyll Bootstrap 搭建一个免费的，无限流量的Blog—-github Pages和Jekyll入门 告别wordpress，拥抱jekyll 像黑客一样写博客——Jekyll入门 用 Jekyll 和 Octopress 搭建博客，哪个更合适？ Play with Jekyll 创造者 非常规思维研究所 Keep on Fighting! Havee’s Space 闭门造轮子]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>jekyll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH远程连接时环境变量问题]]></title>
    <url>%2F2014%2F01%2F18%2Fbash-problem-when-ssh-access%2F</url>
    <content type="text"><![CDATA[1. 问题RHEL服务器A有个启动脚本（普通用户user01运行），里面使用ifconfig获取ip地址如下： 1Localhost_ip=$(ifconfig |awk -F &apos;addr:|Bcast&apos; &apos;/Bcast/&#123;print $2&#125;&apos;) 由于普通用户user01不能直接识别ifconfig命令，只能使用全路径/sbin/ifconfig，目前处理方式为修改~/.bash_profile文件添加环境变量如下： 1PATH=$PATH:$HOME/bin 改成如下： 1PATH=$PATH:$HOME/bin:/sbin 经过如上配置后服务器本机user01用户登录执行XX.sh脚本可以识别ifconfig命令。 出现如下问题： 远程主机B通过ssh远程执行启动脚本XX.sh，报错如下： 1bash: ifconfg: command not found 2. 问题分析测试前准备，追加调用说明语句，如下： 123456789101112/etc/profile[root@node3 ~]# vim /etc/profileecho &quot;/etc/profile begin:&quot;echo &quot;$PATH&quot;... unset iunset pathmunge echo &quot;invoke /etc/profile&quot;echo &quot;$PATH&quot;echo &quot;&quot; 1234567[root@node3 ~]# vim /etc/bashrc...# vim:ts=4:sw=4 echo &quot;invoke /etc/bashrc&quot;echo &quot;$PATH&quot;echo &quot;&quot; 12345678[root@node3 ~]# vim /root/.bash_profile# User specific environment and startup programsPATH=$PATH:$HOME/binexport PATH echo &quot;invoke ~/.bash_profile&quot;echo &quot;$PATH&quot;echo &quot;&quot; 12345678910[root@node3 ~]# vim /root/.bashrc...# Source global definitionsif [ -f /etc/bashrc ]; then . /etc/bashrcfi echo &quot;invoke ~/.bashrc&quot;echo &quot;$PATH&quot;echo &quot;&quot; 123456789[root@node3 ~]# vim /home/user01/.bash_profile...# User specific environment and startup programsPATH=$PATH:$HOME/binexport PATH echo &quot;invoke ~/.bashrc&quot;echo &quot;$PATH&quot;echo &quot;&quot; 12345678910111213141516171819202122[root@node3 ~]# vim /home/user01/.bashrc...# User specific aliases and functions echo &quot;invoke ~/.bashrc&quot;echo &quot;$PATH&quot;echo &quot;&quot;~~~ 分 user 和 root 用户，3 种场景进行测试，如下：## 普通用户 User### 场景1：本机使用 su 命令切换到普通用户 （属于 Login 方式）结论：- Login 之前，系统 PATH 为：`/usr/local/bin:/bin:/usr/bin`- Login 方式，文件调用顺序为： `/etc/profile -&gt; /etc/bashrc -&gt; ~/.bashrc -&gt; ~/.bash_profile`- Login 之后，系统 PATH 为：`/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/user01/bin` su - user[root@node3 ~]# hostname -i192.168.122.33 12 [root@node3 ~]# su - user01/etc/profile begin:/usr/local/bin:/bin:/usr/bininvoke /etc/profile/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin invoke /etc/bashrc/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin invoke ~/.bashrc/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin invoke ~/.bash_profile/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/user01/bin 12345678### 场景2：远程机使用 ssh 命令以普通用户身份登陆到主机 （属于 Login 方式）结论：- 与在本机使用 su 命令切换到普通用户的效果完全一样！ ssh user@remote_server_ip[root@node1 ~]# hostname -i192.168.122.31 12 [root@node1 ~]# ssh user01@192.168.122.33user01@192.168.122.33‘s password:Last login: Tue Jul 9 16:23:33 2013 from 192.168.122.31/etc/profile begin:/usr/local/bin:/bin:/usr/bininvoke /etc/profile/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin invoke /etc/bashrc/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin invoke ~/.bashrc/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin invoke ~/.bash_profile/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/user01/bin 12345678910## 场景3：远程机使用 ssh 命令以普通用户身份连接到主机执行获取 PATH 的命令 （属于 NoLogin 方式）结论：- NoLogin 方式，命令获取的 PATH 为该远程机的，并未拿到目标主机的 PATH- NoLogin 方式，文件调用顺序为：`/etc/bashrc -&gt; ~/.bashrc`- NoLogin 方式，目标主机 User 用户 PATH 为：`/usr/local/bin:/bin:/usr/bin` ssh user@remote_server_ip command[root@node1 ~]# hostname -i192.168.122.31 123456789101112131415~~~ [root@node1 ~]# echo $PATH/usr/local/rabbitmq/sbin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin [root@node1 ~]# ssh user01@192.168.122.33 &quot;echo $PATH&quot;user01@192.168.122.33&apos;s password: invoke /etc/bashrc/usr/local/bin:/bin:/usr/bin invoke ~/.bashrc/usr/local/bin:/bin:/usr/bin /usr/local/rabbitmq/sbin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin 对比 root 用户场景1：本机使用 su 命令切换到 root （属于 Login 方式） 结论： Login 之前，系统 PATH 为：/usr/local/bin:/bin:/usr/bin Login 方式，root 用户，文件调用顺序为：/etc/profile -&gt; /etc/bashrc -&gt; ~/.bashrc -&gt; ~/.bash_profile Login 之后，系统 PATH 为：/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin 123su - root[root@node3 ~]# hostname -i192.168.122.33 1234567891011121314[root@node3 ~]# su - root/etc/profile begin:/usr/local/bin:/bin:/usr/bininvoke /etc/profile/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin invoke /etc/bashrc/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin invoke ~/.bashrc/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin invoke ~/.bash_profile/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin 场景2：远程机使用 ssh 命令以 root 用户身份登陆到主机 （属于 Login 方式） 结论： 与在本机使用 su 命令切换到 root 用户的效果完全一样！ 123ssh root@remote_server_ip[root@node1 ~]# hostname -i192.168.122.31 12345678910111213141516[root@node1 ~]# ssh root@192.168.122.33root@192.168.122.33&apos;s password: Last login: Tue Jul 9 15:54:53 2013 from 192.168.122.1/etc/profile begin:/usr/local/bin:/bin:/usr/bininvoke /etc/profile/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin invoke /etc/bashrc/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin invoke ~/.bashrc/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin invoke ~/.bash_profile/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin 场景3远程机使用 ssh 命令以 root 用户身份连接到主机执行获取 PATH 的命令 （属于 NoLogin 方式） 结论： NoLogin 方式，命令获取的 PATH 为该远程机的，并未拿到目标主机的 PATH NoLogin 方式，文件调用顺序为：/etc/bashrc -&gt; ~/.bashrc NoLogin 方式，目标主机 root 用户 PATH 为：/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin 123ssh root@remote_server_ip command[root@node1 ~]# hostname -i192.168.122.31 12345678910111213141516171819[root@node1 ~]# echo $PATH/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin [root@node1 ~]# ssh root@192.168.122.33 &quot;echo $PATH&quot;root@192.168.122.33&apos;s password: invoke /etc/bashrc/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin invoke ~/.bashrc/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin /usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin~~~ ### 关于/etc/profile 文件部分代码分析结论：- 无论 root 还是 user ，只有调用此文件，其 PATH 中才会被追加 sbin 相关路径。而由以上测试场景可知，只有 Login 时，`/etc/profile` 文件才会被调用。 pathmunge () { case “:${PATH}:” in :”$1”:) ;; *) if [ “$2” = “after” ] ; then PATH=$PATH:$1 else PATH=$1:$PATH fi esac} if [ -x /usr/bin/id ]; then if [ -z “$EUID” ]; then # ksh workaround EUID=`id -u` UID=`id -ru` fi USER=&quot;`id -un`&quot; LOGNAME=$USER MAIL=&quot;/var/spool/mail/$USER&quot;fi Path manipulationif [ “$EUID” = “0” ]; then pathmunge /sbin pathmunge /usr/sbin pathmunge /usr/local/sbinelse pathmunge /usr/local/sbin after pathmunge /usr/sbin after pathmunge /sbin after fi # 3. 总结 综上，如需修改 PATH，建议修改 bashrc 文件，从而保证任何方式访问时 PATH 的正确性。]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase笔记：Region拆分策略]]></title>
    <url>%2F2014%2F01%2F16%2Fhbase-region-split-policy%2F</url>
    <content type="text"><![CDATA[Region 概念Region是表获取和分布的基本元素，由每个列族的一个Store组成。对象层级图如下： 123456Table (HBase table) Region (Regions for the table) Store (Store per ColumnFamily for each Region for the table) MemStore (MemStore for each Store for each Region for the table) StoreFile (StoreFiles for each Store for each Region for the table) Block (Blocks within a StoreFile within a Store for each Region for the table) Region 大小Region的大小是一个棘手的问题，需要考量如下几个因素。 Region是HBase中分布式存储和负载均衡的最小单元。不同Region分布到不同RegionServer上，但并不是存储的最小单元。 Region由一个或者多个Store组成，每个store保存一个columns family，每个Strore又由一个memStore和0至多个StoreFile 组成。memStore存储在内存中， StoreFile存储在HDFS上。 HBase通过将region切分在许多机器上实现分布式。也就是说，你如果有16GB的数据，只分了2个region， 你却有20台机器，有18台就浪费了。 region数目太多就会造成性能下降，现在比以前好多了。但是对于同样大小的数据，700个region比3000个要好。 region数目太少就会妨碍可扩展性，降低并行能力。有的时候导致压力不够分散。这就是为什么，你向一个10节点的HBase集群导入200MB的数据，大部分的节点是idle的。 RegionServer中1个region和10个region索引需要的内存量没有太多的差别。 最好是使用默认的配置，可以把热的表配小一点(或者受到split热点的region把压力分散到集群中)。如果你的cell的大小比较大(100KB或更大)，就可以把region的大小调到1GB。region的最大大小在hbase配置文件中定义： 1234&lt;property&gt; &lt;name&gt;hbase.hregion.max.filesize&lt;/name&gt; &lt;value&gt;10 * 1024 * 1024 * 1024&lt;/value&gt; &lt;/property&gt; 说明： 当region中的StoreFile大小超过了上面配置的值的时候，该region就会被拆分，具体的拆分策略见下文。 上面的值也可以针对每个表单独设置，例如在hbase shell中设置： 1234create 't','f'disable 't'alter 't', METHOD =&gt; 'table_att', MAX_FILESIZE =&gt; '134217728'enable 't' Region 拆分策略Region的分割操作是不可见的，因为Master不会参与其中。RegionServer拆分region的步骤是，先将该region下线，然后拆分，将其子region加入到META元信息中，再将他们加入到原本的RegionServer中，最后汇报Master。 执行split的线程是CompactSplitThread。 自定义拆分策略可以通过设置RegionSplitPolicy的实现类来指定拆分策略，RegionSplitPolicy类的实现类有： 1234ConstantSizeRegionSplitPolicy IncreasingToUpperBoundRegionSplitPolicy DelimitedKeyPrefixRegionSplitPolicy KeyPrefixRegionSplitPolicy 对于split，并不是设置了hbase.hregion.max.filesize（默认10G）为很大就保证不split了，需要有以下的算法： IncreasingToUpperBoundRegionSplitPolicy，0.94.0默认region split策略。根据公式min(r^2*flushSize，maxFileSize)确定split的maxFileSize，其中r为在线region个数，maxFileSize由hbase.hregion.max.filesize指定。 ConstantSizeRegionSplitPolicy，仅仅当region大小超过常量值（hbase.hregion.max.filesize大小）时，才进行拆分。 DelimitedKeyPrefixRegionSplitPolicy，保证以分隔符前面的前缀为splitPoint，保证相同RowKey前缀的数据在一个Region中 KeyPrefixRegionSplitPolicy，保证具有相同前缀的row在一个region中（要求设计中前缀具有同样长度）。指定rowkey前缀位数划分region，通过读取table的prefix_split_key_policy.prefix_length属性，该属性为数字类型，表示前缀长度，在进行split时，按此长度对splitPoint进行截取。此种策略比较适合固定前缀的rowkey。当table中没有设置该属性，或其属性不为Integer类型时，指定此策略效果等同与使用IncreasingToUpperBoundRegionSplitPolicy。 IncreasingToUpperBoundRegionSplitPolicy这是0.94.0默认region split策略。根据根据公式min(r^2*flushSize，maxFileSize)确定split的maxFileSize，这里假设flushSize为128M： 123456第一次拆分大小为：min(10G，1*1*128M)=128M第二次拆分大小为：min(10G，3*3*128M)=1152M第三次拆分大小为：min(10G，5*5*128M)=3200M第四次拆分大小为：min(10G，7*7*128M)=6272M第五次拆分大小为：min(10G，9*9*128M)=10G第五次拆分大小为：min(10G，11*11*128M)=10G 可以看到，只有在第四次之后的拆分大小才为10G 配置拆分策略你可以在hbase配置文件中定义全局的拆分策略，设置hbase.regionserver.region.split.policy的值即可，也可以在创建和修改表时候指定： 1234567891011// 更新现有表的split策略HBaseAdmin admin = new HBaseAdmin( conf);HTable hTable = new HTable( conf, "test" );HTableDescriptor htd = hTable.getTableDescriptor();HTableDescriptor newHtd = new HTableDescriptor(htd);newHtd.setValue(HTableDescriptor. SPLIT_POLICY, KeyPrefixRegionSplitPolicy.class .getName());// 指定策略newHtd.setValue("prefix_split_key_policy.prefix_length", "2");newHtd.setValue("MEMSTORE_FLUSHSIZE", "5242880"); // 5Madmin.disableTable( "test");admin.modifyTable(Bytes. toBytes("test"), newHtd);admin.enableTable( "test"); 说明： 上面的不同策略可以在不同的业务场景下使用，特别是第三种和第四种一般关注和使用的比较少。 如果想关闭自动拆分改为手动拆分，建议同时修改hbase.hregion.max.filesize和hbase.regionserver.region.split.policy值。 参考文章 [1] HBase的Compact和Split源码分析与应用–基于0.94.5 [2] HBase源码分析之org.apache.hadoop.hbase.regionserver包 [3] HBase 官方文档中文版 [4] hbase region split策略]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim配置和插件管理]]></title>
    <url>%2F2014%2F01%2F14%2Fvim-config-and-plugins%2F</url>
    <content type="text"><![CDATA[这篇文章主要是记录vim配置中各个配置项的含义并且收藏一些常用的插件及其使用方法。 1. Vim配置目前我的vimrc配置放置在:https://github.com/javachen/snippets/blob/master/dotfiles/.vimrc，其中大多数用英文注释。 2. 插件管理使用 pathogen来管理插件 项目地址: https://github.com/tpope/vim-pathogen 安装方法： 12$ mkdir -p ~/.vim/autoload ~/.vim/bundle &amp;&amp; \$ curl -LSso ~/.vim/autoload/pathogen.vim https://tpo.pe/pathogen.vim 要记得把以下内容加入到vimrc文件中: 1execute pathogen#infect() 3. 安装插件3.1 NERDTreeNERD tree允许你在Vim编辑器中以树状方式浏览系统中的文件和目录, 支持快捷键与鼠标操作, 使用起来十分方便. NERD tree能够以不同颜色高亮显示节点类型, 并包含书签, 过滤等实用功能. 配合taglist或txtviewer插件, 右边窗口显示本文件夹的文件, 左边窗口显示本文的文档结构, 将会使管理一个工程变得相当容易. 项目地址: https://github.com/scrooloose/nerdtree 安装方法很简单，只要把项目clone一份到bundle目录就可以了。 12cd ~/.vim/bundlegit clone https://github.com/scrooloose/nerdtree.git 之后的插件也都是这么安装。 使用： 在linux命令行界面，输入vim 输入:NERDTree ，回车，默认打开当前目录，当然可以打开指定目录，如 :NERDTree /home/ 打开 入当前目录的树形界面，通过小键盘上下键，能移动选中的目录或文件 目录前面有+号，摁 Enter 会展开目录，文件前面是-号，摁 Enter 会在右侧窗口展现该文件的内容，并光标的焦点focus右侧。 ctr+w+h 光标 focus 左侧树形目录，ctrl+w+l 光标 focus 右侧文件显示窗口。多次摁 ctrl+w，光标自动在左右侧窗口切换 光标focus左侧树形窗口，按 ? 弹出NERDTree的帮助，再次按 ？关闭帮助显示 输入 :q 回车，关闭光标所在窗口 除了使用鼠标可以基本操作以外，还可以使用键盘。下下面列出常用的快捷键： j、k 分别下、上移动光标 o 或者回车打开文件或是文件夹，如果是文件的话，光标直接定位到文件中，想回到目录结构中，按住 Ctrl，然后点两下 w 就回来了 go 打开文件，但是光标不动，仍然在目录结构中 i、s 分别是水平、垂直打开文件，就像vim命令的 :vs、:sp一样 gi、gs 水平、垂直打开文件，光标不动 p 快速定位到上层目录 P 快速定位到根目录 K、J 快速定位到同层目录第一个、最后一个节点 q 关闭 3.2 NERDTree-Tabs项目地址:https://github.com/jistr/vim-nerdtree-tabs 安装完 NERDTree 以后我觉得还需要安装一下 NERDTree-Tabs 这个插件，提供了很多 NERDTree 的加强功能，包括保持 目录树状态、优化tab标题等等。 安装方法： 12$ cd ~/.vim/bundle$ git clone https://github.com/jistr/vim-nerdtree-tabs.git 可以把一下内容添加到 vimrc 文件中 12let g:nerdtree_tabs_open_on_console_startup=1 &quot;设置打开vim的时候默认打开目录树map &lt;leader&gt;n &lt;plug&gt;NERDTreeTabsToggle &lt;CR&gt; &quot;设置打开目录树的快捷键 3.3 supertabSuperTab使键入Tab键时具有上下文提示及补全功能。如下图（图片来自 图灵社区）： 项目地址: https://github.com/ervandew/supertab 安装方法： 12$ cd ~/.vim/bundle$ git clone git@github.com:ervandew/supertab.git 打开vim配置文件，vim ~/.vimrc，在最后加上一行内容 1let g:SuperTabDefaultCompletionType=&quot;context&quot; 3.4 ctrlp项目地址: https://github.com/kien/ctrlp.vim 安装方法： 12$ cd ~/.vim/bundle$ git clone git@github.com:kien/ctrlp.vim.git 快捷键：ctrl+p 4. 参考 [1] https://github.com/wklken/k-vim [2] http://www.zhihu.com/question/19989337 [3] http://www.cnblogs.com/ma6174/archive/2011/12/10/2283393.html]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重装Linux-Mint系统之后]]></title>
    <url>%2F2014%2F01%2F09%2Fafter-reinstall-the-system%2F</url>
    <content type="text"><![CDATA[本文主要记录重装Linux-Mint系统之后的一些软件安装和环境变量配置。 安装常用工具1sudo apt-get install ctags curl vsftpd git vim tmux meld htop putty subversion nload iptraf iftop tree openssh-server gconf-editor gnome-tweak-tool 挂载exfat格式磁盘1sudo apt-get install exfat-fuse exfat-utils 安装ibus在终端输入命令: 123sudo add-apt-repository ppa:shawn-p-huang/ppasudo apt-get updatesudo apt-get install ibus-gtk ibus-pinyin 启用IBus框架: 1im-switch -s ibus 启动ibus： 1ibus-daemon 安装gedit-markdown1234wget https://gitorious.org/projets-divers/gedit-markdown/archive/master.zipunzip master.zipcd projets-divers-gedit-markdown./gedit-markdown.sh install 安装wiz123sudo add-apt-repository ppa:wiznote-teamsudo apt-get updatesudo apt-get install wiznote 安装 oh-my-zsh1sudo apt-get install zsh 把默认 Shell 换为 zsh。 1chsh -s /bin/zsh 然后用下面的两句（任选其一）可以自动安装 oh-my-zsh： 1curl -L https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh | sh 1wget --no-check-certificate https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O - | sh 编辑~/.zshrc： 12echo &apos;source ~/.bashrc&apos; &gt;&gt;~/.zshrcecho &apos;source ~/.bash_profile&apos; &gt;&gt;~/.zshrc 博客相关安装Ruby通过rvm安装ruby： 123curl -L get.rvm.io | bash -s stablesource ~/.bash_profilesed -i -e &apos;s/ftp\.ruby-lang\.org\/pub\/ruby/ruby\.taobao\.org\/mirrors\/ruby/g&apos; ~/.rvm/config/db 安装rvm: 123sudo apt-get install clang -yrvm install 1.9.3 --with-gcc=clangrvm --default 1.9.3 安装jekyll12gem update --systemgem install rdoc jekyll redcarpet 安装七牛同步脚本1234wget http://devtools.qiniudn.com/linux_amd64/qiniu-devtools.zipunzip qiniu-devtools.zipmv _package_linux_amd64/* /usr/bin/source ~/.bashrc 更新ssh-key更新github和gitcafe的ssh-key 安装 virtualbox123wget -q http://download.virtualbox.org/virtualbox/debian/oracle_vbox.asc -O- | sudo apt-key add -sudo apt-get updatesudo apt-get install virtualbox-4.3 安装fortune-zh1sudo apt-get install fortune-zh /usr/bin/mint-fortune 中调用语句改为: 1/usr/games/fortune 70% tang300 30% song100 | $command -f $cow -n 这里的70%与30%是显示唐诗与宋词的概率。 1gsettings set com.linuxmint.terminal show-fortunes true 修改分区权限1sudo chown -R june:june /chan 重命名home下目录先手动重命名: 12345文档 ---&gt; projects音乐 ---&gt; app图片 ---&gt; codes视频 ---&gt; workspace下载 ---&gt; downloads 然后,删除这些目录,建立软连接: 123456rm -rf projects app codes workspace downloadsln -s /chan/app ~/appln -s /chan/codes ~/codesln -s /chan/projects ~/projectsln -s /chan/workspace ~/workspaceln -s /chan/downloads ~/downloads 安装开发环境配置ant、maven和ivy仓库 1234567chmod +x /chan/app/apache/apache-maven-3.0.5/bin/mvnchmod +x /chan/app/apache/apache-ant-1.9.4/bin/antrm -rf /home/june/.ivy2/cache /home/june/.m2/repositorymkdir -p /home/june/.ivy2 /home/june/.m2ln -s /chan/app/repository/cache/ /home/june/.ivy2/cacheln -s /chan/app/repository/m2/ /home/june/.m2/repository 安装jdk1.6 12wget http://archive.cloudera.com/cm4/ubuntu/precise/amd64/cm/pool/contrib/o/oracle-j2sdk1.6/oracle-j2sdk1.6_1.6.0+update31_amd64.debdpkg -i oracle-j2sdk1.6_1.6.0+update31_amd64.deb 配置环境变量： 12345678910111213141516171819sudo mkdir -p /usr/java/sudo ln -s /usr/lib/jvm/j2sdk1.6-oracle /usr/java/latestsudo update-alternatives --install /usr/bin/java java /usr/java/latest/bin/java 5sudo update-alternatives --install /usr/bin/javac javac /usr/java/latest/bin/javac 5sudo update-alternatives --set java /usr/java/latest/bin/javaif [ -f ~/.bashrc ] ; then sed -i &apos;/^export[[:space:]]\&#123;1,\&#125;JAVA_HOME[[:space:]]\&#123;0,\&#125;=/d&apos; ~/.bashrc sed -i &apos;/^export[[:space:]]\&#123;1,\&#125;CLASSPATH[[:space:]]\&#123;0,\&#125;=/d&apos; ~/.bashrc sed -i &apos;/^export[[:space:]]\&#123;1,\&#125;PATH[[:space:]]\&#123;0,\&#125;=/d&apos; ~/.bashrcfiecho &quot;export JAVA_HOME=/usr/java/latest&quot; &gt;&gt; ~/.bashrcecho &quot;export CLASSPATH=.:\$JAVA_HOME/lib/tools.jar:\$JAVA_HOME/lib/dt.jar&quot;&gt;&gt;~/.bashrcecho &quot;export MVN_HOME=/chan/app/apache/apache-maven-3.0.5&quot; &gt;&gt; ~/.bashrcecho &quot;export ANT_HOME=/chan/app/apache/apache-ant-1.9.4&quot; &gt;&gt; ~/.bashrcecho &quot;export PATH=\$JAVA_HOME/bin:\$MVN_HOME/bin:\$ANT_HOME/bin:\$PATH&quot; &gt;&gt; ~/.bashrcsource ~/.bashrc]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive使用HAProxy配置HA]]></title>
    <url>%2F2014%2F01%2F08%2Fhive-ha-by-haproxy%2F</url>
    <content type="text"><![CDATA[HAProxy是一款提供高可用性、负载均衡以及基于TCP（第四层）和HTTP（第七层）应用的代理软件，HAProxy是完全免费的、借助HAProxy可以快速并且可靠的提供基于TCP和HTTP应用的代理解决方案。 免费开源，稳定性也是非常好，这个可通过我做的一些小项目可以看出来，单Haproxy也跑得不错，稳定性可以与硬件级的F5相媲美。根据官方文档，HAProxy可以跑满10Gbps-New benchmark of HAProxy at 10 Gbps using Myricom’s 10GbE NICs （Myri-10G PCI-Express），这个数值作为软件级负载均衡器是相当惊人的。 HAProxy 支持连接拒绝 : 因为维护一个连接的打开的开销是很低的，有时我们很需要限制攻击蠕虫（attack bots），也就是说限制它们的连接打开从而限制它们的危害。 这个已经为一个陷于小型DDoS攻击的网站开发了而且已经拯救了很多站点，这个优点也是其它负载均衡器没有的。 HAProxy 支持全透明代理（已具备硬件防火墙的典型特点）: 可以用客户端IP地址或者任何其他地址来连接后端服务器. 这个特性仅在Linux 2.4/2.6内核打了cttproxy补丁后才可以使用. 这个特性也使得为某特殊服务器处理部分流量同时又不修改服务器的地址成为可能。 HAProxy现多于线上的Mysql集群环境，我们常用于它作为MySQL（读）负载均衡； 自带强大的监控服务器状态的页面，实际环境中我们结合Nagios进行邮件或短信报警，这个也是我非常喜欢它的原因之一； HAProxy支持虚拟主机，许多朋友说它不支持虚拟主机是错误的，通过测试我们知道，HAProxy是支持虚拟主机的。 HAProxy特别适用于那些负载特大的web站点， 这些站点通常又需要会话保持或七层处理。HAProxy运行在当前的硬件上，完全可以支持数以万计的并发连接。并且它的运行模式使得它可以很简单安全的整合进您当前的架构中， 同时可以保护你的web服务器不被暴露到网络上。 安装配置在HAProxy官网下载安装包并编译 1234wget http://haproxy.1wt.eu/download/1.4/src/haproxy-1.4.24.tar.gz|tar zxvfmv haproxy-1.4.24 /opt/haproxy-1.4.24cd /opt/haproxy-1.4.24make TARGET=linux26 添加配置文件在/opt/haproxy-1.4.24目录下创建一个config.cfg文件，添加如下内容： 123456789101112131415161718192021222324252627282930313233343536global daemon nbproc 1 pidfile /var/run/haproxy.pid ulimit-n 65535defaults mode tcp #mode &#123; tcp|http|health &#125;，tcp 表示4层，http表示7层，health仅作为健康检查使用 retries 2 #尝试2次失败则从集群摘除 option redispatch #如果失效则强制转换其他服务器 option abortonclose #连接数过大自动关闭 maxconn 1024 #最大连接数 timeout connect 1d #连接超时时间，重要，hive查询数据能返回结果的保证 timeout client 1d #同上 timeout server 1d #同上 timeout check 2000 #健康检查时间 log 127.0.0.1 local0 err #[err warning info debug]listen admin_stats #定义管理界面 bind 0.0.0.0:1090 #管理界面访问IP和端口 mode http #管理界面所使用的协议 maxconn 10 #最大连接数 stats refresh 30s #30秒自动刷新 stats uri / #访问url stats realm Hive\ Haproxy #验证窗口提示 stats auth admin:123456 #401验证用户名密码listen hive #hive后端定义 bind 0.0.0.0:10001 #ha作为proxy所绑定的IP和端口 mode tcp #以4层方式代理，重要 balance leastconn #调度算法 &apos;leastconn&apos; 最少连接数分配，或者 &apos;roundrobin&apos;，轮询分配 maxconn 1024 #最大连接数 server hive_1 192.168.1.1:10000 check inter 180000 rise 1 fall 2 server hive_2 192.168.1.2:10000 check inter 180000 rise 1 fall 2 #释义：server 主机代名(你自己能看懂就行)，IP:端口 每180000毫秒检查一次。也就是三分钟。 #hive每有10000端口的请求就会创建一个log，设置短了，/tmp下面会有无数个log文件，删不完。 如何启动在HAProxy目录下执行如下命令： 1haproxy -f conf.cfg 如何使用在hive-server或者hive-server2中jdbc的连接信息修改url和port，如hive-server2: 1jdbc:hive2://$&#123;haproxy.hostname&#125;:$&#123;haproxy.hive.bind.port&#125;/$&#123;hive.database&#125; 上面haproxy.hostname为你安装haproxy的机器名；haproxy.hive.bind.port为conf.cfg中定义的监听hive的端口（上面中定义的为10001） 参考资料 [1] HAProxy—HAProxy简介 [2] HAProxy+Hive构建高可用数据挖掘集群]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git配置和一些常用命令]]></title>
    <url>%2F2013%2F12%2F27%2Fsome-git-configs-and-cammands%2F</url>
    <content type="text"><![CDATA[Git是一个分布式版本控制／软件配置管理软件，原来是linux内核开发者林纳斯·托瓦兹（Linus Torvalds）为了更好地管理linux内核开发而创立的。 Git配置12345678git config --global user.name &quot;javachen&quot;git config --global user.email &quot;june.chan@foxmail.com&quot;git config --global color.ui truegit config --global alias.co checkoutgit config --global alias.ci commitgit config --global alias.st statusgit config --global alias.br branchgit config -l # 列举所有配置 用户的git配置文件在~/.gitconfig，我的配置： 123456789101112131415161718192021222324252627282930313233343536june@june-mint ~/workspace/snippets/dotfiles $ cat .gitconfig [user] email = june.chan@foxmail.com name = javachen[color] ui = auto[color &quot;branch&quot;] current = yellow reverse local = yellow remote = green[color &quot;diff&quot;] meta = yellow bold frag = magenta bold old = red bold new = green bold[color &quot;status&quot;] added = yellow changed = green untracked = cyan[alias] st = &quot;status&quot; co = checkout ls = &quot;ls-files&quot; ci = commit br = branch rt = reset --hard unstage = reset HEAD uncommit = reset --soft HEAD^ l = log --pretty=oneline --abbrev-commit --graph --decorate amend = commit --amend who = shortlog -n -s --no-merges g = grep -n --color -E cp = cherry-pick -x cb = checkout -b [core] filemode = true Git常用命令查看、帮助命令123git help &lt;command&gt; # 显示command的helpgit show # 显示某次提交的内容git show $id 查看提交记录12345678910git loggit log &lt;file&gt; # 查看该文件每次提交记录git log -p &lt;file&gt; # 显示版本历史，以及版本间的内容差异git log -p -2 # 查看最近两次详细修改内容的diffgit log --stat # 查看提交统计信息git log --since=&quot;6 hours&quot; # 显示最近6小时提交git log --before=&quot;2 days&quot; # 显示2天前提交git log -1 HEAD~3 # 显示比HEAD早3个提交的那个提交git log -1 HEAD^^^git reflog # 查看操作记录 添加、提交、删除、找回，重置修改文件12git add &lt;file&gt; # 将工作文件修改提交到本地暂存区git add . # 将所有修改过的工作文件提交暂存区 1234git co -- &lt;file&gt; # 抛弃工作区修改git co . # 抛弃工作区修改git co HEAD &lt;file&gt; # 抛弃工作目录区中文件的修改git co HEAD~3 # 回退三个版本 12345git ci &lt;file&gt;git ci .git ci -a # 将git add, git rm和git ci等操作都合并在一起做git ci -am &quot;some comments&quot;git ci --amend # 修改最后一次提交记录 12git rm &lt;file&gt; # 从版本库中删除文件git rm &lt;file&gt; --cached # 从版本库中删除文件，但不删除文件 1git mv &lt;file1&gt; &lt;file2&gt; # 重命名文件 12345git reset --hard HEAD^ # 恢复最近一次提交过的状态，即放弃上次提交后的所有本次修改git reset --hard &lt;commit id&gt; # 恢复到某一次提交的状态git reset HEAD &lt;file&gt; # 抛弃暂存区中文件的修改git reset &lt;file&gt; # 从暂存区恢复到工作文件git reset -- . # 从暂存区恢复到工作文件 12git revert &lt;$id&gt; # 恢复某次提交的状态，恢复动作本身也创建了一次提交对象git revert HEAD # 恢复最后一次提交的状态 查看文件diff12345678910git diff &lt;file&gt; # 比较当前文件和暂存区文件差异git diffgit diff &lt;$id1&gt; &lt;$id2&gt; # 比较两次提交之间的差异git diff &lt;branch1&gt; &lt;branch2&gt; # 在两个分支之间比较 git diff --staged # 比较暂存区和版本库差异git diff --cached # 比较暂存区和版本库差异git diff --stat # 仅仅比较统计信息git diff &quot;@&#123;yesterday&#125;&quot; # 查看昨天的改变git diff 1b6d &quot;master~2&quot; # 查看一个特定版本与倒数第二个变更之间的改变 Git 本地分支管理查看、切换、创建和删除分支12345678910git br -r # 查看远程分支git br -v # 查看各个分支最后提交信息git br -a # 列出所有分支git br --merged # 查看已经被合并到当前分支的分支git br --no-merged # 查看尚未被合并到当前分支的分支git br &lt;new_branch&gt; # 基于当前分支创建新的分支git br &lt;new_branch&gt; &lt;start_point&gt; # 基于另一个起点（分支名称，提交名称或则标签名称），创建新的分支git br -f &lt;existing_branch&gt; &lt;start_point&gt; # 创建同名新分支，覆盖已有分支git br -d &lt;branch&gt; # 删除某个分支git br -D &lt;branch&gt; # 强制删除某个分支 (未被合并的分支被删除的时候需要强制) 12345678git co &lt;branch&gt; # 切换到某个分支git co -b &lt;new_branch&gt; # 创建新的分支，并且切换过去git co -b &lt;new_branch&gt; &lt;branch&gt; # 基于branch创建新的new_branchgit co -m &lt;existing_branch&gt; &lt;new_branch&gt; # 移动或重命名分支，当新分支不存在时git co -M &lt;existing_branch&gt; &lt;new_branch&gt; # 移动或重命名分支，当新分支存在时就覆盖git co $id # 把某次历史提交记录checkout出来，但无分支信息，切换到其他分支会自动删除git co $id -b &lt;new_branch&gt; # 把某次历史提交记录checkout出来，创建成一个分支 分支合并和rebase1234567git merge &lt;branch&gt; # 将branch分支合并到当前分支git merge origin/master --no-ff # 不要Fast-Foward合并，这样可以生成merge提交git merge --no-commit &lt;branch&gt; # 合并但不提交git merge --squash &lt;branch&gt; # 把一条分支上的内容合并到另一个分支上的一个提交git rebase master &lt;branch&gt; # 将master rebase到branch，相当于：git co &lt;branch&gt; &amp;&amp; git rebase master &amp;&amp; git co master &amp;&amp; git merge &lt;branch&gt; Git补丁管理1234git diff &gt; ../sync.patch # 生成补丁git apply ../sync.patch # 打补丁git apply --check ../sync.patch # 测试补丁能否成功git format-patch -X # 根据提交的log生成patch，X为数字，表示最近的几个日志 Git暂存管理1234git stash # 暂存git stash list # 列所有stashgit stash apply # 恢复暂存的内容git stash drop # 删除暂存区 Git远程分支管理123456789git pull # 抓取远程仓库所有分支更新并合并到本地git pull --no-ff # 抓取远程仓库所有分支更新并合并到本地，不要快进合并git fetch origin # 抓取远程仓库所有更新git fetch origin remote-branch:local-branch #抓取remote-branch分支的更新git fetch origin --tags # 抓取远程上的所有分支git checkout -b &lt;new-branch&gt; &lt;remote_tag&gt; # 抓取远程上的分支git merge origin/master # 将远程主分支合并到本地当前分支git co --track origin/branch # 跟踪某个远程分支创建相应的本地分支git co -b &lt;local_branch&gt; origin/&lt;remote_branch&gt; # 基于远程分支创建本地分支，功能同上 123456git push # push所有分支git push origin master # 将本地主分支推到远程主分支git push -u origin master # 将本地主分支推到远程(如无远程主分支则创建，用于初始化远程仓库)git push origin &lt;local_branch&gt; # 创建远程分支， origin是远程仓库名git push origin &lt;local_branch&gt;:&lt;remote_branch&gt; # 创建远程分支git push origin :&lt;remote_branch&gt; #先删除本地分支(git br -d &lt;branch&gt;)，然后再push删除远程分支 Git远程仓库管理123456789git remote -v # 查看远程服务器地址和仓库名称git remote show origin # 查看远程服务器仓库状态git remote add origin git@github:XXX/test.git # 添加远程仓库地址git remote set-url origin git@github.com:XXX/test.git # 设置远程仓库地址(用于修改远程仓库地址)git remote rm &lt;repository&gt; # 删除远程仓库git remote set-head origin master # 设置远程仓库的HEAD指向master分支git branch --set-upstream master origin/mastergit branch --set-upstream develop origin/develop 实例打patch过程1234git add .git statusgit diff --cached &gt;XXX.patchgit ci -m &apos;add patch&apos; 分支策略在实际开发中，我们应该按照几个基本原则进行分支管理： 首先，master分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活； 那在哪干活呢？干活都在dev分支上，也就是说，dev分支是不稳定的，到某个时候，比如1.0版本发布时，再把dev分支合并到master上，在master分支发布1.0版本； 你和你的小伙伴们每个人都在dev分支上干活，每个人都有自己的分支，时不时地往dev分支上合并就可以了。]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SaltStack学习笔记]]></title>
    <url>%2F2013%2F11%2F18%2Fstudy-note-of-saltstack%2F</url>
    <content type="text"><![CDATA[1. 关于本文档这份文档如其名，是我自己整理的学习 SaltStack 的过程记录。只是过程记录，没有刻意像教程那样去做。所以呢，从前至后，中间不免有一些概念不清不明的地方。因为事实上，在某个阶段对于一些概念本来就不可能明白。所以，整个过程只求在形式上的能用即可。前面就不要太纠结概念和原理，知道怎么用就好。 希望这篇文章能够让你快速了解并使用saltstack。文章还在编写中。 2. 关于SaltStack2.1. 什么是SaltStackSaltStack是开源的管理基础设置的轻量级工具，容易搭建，为远程管理服务器提供一种更好、更快速、更有扩展性的解决方案。通过简单、可管理的接口，Salt实现了可以管理成千上百的服务器和处理大数据的能力。 轻量级配置管理系统，能够维持远端节点运行在预定状态（例如，确保指定的软件包已经安装和特定的系统服务正在运行） 分布式远程执行系统，用于在远端节点执行命令和查询数据，可以是单独，也可以是选定的条件 2.2. SaltStack特点简单兼顾大规模部署与更小的系统的同时提供多功能性是很困难的，Salt是非常简单配置和维护，不管项目的大小。Salt可以胜任管理任意的数量的服务器，不管是本地网络，还是跨数据中心。架构采用C/S模式，在一个后台程序中集成必要功能。默认不需要复杂的配置就可以工作，同时可以定制用于特殊的需求。 并行执行Salt的核心功能： 通过并行方式让远端节点执行命令 采用安全的加密/解析协议 最小化使用网络和负载 提供简单的程序接口 Salt引入了更细粒度的控制，允许不通过目标名字，二是通过系统属性分类 构建在成熟技术之上Salt采用了很多技术和技巧。网络层采用优秀的ZeroMQ库，所以守护进程里面包含AMQ代理。Salt采用公钥和主控通讯，同时使用更快的AES加密通信，验证和加密都已经集成在Salt里面。Salt使用msgpack通讯，所以更快速和更轻量网络交换。 Python 客户端接口为了实现简单的扩展，Salt执行例程可以写成简单的Python模块。客户端程序收集的数据可以发送回主控端，可以是其他任意程序。可以通过Python API调用Salt程序，或者命令行，因此，Salt可以用来执行一次性命令，或者大型应用程序中的一部分模块。 快速，灵活，可扩展结果是一个系统可以高速在一台或者一组服务器执行命令。Salt速度很快，配置简单，扩展性好，提供了一个远程执行架构，可以管理多样化需求的任何数量的服务器。整合了世界上最好的远程执行方法，增强处理能力，扩展使用范围，使得可以适用任何多样化复杂的网络。 开源Salt基于Apache 2.0 licence开发，可以用于开源或者自有项目。请反馈你的扩展给项目组，以便更多人受益，共同促进Salt发展。请在你的系统部署 系统，让运维更便捷。 开发语言：Python 2.3. 支持的系统常见的系统包可以直接下载安装使用： Fedora RedHat Enterprise Linux / Centos (EPEL 5, EPEL 6) Ubuntu (PPA) Arch (AUR) FreeBSD Gentoo Debian (sid) Debian (experimental) 3. 安装SaltStack3.1. 依赖SaltStack只能安装在类unix的操作系统上并依赖以下组件： Python 2.6 &gt;= 2.6 &lt;3.0 ZeroMQ &gt;= 2.1.9 pyzmq &gt;= 2.1.9 - ZeroMQ Python bindings PyCrypto - The Python cryptography toolkit msgpack-python - High-performance message interchange format YAML - Python YAML bindings Jinja2 - parsing Salt States (configurable in the master settings) 可选的依赖： mako - an optional parser for Salt States (configurable in the master settings) gcc - dynamic Cython module compiling 3.2. 快速安装可以使用官方提供的Salt Bootstrap来快速安装SaltStack。 安装master： 1curl -L http://bootstrap.saltstack.org | sudo sh -s -- -M -N 安装minion： 1wget -O - http://bootstrap.saltstack.org | sudo sh 当前Salt Bootstrap已经在以下操作系统测试通过： Ubuntu 10.x/11.x/12.x Debian 6.x CentOS 6.3 Fedora Arch FreeBSD 9.0 3.3. 通过rpm安装3.3.1. 下载EPEL yum源：RHEL 5系统: 1rpm -Uvh http://mirror.pnl.gov/epel/5/i386/epel-release-5-4.noarch.rpm RHEL 6系统: 1rpm -Uvh http://ftp.linux.ncsu.edu/pub/epel/6/i386/epel-release-6-8.noarch.rpm 3.3.2. 安装包在master上运行： 1yum install salt-master 在minion上运行： 1yum install salt-minion 3.3.3. 安装后master上设置开启启动并启动服务： 12chkconfig salt-master onservice salt-master start minion上设置开启启动并启动服务： 12chkconfig salt-minion onservice salt-minion start 3.4. 排错当前最新版为0.17.1，如果你也是使用的这个版本并且启动提示如下错误： 1234567891011[root@sk1 vagrant]# /etc/init.d/salt-master startStarting salt-master daemon: Traceback (most recent call last): File &quot;/usr/bin/salt-master&quot;, line 10, in &lt;module&gt; salt_master() File &quot;/usr/lib/python2.6/site-packages/salt/scripts.py&quot;, line 20, in salt_master master.start() File &quot;/usr/lib/python2.6/site-packages/salt/__init__.py&quot;, line 114, in start if check_user(self.config[&apos;user&apos;]): File &quot;/usr/lib/python2.6/site-packages/salt/utils/verify.py&quot;, line 296, in check_user if user in e.gr_mem] + [pwuser.gid])AttributeError: &apos;pwd.struct_passwd&apos; object has no attribute &apos;gid&apos; 通过google搜索关键字saltstack &#39;pwd.struct_passwd&#39; object has no attribute &#39;gid&#39;，可以找到这个issues，查看评论可以发现这是一个bug，将会在0.17.2中被修复。 我的解决方法是：下载saltstack源码重新编译 1234wget https://github.com/saltstack/salt/archive/develop.zipunzip developcd salt-develop/python2.6 setup.py install 4. 配置SaltStack4.1. master配置master不修改配置文件就可以运行，而minion必须修改配置文件中的master id才能和master通讯，配置文件分别在/etc/salt/master 和 /etc/salt/minion。 master默认监控0.0.0.0上4505和4506端口，你可以在/etc/salt/master中修改为指定ip。 master关键配置： interface publish_port user max_open_files worker_threads ret_port pidfile root_dir pki_dir cachedir keep_jobs job_cache ext_job_cache minion_data_cache enforce_mine_cache sock_dir Master Security配置 open_mode auto_accept autosign_file client_acl client_acl_blacklist external_auth token_expire Master Module管理 runner_dirs cython_enable Master State设置 state_verbose state_output state_top external_nodes renderer failhard test Master File Server设置 fileserver_backend file_roots hash_type file_buffer_size Pillar配置 pillar_roots ext_pillar Syndic Server设置 order_masters syndic_master syndic_master_port syndic_log_file syndic_pidfile Peer发布设置 peer peer_run Node Groups设置 Master Logging设置 log_file log_level log_level_logfile log_datefmt log_datefmt_logfile log_fmt_console log_fmt_logfile log_granular_levels Include配置 default_include include 4.4. minion配置Minion主要配置： master master_port user pidfile root_dir pki_dir id append_domain cachedir verify_env cache_jobs sock_dir backup_mode acceptance_wait_time random_reauth_delay cceptance_wait_time_max dns_check ipc_mode tcp_pub_port tcp_pull_port Minion Module管理 disable_modules disable_returners module_dirs returner_dirs states_dirs render_dirs cython_enable providers State Management 设置 renderer state_verbose state_output autoload_dynamic_modules environment File目录设置 file_client file_roots hash_type pillar_roots Security设置 open_mode 线程设置 multiprocessing Minion日志设置 log_file log_level log_level_logfile log_datefmt log_datefmt_logfile log_fmt_console log_fmt_logfile log_granular_levels Include配置 default_include include Frozen Build Update Settings update_url update_restart_services 5. 初识SaltStack5.1. 配置参考上面的配置文件说明修改master和minion配置。这里我的master主机名为sk1，minion主机名为sk2。 修改master配置文件/etc/salt/master 12interface: 0.0.0.0auto_accept: True 修改minion配置文件/etc/salt/minion 12master: sk1id: sk2 5.2. 运行salt然后运行salt，启动master（添加-d参数可以让其后台运行）： 1salt-master 启动minion（添加-d参数可以让其后台运行）： 1salt-minion 如果需要排错，可以添加设置日志级别： 1salt-master --log-level=debug 如果想以非用户运行，可以添加--user参数 5.3. 管理KeySalt在master和minion通信之间使用AES加密。在master和minion通信之前，minion上的key需要发送到master并被master接受。可以在master上查看已经接受的key： 12345[root@sk1 pillar]# salt-key -LAccepted Keys:Unaccepted Keys:sk2Rejected Keys: 然后运行下面命令可以接受所有未被接受的key： 1[root@sk1 pillar]# salt-key -A 5.4. 发送命令在master上发送ping命令检测minon是否被认证成功： 123[root@sk1 pillar]# salt &apos;*&apos; test.pingsk2:salt &apos;*&apos; test.ping True True表明测试成功。 6. 配置管理6.1 states6.1.1 states文件salt states的核心是sls文件，该文件使用YAML语法定义了一些k/v的数据。 sls文件存放根路径在master配置文件中定义，默认为/srv/salt,该目录在操作系统上不存在，需要手动创建。 在salt中可以通过salt://代替根路径，例如你可以通过salt://top.sls访问/srv/salt/top.sls。 在states中top文件也由master配置文件定义，默认为top.sls，该文件为states的入口文件。 一个简单的sls文件如下： 1234567apache: pkg: - installed service: - running - require: - pkg: apache 说明：此SLS数据确保叫做”apache”的软件包(package)已经安装,并且”apache”服务(service)正在运行中。 第一行，被称为ID说明(ID Declaration)。ID说明表明可以操控的名字。 第二行和第四行是State说明(State Declaration)，它们分别使用了pkg和service states。pkg state通过系统的包管理其管理关键包，service state管理系统服务(daemon)。 在pkg及service列下边是运行的方法。方法定义包和服务应该怎么做。此处是软件包应该被安装，服务应该处于运行中。 第六行使用require。本方法称为”必须指令”(Requisite Statement)，表明只有当apache软件包安装成功时，apache服务才启动起来。 state和方法可以通过点连起来，上面sls文件和下面文件意思相同。 12345apache: pkg.installed service.running - require: - pkg: apache 将上面sls保存为init.sls并放置在sal://apache目录下，结果如下： 1234/srv/salt├── apache│ └── init.sls└── top.sls top.sls如何定义呢？ master配置文件中定义了三种环境，每种环境都可以定义多个目录，但是要避免冲突，分别如下： 123456789# file_roots:# base:# - /srv/salt/# dev:# - /srv/salt/dev/services# - /srv/salt/dev/states# prod:# - /srv/salt/prod/services# - /srv/salt/prod/states top.sls可以这样定义： 123base: &apos;*&apos;: - apache 说明： 第一行，声明使用base环境 第二行，定义target，这里是匹配所有 第三行，声明使用哪些states目录，salt会寻找每个目录下的init.sls文件。 6.1.2 运行states一旦创建完states并修改完top.sls之后，你可以在master上执行下面命令： 123456789101112131415161718192021222324252627282930313233[root@sk1 salt]# salt &apos;*&apos; state.highstatesk2:---------- State: - pkg Name: httpd Function: installed Result: True Comment: The following packages were installed/updated: httpd. Changes: ---------- httpd: ---------- new: 2.2.15-29.el6.centos old:---------- State: - service Name: httpd Function: running Result: True Comment: Service httpd has been enabled, and is running Changes: ---------- httpd: TrueSummary------------Succeeded: 2Failed: 0------------Total: 2 上面命令会触发所有minion从master下载top.sls文件以及其中定一个的states，然后编译、执行。执行完之后，minion会将执行结果的摘要信息汇报给master。 6.2. GrainsPillarRenderers远程执行命令在上面的例子中，test.ping是最简单的一个远程执行的命令，你还可以执行一些更加复杂的命令。 salt执行命令的格式如下： 1salt &apos;&lt;target&gt;&apos; &lt;function&gt; [arguments] target: 执行salt命令的目标，可以使用正则表达式 function： 方法，由module提供 arguments：function的参数 target可以使用正则表达式匹配： 12salt &apos;*&apos; test.pingsalt &apos;sk2&apos; test.ping function是module提供的方法。通过下面命令可以查看所有的function： 1salt &apos;*&apos; sys.doc function可以接受参数： 1salt &apos;*&apos; cmd.run &apos;uname -a&apos; 并且支持关键字参数： 1salt &apos;*&apos; cmd.run &apos;uname -a&apos; cwd=/ user=salt 上面例子意思是：在所有minion上切换到/目录以salt用户运行uname -a命令。 关键字参数可以参考module的api,通过api你可以查看cmd.run的定义： 1234salt.states.cmd.mod_watch(name, **kwargs)Execute a cmd function based on a watch callsalt.states.cmd.run(name, onlyif=None, unless=None, cwd=None, user=None, group=None, shell=None, env=(), stateful=False, umask=None, quiet=False, timeout=None, **kwargs) 所有module中的方法定义都与上面类似，说明： name： 第一个参数，为执行的命令 中间的key=alue为keyword参数，都有默认值 最后一个参数为name中命令的输入参数 TARGETINGReturnersMine参考文章 [1]Saltstack服务器集中管理和并行下发命令工具]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>saltstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用SaltStack安装JBoss]]></title>
    <url>%2F2013%2F11%2F16%2Finstall-jboss-with-saltstack%2F</url>
    <content type="text"><![CDATA[SaltStack是一个具备puppet与func功能为一身的集中化管理平台，其基于python实现，功能十分强大，各模块融合度及复用性极高。SaltStack 采用 zeromq 消息队列进行通信，和 Puppet/Chef 比起来，SaltStack 速度快得多。 在开始使用SaltStack之前，首先要对SaltStack的基础进行一系列的学习，这里，强烈推荐官网的Tutorial,在完成了整个Tutorial之后，通过Module Index页面，我们能够快速查阅Salt所有模块的功能与用法:http://docs.saltstack.com/py-modindex.html 安装saltstack安装过程请参考：安装saltstack和halite 添加pillar你可以执行下面命令查看minion拥有哪些Pillar数据： 1$ salt &apos;*&apos; pillar.data saltstack的默认states目录为/srv/salt，默认为/srv/pillar，如果不存在请先创建。 12345678910[root@sk1 /]# tree /srv/ -L 3/srv/├── pillar│ ├── jboss│ ├── params.sls│ └── top.sls├── salt│ ├── jboss│ ├── _modules│ └── top.sls 在/srv/pillar/下创建top.sls，该文件引入jboss下的params.sls： 123base: '*': - jboss.params 创建jboss目录并添加params.sls如下： 123456789101112131415161718jboss_home: /home/jboss/jboss-eap-5.1/jboss-asprofile_port: default1: http_port: ports-default jmx_port: 1099 default2: http_port: ports-01 jmx_port: 1199 default3: http_port: ports-02 jmx_port: 1299 default4: http_port: ports-03 jmx_port: 1399jmx: username: admin password: admin 该文件定义了如下变量，你可以按需要定义自己的变量： jboss_home：mimion机器上jboss的home目录 profile_port：定义有多少个profile以及每个profile下的http端口和jmx端口 jmx:定义jmx-console用户名和密码 定义变量之后，你可以在sates文件中这样引用： 1pillar['profile_port']['default1'] 下面是个复杂的例子，使用了python中的模板引擎语言： undefined 你还可以在python脚本中或者是saltstack自定义module中这样引用变量： 12__pillar__[&apos;jboss_home&apos;]__pillar__[&apos;profile_port&apos;][&apos;default1&apos;][&apos;jmx_port&apos;] 在master上修改Pilla文件后，需要用以下命令刷新minion上的数据： 1$ salt &apos;*&apos; saltutil.refresh_pillar 如果定义好的pillar不生效，建议刷新一下或者重启salt试试。 编写states/srv/salt目录如下： 12345678910[root@sk1 salt]# tree -L 3.├── jboss│ ├── files│ │ └── jboss-eap-5.1.zip│ └── init.sls├── _modules│ └── jboss.py├── top.sls top.sls为sates入口，定义如下： 123base: *: - jboss 创建jboss目录并编写init.sls文件： undefined 说明： 1.上面文件中创建了jboss ID，其包括两部分：拷贝文件和解压缩，分别对应file.managed和cmd.run。 2.然后通过脚本语言读取pillar中定义的变量并以依次遍历生成多个ID，ID名称由变量中值定义。本例中，是读取profile_port的值，然后创建多个profile。profile_port变量在/srv/pillar/jboss/params.sls中定义。 编写完sates文件之后，你可以通过执行以下命令让所有minion执行sates文件中定义的state： 1$ salt &apos;*&apos; state.highstate 你也可以单独执行jboss这个states： 1$ salt &apos;*&apos; state.sls jboss 自定义grains_module自定义的grains_module存放在/srv/salt/_grains目录，下面定义一个获取max_open_file的grains： 12345678910111213import os,sys,commands def Grains(): grains = &#123;&#125; max_open_file=65536 try: getulimit=commands.getstatusoutput('source /etc/profile;ulimit -n') except Exception,e: pass if getulimit[0]==0: max_open_file=int(getulimit[1]) grains['max_open_file'] = max_open_file return grains 然后，同步grains模块： 12345678910$ salt '*' saltutil.sync_allsk2: ---------- grains: - grains.max_open_file modules: outputters: renderers: returners: states: 刷新模块(让minion编译模块)： 1$ salt &apos;*&apos; sys.reload_modules 然后，验证max open file的value： 123$ salt &apos;*&apos; grains.item max_open_filesk2: max_open_file: 1024 自定义module你通过执行states可以完成bao的安装、配置和部署，如果你想对他们做管理，你可以自定义module来执行一些远程命令。 自定义的module需要存放在/srv/salt/_modules目录下，为了对jboss实例进行启动、停止、查看运行状态，编写jboss.py模块： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162def _simple_cmd_retcode(cmd): home = __pillar__['jboss_home']+'/bin' out = __salt__['cmd.retcode'](cmd,cwd=home) ret = &#123;&#125; ret['Cmd']=cmd ret['Msg']=out ret['Result']=True return retdef _simple_cmd(cmd): home = __pillar__['jboss_home']+'/bin' out = __salt__['cmd.run'](cmd,cwd=home).splitlines() ret = &#123;&#125; ret['Cmd']=cmd ret['Msg']=out ret['Result']=True return retdef running(profile): ret =_simple_cmd("ps -ef|grep -v grep|grep java|grep "+profile+" |grep "+__pillar__['profile_port'][profile]['http_port']) if ret['Msg']=='' or len(ret['Msg'])==0: return False return Truedef start(profile): ip = __grains__['id'][0] port = __pillar__['profile_port'][profile]['http_port'] cmd = 'nohup ./run.sh -b '+ip+' -c '+profile+' -Djboss.service.binding.set='+port+' &amp;' ret=&#123;&#125; if running(profile): ret['Cmd']=cmd ret['Msg']=profile+' has started' ret['Result']=False return ret return _simple_cmd_retcode(cmd)def status(profile): ip = __grains__['id'][0] port = __pillar__['profile_port'][profile]['jmx_port'] username = __pillar__['jmx']['username'] password = __pillar__['jmx']['password'] cmd="./twiddle.sh -s "+ip+":"+str(port)+" -u "+username+" -p "+password+" get jboss.system:type=Server Started" ret=_simple_cmd(cmd) for line in ret['Msg']: if not line: continue if 'ERROR' in line: ret['Result']=False ret['Msg']=ret['Msg'][0:2] break return ret def stop(profile): ip = __grains__['id'][0] port = __pillar__['profile_port'][profile]['jmx_port'] username = __pillar__['jmx']['username'] password = __pillar__['jmx']['password'] cmd="./shutdown.sh -S -s "+ip+":"+str(port)+" -u "+username+" -p "+password return _simple_cmd(cmd) 以上python脚本定义了对单个profile的启动、停止和查看运行状态的方法，你可以修改或者扩增代码，添加更多的方法。 然后运行下面命令同步所有模块： 12345678910$ salt &apos;*&apos; saltutil.sync_allsk2: ---------- grains: modules: - modules.jboss outputters: renderers: returners: states: 刷新自定义模块(让minion编译模块)： 1$ salt &apos;*&apos; sys.reload_modules 如果你想启动jboss的default1实例，只需要执行以下方法： 1$ salt &apos;*&apos; jboss.start default1 同样，查看状态： 1$ salt &apos;*&apos; jboss.status default1 jboss为自定义模块的名称，也是jboss.py的名称，start或者status为jboss.py中定义的方法。 如果运行出错，请查看minion的日志，路径为/var/log/salt/minion 同步配置文件现在需要修改jmx-console-users.properties文件中的用户名和密码并将其同步到所有jboss实例中。 将jmx-console-users.properties文件放置在/srv/salt/jboss/files目录下，并修改该文件如下： 12#A sample users.properties file for use with the UsersRolesLoginModulepillar[&apos;jmx&apos;][&apos;username&apos;]= pillar[&apos;jmx&apos;][&apos;password&apos;] 上面文件中使用了pillar获取变量，你还可以使用模板语言如if、for语句来丰富你的文件内容，saltstack支持的模板引擎有jinja等。 然后在init.sls中添加： 12345jmx-console-users.properties: file.managed: - name: /home/jboss/jboss-eap-5.1/jboss-as/server/default/conf/props/jmx-console-users.properties - source: salt://jboss/files/jmx-console-users.properties - template: jinja]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>saltstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装SaltStack和Halite]]></title>
    <url>%2F2013%2F11%2F11%2Finstall-saltstack-and-halite%2F</url>
    <content type="text"><![CDATA[本文记录安装SaltStack和halite过程。 首先准备两台rhel或者centos虚拟机sk1和sk2，sk1用于安装master，sk2安装minion。 配置yum源在每个节点上配置yum源： 1$ rpm -ivh http://mirrors.sohu.com/fedora-epel/6/x86_64/epel-release-6-8.noarch.rpm 然后通过下面命令查看epel参考是否安装成功： 1$ yum list #或者查看/etc/yum.repos.d目录下是否有epel.repo 如果没有安装成功，则可以手动下载epel-release-6-8.noarch.rpm，然后打开该rpm找到./etc/yum.repos.d/epel.repo，将其拷贝到/etc/yum.repos.d目录 安装依赖因为我用jinja2作为SaltStack的渲染引擎，故需要在每个节点上安装python-jinja2： 1$ yum install python-jinja2 -y 安装saltstack在sk1上安装master： 1$ yum install salt-master 在sk1上安装minion： 1$ yum install salt-minion 关闭防火墙12$ iptables -F$ setenforce 0 修改配置文件修改master配置文件，使其监听0.0.0.0地址，并设置自动接受minion的请求。 123$ vim /etc/salt/master interface: 0.0.0.0 #去掉对该行的注释 auto_accept: True #去掉对该行的注释,并修改False为True 在所有的minion节点配置master的id和自己的id： 123$ vim /etc/salt/minion master: sk1 id: sk2 启动分别在sk1和sk2上配置开机启动： 12$ chkconfig salt-master on$ chkconfig salt-minion on 分别在sk1和sk2上以service方式启动： 12$ /etc/init.d/salt-master start$ /etc/init.d/salt-minion start 你可以在sk2上以后台运行salt-minion 1$ salt-minion -d 或者在sk2上debug方式运行： 1$ salt-minion -l debug 排错如果启动提示如下错误： 1234567891011$ /etc/init.d/salt-master startStarting salt-master daemon: Traceback (most recent call last): File &quot;/usr/bin/salt-master&quot;, line 10, in &lt;module&gt; salt_master() File &quot;/usr/lib/python2.6/site-packages/salt/scripts.py&quot;, line 20, in salt_master master.start() File &quot;/usr/lib/python2.6/site-packages/salt/__init__.py&quot;, line 114, in start if check_user(self.config[&apos;user&apos;]): File &quot;/usr/lib/python2.6/site-packages/salt/utils/verify.py&quot;, line 296, in check_user if user in e.gr_mem] + [pwuser.gid])AttributeError: &apos;pwd.struct_passwd&apos; object has no attribute &apos;gid&apos; 请下载saltstack源码重新编译： 1234$ wget https://github.com/saltstack/salt/archive/develop.zip$ unzip develop$ cd salt-develop/$ python2.6 setup.py install 如果你通过’cmd.run’命令去运行java命令，你会得到这样的结果： 123[root@sk1 salt]# salt &apos;*&apos; cmd.run &apos;java&apos; sk2: /bin/bash: java: command not found 这是因为minion在启动过程中并没有加载系统的环境变量，解决这个问题有两种方式： 运行java命令前先source环境变量 修改minion启动脚本，添加source命令： 12345678910# Source function library.if [ -f $DEBIAN_VERSION ]; then breakelif [ -f $SUSE_RELEASE -a -r /etc/rc.status ]; then . /etc/rc.statuselse . /etc/rc.d/init.d/functions . ~/.bashrc . /etc/profilefi salt minion和master的认证过程 minion在第一次启动时，会在/etc/salt/pki/minion/下自动生成minion.pem(private key), minion.pub(public key)，然后将minion.pub发送给master master在接收到minion的public key后，通过salt-key命令accept minion public key，这样在master的/etc/salt/pki/master/minions下的将会存放以minion id命名的public key, 然后master就能对minion发送指令了 master上执行： 1234[root@sk1 pillar]# salt-key -LAccepted Keys:Unaccepted Keys:Rejected Keys: 接受所有的认证请求： 1[root@sk1 pillar]# salt-key -A 再次查看： 12345[root@sk1 pillar]# salt-key -LAccepted Keys:sk2Unaccepted Keys:Rejected Keys: salt-key更多说明：http://docs.saltstack.com/ref/cli/salt-key.html 测试运行在master上运行ping： 123[root@sk1 pillar]# salt &apos;*&apos; test.pingsk2:salt &apos;*&apos; test.ping True True表明测试成功。 安装halite下载代码1$ git clone https://github.com/saltstack/halite 生成index.html12$ cd halite/halite$ ./genindex.py -C 安装salt-api1$ yum install salt-api 配置salt master文件配置salt的master文件，添加： 123456789101112rest_cherrypy: host: 0.0.0.0 port: 8080 debug: true static: /root/halite/halite app: /root/halite/halite/index.htmlexternal_auth: pam: admin: - .* - '@runner' - '@wheel' 重启master; 1$ /etc/init.d/salt-master restart 添加登陆用户12$ useradd admin$ echo admin|passwd –stdin admin 启动 salt-api12$ cd halite/halite$ python2.6 server_bottle.py -d -C -l debug -s cherrypy 然后打开http://ip:8080/app，通过admin/admin登陆即可。]]></content>
      <categories>
        <category>devops</category>
      </categories>
      <tags>
        <tag>saltstack</tag>
        <tag>halite</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Eclipse中调试运行HBase]]></title>
    <url>%2F2013%2F11%2F01%2Fdebug-hbase-in-eclipse%2F</url>
    <content type="text"><![CDATA[这篇文章记录一下如何在eclipse中调试运行hbase。 下载并编译源代码请参考编译hbase源代码并打补丁 修改配置文件修改 conf/hbase-site.xml文件： 123456789&lt;property&gt;&lt;name&gt;hbase.defaults.for.version&lt;/name&gt;&lt;value&gt;0.94.6-cdh4.4.0&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.rootdir&lt;/name&gt;&lt;value&gt;file:///home/june/tmp/data&lt;/value&gt;&lt;/property&gt; 把conf文件夹加到Classpath中 运行HMaster新建一个Debug Configuration, main class 是org.apache.hadoop.hbase.master.HMaster, 参数填start 调试运行该类，运行成功之后日志如下： 1234567891011121314151613/10/29 14:38:29 WARN zookeeper.RecoverableZooKeeper: Node /hbase/table/.META. already deleted, and this is not a retry13/10/29 14:38:29 INFO regionserver.HRegionServer: Received request to open region: .META.,,1.102878519213/10/29 14:38:29 INFO regionserver.HRegion: Setting up tabledescriptor config now ...13/10/29 14:38:29 INFO regionserver.Store: time to purge deletes set to 0ms in store info13/10/29 14:38:29 INFO regionserver.HRegion: Onlined .META.,,1.1028785192; next sequenceid=113/10/29 14:38:29 INFO regionserver.HRegionServer: Post open deploy tasks for region=.META.,,1.1028785192, daughter=false13/10/29 14:38:29 INFO catalog.MetaEditor: Updated row .META.,,1.1028785192 with server=june-mint,47477,138302870187113/10/29 14:38:29 INFO regionserver.HRegionServer: Done with post open deploy task for region=.META.,,1.1028785192, daughter=false13/10/29 14:38:29 INFO handler.OpenedRegionHandler: Handling OPENED event for .META.,,1.1028785192 from june-mint,47477,1383028701871; deleting unassigned node13/10/29 14:38:29 INFO master.AssignmentManager: The master has opened the region .META.,,1.1028785192 that was online on june-mint,47477,138302870187113/10/29 14:38:29 INFO master.HMaster: .META. assigned=2, rit=false, location=june-mint,47477,138302870187113/10/29 14:38:29 INFO catalog.MetaMigrationRemovingHTD: Meta version=0; migrated=true13/10/29 14:38:29 INFO catalog.MetaMigrationRemovingHTD: ROOT/Meta already up-to date with new HRI.13/10/29 14:38:29 INFO master.AssignmentManager: Clean cluster startup. Assigning userregions13/10/29 14:38:29 INFO master.HMaster: Registered HMaster MXBean13/10/29 14:38:29 INFO master.HMaster: Master has completed initialization 如果想修改日志级别，请修改conf/log4j.properties中级别为INFO: 12#Logging Thresholdlog4j.threshold=INFO 运行HRegionServer参考上面的方法，运行HRegionServer，这时候会出现如下日志： 123413/11/04 11:50:47 INFO util.VersionInfo: HBase 0.94.6-cdh4.4.013/11/04 11:50:47 INFO util.VersionInfo: Subversion git://june-mint/chan/workspace/hadoop/hbase -r 979969e1d0d95ce3b8c1d14593f55148da8bc98f13/11/04 11:50:47 INFO util.VersionInfo: Compiled by june on Tue Oct 29 15:11:51 CST 201313/11/04 11:50:47 WARN regionserver.HRegionServerCommandLine: Not starting a distinct region server because hbase.cluster.distributed is false 这是因为当hbase.cluster.distributed=false时，hbase为本地模式，master和regionserver在同一个jvm启动，并且会启动一个最小化的zookeeper集群。请参看：HMasterCommandLine.java的startMaster()方法。 如果你把该值设为true，则hbase集群为分布式模式，这时候默认会连接127.0.0.1：2181对应的zookeeper集群（该集群需要在master启动之前启动）。当然，你可以修改参数让hbase自己维护一个zookeeper集群。 调试hbase shell新建一个Debug Configuration, main class 是org.jruby.Main，在程序参数中添加bin/hirb.rb,然后运行即可。 一些技巧 调试java代码的时候, byte[]的变量总是显示成数字,如果要显示对应的字符 1Window-&gt;Preference-&gt;Java-&gt;Debug-&gt;Primitive Display Options-&gt;Check some of them hbase源码中默认依赖的是hadoop 1.0.x版本，所以mavne依赖中会引入hadoop-core-1.0.4.jar，你可以修改pom.xml文件，将默认的profile修改为你需要的hadoop版本，如2.0版本的hadoop。这样做之后，当你看HMaster的源代码时，你会很方便的关联并浏览ToolRunner类中的源代码。 默认的profile是hadoop-1.0，配置文件如下： 123456789101112131415&lt;!-- profile for building against Hadoop 1.0.x: This is the default. --&gt; &lt;profile&gt; &lt;id&gt;hadoop-1.0&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;!hadoop.profile&lt;/name&gt; &lt;/property&gt; &lt;profile&gt; &lt;id&gt;hadoop-2.0&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;hadoop.profile&lt;/name&gt; &lt;value&gt;2.0&lt;/value&gt; &lt;/property&gt; 你可以将默认的profile改为hadoop-2.0,修改之后的配置文件如下： 1234567891011121314 &lt;profile&gt; &lt;id&gt;hadoop-1.0&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;hadoop.profile&lt;/name&gt;&lt;value&gt;1.0&lt;/value&gt; &lt;/property&gt;&lt;profile&gt; &lt;id&gt;hadoop-2.0&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;!hadoop.profile&lt;/name&gt; &lt;/property&gt;]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译CDH HBase源代码并打补丁]]></title>
    <url>%2F2013%2F10%2F28%2Fcompile-hbase-source-code-and-apply-patches%2F</url>
    <content type="text"><![CDATA[写了一篇博客记录编译CDH HBase源代码并打补丁的过程，如有不正确的，欢迎指出！ 下载源代码从Cloudera github上下载最新分支源代码，例如：当前最新分支为cdh4-0.94.6_4.4.0 1$ git clone git@github.com:cloudera/hbase.git -b cdh4-0.94.6_4.4.0 cdh4-0.94.6_4.4.0 说明： -b 指定下载哪个分支 最后一个参数指定下载下来的文件名称 添加snappy压缩支持编译snappy12345$ svn checkout http://snappy.googlecode.com/svn/trunk/ snappy$ cd snappy$ sh autogen.sh$ ./configure$ sudo make install 编译hadoop-snappy降低gcc版本到4.4: 123$ sudo yum install gcc-4.4$ rm /usr/bin/gcc$ ln -s /usr/bin/gcc-4.4 /usr/bin/gcc 建立libjvm软连接 1$ sudo ln -s /usr/java/latest/jre/lib/amd64/server/libjvm.so /usr/local/lib/ 下载并编译hadoop-snappy 123$ svn checkout http://hadoop-snappy.googlecode.com/svn/trunk/ hadoop-snappy$ cd hadoop-snappy$ make package -Dsnappy.prefix=/usr/local/ 安装jar包到本地仓库 12$ mvn install:install-file -DgroupId=org.apache.hadoop -DartifactId=hadoop-snappy -Dversion=0.0.1-SNAPSHOT -Dpackaging=jar -Dfile=./target/hadoop-snappy-0.0.1-SNAPSHOT.jar$ mvn install:install-file -DgroupId=org.apache.hadoop -DartifactId=hadoop-snappy -Dversion=0.0.1-SNAPSHOT -Dclassifier=Linux-amd64-64 -Dpackaging=tar -Dfile=./target/hadoop-snappy-0.0.1-SNAPSHOT-Linux-amd64-64.tar 添加lzo压缩支持暂不在此列出，请参考网上文章。 编译Protobuf注意：目前只能装2.4.1版本的，装最新版本的可能会缺少文件。 123456$ wget https://protobuf.googlecode.com/files/protobuf-2.4.1.zip$ unzip protobuf-2.4.1.zip$ cd protobuf-2.4.1$ ./configure$ make$ sudo make install 测试是否安装成功，如果成功你会看到： 12$ protocMissing input file. 如果安装失败，你可能会看到： 12$ protocprotoc: error while loading shared libraries: libprotobuf.so.7: cannot open shared object file: No such file or directory 编译hbase进入到cdh4-0.94.6_4.4.0 目录，然后运行mvn基本命令。 12$ cd cdh4-0.94.6_4.4.0$ mvn clean install 忽略测试，请添加如下参数： 1-DskipTests 添加MAVEN运行时jvm大小，请在mvn前面添加如下参数： 1MAVEN_OPTS=&quot;-Xmx2g&quot; 生成javadoc和文档，请添加如下参数： 1javadoc:aggregate site assembly:single 生成release加入security和native包，请添加如下参数： 1-Prelease,security,native 基于hadoop2.0进行编译，请添加如下参数： 1-Dhadoop.profile=2.0 添加hadoop-snappy支持，请添加如下参数： 1-Prelease,hadoop-snappy -Dhadoop-snappy.version=0.0.1-SNAPSHOT 如果你添加了一些java代码，在每个文件头没有添加license，则需要添加如下参数： 1-Drat.numUnapprovedLicenses=200 综上，完整命令如下： 1$ MAVEN_OPTS="-Xmx2g" mvn clean install javadoc:aggregate site assembly:single -DskipTests -Prelease,security,native,hadoop-snappy -Drat.numUnapprovedLicenses=200 -Dhadoop.profile=2.0 -Dhadoop-snappy.version=0.0.1-SNAPSHOT 生成patch修改代码之后，在提交代码之前，运行如下命令生成patch： 1$ git diff &gt;../XXXXX.patch 如果你已经将该动文件加入到提交缓存区，即执行了如下代码： 1$ git add . 你可以使用如下代码打补丁： 1$ git diff --staged &gt;../XXXXX.patch 如果在提交之后，想生成patch，执行如下命令： 1$ git format-patch -1 git format-patch 的详细说明请参考：git patch操作 更多diff的命令如下： 1234567git diff &lt;file&gt; # 比较当前文件和暂存区文件差异git diffgit diff &lt;$id1&gt; &lt;$id2&gt; # 比较两次提交之间的差异git diff &lt;branch1&gt;..&lt;branch2&gt; # 在两个分支之间比较git diff --staged # 比较暂存区和版本库差异git diff --cached # 比较暂存区和版本库差异git diff --stat # 仅仅比较统计信息 打patch打patch： 1$ git apply ../XXXXX.patch 测试patch是否打成功： 1$ git apply --check ../add-aggregate-in-hbase-shell.patch 如果出现以下错误： 12$ git apply ../XXXXX.patchfatal: git apply: bad git-diff - expected /dev/null on line 4 请安装dos2unix： 1$ yum install dos2unix -y 然后，执行如下代码： 1$ dos2unix ../add-aggregate-in-hbase-shell.patch 最后再尝试打补丁。 注意： 请注意，git apply 是一个事务性操作的命令，也就是说，要么所有补丁都打上去，要么全部放弃。 对于传统的 diff 命令生成的补丁，则只能用 git apply 处理。对于 format-patch 制作的新式补丁，应当使用 git am 命令。 升级版本当你fork了Cloudera github代码之后，cloudera会继续更新代码、发布新的分支，如何将其最新的分支下载到自己的hbase仓库呢？例如，你的仓库中hbase最新分支为cdh4-0.94.6_4.3.0，而cdh最新分支为cdh4-0.94.6_4.4.0，现在如何将cdh上的分支下载到自己的参考呢？ 查看远程服务器地址和仓库名称： 123$ git remote -vorigin git@github.com:javachen/hbase.git (fetch)origin git@github.com:javachen/hbase.git (push) 添加远程仓库地址： 1$ git remote add cdh git@github.com:cloudera/hbase.git 再一次查看远程服务器地址和仓库名称： 12345$ git remote -vcdh https://github.com/cloudera/hbase (fetch)cdh https://github.com/cloudera/hbase (push)origin git@github.com:javachen/hbase.git (fetch)origin git@github.com:javachen/hbase.git (push) 抓取远程仓库更新： 1$ git fetch cdh 然后，再执行下面命令查看远程分支： 1$ git branch -r 下载cdh上的cdh4-0.94.6_4.4.0分支，在本地命名为cdh4-0.94.6_4.4.0： 1$ git checkout -b cdh4-0.94.6_4.4.0 cdh/cdh4-0.94.6_4.4.0 将本地的cdh4-0.94.6_4.4.0分支其提交到自己的远程仓库： 1$ git push origin cdh4-0.94.6_4.4.0:cdh4-0.94.6_4.4.0 排错如果在启动 hbase 的服务过程中出现如下日志： 1232013-10-24 22:44:59,921 INFO org.apache.hadoop.hbase.util.VersionInfo: HBase Unknown2013-10-24 22:44:59,921 INFO org.apache.hadoop.hbase.util.VersionInfo: Subversion Unknown -r Unknown2013-10-24 22:44:59,921 INFO org.apache.hadoop.hbase.util.VersionInfo: Compiled by Unknown on Unknown 请查看 src/saveVersion.sh 文件的编码及换行符是否和你的操作系统一致。编码应该设置为 UTF-8，如果你使用的是 linux 系统，则换行符应该为 unix/linux 换行符，不应该为 window 换行符。 参考文章 [1] Git常用命令备忘 [2] git patch操作 [3] Git Fetch拉取他人分支 [4] git根据commit生成patch]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive连接产生笛卡尔集]]></title>
    <url>%2F2013%2F10%2F17%2Fcartesian-product-in-hive-inner-join%2F</url>
    <content type="text"><![CDATA[在使用hive过程中遇到这样的一个异常： 1FAILED: ParseException line 1:18 Failed to recognize predicate &apos;a&apos;. Failed rule: &apos;kwInner&apos; in join type specifier 执行的hql语句如下： 1[root@javachen.com ~]# hive -e &apos;select a.* from t a, t b where a.id=b.id&apos; 从异常信息中很难看出出错原因，hive.log中也没有打印出详细的异常对战信息。改用jdbc连接hive-server2，可以看到hive-server2中提示如下异常信息： 123456789101112131415161718192021222313/10/17 09:57:48 ERROR ql.Driver: FAILED: ParseException line 1:18 Failed to recognize predicate &apos;a&apos;. Failed rule: &apos;kwInner&apos; in join type specifierorg.apache.hadoop.hive.ql.parse.ParseException: line 1:18 Failed to recognize predicate &apos;a&apos;. Failed rule: &apos;kwInner&apos; in join type specifier at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:446) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:441) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:349) at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:355) at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:95) at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:76) at org.apache.hive.service.cli.operation.SQLOperation.run(SQLOperation.java:114) at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:194) at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:155) at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:191) at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1193) at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1) at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) at org.apache.hive.service.cli.thrift.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:38) at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662) 从异常信息可以看到是在编译hql语句进行语法解析时出现了错误，到底为什么会出现Failed rule: &#39;kwInner&#39; in join type specifier这样的异常信息呢？ 在eclipse中查找关键字并没有找到相应代码，在Hive.g 中查找关键字“kwInner”，可以看到如下内容： 123456789101112joinToken@init &#123; msgs.push(&quot;join type specifier&quot;); &#125;@after &#123; msgs.pop(); &#125; : KW_JOIN -&gt; TOK_JOIN | kwInner KW_JOIN -&gt; TOK_JOIN | KW_CROSS KW_JOIN -&gt; TOK_CROSSJOIN | KW_LEFT KW_OUTER KW_JOIN -&gt; TOK_LEFTOUTERJOIN | KW_RIGHT KW_OUTER KW_JOIN -&gt; TOK_RIGHTOUTERJOIN | KW_FULL KW_OUTER KW_JOIN -&gt; TOK_FULLOUTERJOIN | KW_LEFT KW_SEMI KW_JOIN -&gt; TOK_LEFTSEMIJOIN ; 从上面可以看出hive支持的连接包括： join inner join cross join (as of Hive 0.10) left outer join right outer join full outer join left semi join kwInner为什么是小写呢，其含义是什么呢？搜索关键字，找到如下代码： 123kwInner:&#123;input.LT(1).getText().equalsIgnoreCase(&quot;inner&quot;)&#125;? Identifier; 上面的大概意思是找到输入左边的内容并判断其值在忽略大小写情况下是否等于inner，大概意思是hql语句中缺少inner关键字吧？修改下hql语句如下，然后执行： 1[root@javachen.com ~]# hive -e &apos;select a.* from t a inner join t b where a.id=b.id&apos; 修改后的hql语句能够正常运行，并且变成了内连接。在JION接连查询中没有ON连接key而通过WHERE条件语句会产生笛卡尔集。 Hive本身是不支持笛卡尔集的，不能用select T1.*, T2.* from table1, table2这种语法。但有时候确实需要用到笛卡尔集的时候，可以用下面的语法来实现同样的效果： 1select T1.*, T2.* from table1 T1 join table2 T2 where 1=1; 注意在Hive的Strict模式下不能用这种语法，因为这样会产生笛卡尔集，而这种模式禁止产生笛卡尔集。需要先用set hive.mapred.mode=nonstrict;设为非strict模式就可以用了，或者将where改为on连接。 1select T1.*, T2.* from table1 T1 join table2 T2 on T1.id=T2.id; 关于Strict ModeHive中的严格模式可以防止用户发出（可以有问题）的查询无意中造成不良的影响。 将hive.mapred.mode设置成strict可以禁止三种类型的查询： 1）、在一个分区表上，如果没有在WHERE条件中指明具体的分区，那么这是不允许的，换句话说，不允许在分区表上全表扫描。这种限制的原因是分区表通常会持非常大的数据集并且可能数据增长迅速，对这样的一个大表做全表扫描会消耗大量资源，必须要再WHERE过滤条件中具体指明分区才可以执行成功的查询。 2）、第二种是禁止执行有ORDER BY的排序要求但没有LIMIT语句的HiveQL查询。因为ORDER BY全局查询会导致有一个单一的reducer对所有的查询结果排序，如果对大数据集做排序，这将导致不可预期的执行时间，必须要加上limit条件才可以执行成功的查询。 3）、第三种是禁止产生笛卡尔集。在JION接连查询中没有ON连接key而通过WHERE条件语句会产生笛卡尔集，需要改为JOIN…ON语句。 参考文章 [1] 深入学习《Programing Hive》：Tuning [2] Hive Tips]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HiveServer2中使用jdbc客户端用户运行mapreduce]]></title>
    <url>%2F2013%2F10%2F17%2Frun-mapreduce-with-client-user-in-hive-server2%2F</url>
    <content type="text"><![CDATA[最近做了个web系统访问hive数据库，类似于官方自带的hwi、安居客的hwi改进版和大众点评的polestar(github地址)系统，但是和他们的实现不一样，查询Hive语句走的不是cli而是通过jdbc连接hive-server2。为了实现mapreduce任务中资源按用户调度，需要hive查询自动绑定当前用户、将该用户传到yarn服务端并使mapreduce程序以该用户运行。本文主要是记录实现该功能过程中遇到的一些问题以及解决方法,如果你有更好的方法和建议，欢迎留言发表您的看法！ 说明集群环境使用的是cdh4.3，没有开启kerberos认证。 写完这篇文章之后，在微博上收到@单超eric的评论，发现cdh4.3中hive-server2已经实现Impersonation功能，再此对@单超eric的帮助表示感谢。 so，你可以完全忽略本文后面的内容，直接看cloudera的HiveServer2 Impersonation是怎么做的。 hive-server2的启动先从hive-server2服务的启动开始说起。 如果你是以服务的方式启动hive-server2进程，则启动hive-server2的用户为hive,运行mapreduce的用户也为hive，启动脚本如下： 1/etc/init.d/hive-server2 start 如果你以命令行方式启动hive-server2进程，则启动hive-server2的用户为root,运行mapreduce的用户也为root，启动脚本如下： 1hive --service hiveserver2 为什么是上面的结论？这要从hive-server2的启动过程开始说明。 查看HiveServer2.java的代码可以看到，hive-server2启动时会依次启动cliService和thriftCLIService，查看cliService的init()方法，可以看到如下代码： 12345678910111213141516public synchronized void init(HiveConf hiveConf) &#123; this.hiveConf = hiveConf; sessionManager = new SessionManager(); addService(sessionManager); try &#123; HiveAuthFactory.loginFromKeytab(hiveConf); serverUserName = ShimLoader.getHadoopShims(). getShortUserName(ShimLoader.getHadoopShims().getUGIForConf(hiveConf)); &#125; catch (IOException e) &#123; throw new ServiceException(&quot;Unable to login to kerberos with given principal/keytab&quot;, e); &#125; catch (LoginException e) &#123; throw new ServiceException(&quot;Unable to login to kerberos with given principal/keytab&quot;, e); &#125; super.init(hiveConf); &#125; 从上面的代码可以看到在cliService初始化过程中会做登陆（从kertab中登陆）和获取用户名的操作： 1ShimLoader.getHadoopShims().getUGIForConf(hiveConf) 上面代码最终会调用HadoopShimsSecure类的getUGIForConf方法： 1234@Overridepublic UserGroupInformation getUGIForConf(Configuration conf) throws IOException &#123; return UserGroupInformation.getCurrentUser();&#125; UserGroupInformation.getCurrentUser()代码如下： 12345678910public synchronized static UserGroupInformation getCurrentUser() throws IOException &#123; AccessControlContext context = AccessController.getContext(); Subject subject = Subject.getSubject(context); if (subject == null || subject.getPrincipals(User.class).isEmpty()) &#123; return getLoginUser(); &#125; else &#123; return new UserGroupInformation(subject); &#125; &#125; 因为这时候服务刚启动，subject为空，故if分支会调用getLoginUser()方法，其代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940public synchronized static UserGroupInformation getLoginUser() throws IOException &#123; if (loginUser == null) &#123; try &#123; Subject subject = new Subject(); LoginContext login; if (isSecurityEnabled()) &#123; login = newLoginContext(HadoopConfiguration.USER_KERBEROS_CONFIG_NAME, subject, new HadoopConfiguration()); &#125; else &#123; login = newLoginContext(HadoopConfiguration.SIMPLE_CONFIG_NAME, subject, new HadoopConfiguration()); &#125; login.login(); loginUser = new UserGroupInformation(subject); loginUser.setLogin(login); loginUser.setAuthenticationMethod(isSecurityEnabled() ? AuthenticationMethod.KERBEROS : AuthenticationMethod.SIMPLE); loginUser = new UserGroupInformation(login.getSubject()); String fileLocation = System.getenv(HADOOP_TOKEN_FILE_LOCATION); if (fileLocation != null) &#123; // Load the token storage file and put all of the tokens into the // user. Don&apos;t use the FileSystem API for reading since it has a lock // cycle (HADOOP-9212). Credentials cred = Credentials.readTokenStorageFile( new File(fileLocation), conf); loginUser.addCredentials(cred); &#125; loginUser.spawnAutoRenewalThreadForUserCreds(); &#125; catch (LoginException le) &#123; LOG.debug(&quot;failure to login&quot;, le); throw new IOException(&quot;failure to login&quot;, le); &#125; if (LOG.isDebugEnabled()) &#123; LOG.debug(&quot;UGI loginUser:&quot;+loginUser); &#125; &#125; return loginUser;&#125; 因为是第一次调用getLoginUser(),故loginUser为空，接下来会创建LoginContext并调用其login方法，login方法最终会调用HadoopLoginModule的commit()方法。 下图是从hive-server2启动到执行HadoopLoginModule的commit()方法的调用图： 获取登陆用户的关键代码就在commit()，逻辑如下： 如果使用了kerberos，则为kerberos登陆用户。hive-server2中如何使用kerberos登陆，请查看官方文档。 如果kerberos用户为空并且没有开启security，则从系统环境变量中取HADOOP_USER_NAME的值 如果环境变量中没有设置HADOOP_USER_NAME，则使用系统用户，即启动hive-server2进程的用户。 小结hive-server2启动过程中会做登陆操作并获取到登陆用户，启动之后再次调用UserGroupInformation.getCurrentUser()取到的用户就为登陆用户了，这样会导致所有请求到hive-server2的hql最后都会以这个用户来运行mapreduce。 提交hive任务现在来看hive任务是怎么提交到yarn服务端然后运行mapreduce的。 为了调试简单，我在本机eclipse的hive源代码中配置hive-site.xml、core-site.xml、mapred.xml、yarn-site.xml连接测试集群,添加缺少的yarn依赖并解决hive-builtins中报错的问题，然后运行HiveServer2类的main方法。_注意_，我的电脑当前登陆用户为june，故启动hive-server2的用户为june。 然后，在运行jdbc测试类，运行一个简单的sql语句，大概如下： 1234567891011121314151617181920212223public static void test() &#123; try &#123; Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;); Connection conn = DriverManager.getConnection( &quot;jdbc:hive2://june-mint:10000/default&quot;, &quot;&quot;, &quot;&quot;); Statement stmt = conn.createStatement(); ResultSet rs = stmt.executeQuery(&quot;select count(1) from t&quot;); while (rs.next()) System.out.println(rs.getString(1)); rs.close(); stmt.close(); conn.close(); &#125; catch (SQLException se) &#123; se.printStackTrace(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 查看yarn监控地址http://192.168.56.101:8088/cluster，可以看到提交的mapreduce任务由june用户来运行。 如何修改mapreduce任务的运行用户呢？如果了解hive提交mapreduce任务的过程的话，就应该知道hive任务会通过org.apache.hadoop.mapred.JobClient来提交。在JobClient的init方法中有如下代码： 12345public void init(JobConf conf) throws IOException &#123; setConf(conf); cluster = new Cluster(conf); clientUgi = UserGroupInformation.getCurrentUser();&#125; JobClient类中提交mapreduce任务的代码如下，见submitJobInternal方法： 123456789Job job = clientUgi.doAs(new PrivilegedExceptionAction&lt;Job&gt; () &#123; @Override public Job run() throws IOException, ClassNotFoundException, InterruptedException &#123; Job job = Job.getInstance(conf); job.submit(); return job; &#125;&#125;); 从前面知道，hive-server2启动中会进行登陆操作并且登陆用户为june，故clientUgi对应的登陆用户也为june，故提交的mapreduce任务也通过june用户来运行。 如何修改源代码从上面代码可以知道，修改clientUgi的获取方式就可以改变提交任务的用户。UserGroupInformation中存在如下静态方法： 12345678910public static UserGroupInformation createRemoteUser(String user) &#123; if (user == null || &quot;&quot;.equals(user)) &#123; throw new IllegalArgumentException(&quot;Null user&quot;); &#125; Subject subject = new Subject(); subject.getPrincipals().add(new User(user)); UserGroupInformation result = new UserGroupInformation(subject); result.setAuthenticationMethod(AuthenticationMethod.SIMPLE); return result;&#125; 故可以尝试使用该方法，修改JobClient的init方法如下： 12345678910public void init(JobConf conf) throws IOException &#123; setConf(conf); cluster = new Cluster(conf); if(UserGroupInformation.isSecurityEnabled())&#123; clientUgi = UserGroupInformation.getCurrentUser(); &#125;else&#123; String user = conf.get(&quot;myExecuteName&quot;,&quot;NoName&quot;); clientUgi = UserGroupInformation.createRemoteUser(user); &#125; &#125; 上面代码是在没有开启security的情况下，从环境变量（myExecuteName）获取jdbc客户端指定的用户名，然后创建一个远程的UserGroupInformation。 为什么从环境变量中获取用户名称？ 在不考虑安全的情况下，可以由客户端任意指定用户。 没有使用jdbc连接信息中的用户，是因为这样会导致每次获取jdbc连接的时候都要指定用户名，这样就没法使用已有的连接池。 编译代码、替换class文件，然后重新运行HiveServer2以及jdbc测试类，查看yarn监控地址http://192.168.56.101:8088/cluster，截图如下： 这时候mapreduce的运行用户变为NoName，这是因为从JobConf环境变量中找不到myExecuteName变量而使用默认值NoName的原因。 查看hive-server2运行日志，会发现任务运行失败，关键异常信息如下： 123456789101112Caused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=NoName, access=WRITE, inode=&quot;/tmp/hive-june/hive_2013-10-18_21-18-12_812_378750610917949668/_tmp.-ext-10001&quot;:june:hadoop:drwxr-xr-x at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:224) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:204) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:149) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:4705) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:4687) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:4661) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renameToInternal(FSNamesystem.java:2696) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renameToInt(FSNamesystem.java:2663) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renameTo(FSNamesystem.java:2642) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rename(NameNodeRpcServer.java:610) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.rename 出现上述异常是因为，mapreduce任务在运行过程中会生成一些临时文件，而NoName用户对临时文件没有写的权限，这些临时文件属于june用户。查看hdfs文件如下： 1234[root@edh1 lib]# hadoop fs -ls /tmp/Found 6 itemsdrwx------ - june hadoop 0 2013-10-15 01:33 /tmp/hadoop-yarndrwxr-xr-x - june hadoop 0 2013-10-16 06:52 /tmp/hive-june /tmp/hive-june是hive执行过程中保存在hdfs的路径，由hive.exec.scratchdir定义，其默认值为/tmp/hive-${user.name}，而且这个文件是在org.apache.hadoop.hive.ql.Context类的构造方法中获取并在ExecDriver类的execute(DriverContext driverContext)方法中创建的。 类似这样的权限问题还会出现在hdfs文件重命名、删除临时目录的时候。为了避免出现这样的异常，需要修改hive.exec.scratchdir为当前用户对应的临时目录路径，并使用当前登陆用户创建、重命名、删除临时目录。 修改获取hive.exec.scratchdir对应的临时目录代码如下，在Context类的够找方法中修改： 12345678910String user = conf.get(myExecuteName，“”);if (user != null &amp;&amp; user.trim().length() &gt; 0) &#123; nonLocalScratchPath = new Path(&quot;/tmp/hive-&quot; + user, executionId);&#125; else &#123; nonLocalScratchPath = new Path(HiveConf.getVar(conf, HiveConf.ConfVars.SCRATCHDIR), executionId);&#125; 找到这些操作对应的代码似乎太过复杂了，修改的地方也有很多，因为这里是使用的hive-server2，故在对应的jdbc代码中修改似乎会简单很多，例如修改HiveSessionImpl类的以下三个方法： 12345public OperationHandle executeStatement(String statement, Map&lt;String, String&gt; confOverlay) throws HiveSQLException&#123;&#125;public void cancelOperation(final OperationHandle opHandle) throws HiveSQLException &#123;&#125;public void closeOperation(final OperationHandle opHandle) throws HiveSQLException &#123;&#125; 第一个方法是运行sql语句，第二个方法是取消运行，第三个方法是关闭连接。 executeStatement中所做的修改如下，将operation.run();改为： 12345678910111213141516171819if (operation instanceof SQLOperation) &#123; try &#123; String user = hiveConf.getVar(ConfVars.HIVE_SERVER2_MAPREDUCE_USERNAME); ugi = UserGroupInformation.createRemoteUser(user); ugi.doAs(new PrivilegedExceptionAction&lt;CommandProcessorResponse&gt;() &#123; @Override public CommandProcessorResponse run() throws HiveSQLException &#123; operation.run(); return null; &#125; &#125;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; operation.run(); &#125; 这里添加了判断，当operation操作时，才执行下面代码，这是为了保证从hive环境变量中获取myExecuteName的值不为空时才创建UserGroupInformation。 myExecuteName是新定义的hive变量，主要是用于jdbc客户端通过set语句设置myExecuteName的值为当前登陆用户名称，然后在执行sql语句。代码如下： 1234567Statement stmt = conn.createStatement();stmt.execute(&quot;set myExecuteName=aaaa&quot;);ResultSet rs = stmt.executeQuery(&quot;select count(1) from t&quot;);while (rs.next()) System.out.println(rs.getString(1)); 小结上面修改的类包括： 123org.apache.hadoop.mapred.JobClient //从环境变量获取从jdbc客户端传过来的用户，即myExecuteName的值，然后以该值运行mapreduce用户org.apache.hadoop.hive.ql.Context //修改hive.exec.scratchdir的地址为从jdbc客户端传过来的用户对应的临时目录org.apache.hive.service.cli.session.HiveSessionImpl //修改运行sql、取消操作、关闭连接对应的方法 测试是用javachen用户测试,hdfs上的临时目录如下： 1234567[root@edh1 lib]# hadoop fs -ls /tmp/Found 7 itemsdrwx------ - june hadoop 0 2013-10-15 01:33 /tmp/hadoop-yarndrwxr-xr-x - javachen.com hadoop 0 2013-10-16 07:30 /tmp/hive-javachen.comdrwxr-xr-x - june hadoop 0 2013-10-16 06:52 /tmp/hive-junedrwxr-xr-x - root hadoop 0 2013-10-15 14:13 /tmp/hive-rootdrwxrwxrwt - yarn mapred 0 2013-10-16 07:30 /tmp/logs 监控页面截图： 除了简单测试之外，还需要测试修改后的代码是否影响源代码的运行以及hive cli的运行。 参考文章 HiveServer2 Impersonation CDH4 HiveServer2 Security Configuration Enjoy it ！]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>hiveserver2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最近的工作]]></title>
    <url>%2F2013%2F09%2F08%2Frecent-work%2F</url>
    <content type="text"><![CDATA[最近一直在构思这篇博客的内容，到现在还是不知道从何下手。自从将博客从wordpress迁移到github上之后，就很少在博客写一些关于工作和生活的文章，所以想写一篇关于工作的博客，记录最近做过的事情以及一些当时的所思所想。 最近半年多一直在做hadoop方面的工作，也就是接触hadoop才半年多时间。最开始接触hadoop是去年的11月21日，那天去Intel公司参加了两天的hadoop培训。培训的内容很多干货也有很多枯燥的东西，所以边听边瞌睡的听完了两天的培训内容。培训的ppt打印出来了，时不时地会翻看上面讲述的内容，然后在网上搜索些相关的资料。 最先接触的hadoop发行版是Intel的IDH，刚开始使用IDH也就是用他的管理界面安装、部署hadoop集群，然后在8节点的集群上作hive两表关联的测试。测试结果不是很满意，但是对IDH倒是印象深刻。IDH的前端使用GWT开发，界面简洁，操作也比较方便，只是同步配置文件有时候非常慢，要等上一杯咖啡的时间。 IDH的源代码不开源，所以遇到一些问题的时候，只能先自己摸索。我不喜欢闭源也不喜欢许可证以及混淆代码什么的。虽然java代码的混淆了，但是shell和puppet脚本还是能够很容易读懂的。参考IDH的部署安装脚本，我在试着用shell编写一个hadoop的安装部署脚本，这个工作还在慢慢进行中。也许在弄懂puppet的原理和代码之后，我会简化、改进IDH的脚本；也许会使用公司使用的saltstack来完善这个部署脚本。 IDH实现了hive over hbase，这是一个很不错的特性，其基于hbase的协作器，代码实现并不复杂。IDH中有个hmon用于监控hadoop，我已经把这部分代码反编译了。 在hadoop版本选择的过程中，体验了Cloudera的CDH。首先是CDH的压缩包手动安装hadoop集群，一点点的修改配置文件，直到最后集群能够成功启动，这种方式安装的hadoop集群不包含本地lib文件；然后又试着解压缩Cloudera-manager.bin文件，尝试在虚拟机中不连接网络的情况下通过Cloudera-manager来安装配置集群，在使用了一段时间之后，发现Cloudera-manager没有使用操作系统的service服务来管理hadoop组件的启动和停止，而是有自己一套实现来管理集群，再加上Cloudera-manager也不开源，故放弃了使用Cloudera-manager来安装集群的方式；最后，最后是使用rpm方式来安装hadoop，安装过程倒是不复杂，只是以后如果自己修改了源代码时候升级不是很方便了，Cloudera的github上有个cdh-package项目，这个其实就是apache的bigtop项目，试过通过这个来编译出hadoop的rpm包，没有成功。 IDH通过本地文件来保存集群的配置信息，而CDH将这些信息保存到数据库了。CDH的Cloudera-manager的web界面基于bootstrap和jquery插件，ui做的很不错，通过反编译java代码，已经知道了其web界面的实现方式以及编译成功了部分java代码。CDH的Cloudera-manager和IDH-manager也很大不同，我想在这两个的基础上也实现一个hadoop的manager，这是我个人想法，还需要研究、整理出他们的实现思路，然后一点点的构思自己该怎么做。Cloudera-manager免费版简称CMF。 差不多两个月前，公司想做个hive的查询界面，这个东西其实就是和hwi、hue差不多的个东西。基于Spring，我很快搭好了框架，然后把CMF其中的css和js都迁移过来了。这是我第一次接触bootstrap，稍微修改下代码一个前台框架就弄好了，CMF最主要是使用了require.js使javascript代码模块化，这东西改起来也很简单，我把CMF中大部分基础javascript代码都移到了我搭好的框架中。搭好这个框架没花多少时间，但后来公司不打算使用这一套框架，以后有时间我会继续基于这个框架开发个hadoop的管理界面。 在使用ganglia监控hadoop的过程中，有时候需要找一个监控指标需要花好长时间，而且一个页面上显示的指标太多的时候，这个页面会反应不过来。hortonworks的HDP发行版中似乎对ganglia做了些修改，具体不知道改了什么。HDP发行版没有使用和研究过，只是下载了1.3和2.0两个版本的VM，然后看了看其中的hue，觉得做的很不错，只可惜是python写的。 在使用hadoop的过程中，觉得hadoop的门槛对于用户来说还是有点高。用户需要学习hive语法，写出的sql语句通常都不是最优sql，如果有个sql优化器自动帮用户优化sql语句就好了。hbase用于监控业务日志，数据建模和编写代码查询数据对于业务人员来说难度也太大了，如果能够适度封装，让业务人员不用关心hadoop的细节，只需要编写sql语句就能查询数据该多好啊！ 其他工作：hive和mapreduce调优。 上面是最近在做的一些事情，包括暂停没做的、正在做的以及还未做的。有时候觉得有些事情很简单，但没有时间、没有精力也没有自由一下子做完，有些时候人在江湖，身不由己。]]></content>
      <categories>
        <category>work</category>
      </categories>
      <tags>
        <tag>work</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive中如何确定map数]]></title>
    <url>%2F2013%2F09%2F04%2Fhow-to-decide-map-number%2F</url>
    <content type="text"><![CDATA[Hive 是基于 Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的 sql 查询功能，可以将 sq l语句转换为 MapReduce 任务进行运行。当运行一个 hql 语句的时候，map 数是如何计算出来的呢？有哪些方法可以调整 map 数呢？ 本文测试集群版本：cdh-4.3.0 。 hive 默认的 input format在 cdh-4.3.0 的 hive 中查看 hive.input.format 值（为什么是hive.input.format？）： 12hive&gt; set hive.input.format;hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 可以看到默认值为 CombineHiveInputFormat，如果你使用的是 IDH 的hive，则默认值为： 12hive&gt; set hive.input.format;hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat; CombineHiveInputFormat 类继承自 HiveInputFormat，而 HiveInputFormat 实现了 org.apache.hadoop.mapred.InputFormat 接口，关于 InputFormat 的分析，可以参考Hadoop深入学习：InputFormat组件. InputFormat 接口功能简单来说，InputFormat 主要用于描述输入数据的格式，提供了以下两个功能： 1)、数据切分，按照某个策略将输入数据且分成若干个 split，以便确定 Map Task 的个数即 Mapper 的个数，在 MapReduce 框架中，一个 split 就意味着需要一个 Map Task; 2)、为 Mapper 提供输入数据，即给定一个 split(使用其中的 RecordReader 对象)将之解析为一个个的 key/value 键值对。 该类接口定义如下： 1234public interface InputFormat&lt;K,V&gt;&#123; public InputSplit[] getSplits(JobConf job,int numSplits) throws IOException; public RecordReader&lt;K,V&gt; getRecordReader(InputSplit split,JobConf job,Reporter reporter) throws IOException; &#125; 其中，getSplit() 方法主要用于切分数据，每一份数据由，split 只是在逻辑上对数据分片，并不会在磁盘上将数据切分成 split 物理分片，实际上数据在 HDFS 上还是以 block 为基本单位来存储数据的。InputSplit 只记录了 Mapper 要处理的数据的元数据信息，如起始位置、长度和所在的节点。 MapReduce 自带了一些 InputFormat 的实现类： hive 中有一些 InputFormat 的实现类，如： 123456789AvroContainerInputFormatRCFileBlockMergeInputFormatRCFileInputFormatFlatFileInputFormatOneNullRowInputFormatReworkMapredInputFormatSymbolicInputFormatSymlinkTextInputFormatHiveInputFormat HiveInputFormat 的子类有： HiveInputFormat以 HiveInputFormat 为例，看看其getSplit()方法逻辑： 1234567891011121314151617181920212223242526for (Path dir : dirs) &#123; PartitionDesc part = getPartitionDescFromPath(pathToPartitionInfo, dir); // create a new InputFormat instance if this is the first time to see this // class Class inputFormatClass = part.getInputFileFormatClass(); InputFormat inputFormat = getInputFormatFromCache(inputFormatClass, job); Utilities.copyTableJobPropertiesToConf(part.getTableDesc(), newjob); // Make filter pushdown information available to getSplits. ArrayList&lt;String&gt; aliases = mrwork.getPathToAliases().get(dir.toUri().toString()); if ((aliases != null) &amp;&amp; (aliases.size() == 1)) &#123; Operator op = mrwork.getAliasToWork().get(aliases.get(0)); if ((op != null) &amp;&amp; (op instanceof TableScanOperator)) &#123; TableScanOperator tableScan = (TableScanOperator) op; pushFilters(newjob, tableScan); &#125; &#125; FileInputFormat.setInputPaths(newjob, dir); newjob.setInputFormat(inputFormat.getClass()); InputSplit[] iss = inputFormat.getSplits(newjob, numSplits / dirs.length); for (InputSplit is : iss) &#123; result.add(new HiveInputSplit(is, inputFormatClass.getName())); &#125;&#125; 上面代码主要过程是： 遍历每个输入目录，然后获得 PartitionDesc 对象，从该对象调用 getInputFileFormatClass 方法得到实际的 InputFormat 类，并调用其 getSplits(newjob, numSplits / dirs.length) 方法。 按照上面代码逻辑，似乎 hive 中每一个表都应该有一个 InputFormat 实现类。在 hive 中运行下面代码，可以查看建表语句： 123456789101112131415161718192021222324252627282930hive&gt; show create table info; OKCREATE TABLE info( statist_date string, statistics_date string, inner_code string, office_no string, window_no string, ticket_no string, id_kind string, id_no string, id_name string, area_center_code string)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\;' LINES TERMINATED BY '\n' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'LOCATION 'hdfs://node:8020/user/hive/warehouse/info'TBLPROPERTIES ( 'numPartitions'='0', 'numFiles'='1', 'transient_lastDdlTime'='1378245263', 'numRows'='0', 'totalSize'='301240320', 'rawDataSize'='0')Time taken: 0.497 seconds 从上面可以看到 info 表的 INPUTFORMAT 为org.apache.hadoop.mapred.TextInputFormat，TextInputFormat 继承自FileInputFormat。FileInputFormat 是一个抽象类，它最重要的功能是为各种 InputFormat 提供统一的 getSplits()方法，该方法最核心的是文件切分算法和 Host 选择算法。 算法如下： 123456789101112131415long length = file.getLen();long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);long minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);long blockSize = file.getBlockSize();long splitSize = computeSplitSize(goalSize, minSize, blockSize);long bytesRemaining = length;while (((double) bytesRemaining)/splitSize &gt; SPLIT_SLOP) &#123;String[] splitHosts = getSplitHosts(blkLocations, length-bytesRemaining, splitSize, clusterMap); splits.add(makeSplit(path, length-bytesRemaining, splitSize, splitHosts)); bytesRemaining -= splitSize;&#125; 华丽的分割线：以下摘抄自Hadoop深入学习：InputFormat组件 1）文件切分算法 文件切分算法主要用于确定InputSplit的个数以及每个InputSplit对应的数据段，FileInputSplit以文件为单位切分生成InputSplit。有三个属性值来确定InputSplit的个数： goalSize：该值由 totalSize/numSplits 来确定 InputSplit 的长度，它是根据用户的期望的 InputSplit 个数计算出来的；numSplits 为用户设定的 Map Task 的个数，默认为1。 minSize：由配置参数 mapred.min.split.size（或者 mapreduce.input.fileinputformat.split.minsize）决定的 InputForma t的最小长度，默认为1。 blockSize：HDFS 中的文件存储块block的大小，默认为64MB。 numSplits=mapred.map.tasks 或者 mapreduce.job.maps 这三个参数决定一个 InputFormat 分片的最终的长度，计算方法如下： 1splitSize = max&#123;minSize,min&#123;goalSize,blockSize&#125;&#125; 计算出了分片的长度后，也就确定了 InputFormat 的数目。 2）host 选择算法 InputFormat 的切分方案确定后，接下来就是要确定每一个 InputSplit 的元数据信息。InputSplit 元数据通常包括四部分，&lt;file,start,length,hosts&gt;其意义为： file 标识 InputSplit 分片所在的文件； InputSplit 分片在文件中的的起始位置； InputSplit 分片的长度； 分片所在的 host 节点的列表。 InputSplit 的 host 列表的算作策略直接影响到运行作业的本地性。 我们知道，由于大文件存储在 HDFS上的 block 可能会遍布整个 Hadoop 集群，而一个 InputSplit 分片的划分算法可能会导致一个 split 分片对应多个不在同一个节点上的 blocks，这就会使得在 Map Task 执行过程中会涉及到读其他节点上的属于该 Task 的 block 中的数据，从而不能实现数据本地性，而造成更多的网络传输开销。 一个 InputSplit 分片对应的 blocks 可能位于多个数据节点地上，但是基于任务调度的效率，通常情况下，不会把一个分片涉及的所有的节点信息都加到其host列表中，而是选择包含该分片的数据总量的最大的前几个节点，作为任务调度时判断是否具有本地性的主要凭证。 FileInputFormat 使用了一个启发式的 host 选择算法：首先按照 rack 机架包含的数据量对 rack 排序，然后再在 rack 内部按照每个 node 节点包含的数据量对 node 排序，最后选取前 N 个(N 为 block 的副本数)，node 的 host 作为 InputSplit 分片的 host 列表。当任务地调度 Task 作业时，只要将 Task 调度给 host 列表上的节点，就可以认为该 Task 满足了本地性。 从上面的信息我们可以知道，当 InputSplit 分片的大小大于 block 的大小时，Map Task 并不能完全满足数据的本地性，总有一本分的数据要通过网络从远程节点上读数据，故为了提高 Map Task 的数据本地性，减少网络传输的开销，应尽量是 InputFormat 的大小和 HDFS 的 block 块大小相同。 CombineHiveInputFormatgetSplits(JobConf job, int numSplits) 代码运行过程如下： 12345init(job);CombineFileInputFormatShim combine = ShimLoader.getHadoopShims().getCombineFileInputFormat(); ShimLoader.loadShims(HADOOP_SHIM_CLASSES, HadoopShims.class); Hadoop23Shims HadoopShimsSecure.getCombineFileInputFormat() CombineFileInputFormatShim 继承了org.apache.hadoop.mapred.lib.CombineFileInputFormat，CombineFileInputFormatShim 的 getSplits 方法代码如下： 12345678910111213141516171819202122232425public InputSplitShim[] getSplits(JobConf job, int numSplits) throws IOException &#123; long minSize = job.getLong("mapred.min.split.size", 0); // For backward compatibility, let the above parameter be used if (job.getLong("mapred.min.split.size.per.node", 0) == 0) &#123; super.setMinSplitSizeNode(minSize); &#125; if (job.getLong("mapred.min.split.size.per.rack", 0) == 0) &#123; super.setMinSplitSizeRack(minSize); &#125; if (job.getLong("mapred.max.split.size", 0) == 0) &#123; super.setMaxSplitSize(minSize); &#125; InputSplit[] splits = (InputSplit[]) super.getSplits(job, numSplits); InputSplitShim[] isplits = new InputSplitShim[splits.length]; for (int pos = 0; pos &lt; splits.length; pos++) &#123; isplits[pos] = new InputSplitShim((CombineFileSplit)splits[pos]); &#125; return isplits;&#125; 从上面代码可以看出，如果为 CombineHiveInputFormat，则以下四个参数起作用： mapred.min.split.size 或者 mapreduce.input.fileinputformat.split.minsize。 mapred.max.split.size 或者 mapreduce.input.fileinputformat.split.maxsize。 mapred.min.split.size.per.rack 或者 mapreduce.input.fileinputformat.split.minsize.per.rack。 mapred.min.split.size.per.node 或者 mapreduce.input.fileinputformat.split.minsize.per.node。 CombineFileInputFormatShim 的 getSplits 方法最终会调用父类的 getSplits 方法，拆分算法如下： 123456789101112131415161718192021long left = locations[i].getLength();long myOffset = locations[i].getOffset();long myLength = 0;do &#123; if (maxSize == 0) &#123; myLength = left; &#125; else &#123; if (left &gt; maxSize &amp;&amp; left &lt; 2 * maxSize) &#123; myLength = left / 2; &#125; else &#123; myLength = Math.min(maxSize, left); &#125; &#125; OneBlockInfo oneblock = new OneBlockInfo(path, myOffset, myLength, locations[i].getHosts(), locations[i] .getTopologyPaths()); left -= myLength; myOffset += myLength; blocksList.add(oneblock);&#125; while (left &gt; 0); hive 中如何确定 map 数总上总结如下： 如果 hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat，则这时候的参数如下： 123456hive&gt; set mapred.min.split.size;mapred.min.split.size=1hive&gt; set mapred.map.tasks;mapred.map.tasks=2hive&gt; set dfs.blocksize;dfs.blocksize=134217728 上面参数中 mapred.map.tasks 为2，dfs.blocksize（使用的是 cdh-4.3.0 版本的 hadoop，这里 block 和 size 之间没有逗号）为128M。 假设有一个文件为200M，则按上面 HiveInputFormat 的 split 算法： 1、文件总大小为200M，goalSize=200M /2 =100M，minSize=1 ，splitSize = max{1,min{100M,128M}} =100M 2、200M / 100M &gt;1.1,故第一块大小为100M 3、剩下文件大小为100M，小于128M，故第二块大小为100M。 如果 hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat，则这时候的参数如下： 12345678910hive&gt; set mapred.min.split.size;mapred.min.split.size=1hive&gt; set mapred.max.split.size;mapred.max.split.size=67108864hive&gt; set mapred.min.split.size.per.rack;mapred.min.split.size.per.rack=1hive&gt; set mapred.min.split.size.per.node;mapred.min.split.size.per.node=1hive&gt; set dfs.blocksize;dfs.blocksize=134217728 上面参数中 mapred.max.split.size 为64M，dfs.blocksize 为128M。 假设有一个文件为200M，则按上面 CombineHiveInputFormat 的 split 算法： 1、128M &lt; 200M &lt;128M X 2，故第一个block大小为128M 2、剩下文件大小为200M-128M=72M，72M &lt; 128M,故第二块大小为72M 总结网上有一些文章关于 hive 中如何控制 map 数的文章是否考虑的不够全面，没有具体情况具体分析。简而言之，当 InputFormat 的实现类为不同类时，拆分块算法都不一样，相关设置参数也不一样，需要具体分析。 1. map 数不是越多越好如果一个任务有很多小文件（远远小于块大小128m）,则每个小文件也会被当做一个块，用一个 map 任务来完成，而一个 map 任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的 map 数是受限的。 2. 如何适当的增加 map 数？ 将数据导入到 hive 前，手动将大文件拆分为小文件 指定 map 数，使用 insert 或者 create as select 语句将一个表导入到另一个表，然后对另一张表做查询 3. 一些经验 合并小文件可以减少 map 数，但是会增加网络 IO。 尽量使拆分块大小和 hdfs 的块大小接近，避免一个拆分块大小上的多个 hdfs 块位于不同数据节点，从而降低网络 IO。 根据实际情况，控制 map 数量需要遵循两个原则：使大数据量利用合适的map数；使单个map任务处理合适的数据量。 参考文章 [1] hive的查询注意事项以及优化总结 [2] Hadoop中map数的计算 [3] [Hive]从一个经典案例看优化mapred.map.tasks的重要性 [4] hive优化之——控制hive任务中的map数和reduce数 [5] Hadoop Job Tuning [6] Hive配置项的含义详解（2） [7] Hive小文件合并调研 [8] Hadoop深入学习：InputFormat组件]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的jekyll配置和修改]]></title>
    <url>%2F2013%2F08%2F31%2Fmy-jekyll-config%2F</url>
    <content type="text"><![CDATA[主要记录使用jekyll搭建博客时的一些配置和修改。 注意： 使用时请删除{和%以及{和{之间的空格。 预览文章source ~/.bash_profile jekyll server添加about me 边栏参考the5fire的技术博客在index.html页面加入如下代码： 1234567891011&lt;section&gt;&lt;h4&gt;About me&lt;/h4&gt;&lt;div&gt; 一个Java方案架构师，主要从事hadoop相关工作。&lt;a href=&quot;/about.html&quot;&gt;更多信息&lt;/a&gt; &lt;br/&gt;&lt;br/&gt;&lt;strong&gt;&lt;font color=&quot;red&quot;&gt;&lt;a href=&quot;/atom.xml&quot; target=&quot;_blank&quot;&gt;订阅本站&lt;/a&gt;&lt;/font&gt;&lt;/strong&gt;&lt;br/&gt;&lt;br/&gt;联系博主：javachen.june[a]gmail.com&lt;/div&gt;&lt;/section&gt; 添加about页面在根目录创建about.md并修改，注意：文件开头几行内容如下 title: About layout: page group: navigation设置固定链接在 _config.yml 里，找到 permalink，设置如下： permalink: /:categories/:year/:month/:day/:title 修改，markdown实现为redcarpet首先通过gem安装redcarpet，然后修改_config.yml： 12redcarpet: extensions: [&quot;no_intra_emphasis&quot;, &quot;fenced_code_blocks&quot;, &quot;autolink&quot;, &quot;tables&quot;, &quot;strikethrough&quot;, &quot;superscript&quot;, &quot;with_toc_data&quot;, &quot;highlight&quot;, &quot;prettify&quot;] 首页添加最近文章在index.html页面 12345678&lt;section&gt;&lt;h4&gt;Recent Posts&lt;/h4&gt;&lt;ul id=&quot;recent_posts&quot;&gt;&#123; % for rpost in site.posts limit: 15 %&#125;&lt;li class=&quot;post&quot;&gt;&lt;a href=&quot;&#123;&#123; BASE_PATH &#125;&#125;&#123;&#123; rpost.url &#125;&#125;&quot;&gt;&#123;&#123; rpost.title &#125;&#125;&lt;/a&gt;&lt;/li&gt;&#123; % endfor %&#125;&lt;/ul&gt;&lt;/section&gt; 首页为每篇文章添加分类、标签、发表日期以及评论连接在index.html页面找到&lt;h3&gt;&lt;a href=&quot;{ { BASE_PATH }}{ { post.url }}&quot;&gt;{ { post.title }}&lt;/a&gt;&lt;/h3&gt;，在下面添加： 1234567891011121314151617181920 &lt;div class=&quot;c9&quot;&gt; Categories： &#123; %for cg in post.categories % &#125; &lt;a href=&quot;/categories.html#&#123;&#123;cg&#125;&#125;-ref&quot;&gt;&#123;&#123;cg&#125;&#125;&lt;/a&gt; &#123; %if forloop.index &lt; forloop.length % &#125; , &#123; %endif%&#125; &#123; %endfor%&#125; | Tags： &#123; %for cg in post.tags %&#125; &lt;a href=&quot;/tags.html#&#123;&#123;cg&#125;&#125;-ref&quot;&gt;&#123;&#123;cg&#125;&#125;&lt;/a&gt; &#123; %if forloop.index &lt; forloop.length %&#125; , &#123; %endif%&#125; &#123; %endfor%&#125; | Time：&lt;time date=&quot;&#123; &#123; post.date|date: &apos;%Y-%m-%d&apos; &#125;&#125;&quot;&gt;&#123;&#123; post.date|date: &quot;%Y-%m-%d&quot;&#125;&#125;&lt;/time&gt; &lt;a href=&apos;&#123;&#123;post.url&#125;&#125;#comments&apos; title=&apos;分享文章、查看评论&apos; style=&quot;float:right;margin-right:.5em;&quot;&gt;Comments&lt;/a&gt;&lt;/div&gt; 修改h1、h2等标题字体主要是参考图灵社区的css，在assets/themes/twitter/css/style.css中添加如下css代码： h1,h2,h3,h4,h5,h6{margin:18px 0 9px;font-family:inherit;font-weight:normal;color:inherit;text-rendering:optimizelegibility;}h1 small,h2 small,h3 small,h4 small,h5 small,h6 small{font-weight:normal;color:#999999;} h1{font-size:30px;line-height:36px;}h1 small{font-size:18px;} h2{font-size:24px;line-height:36px;}h2 small{font-size:18px;} h3{font-size:18px;line-height:27px;}h3 small{font-size:14px;} h4,h5,h6{line-height:18px;} h4{font-size:14px;}h4 small{font-size:12px;} h5{font-size:12px;} h6{font-size:11px;color:#999999;text-transform:uppercase;}添加返回顶部功能同样是参考了图灵社区的css和网上的一篇js实现。在assets/themes/twitter/css/style.css： .backToTop { display: block; width: 40px; height: 32px; font-size: 26px; line-height: 32px; font-family: verdana, arial; padding: 5px 0; background-color: #000; color: #fff; text-align: center; position: fixed; _position: absolute; right: 10px; bottom: 100px; _bottom: &quot;auto&quot;; cursor: pointer; opacity: .6; filter: Alpha(opacity=60); }在assets/themes/twitter/js添加jquery和main.js，main.js内容如下： jQuery.noConflict(); jQuery(document).ready(function(){ var backToTopTxt = &quot;▲&quot;, backToTopEle = jQuery(&apos;&lt;div class=&quot;backToTop&quot;&gt;&lt;/div&gt;&apos;).appendTo(jQuery(&quot;body&quot;)).text(backToTopTxt).attr(&quot;title&quot;,&quot;Back top top&quot;).click(function() { jQuery(&quot;html, body&quot;).animate({ scrollTop: 0 }, 120); }), backToTopFun = function() { var st = jQuery(document).scrollTop(), winh = jQuery(window).height(); (st &gt; 200)? backToTopEle.show(): backToTopEle.hide(); //IE6下的定位 if (!window.XMLHttpRequest) { backToTopEle.css(&quot;top&quot;, st + winh - 166); } }; backToTopEle.hide(); jQuery(window).bind(&quot;scroll&quot;, backToTopFun); jQuery(&apos;div.main a,div.pic a&apos;).attr(&apos;target&apos;, &apos;_blank&apos;); });添加文章版权说明在_includes/themes/twitter/post.html中文章主体下面添加如下代码： &lt;div class=&quot;well&quot;&gt; 本文基于&lt;a target=&quot;_blank&quot; title=&quot;Creative Commons Attribution 2.5 China Mainland License&quot; href=&quot;http://creativecommons.org/licenses/by/2.5/cn/&quot;&gt;署名2.5中国大陆许可协议&lt;/a&gt;发布，转载请注明：&lt;br/&gt; 作者：&lt;a href=&quot;{{site.url}}/about.html&quot;&gt;{{ site.author.name }}&lt;/a&gt;&lt;br/&gt; 链接：&lt;a href=&quot;{{page.url}}&quot;&gt;{{site.url}}{{page.url}}&lt;/a&gt;&lt;br/&gt; 欢迎转载、演绎或用于商业目的，但是必须保留本文署名和文章链接。如您有任何疑问或者授权方面的协商，请邮件联系我&lt;/a&gt;。 &lt;/div&gt;添加read more功能参考Jekyll - Read More without plugin，在index.html找到 ，然后修改为： { % if post.content contains &quot;&lt;!-- more --&gt;&quot; %} { { post.content | split:&quot;&lt;!-- more --&gt;&quot; | first % }} &lt;h4&gt;&lt;a href=&apos;{ {post.url}}&apos; title=&apos;Read more...&apos;&gt;Read more...&lt;/a&gt;&lt;/h4&gt; { % else %} { { post.content}} { % endif %}然后，在文章中添加&lt;!-- more --&gt;即可。 添加搜索栏参考Jekyll Bootstrap - Create Simple Search box，在_includes/themes/twitter/default.html导航菜单下面添加： &lt;form class=&quot;navbar-search pull-left&quot; id=&quot;search-form&quot;&gt; &lt;input type=&quot;text&quot; id=&quot;google-search&quot; class=&quot;search-query&quot; placeholder=&quot;Search&quot;&gt; &lt;/form添加js： jQuery(&quot;#search-form&quot;).submit(function(){ var query = document.getElementById(&quot;google-search&quot;).value; window.open(&quot;http://google.com/search?q=&quot; + query+ &quot;%20site:&quot; + &quot;http://blog.javachen.com&quot;); });添加文章访问量功能在_includes目录下的head.html中添加 &lt;script async src=&quot;//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt;在_includes目录下的footer.html中添加如下代码，这样文章底部有了统计访问量功能 &lt;span id=&quot;busuanzi_container_site_pv&quot;&gt;本站总访问量：&lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt;次&lt;/span&gt;在_layouts目录下的post.html中添加如下代码，这样每篇文章有了统计访问量功能 &lt;span id=&quot;busuanzi_container_page_pv&quot;&gt; | 访问量：&lt;span id=&quot;busuanzi_value_page_pv&quot;&gt;&lt;/span&gt; 次&lt;/span&gt;其他 添加404页面 修改博客主体为宽屏模式 TODO 添加语法高亮，参考Jekyll - Syntax highlighting]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>jekyll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用ZooKeeper实现配置同步]]></title>
    <url>%2F2013%2F08%2F23%2Fpublish-proerties-using-zookeeper%2F</url>
    <content type="text"><![CDATA[前言应用项目中都会有一些配置信息，这些配置信息数据量少，一般会保存到内存、文件或者数据库，有时候需要动态更新。当需要在多个应用服务器中修改这些配置文件时，需要做到快速、简单、不停止应用服务器的方式修改并同步配置信息到所有应用中去。本篇文章就是介绍如何使用ZooKeeper来实现配置的动态同步。 ZooKeeper在《hive Driver类运行过程》一文中可以看到hive为了支持并发访问引入了ZooKeeper来实现分布式锁。参考《ZooKeeper典型应用场景一览》一文，ZooKeeper还可以用作其他用途，例如： 数据发布与订阅（配置中心） 负载均衡 命名服务(Naming Service) 分布式通知/协调 集群管理与Master选举 分布式锁 分布式队列 一些在线系统在运行中，需要在不停止程序的情况下能够动态调整某一个变量的值并且能够及时生效。特别是当部署了多台应用服务器的时候，需要能够做到在一台机器上修改配置文件，然后在同步到所有应用服务器。这时候使用ZooKeeper来实现就很合适了。 数据发布与订阅发布与订阅模型，即所谓的配置中心，顾名思义就是发布者将数据发布到ZK节点上，供订阅者动态获取数据，实现配置信息的集中式管理和动态更新。例如全局的配置信息，服务式服务框架的服务地址列表等就非常适合使用。 使用ZooKeeper的发布与订阅模型，可以将应用中用到的一些配置信息放到ZK上进行集中管理。这类场景通常是这样：应用在启动的时候会主动来获取一次配置，同时，在节点上注册一个Watcher，这样一来，以后每次配置有更新的时候，都会实时通知到订阅的客户端，从来达到获取最新配置信息的目的。这样的场景适合数据量很小，但是数据更新可能会比较快的需求。 配置存储方案配置文件通常有如下几种保存方式： 将配置信息保存在程序代码中这种方案简单，但每次修改配置都要重新编译、部署应用程序。显然这种方案很不方便，也不可靠，更无法做到修改的实时生效。 将配置信息保存在xml文件或者属性文件中在参数信息保存在xml或者属性文件中，当需要修改参数时，直接修改 xml 文件。这样无需重新编译，只需重新部署修改的文件即可。但然后对所有的应用进行重新部署。这样做的缺点显而易见，要往上百台机器上重新部署应用，简直是一个噩梦。同时该方案还有一个缺点，就是配置修改无法做到实时生效。修改后往往过一段时间才能生效。 将配置信息保存在数据库中当需要修改参数时，直接修改数据库，然后重启分布式应用程序，或者刷新分布式应用的缓存。尽管这种做法比以上两种方案简单，但却面临着单点失效问题。如果数据库服务器停机，则分布式应用程序的配置信息将无法更新。另外这种方案的配置修改生效实时性虽然比第二种方案好些，但仍然不能达到某些情况下的要求。 基于ZooKeeper的配置信息同步方案如果使用ZooKeeper来实现，就可以直接把配置信息保存到ZooKeeper中，或者把属性文件内容保存到ZooKeeper中，当属性文件内容发生变化时，就通知监听者如应用程序去重新读取配置文件。 在网上搜索了一下，很能找到好用的现成的代码实现。有的基于ZooKeeper来扩张jdk的hashmap来存储配置参数，如：使用ZooKeeper实现静态数据中心化配置管理，也有人直接实现了一个基于java并发框架的工具包，如：menagerie。 注意:以下部分文字和图来自：基于ZooKeeper的配置信息存储方案的设计与实现1.pdf 基于ZooKeeper的特性,借助ZooKeeper可以实现一个可靠的、简单的、修改配置能够实时生效的配置信息存储方案,整体的设计方案如图： 整个配置信息存储方案由三部分组成:ZooKeeper服务器集群、配置管理程序、分布式应用程序。 ZooKeeper服务器集群存储配置信息,在服务器上创建一个保存数据的节点(创建节点操作);配置管理程序提供一个配置管理的UI界面或者命令行方式,用户通过配置界面修改ZooKeeper服务器节点上配置信息(设置节点数据操作);分布式应用连接到ZooKeeper集群上(创建ZooKeeper客户端操作),监听配置信息的变化(使用获取节点数据操作,并注册一个watcher)。 当配置信息发生变化时，分布式应用会更新程序中使用配置信息。 源代码找到一个淘宝工程师写的实现方式， 代码见：zkpublisher 优点借助 ZooKeeper我们实现的配置信息存储方案具有的优点如下: 简单。尽管前期搭建ZooKeeper服务器集群较为麻烦,但是实现该方案后,修改配置整个过程变得简单很多。用户只要修改配置,无需进行其他任何操作,配置自动生效。 可靠。ZooKeeper服务集群具有无单点失效的特性,使整个系统更加可靠。即使ZooKeeper 集群中的一台机器失效,也不会影响整体服务,更不会影响分布式应用配置信息的更新。 实时。ZooKeeper的数据更新通知机制,可以在数据发生变化后,立即通知给分布式应用程序,具有很强的变化响应能力。 总结本文参考了网上的一些文章，给出了基于ZooKeeper的配置信息同步方案,解决了传统配置信息同步方案的缺点如实时性差、可靠性差、复杂等。 参考文章 ZooKeeper典型应用场景一览 使用ZooKeeper实现静态数据中心化配置管理 menagerie 基于ZooKeeper的配置信息存储方案的设计与实现1.pdf]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive源码分析：Driver类运行过程]]></title>
    <url>%2F2013%2F08%2F22%2Fhive-Driver%2F</url>
    <content type="text"><![CDATA[说明： 本文的源码分析基于hive-0.12.0-cdh5.0.1。 概括从《hive cli的入口类》中可以知道hive中处理hive命令的处理器一共有以下几种： 123456（1）set SetProcessor，设置修改参数,设置到SessionState的HiveConf里。 （2）dfs DfsProcessor，使用hadoop的FsShell运行hadoop的命令。 （3）add AddResourceProcessor，添加到SessionState的resource_map里，运行提交job的时候会写入Hadoop的Distributed Cache。 （4）delete DeleteResourceProcessor，从SessionState的resource_map里删除。（5）reset RestResourceProcessor，重置终端输出（6）其他命令 Driver Driver类的主要作用是用来编译并执行hive命令，然后返回执行结果。这里主要分析Driver类的运行逻辑，其时序图如下： 从时序图上可以看出有以下步骤： run方法调用内部方法runInternal 在runInternal方法内部先，调用HiveDriverRunHookContext的preDriverRun方法 调用compileInternal方法 compileInternal方法内部调用compile方法 compile方法内，先调用HiveSemanticAnalyzerHookContext的preAnalyze方法 再进行语法分析，调用BaseSemanticAnalyzer的analyze方法 调用HiveSemanticAnalyzerHookContext的postAnalyze方法 再进行语法校验，调用BaseSemanticAnalyzer的validate方法 compileInternal方法运行完成之后，调用checkConcurrency方法 再来运行execute方法，该方法用于运行任务 最后，调用HiveDriverRunHookContext的postDriverRun方法 Driver初始化在继续分析之前，需要弄清楚Driver类初始化时做了什么事情。 在CliDriver的processCmd(String cmd)方法中可以看到proc是在CommandProcessorFactory类中new出来的并调用了init方法。 1234&#125; else &#123; // local mode CommandProcessor proc = CommandProcessorFactory.get(tokens[0], (HiveConf) conf); ret = processLocalCmd(cmd, proc, ss);&#125; CommandProcessorFactory.get方法代码片段： 12345678910if (conf == null) &#123;return new Driver();&#125;Driver drv = mapDrivers.get(conf);if (drv == null) &#123;drv = new Driver();mapDrivers.put(conf, drv);&#125;drv.init(); init方法和构造方法代码如下： 123456789public void init() &#123; Operator.resetId();&#125;public Driver() &#123; if (SessionState.get() != null) &#123; conf = SessionState.get().getConf(); &#125; &#125; 从上可以看出仅仅是初始化了conf属性和重置了Operator的id。 run方法过程1、调用runInternal方法，根据该方法返回值判断是否出错。 2、runInternal方法内，运行HiveDriverRunHook的前置方法preDriverRun 3、判断是否需要编译，如果需要，则运行compileInternal(command)方法，并根据返回值判断是否该释放Hive锁。hive中可以配置hive.support.concurrency值为true并设置zookeeper的服务器地址和端口，基于zookeeper实现分布式锁以支持hive的多并发访问。这部分内容不是本文重点故不做介绍。 compileInternal(command)方法内部代码说明见下文。 4、判断是否需要对Task加锁。如果需要，则调用checkConcurrency方法。 5、调用execute()方法执行任务。 执行计划开始：plan.setStarted(); 先运行ExecuteWithHookContext的前置hook方法，ExecuteWithHookContext类型有三种：前置、运行失败、后置。 然后创建DriverContext用于维护正在运行的task任务，正在运行的task任务会添加到队列runnable中去。 其次，在while循环中遍历队列中的任务，然后启动任务让其执行，并且轮训任务执行结果，如果任务运行完成，则将其从running中删除并将当前任务的子任务加入队列中；如果运行失败，则会启动备份的任务，并运行失败的hook。 1234while (runnable.peek() != null &amp;&amp; running.size() &lt; maxthreads) &#123; Task&lt;? extends Serializable&gt; tsk = runnable.remove(); launchTask(tsk, queryId, noName, running, jobname, jobs, driverCxt);&#125; 在launchTask方法中，先判断是否支持并发执行，如果支持则调用TaskRunner的start()方法，否则调用tskRun.runSequential()方法顺序执行，只有当是MapReduce任务时，才执行并发执行： 123456789101112131415161718192021222324252627public void launchTask(Task&lt;? extends Serializable&gt; tsk, String queryId, boolean noName, Map&lt;TaskResult, TaskRunner&gt; running, String jobname, int jobs, DriverContext cxt) &#123; if (SessionState.get() != null) &#123; SessionState.get().getHiveHistory().startTask(queryId, tsk, tsk.getClass().getName()); &#125; if (tsk.isMapRedTask() &amp;&amp; !(tsk instanceof ConditionalTask)) &#123; if (noName) &#123; conf.setVar(HiveConf.ConfVars.HADOOPJOBNAME, jobname + "(" + tsk.getId() + ")"); &#125; cxt.incCurJobNo(1); console.printInfo("Launching Job " + cxt.getCurJobNo() + " out of " + jobs); &#125; tsk.initialize(conf, plan, cxt); TaskResult tskRes = new TaskResult(); TaskRunner tskRun = new TaskRunner(tsk, tskRes); // Launch Task if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.EXECPARALLEL) &amp;&amp; tsk.isMapRedTask()) &#123; // Launch it in the parallel mode, as a separate thread only for MR tasks tskRun.start(); &#125; else &#123; tskRun.runSequential(); &#125; running.put(tskRes, tskRun); return; &#125; 最后任务的执行情况，就要看具体的Task&lt;? extends Serializable&gt;的实现类的逻辑了。 执行计划完成：plan.setDone(); 6、运行HiveDriverRunHook的后置方法postDriverRun compileInternal方法过程1、保存当前查询状态 1234567891011QueryState queryState = new QueryState();if (plan != null) &#123; close(); plan = null;&#125;if (resetTaskIds) &#123; TaskFactory.resetId();&#125;saveSession(queryState); QueryState中保存了HiveOperation以及当前查询语句或者命令。 2、创建Context上下文 12345command = new VariableSubstitution().substitute(conf,command);ctx = new Context(conf);ctx.setTryCount(getTryCount());ctx.setCmd(command);ctx.setHDFSCleanup(true); 3、创建ParseDriver对象，然后解析命令、生成AST树。语法和词法分析内容，不是本文重点故不做介绍。 123ParseDriver pd = new ParseDriver();ASTNode tree = pd.parse(command, ctx);tree = ParseUtils.findRootNonNullToken(tree); 简单归纳来说，解析程包括如下： 词法分析，生成AST树，ParseDriver完成。 分析AST树，AST拆分成查询子块，信息记录在QB，这个QB在下面几个阶段都需要用到，SemanticAnalyzer.doPhase1完成。 从metastore中获取表的信息，SemanticAnalyzer.getMetaData完成。 生成逻辑执行计划，SemanticAnalyzer.genPlan完成。 优化逻辑执行计划，Optimizer完成，ParseContext作为上下文信息进行传递。 生成物理执行计划，SemanticAnalyzer.genMapRedTasks完成。 物理计划优化，PhysicalOptimizer完成，PhysicalContext作为上下文信息进行传递。 4、读取环境变量，如果配置了语法分析的hook，参数为：hive.semantic.analyzer.hook，则:先用反射得到AbstractSemanticAnalyzerHook的集合，调用hook.preAnalyze(hookCtx, tree)方法,然后再调用sem.analyze(tree, ctx)方法，该方法才是用来作语法分析的,最后再调用hook.postAnalyze(hookCtx, tree)方法执行一些用户定义的后置操作； 否则，直接调用sem.analyze(tree, ctx)进行语法分析。 123456789101112131415161718192021BaseSemanticAnalyzer sem = SemanticAnalyzerFactory.get(conf, tree);List&lt;AbstractSemanticAnalyzerHook&gt; saHooks = getHooks(HiveConf.ConfVars.SEMANTIC_ANALYZER_HOOK, AbstractSemanticAnalyzerHook.class);// Do semantic analysis and plan generationif (saHooks != null) &#123; HiveSemanticAnalyzerHookContext hookCtx = new HiveSemanticAnalyzerHookContextImpl(); hookCtx.setConf(conf); hookCtx.setUserName(userName); for (AbstractSemanticAnalyzerHook hook : saHooks) &#123; tree = hook.preAnalyze(hookCtx, tree); &#125; sem.analyze(tree, ctx); hookCtx.update(sem); for (AbstractSemanticAnalyzerHook hook : saHooks) &#123; hook.postAnalyze(hookCtx, sem.getRootTasks()); &#125;&#125; else &#123; sem.analyze(tree, ctx);&#125; 5、校验执行计划：sem.validate() 6、创建查询计划QueryPlan。 12plan = new QueryPlan(command, sem, perfLogger.getStartTime(PerfLogger.DRIVER_RUN), SessionState.get().getCommandType()); 7、初始化FetchTask。 123if (plan.getFetchTask() != null) &#123; plan.getFetchTask().initialize(conf, plan, null);&#125; 8、得到schema 1schema = getSchema(sem, conf); 9、授权校验工作。 hive中支持的hook上面分析中，提到了hive的hook机制，hive中一共存在以下几种hook。 12345678910hive.semantic.analyzer.hookhive.exec.filter.hookhive.exec.driver.run.hookshive.server2.session.hookhive.exec.pre.hookshive.exec.post.hookshive.exec.failure.hookshive.client.stats.publishershive.metastore.ds.connection.url.hookhive.metastore.init.hooks 通过hook机制，可以在运行前后做一些用户想做的事情。如：你可以在语法分析的hook中对hive的操作做一些超级管理员级别的权限判断；你可以对hive-server2做一些session级别的控制。 cloudera的github仓库access中关于hive的访问控制就是使用了hive的hook机制。 twitter的mapreduce可视化项目监控项目ambrose也利用了hive的hook机制，有兴趣的话，你可以去看看其是如何使用hive的hook并且你也可以扩增hook做些自己想做的事情。 总结本文主要介绍了hive运行过程，包括hive语法词法解析以及hook机制，任务的最后运行过程取决于具体的Task&lt;? extends Serializable&gt;的实现类的逻辑。关于hive语法词法解析，这一部分没有做详细的解释。 hive Driver类的执行过程如下（该图是根据hive-0.11版本画出来的）： 参考文章 hive 初始化运行流程 Cloudera access twitter ambrose]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive源码分析：CLI入口类]]></title>
    <url>%2F2013%2F08%2F21%2Fhive-CliDriver%2F</url>
    <content type="text"><![CDATA[说明： 本文的源码分析基于hive-0.10.0-cdh4.3.0。 启动脚本从shell脚本/usr/lib/hive/bin/ext/cli.sh可以看到hive cli的入口类为org.apache.hadoop.hive.cli.CliDriver 12345678cli () &#123; CLASS=org.apache.hadoop.hive.cli.CliDriver execHiveCmd $CLASS "$@"&#125;cli_help () &#123; CLASS=org.apache.hadoop.hive.cli.CliDriver execHiveCmd $CLASS "--help"&#125; 入口类java中的类如果有main方法就能运行，故直接查找org.apache.hadoop.hive.cli.CliDriver中的main方法即可。 CliDriver类中的方法有： main方法代码如下： 1234public static void main(String[] args) throws Exception &#123; int ret = run(args); System.exit(ret);&#125; 阅读run函数可以看到，主要做了以下几件事情： 读取main方法的参数 重置默认的log4j配置并为hive重新初始化log4j，注意，在这里是读取hive-log4j.properties来初始化log4j。 创建CliSessionState，并初始化in、out、info、error等stream流。CliSessionState是一次命令行操作的session会话，其继承了SessionState。 重命令行参数中读取参数并设置到CliSessionState中。 启动SessionState并连接到hive server 如果cli是本地模式运行，则加载hive.aux.jars.path参数配置的jar包到classpath 创建一个CliDriver对象，并设置当前选择的数据库。可以在命令行参数添加-database database来选择连接那个数据库，默认为default数据库。 加载初始化文件.hiverc，该文件位于当前用户主目录下，读取该文件内容后，然后调用processFile方法处理文件内容。 如果命令行中有-e参数，则运行指定的sql语句；如果有-f参数，则读取该文件内容并运行。注意：不能同时指定这两个参数。 12hive -e 'show tables'hive -f /root/hive.sql 如果没有指定上面两个参数，则从当前用户主目录读取.hivehistory文件，如果不存在则创建。该文件保存了当前用户所有运行的hive命令。 在while循环里不断读取控制台的输入内容，每次读取一行，如果行末有分号，则调用CliDriver的processLine方法运行读取到的内容。 每次调用processLine方法时，都会创建SignalHandler用于捕捉用户的输入，当用户输入Ctrl+C时，会kill当前正在运行的任务以及kill掉当前进程。kill当前正在运行的job的代码如下. 1HadoopJobExecHelper.killRunningJobs(); 处理hive命令。 处理hive命令过程如果输入的是quit或者exit,则程序退出。 如果命令开头是source，则会读取source 后面文件内容，然后执行该文件内容。通过这种方式，你可以在hive命令行模式运行一个文件中的hive命令。 如果命令开头是感叹号，执行操作系统命令（如!ls，列出当前目录的文件信息）。通过以下代码来运行： 1234567891011Process executor = Runtime.getRuntime().exec(shell_cmd);StreamPrinter outPrinter = new StreamPrinter(executor.getInputStream(), null, ss.out);StreamPrinter errPrinter = new StreamPrinter(executor.getErrorStream(), null, ss.err);outPrinter.start();errPrinter.start();ret = executor.waitFor();if (ret != 0) &#123; console.printError("Command failed with exit code = " + ret);&#125; shell_cmd的内容大概如下： 1shell_cmd = "/bin/bash -c \'" + shell_cmd + "\'" 如果命令开头是list，列出jar/file/archive 如果是远程模式运行命令行，则通过HiveClient来运行命令；否则，调用processLocalCmd方法运行本地命令。 以本地模式运行时，会通过CommandProcessorFactory工厂解析输入的语句来获得一个CommandProcessor，CommandProcessor接口的实现类见下图： 从上图可以看到指定的命令(set/dfs/add/delete/reset)交给指定的CommandProcessor处理，其余的(指hql语句)交给Driver类来处理。 故，org.apache.hadoop.hive.ql.Driver类是hql查询的起点，而run()方法会先后调用compile()和execute()两个函数来完成查询，所以一个command的查询分为compile和execute两个阶段。 总结作为尝试，第一次使用思维导图分析代码逻辑，简单整理了一下CliDriver类的运行逻辑，如下图。以后还需要加强画图和表达能力。 参考文章 hive 初始化运行流程]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[远程调试Hadoop各组件]]></title>
    <url>%2F2013%2F08%2F01%2Fremote-debug-hadoop%2F</url>
    <content type="text"><![CDATA[远程调试对应用程序开发十分有用。例如，为不能托管开发平台的低端机器开发程序，或在专用的机器上（比如服务不能中断的 Web 服务器）调试程序。其他情况包括：运行在内存小或 CUP 性能低的设备上的 Java 应用程序（比如移动设备），或者开发人员想要将应用程序和开发环境分开，等等。 为了进行远程调试，必须使用 Java Virtual Machine (JVM) V5.0 或更新版本。 JPDA 简介Sun Microsystem 的 Java Platform Debugger Architecture (JPDA) 技术是一个多层架构，使您能够在各种环境中轻松调试 Java 应用程序。JPDA 由两个接口（分别是 JVM Tool Interface 和 JDI）、一个协议（Java Debug Wire Protocol）和两个用于合并它们的软件组件（后端和前端）组成。它的设计目的是让调试人员在任何环境中都可以进行调试。 更详细的介绍，您可以参考使用 Eclipse 远程调试 Java 应用程序 JDWP 设置JVM本身就支持远程调试，Eclipse也支持JDWP，只需要在各模块的JVM启动时加载以下参数： -Xdebug -Xrunjdwp:transport=dt_socket, address=8000,server=y,suspend=y 各参数的含义： -Xdebug 启用调试特性 -Xrunjdwp 启用JDWP实现，包含若干子选项： transport=dt_socket JPDA front-end和back-end之间的传输方法。dt_socket表示使用套接字传输。 address=8000 JVM在8000端口上监听请求，这个设定为一个不冲突的端口即可。 server=y y表示启动的JVM是被调试者。如果为n，则表示启动的JVM是调试器。 suspend=y y表示启动的JVM会暂停等待，直到调试器连接上才继续执行。suspend=n，则JVM不会暂停等待。配置hbase远程调试打开/etc/hbase/conf/hbase-env.sh，找到以下内容： # Enable remote JDWP debugging of major HBase processes. Meant for Core Developers # export HBASE_MASTER_OPTS=&quot;$HBASE_MASTER_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8070&quot; # export HBASE_REGIONSERVER_OPTS=&quot;$HBASE_REGIONSERVER_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8071&quot; # export HBASE_THRIFT_OPTS=&quot;$HBASE_THRIFT_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8072&quot; # export HBASE_ZOOKEEPER_OPTS=&quot;$HBASE_ZOOKEEPER_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8073&quot;如果想远程调式hbase-master进程，请去掉对HBASE_MASTER_OPTS的注释，其他依次类推。注意，我这里使用的是cdh-4.3.0中的hbase。 注意（20130817更新）：如果启动hbase时提示check your java command line for duplicate jdwp options，请把上面参数加到/usr/lib/hbase/bin/hbase中if else对应分支中去。 例如，如果你想调试regionserver，请把下面代码加到elif [ &quot;$COMMAND&quot; = &quot;regionserver&quot; ] ; then中去： 1export HBASE_REGIONSERVER_OPTS=&quot;$HBASE_REGIONSERVER_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8071&quot; 配置hive远程调试停止hive-server2进程，然后以下面命令启动hive-server2 1hive --service hiveserver --debug 进程会监听在8000端口等待调试连接。如果想更改监听端口，可以修改配置文件:${HIVE_HOME}bin/ext/debug.sh 如果Hadoop是0.23以上版本，debug模式启动Cli会报错： ERROR: Cannot load this JVM TI agent twice, check your java command line for duplicate jdwp options.打开${Hadoop_HOME}/bin/hadoop，注释掉以下代码 # Always respect HADOOP_OPTS and HADOOP_CLIENT_OPTS HADOOP_OPTS=&quot;$HADOOP_OPTS $HADOOP_CLIENT_OPTS&quot;配置yarn远程调试请在以下代码添加调试参数： if [ &quot;$COMMAND&quot; = &quot;classpath&quot; ] ; then if $cygwin; then CLASSPATH=`cygpath -p -w &quot;$CLASSPATH&quot;` fi echo $CLASSPATH exit elif [ &quot;$COMMAND&quot; = &quot;rmadmin&quot; ] ; then CLASS=&apos;org.apache.hadoop.yarn.client.RMAdmin&apos; YARN_OPTS=&quot;$YARN_OPTS $YARN_CLIENT_OPTS&quot; elif [ &quot;$COMMAND&quot; = &quot;application&quot; ] ; then class=&quot;org&quot;.apache.hadoop.yarn.client.cli.ApplicationCLI YARN_OPTS=&quot;$YARN_OPTS $YARN_CLIENT_OPTS&quot; elif [ &quot;$COMMAND&quot; = &quot;node&quot; ] ; then class=&quot;org&quot;.apache.hadoop.yarn.client.cli.NodeCLI YARN_OPTS=&quot;$YARN_OPTS $YARN_CLIENT_OPTS&quot; elif [ &quot;$COMMAND&quot; = &quot;resourcemanager&quot; ] ; then CLASSPATH=${CLASSPATH}:$YARN_CONF_DIR/rm-config/log4j.properties CLASS=&apos;org.apache.hadoop.yarn.server.resourcemanager.ResourceManager&apos; YARN_OPTS=&quot;$YARN_OPTS $YARN_RESOURCEMANAGER_OPTS&quot; if [ &quot;$YARN_RESOURCEMANAGER_HEAPSIZE&quot; != &quot;&quot; ]; then JAVA_HEAP_MAX=&quot;-Xmx&quot;&quot;$YARN_RESOURCEMANAGER_HEAPSIZE&quot;&quot;m&quot; fi elif [ &quot;$COMMAND&quot; = &quot;nodemanager&quot; ] ; then CLASSPATH=${CLASSPATH}:$YARN_CONF_DIR/nm-config/log4j.properties CLASS=&apos;org.apache.hadoop.yarn.server.nodemanager.NodeManager&apos; YARN_OPTS=&quot;$YARN_OPTS -server $YARN_NODEMANAGER_OPTS&quot; if [ &quot;$YARN_NODEMANAGER_HEAPSIZE&quot; != &quot;&quot; ]; then JAVA_HEAP_MAX=&quot;-Xmx&quot;&quot;$YARN_NODEMANAGER_HEAPSIZE&quot;&quot;m&quot; fi elif [ &quot;$COMMAND&quot; = &quot;proxyserver&quot; ] ; then CLASS=&apos;org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer&apos; YARN_OPTS=&quot;$YARN_OPTS $YARN_PROXYSERVER_OPTS&quot; if [ &quot;$YARN_PROXYSERVER_HEAPSIZE&quot; != &quot;&quot; ]; then JAVA_HEAP_MAX=&quot;-Xmx&quot;&quot;$YARN_PROXYSERVER_HEAPSIZE&quot;&quot;m&quot; fi例如：如果你想调试resourcemanager代码，请在elif [ &quot;$COMMAND&quot; = &quot;resourcemanager&quot; ] 分支内添加如下代码： YARN_RESOURCEMANAGER_OPTS=&quot;$YARN_RESOURCEMANAGER_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=6001&quot;其他进程，参照上面即可。 注意：端口不要冲突。 配置mapreduce远程调试如果想要调试Map 或Reduce Task，则修改bin/hadoop已经没用了，因为bin/hadoop中没有Map Task的启动参数。 此时需要修改mapred-site.xml &lt;property&gt; &lt;name&gt;mapred.child.java.opts&lt;/name&gt; &lt;value&gt;-Xmx800m -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8000&lt;/value&gt; &lt;/property在一个TaskTracker上，只能启动一个Map Task或一个Reduce Task，否则启动时会有端口冲突。因此要修改所有TaskTracker上的conf/hadoop-site.xml中的配置项： &lt;property&gt; &lt;name&gt;mapred.tasktracker.map.tasks.maximum&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.tasktracker.reduce.tasks.maximum&lt;/name&gt; &lt;value&gt;0&lt;/value&gt; &lt;/property&gt;在Eclipse中使用方法： 打开eclipse，找到Debug Configurations...，添加一个Remout Java Application: 在source中可以关联到hive的源代码,然后，单击Debug按钮进入远程debug模式。 编写个jdbc的测试类，运行代码，这时候因为hive-server2端没有设置端点，故程序可以正常运行直到结束。 在hive代码中设置一个断点，如ExecDriver.java的execute方法中设置断点，然后再运行jdbc测试类。 参考文章 在Eclipse中远程调试Hadoop hive远程调试 使用 Eclipse 远程调试 Java 应用程序]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hbase</tag>
        <tag>hive</tag>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装RHadoop]]></title>
    <url>%2F2013%2F07%2F20%2Finstall-rhadoop%2F</url>
    <content type="text"><![CDATA[1. R Language Install安装相关依赖yum install -y perl* pcre-devel tcl-devel zlib-devel bzip2-devel libX11-devel tk-devel tetex-latex *gfortran* compat-readline5 yum install libRmath-* rpm -Uvh --force --nodeps R-core-2.10.0-2.el5.x86_64.rpm rpm -Uvh R-2.10.0-2.el5.x86_64.rpm R-devel-2.10.0-2.el5.x86_64.rpm 编译安装：R-3.0.1tar -zxvf R-3.0.1 ./configure make make install #R运行 export HADOOP_CMD=/usr/bin/hadoop排错1、错误1 error: --with-readline=yes (default) 安装readline yum install readline*2、错误2 error: No F77 compiler found 安装gfortran 3、错误3 error: –with-x=yes (default) and X11 headers/libs are not available 安装 yum install libXt*4、错误4 error: C++ preprocessor &quot;/lib/cpp&quot; fails sanity check 安装g++或build-essential（redhat6.2安装gcc-c++和glibc-headers） 验证是否安装成功[root@node1 bin]# R R version 3.0.1 (2013-05-16) -- &quot;Good Sport&quot; Copyright (C) 2013 The R Foundation for Statistical Computing Platform: x86_64-unknown-linux-gnu (64-bit) R是自由软件，不带任何担保。 在某些条件下你可以将其自由散布。 用&apos;license()&apos;或&apos;licence()&apos;来看散布的详细条件。 R是个合作计划，有许多人为之做出了贡献. 用&apos;contributors()&apos;来看合作者的详细情况 用&apos;citation()&apos;会告诉你如何在出版物中正确地引用R或R程序包。 用&apos;demo()&apos;来看一些示范程序，用&apos;help()&apos;来阅读在线帮助文件，或 用&apos;help.start()&apos;通过HTML浏览器来看帮助文件。 用&apos;q()&apos;退出R.2. 安装Rhadoop安装rhdfs，rmr2cd Rhadoop/ R CMD javareconf R CMD INSTALL &apos;plyr_1.8.tar.gz&apos; R CMD INSTALL &apos;stringr_0.6.2.tar.gz&apos; R CMD INSTALL &apos;reshape2_1.2.2.tar.gz&apos; R CMD INSTALL &apos;digest_0.6.3.tar.gz&apos; R CMD INSTALL &apos;functional_0.4.tar.gz&apos; R CMD INSTALL &apos;iterators_1.0.6.tar.gz&apos; R CMD INSTALL &apos;itertools_0.1-1.tar.gz&apos; R CMD INSTALL &apos;Rcpp_0.10.3.tar.gz&apos; R CMD INSTALL &apos;rJava_0.9-4.tar.gz&apos; R CMD INSTALL &apos;RJSONIO_1.0-3.tar.gz&apos; R CMD INSTALL &apos;reshape2_1.2.2.tar.gz&apos; R CMD INSTALL &apos;rhdfs_1.0.5.tar.gz&apos; R CMD INSTALL &apos;rmr2_2.2.0.tar.gz&apos;R library(rhdfs)检查是否能正常工作 验证测试Rmr测试命令： &gt; train.mr&lt;-mapreduce( + train.hdfs, + map = function(k, v) { + keyval(k,v$item) + } + ,reduce=function(k,v){ + m&lt;-merge(v,v) + keyval(m$x,m$y) + } + )出现如下错误： packageJobJar: [/tmp/RtmpCuhs7d/rmr-local-env18916b6f86b3, /tmp/RtmpCuhs7d/rmr-global-env18913824c681, /tmp/RtmpCuhs7d/rmr-streaming-map18912d6c2b1c, /tmp/RtmpCuhs7d/rmr-streaming-reduce1891179bb645, /tmp/hadoop-root/hadoop-unjar4575094085541826184/] [] /tmp/streamjob2910108622786868147.jar tmpDir=null 13/06/05 18:22:28 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same. 13/06/05 18:22:28 INFO mapred.FileInputFormat: Total input paths to process : 1 13/06/05 18:22:29 INFO streaming.StreamJob: getLocalDirs(): [/tmp/hadoop-root/mapred/local] 13/06/05 18:22:29 INFO streaming.StreamJob: Running job: job_201306050931_0004 13/06/05 18:22:29 INFO streaming.StreamJob: To kill this job, run: 13/06/05 18:22:29 INFO streaming.StreamJob: /usr/lib/hadoop/bin/hadoop job -Dmapred.job.tracker=cdh1:8021 -kill job_201306050931_0004 13/06/05 18:22:29 INFO streaming.StreamJob: Tracking URL: http://cdh1:50030/jobdetails.jsp?jobid=job_201306050931_0004 13/06/05 18:22:30 INFO streaming.StreamJob: map 0% reduce 0% 13/06/05 18:22:56 INFO streaming.StreamJob: map 100% reduce 100% 13/06/05 18:22:56 INFO streaming.StreamJob: To kill this job, run: 13/06/05 18:22:56 INFO streaming.StreamJob: /usr/lib/hadoop/bin/hadoop job -Dmapred.job.tracker=cdh1:8021 -kill job_201306050931_0004 13/06/05 18:22:56 INFO streaming.StreamJob: Tracking URL: http://cdh1:50030/jobdetails.jsp?jobid=job_201306050931_0004 13/06/05 18:22:56 ERROR streaming.StreamJob: Job not successful. Error: NA 13/06/05 18:22:56 INFO streaming.StreamJob: killJob... Streaming Command Failed! Error in mr(map = map, reduce = reduce, combine = combine, vectorized.reduce, : hadoop streaming failed with error code 1错误解决方法： 通过查看日志，hadoop没有在/usr/bin下找到Rscript,于是从R的安装目录/usr/local/bin下做R和Rscript的符号链接到/usr/bin下，再次执行即可解决次错。 #ln -s /usr/loca/bin/R /usr/bin #ln -s /usr/local/bin/Rscript /usr/bin3. 安装rhbase安装依赖#yum install boost* #yum install openssl*安装thrift#tar -zxvf thrift-0.9.0.tar.gz #mv thrift-0.9.0/lib/cpp/src/thrift/qt/moc_TQTcpServer.cpp thrift-0.9.0/lib/cpp/src/thrift/qt/moc_TQTcpServer.cpp.bak #cd thrift-0.9.0 #./configure --with-boost=/usr/include/boost JAVAC=/usr/java/jdk1.6.0_31/bin/javac #make #make install如果报错：error: “Error: libcrypto required.” #yum install openssl*如果报错： src/thrift/qt/moc_TQTcpServer.cpp:14:2: error: #error &quot;This file was generated using the moc from 4.8.1. It&quot; src/thrift/qt/moc_TQTcpServer.cpp:15:2: error: #error &quot;cannot be used with the include files from this version of Qt.&quot; src/thrift/qt/moc_TQTcpServer.cpp:16:2: error: #error &quot;(The moc has changed too much.)&quot;则运行下面命令： #mv thrift-0.9.0/lib/cpp/src/thrift/qt/moc_TQTcpServer.cpp thrift-0.9.0/lib/cpp/src/thrift/qt/moc_TQTcpServer.cpp.bak配置PKG_CONFIG_PATHexport PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig/ pkg-config --cflags thrift #返回：-I/usr/local/include/thrift为正确 cp /usr/local/lib/libthrift-0.9.0.so /usr/lib/ cp /usr/local/lib/libthrift-0.9.0.so /usr/lib64/启动hbase： /usr/lib/hbase/bin/hbase-daemon.sh start thrift 使用jps查看thrift进程 安装rhbaseR CMD INSTALL &apos;rhbase_1.1.1.tar.gz&apos;验证并测试在R命令行中输入library(rmr2)、library(rhdfs)、library(rhbase)，载入成功即表示安装成功 [root@desktop27 hadoop]# R R version 3.0.1 (2013-05-16) -- &quot;Good Sport&quot; Copyright (C) 2013 The R Foundation for Statistical Computing Platform: x86_64-unknown-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type &apos;license()&apos; or &apos;licence()&apos; for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type &apos;contributors()&apos; for more information and &apos;citation()&apos; on how to cite R or R packages in publications. Type &apos;demo()&apos; for some demos, &apos;help()&apos; for on-line help, or &apos;help.start()&apos; for an HTML browser interface to help. Type &apos;q()&apos; to quit R. &gt; library(rhdfs) Loading required package: rJava HADOOP_CMD=/usr/bin/hadoop Be sure to run hdfs.init() &gt; library(rmr2) Loading required package: Rcpp Loading required package: RJSONIO Loading required package: digest Loading required package: functional Loading required package: stringr Loading required package: plyr Loading required package: reshape2 &gt; library(rhbase) &gt;4. 装RHive环境变量设置环境变量 vim /etc/profile,末行添加如下： export HADOOP_CMD=/usr/bin/hadoop export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig/ export HADOOP_STREAMING=/usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.2.1.jar export HADOOP_HOME=/usr/lib/hadoop export RHIVE_DATA=/hadoop/dfs/rhive/data export HIVE_HOME=/usr/lib/hive安装Rserve：#R CMD INSTALL &apos;Rserve_1.7-1.tar.gz&apos;在安装Rsever用户下，创建一目录，并创建Rserv.conf文件，写入``remote enable’’保存并退出。 #cd /usr/local/lib64/R/ #echo remote enable &gt; Rserv.conf启动Rserve： #R CMD Rserve --RS-conf /usr/local/lib64/R/Rserv.conf检查Rserve启动是否正常： #telnet localhost 6311显示 Rsrv0103QAP1 则表示连接成功 安装RHive创建数据目录： #R CMD INSTALL RHive_0.0-7.tar.gz #cd /usr/local/lib64/R/ mkdir -p rhive/data在上传rhive_udf.jar到hdfs上： hadoop fs -mkdir /rhive/lib cd /usr/local/lib64/R/library/RHive/java hadoop fs -put rhive_udf.jar /rhive/lib hadoop fs -chmod a+rw /rhive/lib/rhive_udf.jar cd /usr/lib/hadoop ln -s /etc/hadoop/conf conf测试RHive安装是否成功： R library（RHive） rhive.connect(&apos;192.168.0.27&apos;)【hive的地址】 rhive.env()]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>rhadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase笔记：存储结构]]></title>
    <url>%2F2013%2F06%2F15%2Fhbase-note-about-data-structure%2F</url>
    <content type="text"><![CDATA[从HBase的架构图上可以看出，HBase中的存储包括HMaster、HRegionServer、HRegion、Store、MemStore、StoreFile、HFile、HLog等，本篇文章统一介绍他们的作用即存储结构。 以下是网络上流传的HBase存储架构图: HBase中的每张表都通过行键按照一定的范围被分割成多个子表（HRegion），默认一个HRegion超过256M就要被分割成两个，这个过程由HRegionServer管理，而HRegion的分配由HMaster管理。 HMaster的作用： 为Region server分配region 负责Region server的负载均衡 发现失效的Region server并重新分配其上的region HDFS上的垃圾文件回收 处理schema更新请求 HRegionServer作用： 维护master分配给他的region，处理对这些region的io请求 负责切分正在运行过程中变的过大的region 可以看到，client访问hbase上的数据并不需要master参与（寻址访问zookeeper和region server，数据读写访问region server），master仅仅维护table和region的元数据信息（table的元数据信息保存在zookeeper上），负载很低。 HRegionServer存取一个子表时，会创建一个HRegion对象，然后对表的每个列族创建一个Store实例，每个Store都会有一个MemStore和0个或多个StoreFile与之对应，每个StoreFile都会对应一个HFile， HFile就是实际的存储文件。因此，一个HRegion有多少个列族就有多少个Store。 一个HRegionServer会有多个HRegion和一个HLog。 HRegiontable在行的方向上分隔为多个Region。Region是HBase中分布式存储和负载均衡的最小单元，即不同的region可以分别在不同的Region Server上，但同一个Region是不会拆分到多个server上。 Region按大小分隔，每个表一行是只有一个region。随着数据不断插入表，region不断增大，当region的某个列族达到一个阈值（默认256M）时就会分成两个新的region。 每个region由以下信息标识： &lt;表名,startRowkey,创建时间&gt; 由目录表(-ROOT-和.META.)可值该region的endRowkey HRegion定位： Region被分配给哪个Region Server是完全动态的，所以需要机制来定位Region具体在哪个region server。 HBase使用三层结构来定位region： 1、 通过zk里的文件/hbase/rs得到-ROOT-表的位置。-ROOT-表只有一个region。 2、通过-ROOT-表查找.META.表的第一个表中相应的region的位置。其实-ROOT-表是.META.表的第一个region；.META.表中的每一个region在-ROOT-表中都是一行记录。 3、通过.META.表找到所要的用户表region的位置。用户表中的每个region在.META.表中都是一行记录。 -ROOT-表永远不会被分隔为多个region，保证了最多需要三次跳转，就能定位到任意的region。client会讲查询的位置信息保存缓存起来，缓存不会主动失效，因此如果client上的缓存全部失效，则需要进行6次网络来回，才能定位到正确的region，其中蚕丝用来发现缓存失效，另外三次用来获取位置信息。 Store每一个region有一个或多个store组成，至少是一个store，hbase会把一起访问的数据放在一个store里面，即为每个ColumnFamily建一个store，如果有几个ColumnFamily，也就有几个Store。一个Store由一个memStore和0或者多个StoreFile组成。 HBase以store的大小来判断是否需要切分region。 MemStorememStore 是放在内存里的。保存修改的数据即keyValues。当memStore的大小达到一个阀值（默认64MB）时，memStore会被flush到文件，即生成一个快照。目前hbase 会有一个线程来负责memStore的flush操作。 StoreFilememStore内存中的数据写到文件后就是StoreFile，StoreFile底层是以HFile的格式保存。 HFileHBase中KeyValue数据的存储格式，是hadoop的二进制格式文件。 首先HFile文件是不定长的，长度固定的只有其中的两块：Trailer和FileInfo。Trailer中又指针指向其他数据块的起始点，FileInfo记录了文件的一些meta信息。 Data Block是hbase io的基本单元，为了提高效率，HRegionServer中又基于LRU的block cache机制。每个Data块的大小可以在创建一个Table的时候通过参数指定（默认块大小64KB），大号的Block有利于顺序Scan，小号的Block利于随机查询。每个Data块除了开头的Magic以外就是一个个KeyValue对拼接而成，Magic内容就是一些随机数字，目的是烦着数据损坏，结构如下。 HFile结构图如下： Data Block段用来保存表中的数据，这部分可以被压缩。 Meta Block段（可选的）用来保存用户自定义的kv段，可以被压缩。 FileInfo段用来保存HFile的元信息，本能被压缩，用户也可以在这一部分添加自己的元信息。 Data Block Index段（可选的）用来保存Meta Blcok的索引。 Trailer这一段是定长的。保存了每一段的偏移量，读取一个HFile时，会首先读取Trailer，Trailer保存了每个段的起始位置(段的Magic Number用来做安全check)，然后，DataBlock Index会被读取到内存中，这样，当检索某个key时，不需要扫描整个HFile，而只需从内存中找到key所在的block，通过一次磁盘io将整个 block读取到内存中，再找到需要的key。DataBlock Index采用LRU机制淘汰。 HFile的Data Block，Meta Block通常采用压缩方式存储，压缩之后可以大大减少网络IO和磁盘IO，随之而来的开销当然是需要花费cpu进行压缩和解压缩。目标HFile的压缩支持两种方式：gzip、lzo。 另外，针对目前针对现有HFile的两个主要缺陷： a) 暂用过多内存 b) 启动加载时间缓慢 提出了HFile Version2设计：https://issues.apache.org/jira/secure/attachment/12478329/hfile_format_v2_design_draft_0.1.pdf HLog其实HLog文件就是一个普通的Hadoop Sequence File，Sequence File的value是key时HLogKey对象，其中记录了写入数据的归属信息，除了table和region名字外，还同时包括sequence number和timestamp，timestamp是写入时间，equence number的起始值为0，或者是最近一次存入文件系统中的equence number。 Sequence File的value是HBase的KeyValue对象，即对应HFile中的KeyValue。 HLog(WAL log)：WAL意为write ahead log，用来做灾难恢复使用，HLog记录数据的所有变更，一旦region server 宕机，就可以从log中进行恢复。 LogFlusher 前面提到，数据以KeyValue形式到达HRegionServer，将写入WAL，之后，写入一个SequenceFile。看过去没问题，但是因为数据流在写入文件系统时，经常会缓存以提高性能。这样，有些本以为在日志文件中的数据实际在内存中。这里，我们提供了一个LogFlusher的类。它调用HLog.optionalSync(),后者根据 hbase.regionserver.optionallogflushinterval (默认是10秒)，定期调用Hlog.sync()。另外，HLog.doWrite()也会根据 hbase.regionserver.flushlogentries (默认100秒)定期调用Hlog.sync()。Sync() 本身调用HLog.Writer.sync()，它由SequenceFileLogWriter实现。 LogRoller Log的大小通过$HBASE_HOME/conf/hbase-site.xml 的 hbase.regionserver.logroll.period 限制，默认是一个小时。所以每60分钟，会打开一个新的log文件。久而久之，会有一大堆的文件需要维护。首先，LogRoller调用HLog.rollWriter()，定时滚动日志，之后，利用HLog.cleanOldLogs()可以清除旧的日志。它首先取得存储文件中的最大的sequence number，之后检查是否存在一个log所有的条目的“sequence number”均低于这个值，如果存在，将删除这个log。 每个region server维护一个HLog，而不是每一个region一个，这样不同region（来自不同的table）的日志会混在一起，这样做的目的是不断追加单个文件相对于同时写多个文件而言，可以减少磁盘寻址次数，因此可以提高table的写性能。带来麻烦的时，如果一个region server下线，为了恢复其上的region，需要讲region server上的log进行拆分，然后分发到其他region server上进行恢复。]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java笔记：IO]]></title>
    <url>%2F2013%2F06%2F09%2Fnote-about-java-io%2F</url>
    <content type="text"><![CDATA[说明，本文内容来源于java io系列01之 “目录”，做了一些删减。 Java库的IO分为输入/输出两部分。 早期的Java 1.0版本的输入系统是InputStream及其子类，输出系统是OutputStream及其子类。 后来的Java 1.1版本对IO系统进行了重新设计。输入系统是Reader及其子类，输出系统是Writer及其子类。 Java1.1之所以要重新设计，主要是为了添加国际化支持(即添加了对16位Unicode码的支持)。具体表现为Java 1.0的IO系统是字节流，而Java 1.1的IO系统是字符流。 字节流，就是数据流中最小的数据单元是8位的字节。 字符流，就是数据流中最小的数据单元是16位的字符。 字节流在操作的时候，不会用到缓冲；而字符流会用到缓冲。所以，字符流的效率会更高一些。 至于为什么用到缓冲会效率更高一些呢？那是因为，缓冲本质上是一段内存区域；而文件大多是存储在硬盘或者Nand Flash上面。读写内存的速度比读写硬盘或Nand Flash上文件的速度快很多！ 目前，文件大多以字节的方式存储的。所以在开发中，字节流使用较为广泛。 Java 1.0和Java 1.1 的IO类的比较基本类对比表 Java 1.0 IO基本类(字节流) Java 1.1 IO基本类(字符流) InputStream Reader OutputStream Writer FileInputStream FileReader FileOutputStream FileWriter StringBufferInputStream StringReader 无 StringWriter ByteArrayInputStream CharArrayReader ByteArrayOutputStream CharArrayWriter PipedInputStream PipedReader PipedOutputStream PipedWriter 装饰器对比表 Java 1.0 IO装饰器(字节流) Java 1.1 IO装饰器(字符流) FilterInputStream FilterReader FilterOutputStream FilterWriter（没有子类的抽象类 BufferedInputStream BufferedReader（也有 readLine()） BufferedOutputStream BufferedWriter DataInputStream 无 PrintStream PrintWriter LineNumberInputStream LineNumberReader StreamTokenizer 无 PushBackInputStream PushBackReader io框架以字节为单位的输入流的框架图： 是以字节为单位的输出流的框架图： 以字节为单位的输入流和输出流关联的框架图： 以字符为单位的输入流的框架图： 以字符为单位的输出流的框架图： 以字符为单位的输入流和输出流关联的框架图： 字节转换为字符流的框架图： 字节和字符的输入流对应关系： 字节和字符的输出流对应关系：]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java笔记：工厂模式]]></title>
    <url>%2F2013%2F06%2F09%2Fnote-about-java-factory-model%2F</url>
    <content type="text"><![CDATA[工厂模式主要是为创建对象提供接口，以便将创建对象的具体过程屏蔽隔离起来，达到提高灵活性的目的。 工厂模式在《Java与模式》中分为三类： 1)简单工厂模式(Simple Factory)：不利于产生系列产品。 2)工厂方法模式(Factory Method)：又称为多形性工厂。 3)抽象工厂模式(Abstract Factory)：又称为工具箱，产生产品族，但不利于产生新的产品。 GOF在《设计模式》一书中将工厂模式分为两类：工厂方法模式(Factory Method)与抽象工厂模式(Abstract Factory)。将简单工厂模式(Simple Factory)看为工厂方法模式的一种特例，两者归为一类。 简单工厂模式在简单工厂模式中，可以根据自变量的不同返回不同类的实例。简单工厂模式专门定义一个类来负责创建其他类的实例，被创建的实例通常都具有共同的父类。 简单工厂模式角色： 工厂类 抽象产品 具体产品 简单工厂模式的优点如下： 工厂类含有必要的判断逻辑，可以决定在什么时候创建哪一个产品类的实例，客户端可以免除直接创建产品对象的责任，而仅仅“消费”产品；简单工厂模式通过这种做法实现了对责任的分割，它提供了专门的工厂类用于创建对象。 客户端无需知道所创建的具体产品类的类名，只需要知道具体产品类所对应的参数即可，对于一些复杂的类名，通过简单工厂模式可以减少使用者的记忆量。 通过引入配置文件，可以在不修改任何客户端代码的情况下更换和增加新的具体产品类，在一定程度上提高了系统的灵活性。 简单工厂模式的缺点如下： 工厂类中包括了创建产品类的业务逻辑，一旦工厂类不能正常工作，整个系统都要受到影响。 系统扩展困难，一旦添加新产品就需要修改工厂逻辑，在产品类型较多时，有可能造成工厂逻辑过于复杂，不利于系统的扩展和维护。 简单工厂模式由于使用了静态工厂方法，造成工厂角色无法形成基于继承的等级结构。 举例： 1234567891011121314151617181920212223242526 //抽象产品角色public interface Car&#123; public void drive();&#125;//具体产品角色public class Benz implements Car&#123; public void drive() &#123; System.out.println("Driving Benz "); &#125;&#125;public class Bmw implements Car&#123; public void drive() &#123; System.out.println("Driving Bmw "); &#125;&#125;//工厂类角色public class CarFactory&#123; //工厂方法.注意 返回类型为抽象产品角色 public static Car create(String s)throws Exception&#123; if(s.equalsIgnoreCase("Benz")) return new Benz(); else if(s.equalsIgnoreCase("Bmw")) return new Bmw(); else throw new Exception(); &#125;&#125; 工厂方法模式在工厂方法模式中，核心的工厂类不再负责所有的产品的创建，而是将具体创建的工作交给子类去做。 工厂方法模式角色: 抽象工厂：是具体工厂角色必须实现的接口或者必须继承的父类。 具体工厂：它含有和具体业务逻辑有关的代码。由应用程序调用以创建对应的具体产品的对象。 抽象产品：它是具体产品继承的父类或者是实现的接口。 具体产品：具体工厂角色所创建的对象就是此角色的实例。 举例： 12345678910111213141516171819202122232425262728293031323334353637383940 //抽象产品角色public interface Car&#123; public void drive();&#125;//具体产品角色public class Benz implements Car&#123; public void drive() &#123; System.out.println("Driving Benz "); &#125;&#125;public class Bmw implements Car&#123; public void drive() &#123; System.out.println("Driving Bmw "); &#125;&#125;//抽象工厂类角色public abstract class CarFactory&#123; public abstract Car create();&#125;public class BenzCarFactory extends CarFactory&#123; public Car create()&#123; return new Benz(); &#125;&#125;public class BmwCarFactory extends CarFactory&#123; public Car create()&#123; return new Bmw(); &#125;&#125;//测试类public class Test &#123; public static void main(String[] args) &#123; CarFactory factory = new BenzCarFactory(); Car m = factory.create(); m.drive(); &#125;&#125; 抽象工厂模式抽象工厂模式提供一个创建一系列或相互依赖的对象的接口，而无需指定它们具体的类。它针对的是有多个产品的等级结构。而工厂方法模式针对的是一个产品的等级结构。 举例： 123456789101112131415161718192021222324252627282930//抽象工厂类public abstract class AbstractFactory &#123; public abstract Vehicle createVehicle(); public abstract Weapon createWeapon(); public abstract Food createFood();&#125;//具体工厂类，其中Food,Vehicle，Weapon是抽象类public class DefaultFactory extends AbstractFactory&#123; @Override public Food createFood() &#123; return new Apple(); &#125; @Override public Vehicle createVehicle() &#123; return new Car(); &#125; @Override public Weapon createWeapon() &#123; return new AK47(); &#125;&#125;//测试类public class Test &#123; public static void main(String[] args) &#123; AbstractFactory f = new DefaultFactory(); Vehicle v = f.createVehicle(); Weapon w = f.createWeapon(); Food a = f.createFood(); &#125;&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java笔记：单例模式]]></title>
    <url>%2F2013%2F06%2F09%2Fnote-about-java-singleton-model%2F</url>
    <content type="text"><![CDATA[什么是单例模式呢？就是在整个系统中，只有一个唯一存在的实例。使用Singleton的好处还在于可以节省内存，因为它限制了实例的个数，有利于Java垃圾回收。 单例模式主要有3个特点： 1、单例类确保自己只有一个实例。 2、单例类必须自己创建自己的实例。 3、单例类必须为其他对象提供唯一的实例。 单例模式的实现方式有五种方法：懒汉，恶汉，双重校验锁，枚举和静态内部类。 懒汉模式：1234567891011public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 这种写法能够在多线程中很好的工作，而且看起来它也具备很好的lazy loading，但是，遗憾的是，效率很低，99%情况下不需要同步。 恶汉模式：12345678public class Singleton &#123; private static Singleton instance = new Singleton(); private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return instance; &#125;&#125; 这种方式基于classloder机制避免了多线程的同步问题，但是没有达到lazy loading的效果。 双重检测：12345678910111213public class Singleton &#123; public static final Singleton singleton = null; private Singleton()&#123;&#125; public static Singleton getInstance()&#123; if(singleton == null)&#123; //如果singleton为空，表明未实例化 synchronize (Singleton.class)&#123; if( singleton == null ) &#123; // double check 进来判断后再实例化。 singleton = new Singleton(); &#125; &#125; return singleton; &#125;&#125; 当两个线程执行完第一个 singleton == null 后等待锁， 其中一个线程获得锁并进入synchronize后，实例化了，然后退出释放锁，另外一个线程获得锁，进入又想实例化，会判断是否进行实例化了，如果存在，就不进行实例化了。 静态内部类（懒汉模式）1234567891011121314//一个延迟实例化的内部类的单例模式public final class Singleton &#123; //一个内部类的容器，调用getInstance时，JVM加载这个类 private static final class SingletonHolder &#123; static final Singleton singleton = new Singleton(); &#125; private Singleton() &#123;&#125; public static Singleton getInstance() &#123; return SingletonHolder.singleton; &#125; &#125; 首先，其他类在引用这个Singleton的类时，只是新建了一个引用，并没有开辟一个的堆空间存放（对象所在的内存空间）。接着，当使用Singleton.getInstance()方法后，Java虚拟机（JVM）会加载SingletonHolder.class（JLS规定每个class对象只能被初始化一次），并实例化一个Singleton对象。 缺点：需要在Java的另外一个内存空间（Java PermGen 永久代内存，这块内存是虚拟机加载class文件存放的位置）占用一个大块的空间。 枚举12345public enum Singleton &#123; INSTANCE; public void whateverMethod() &#123; &#125; &#125; 这种方式是Effective Java作者Josh Bloch 提倡的方式，它不仅能避免多线程同步问题，而且还能防止反序列化重新创建新的对象。 通过这种方式，不能通过反射和序列化来获取一个实例，因为所有的枚举类都继承自java.lang.Enum类, 而不是Object类： 123456789101112protected final Object clone() throws CloneNotSupportedException &#123; throw new CloneNotSupportedException(); &#125; private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException &#123; throw new InvalidObjectException("can't deserialize enum"); &#125; private void readObjectNoData() throws ObjectStreamException &#123; throw new InvalidObjectException("can't deserialize enum"); &#125; 其他需要注意的事项： 不同的CLASSLOADER加载SINGLETON生成的实例不一样，这样，需要自己写classloader，保证Singleton.class的加载唯一。参考文章http://www.oschina.net/question/9709_102019 如果Singleton实现了java.io.Serializable接口，使用反序列化可以生成一个新的对象，这样需要重写readResolve方法，返回静态的单个实例。 单例模式在 Java 标准库中的使用： java.lang.Runtime#getRuntime()是 Java 标准库中常用的方法，它返回与当前Java应用关联的运行时对象。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java笔记：多线程]]></title>
    <url>%2F2013%2F06%2F08%2Fnote-about-java-thread%2F</url>
    <content type="text"><![CDATA[一些概念现在的操作系统是多任务操作系统。多线程是实现多任务的一种方式。 进程 是指一个内存中运行的应用程序，每个进程都有自己独立的一块内存空间，一个进程中可以启动多个线程。比如在 Windows 系统中，一个运行的 exe 就是一个进程。 线程 是指进程中的一个执行流程，一个进程中可以运行多个线程。比如 java.exe 进程中可以运行很多线程。线程总是属于某个进程，进程中的多个线程共享进程的内存。 同时 执行是人的感觉，在线程之间实际上轮换执行。 多线程间堆空间共享，栈空间独立。堆存的是地址，栈存的是变量（如：局部变量）。这部分内容结合 Java 内存模型 来理解。 创建线程两种方式：继承Thread类 或 实现Runnable接口。 Thread 对象代表一个线程，一个 Thread 类实例只是一个对象，像 Java 中的任何其他对象一样，具有变量和方法，生死于堆上。 Java 中，每个线程都有一个调用栈，即使不在程序中创建任何新的线程，线程也在后台运行着。 一个 Java 应用总是从 main() 方法开始运行，mian() 方法运行在一个线程内，它被称为主线程。 一旦创建一个新的线程，就产生一个新的调用栈。 多线程共同访问的同一个对象（临界资源），如果破坏了不可分割的操作（原子操作），就会造成数据不一致的情况。 线程总体分两类：用户线程 和守候线程。 当所有用户线程执行完毕的时候，JVM自动关闭。但是守候线程却不独立于JVM，守候线程一般是由操作系统或者用户自己创建的。 线程状态图 说明：线程共包括以下5种状态。 新建状态(New)： 线程对象被创建后，就进入了新建状态。例如，Thread thread = new Thread()。 就绪状态(Runnable)： 也被称为“可执行状态”。线程对象被创建后，其它线程调用了该对象的 start() 方法，从而来启动该线程。例如，thread.start()。运行中的线程调用 yield() 之后也会进入就绪状态。处于就绪状态的线程，随时可能被 CPU 调度执行。 运行状态(Running)： 线程获取CPU权限进行执行。需要注意的是，线程只能从就绪状态进入到运行状态。 阻塞状态(Blocked)： 阻塞状态是线程因为某种原因放弃 CPU 使用权，暂时停止运行。直到线程进入就绪状态，才有机会转到运行状态。阻塞的情况分三种： a) 等待阻塞 – 通过调用线程的 wait() 方法，让线程等待某工作的完成。 b) 同步阻塞 – 线程在获取 synchronized 同步锁失败(因为锁被其它线程所占用)，它会进入同步阻塞状态。 b) 其他阻塞 – 通过调用线程的 sleep() 或 join() 或发出了 I/O 请求时，线程会进入到阻塞状态。当 sleep() 状态超时、join() 等待线程终止或者超时、或者 I/O 处理完毕时，线程重新转入就绪状态。 死亡状态(Dead)：线程执行完了或者因异常退出了 run() 方法，该线程结束生命周期。 Thread 和 RunnableRunnable 是一个接口，该接口中只包含了一个 run() 方法。它的定义如下： 123public interface Runnable &#123; public abstract void run();&#125; 我们可以定义一个类 A 实现 Runnable 接口；然后，通过 new Thread(new A()) 等方式新建线程。 Thread 是一个类。Thread 本身就实现了 Runnable 接口。它的声明如下： 12345678910public class Thread implements Runnable &#123; public Thread() &#123;&#125; public Thread(Runnable target) &#123;&#125; public Thread(ThreadGroup group, Runnable target)&#123;&#125; public Thread(String name)&#123;&#125; public Thread(ThreadGroup group, String name)&#123;&#125; public Thread(Runnable target, String name)&#123;&#125; public Thread(ThreadGroup group, Runnable target, String name)&#123;&#125; public Thread(ThreadGroup group, Runnable target, String name,long stackSize)&#123;&#125;&#125; 相同点： 都是“多线程的实现方式”。 不同点： Thread 是类，而 Runnable 是接口；Thread 本身是实现了 Runnable 接口的类。我们知道“一个类只能有一个父类，但是却能实现多个接口”，因此 Runnable 具有更好的扩展性。 此外，Runnable 还可以用于“资源的共享”。即，多个线程都是基于某一个Runnable对象建立的，它们会共享这个Runnable对象上的资源。 创建和运行线程的两种方法：12345678910111213141516171819202122232425262728//测试Runnable类实现的多线程程序 public class DoSomething implements Runnable &#123; private String name; public DoSomething(String name) &#123; this.name = name; &#125; public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; for (long k = 0; k &lt; 100000000; k++) ; System.out.println(name + ": " + i); &#125; &#125; &#125;public class TestRunnable &#123; public static void main(String[] args) &#123; DoSomething ds1 = new DoSomething("javachen"); DoSomething ds2 = new DoSomething("blog"); Thread t1 = new Thread(ds1); Thread t2 = new Thread(ds2); t1.start(); t2.start(); &#125; &#125; 1234567891011121314151617181920//测试扩展Thread类实现的多线程程序 public class TestThread extends Thread&#123; public TestThread(String name) &#123; super(name); &#125; public void run() &#123; for(int i = 0;i&lt;5;i++)&#123; for(long k= 0; k &lt;100000000;k++); System.out.println(this.getName()+" :"+i); &#125; &#125; public static void main(String[] args) &#123; Thread t1 = new TestThread("javachen"); Thread t2 = new TestThread("blog"); t1.start(); t2.start(); &#125; &#125; start() 和 run() start()：它的作用是启动一个新线程，新线程会执行相应的 run()方法。start() 不能被重复调用。 run()：和普通的成员方法一样，可以被重复调用。单独调用 run() 的话，会在当前线程中执行 run()，而并不会启动新线程！ 在调用 start()方法之前，线程处于新状态中，新状态指有一个 Thread 对象，但还没有一个真正的线程。 在调用 start()方法之后，发生了一系列复杂的事情： 启动新的执行线程（具有新的调用栈）； 该线程从新状态转移到可运行状态； 当该线程获得机会执行时，其目标 run()方法将运行。 wait(), notify(), notifyAll()在 Object.java 中，定义了 wait(), notify() 和 notifyAll() 等接口。wait() 的作用是让当前线程进入等待状态，同时，wait() 也会让当前线程释放它所持有的锁。而 notify() 和 notifyAll() 的作用，则是唤醒当前对象上的等待线程；notify() 是唤醒单个线程，而 notifyAll()是唤醒所有的线程。 notify(),wait() 依赖于“同步锁”，而“同步锁”是对象锁持有，并且每个对象有且仅有一个！ 在 java 中，任何对象都有一个锁池，用来存放等待该对象锁标记的线程，线程阻塞在对象锁池中时，不会释放其所拥有的其它对象的锁标记。 在 java 中，任何对象都有一个等待队列，用来存放线程，线程 t1对（让）o调用 wait 方法,必须放在对 o 加锁的同步代码块中! t1 会释放其所拥有的所有锁标记; t1会进入 o 的等待队列 t2 对（让）o调用 notify/notifyAll 方法,也必须放在对 o 加锁的同步代码块中! 会从 o 的等待队列中释放一个/全部线程，对 t2 毫无影响，t2 继续执行。 yield()Thread.yield() 方法作用是：暂停当前正在执行的线程对象，并执行其他线程。 yield() 应该做的是让当前运行线程回到可运行状态，以允许具有相同优先级的其他线程获得运行机会。因此，使用 yield() 的目的是让相同优先级的线程之间能适当的轮转执行。 但是，实际中无法保证 yield() 达到让步目的，因为让步的线程还有可能被线程调度程序再次选中。 结论：yield()从未导致线程转到等待/睡眠/阻塞状态。在大多数情况下，yield() 将导致线程从运行状态转到可运行状态，但有可能没有效果。 wait()是会线程释放它所持有对象的同步锁，而yield()方法不会释放锁。 sleep()sleep() 的作用是让当前线程休眠，即当前线程会从 运行状态 进入到 休眠(阻塞)状态 。sleep() 会指定休眠时间，线程休眠的时间会大于/等于该休眠时间；在线程重新被唤醒时，它会由 阻塞状态 变成 就绪状态，从而等待 cpu 的调度执行。 wait()会释放对象的同步锁，而sleep()则不会释放锁。 join()Thread的非静态方法 join() 让一个线程 B “加入” 到另外一个线程 A 的尾部。在 A 执行完毕之前，B 不能工作。例如： 123Thread t = new MyThread();t.start();t.join(); 另外，join() 方法还有带超时限制的重载版本。 例如 t.join(5000); 让线程等待5000毫秒，如果超过这个时间，则停止等待，变为可运行状态。 interrupt()interrupt()的作用是中断本线程。 本线程中断自己是被允许的；其它线程调用本线程的 interrupt() 方法时，会通过 checkAccess() 检查权限。这有可能抛出 SecurityException 异常。 如果本线程是处于阻塞状态：调用线程的wait(), wait(long)或 wait(long, int)会让它进入等待(阻塞)状态，或者调用线程的join(), join(long), join(long, int), sleep(long), sleep(long, int) 也会让它进入阻塞状态。若线程在阻塞状态时，调用了它的 interrupt()方法，那么它的“中断状态”会被清除并且会收到一个 InterruptedException 异常。 例如，线程通过 wait() 进入阻塞状态，此时通过 interrupt() 中断该线程；调用 interrupt() 会立即将线程的中断标记设为“true”，但是由于线程处于阻塞状态，所以该“中断标记”会立即被清除为“false”，同时，会产生一个 InterruptedException 的异常。 如果线程被阻塞在一个 Selector 选择器中，那么通过 interrupt() 中断它时；线程的中断标记会被设置为 true，并且它会立即从选择操作中返回。 如果不属于前面所说的情况，那么通过 interrupt() 中断线程时，它的中断标记会被设置为“true”。中断一个“已终止的线程”不会产生任何操作。 interrupt()常常被用来终止“阻塞状态”线程。 interrupted() 和 isInterrupted()都能够用于检测对象的“中断标记”。 区别是，interrupted()除了返回中断标记之外，它还会清除中断标记(即将中断标记设为false)；而isInterrupted()仅仅返回中断标记。 线程的同步与锁线程的同步 是为了防止多个线程访问一个数据对象时，对数据造成的破坏。 Java中每个对象都有一个内置锁。当程序运行到非静态的 synchronized 同步方法上时，自动获得与正在执行代码类的当前实例有关的锁。获得一个对象的锁也称为获取锁、锁定对象、在对象上锁定或在对象上同步。 当程序运行到 synchronized 同步方法或代码块时才该对象锁才起作用。 一个对象只有一个锁。所以，如果一个线程获得该锁，就没有其他线程可以获得锁，直到第一个线程释放（或返回）锁。这也意味着任何其他线程都不能进入该对象上的 synchronized 方法或代码块，直到该锁被释放。释放锁 是指持锁线程退出了synchronized同步方法或代码块。 关于锁和同步，有一下几个要点： 1）、只能同步方法，而不能同步变量和类； 2）、每个对象只有一个锁；当提到同步时，应该清楚在什么上同步？也就是说，在哪个对象上同步？ 3）、不必同步类中所有的方法，类可以同时拥有同步和非同步方法。 4）、如果两个线程要执行一个类中的 synchronized 方法，并且两个线程使用相同的实例来调用方法，那么一次只能有一个线程能够执行方法，另一个需要等待，直到锁被释放。也就是说：如果一个线程在对象上获得一个锁，就没有任何其他线程可以进入（该对象的）类中的任何一个同步方法。 5）、如果线程拥有同步和非同步方法，则非同步方法可以被多个线程自由访问而不受锁的限制。 6）、线程睡眠时，它所持的任何锁都不会释放。 7）、线程可以获得多个锁。比如，在一个对象的同步方法里面调用另外一个对象的同步方法，则获取了两个对象的同步锁。 8）、同步损害并发性，应该尽可能缩小同步范围。同步不但可以同步整个方法，还可以同步方法中一部分代码块。 9）、在使用同步代码块时候，应该指定在哪个对象上同步，也就是说要获取哪个对象的锁。 举例： 1234567891011121314151617181920212223//对方法同步public synchronized int getX() &#123; return x++;&#125;//对代码块同步public int getX() &#123; synchronized (this) &#123; return x; &#125;&#125; //对静态方法同步public static synchronized int setName(String name)&#123; Xxx.name = name;&#125;//对静态方法中的代码块同步public static int setName(String name)&#123; synchronized(Xxx.class)&#123; Xxx.name = name; &#125;&#125; 参考资料 Java多线程编程总结]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>thread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java笔记：集合框架实现原理]]></title>
    <url>%2F2013%2F06%2F08%2Fjava-collection-framework%2F</url>
    <content type="text"><![CDATA[这篇文章是对http://www.cnblogs.com/skywang12345/category/455711.html中java集合框架相关文章的一个总结，在此对原作者的辛勤整理表示感谢。 Java集合是java提供的工具包，包含了常用的数据结构：集合、链表、队列、栈、数组、映射等。Java集合工具包位置是java.util.* Java集合主要可以划分为4个部分：List列表、Set集合、Map映射、工具类(Iterator迭代器、Enumeration枚举类、Arrays和Collections)。 Java集合工具包框架图(如下)： Collection是一个接口，是以对象为单位来管理元素的。有两个子接口：List接口和Set接口： List接口：元素是有顺序（下标），可以重复。有点像数组，可以用迭代器（Iterator）和数组遍历集合。List的实现类有LinkedList、ArrayList、Vector、Stack。 Set接口：无顺序，不可以重复（内容不重复，而非地址不重复）。只能用迭代器（Iterator）遍历集合。Set接口里本身无方法，方法都是从父接口Collection里继承的。Set的实现类有HastSet和TreeSet。HashSet依赖于HashMap，它实际上是通过HashMap实现的；TreeSet依赖于TreeMap，它实际上是通过TreeMap实现的。 Map:对应一个键对象和一个值对象，可以根据key找value。AbstractMap是个抽象类，它实现了Map接口中的大部分API。而HashMap，TreeMap，WeakHashMap都是继承于AbstractMap。Hashtable虽然继承于Dictionary，但它实现了Map接口。 Iterator用于遍历集合，Enumeration是JDK 1.0引入的抽象类，只能在Hashtable、Vector、Stack中使用。 Arrays和Collections是两个操作数组和集合的工具类，提供一些静态方法。 1. List1.1 ArrayListArrayList 是一个数组列表，相当于动态数组。与Java中的数组相比，它的容量能动态增长。它继承于AbstractList，实现了List、RandomAccess、Cloneable、java.io.Serializable这些接口。 数据结构通过数组保存数据，数组初始大小为10，也可以通过构造函数修改数组初始大小。 当ArrayList容量不足以容纳全部元素时，ArrayList会重新设置容量：新的容量=“(原始容量x3)/2 + 1”。 访问数据当添加和删除元素时，需要移动数组中得元素；当获取元素时，只需要通过数组的索引返回元素，故查询快，增删慢。 ArrayList中的方法没有加同步锁，故线程不安全。 遍历方式1）通过Iterator去遍历 12345Integer value = null;Iterator iter = list.iterator();while (iter.hasNext()) &#123; value = (Integer)iter.next();&#125; 2）随机访问，通过索引值去遍历 12345Integer value = null;int size = list.size();for(int i=0; i&lt;size; i++)&#123; value = (Integer)list.get(i); &#125; 3）for循环遍历 1234Integer value = null;for (Integer integ:list) &#123; value = integ;&#125; 性能比较： 1.2 VectorVector 是矢量列表，它是JDK1.0版本添加的类。继承于AbstractList，实现了List, RandomAccess, Cloneable这些接口。 数据结构通过数组保存数据，数组初始大小为10，容量增加系数为0，也可以通过构造函数修改数组初始大小。 当容量不足以容纳全部元素时，若容量增加系数大于0，则将容量的值增加“容量增加系数”；否则，将容量大小增加一倍。 访问数据当添加和删除元素时，需要移动数组中得元素；当获取元素时，只需要通过数组的索引返回元素，故查询快，增删慢。 方法没有加同步锁，故线程不安全。 遍历方式 1）通过Iterator去遍历 2）随机访问，通过索引值去遍历 3）foreach循环遍历 4）Enumeration遍历 性能比较： 1.3 StackStack是栈。它的特性是：先进后出(FILO, First In Last Out)。 java工具包中的Stack是继承于Vector的，由于Vector是通过数组实现的，这就意味着，Stack也是通过数组实现的，而非链表。 数据结构和 Vector 一致，Stack的push、peek、和pull方法都是对数组末尾的元素进行操作。 1.4 LinkedListLinkedList 是一个继承于AbstractSequentialList的双向链表。它也可以被当作堆栈、队列或双端队列进行操作。它实现了List、Deque、Cloneable、java.io.Serializable接口。 数据结构LinkedList的本质是双向链表。 LinkedList继承于AbstractSequentialList，并且实现了Dequeue接口。 LinkedList包含两个重要的成员：header 和 size。 header是双向链表的表头，它是双向链表节点所对应的类Entry的实例。Entry中包含成员变量： previous, next, element。其中，previous是该节点的上一个节点，next是该节点的下一个节点，element是该节点所包含的值。 size是双向链表中节点的个数。 LinkedList不存在容量不足的问题。 访问数据当添加和删除元素时，只需要将新的元素插入指定位置之前，然后修改链表的指向；当获取元素时（get(int location)），会比较“location”和“双向链表长度的1/2”；若前者大，则从链表头开始往后查找，直到location位置；否则，从链表末尾开始先前查找，直到location位置。故查询慢，增删快。 方法没有加同步锁，故线程不安全。 遍历方式LinkedList支持多种遍历方式。建议不要采用随机访问的方式去遍历LinkedList，而采用foreach逐个遍历的方式。 1.5 总结应用场景如果涉及到“栈”、“队列”、“链表”等操作，应该考虑用List，具体的选择哪个List，根据下面的标准来取舍。 对于需要快速插入，删除元素，应该使用LinkedList。 对于需要快速随机访问元素，应该使用ArrayList。 对于“单线程环境” 或者 “多线程环境，但List仅仅只会被单个线程操作”，此时应该使用非同步的类(如ArrayList)；对于“多线程环境，且List可能同时被多个线程操作”，此时，应该使用同步的类(如Vector)。 Vector和ArrayList相同点： 都继承于AbstractList，并且实现List接口 都实现了RandomAccess和Cloneable接口 都是通过数组实现的，本质上都是动态数组，默认数组容量是10 都支持Iterator和listIterator遍历 不同点： ArrayList是非线程安全，而Vector是线程安全的 ArrayList支持序列化，而Vector不支持 容量增加方式不同，Vector默认增长为原来一培，而ArrayList却是原来的一半+1 Vector支持通过Enumeration去遍历，而List不支持 2. MapMap 是映射接口，Map中存储的内容是键值对(key-value)。 AbstractMap 是继承于Map的抽象类，它实现了Map中的大部分API。其它Map的实现类可以通过继承AbstractMap来减少重复编码。 SortedMap 是继承于Map的接口。SortedMap中的内容是排序的键值对，排序的方法是通过比较器(Comparator)。 NavigableMap 是继承于SortedMap的接口。相比于SortedMap，NavigableMap有一系列的导航方法；如”获取大于/等于某对象的键值对”、“获取小于/等于某对象的键值对”等等。 TreeMap 继承于AbstractMap，且实现了NavigableMap接口；因此，TreeMap中的内容是“有序的键值对” HashMap 继承于AbstractMap，但没实现NavigableMap接口；因此，HashMap的内容是“键值对，但不保证次序” Hashtable 虽然不是继承于AbstractMap，但它继承于Dictionary(Dictionary也是键值对的接口)，而且也实现Map接口；因此，Hashtable的内容也是“键值对，也不保证次序”。但和HashMap相比，Hashtable是线程安全的，而且它支持通过Enumeration去遍历。 WeakHashMap 继承于AbstractMap。它和HashMap的键类型不同，WeakHashMap的键是“弱键”。 2.1 HashMapHashMap 是一个散列表，它存储的内容是键值对(key-value)映射。 HashMap 继承于AbstractMap，实现了Map、Cloneable、java.io.Serializable接口。 HashMap 的实现不是同步的，这意味着它不是线程安全的。它的key、value都可以为null。此外，HashMap中的映射不是有序的。 数据结构HashMap是通过”拉链法”实现的哈希表。它包括几个重要的成员变量：table、size、threshold、loadFactor、modCount。 table是一个Entry[]数组类型，而Entry实际上就是一个单向链表。哈希表的”key-value键值对”都是存储在Entry数组中的。 size是HashMap的大小，它是HashMap保存的键值对的数量。 threshold是HashMap的阈值，用于判断是否需要调整HashMap的容量。threshold的值=”容量*加载因子”，当HashMap中存储数据的数量达到threshold时，就需要将HashMap的容量加倍。 loadFactor就是加载因子。 modCount是用来实现fail-fast机制的。 HashMap中的key-value都是存储在 Entry ç数组中的，而 Entry 实际上就是一个单向链表。 影响HashMap性能的有两个参数：初始容量(initialCapacity) 和加载因子(loadFactor)。 容量是哈希表中桶的数量，初始容量只是哈希表在创建时的容量，默认值为16。 加载因子是哈希表在其容量自动增加之前可以达到多满的一种尺度。当哈希表中的条目数超出了加载因子与当前容量的乘积时，则要对该哈希表进行 rehash 操作（即重建内部数据结构），从而哈希表将具有大约两倍的桶数。 通常，默认加载因子是 0.75, 这是在时间和空间成本上寻求一种折衷。加载因子过高虽然减少了空间开销，但同时也增加了查询成本（在大多数 HashMap 类的操作中，包括 get 和 put 操作，都反映了这一点）。在设置初始容量时应该考虑到映射中所需的条目数及其加载因子，以便最大限度地减少 rehash 操作次数。如果初始容量大于最大条目数除以加载因子，则不会发生 rehash 操作。 访问数据当get（get(Object key)）数据时，先通过key得到一个hash值，然后调用indexFor(hash, table.length)方法得到table数组中得索引值，其次再通过索引得到数组中得 Entry 对象，最后遍历 Entry 对象，判断key和hash是否相等，如果相等则返回该Entry的value属性值。 当put（put(K key, V value)）数据时，若“key为null”，则将该键值对添加到table[0]中；若“key不为null”，则计算该key的哈希值，然后将其添加到该哈希值对应的链表中； 若“该key”对应的键值对已经存在，则用新的value取代旧的value。然后退出；若“该key”对应的键值对不存在，则将“key-value”添加到table中 遍历方式 遍历HashMap的键值对 遍历HashMap的键 遍历HashMap的值 2.2 Hashtable和HashMap一样，Hashtable 也是一个散列表，它存储的内容是键值对(key-value)映射。Hashtable 继承于Dictionary，实现了Map、Cloneable、java.io.Serializable接口。Hashtable 的函数都是同步的，这意味着它是线程安全的。它的key、value都不可以为null。此外，Hashtable中的映射不是有序的。 Hashtable 的实例有两个参数影响其性能：初始容量 和 加载因子。 容量是哈希表中桶 的数量，初始容量就是哈希表创建时的容量，默认值为11。、 加载因子 是对哈希表在其容量自动增加之前可以达到多满的一个尺度。初始容量和加载因子这两个参数只是对该实现的提示。关于何时以及是否调用 rehash 方法的具体细节则依赖于该实现。 通常，默认加载因子是 0.75,这是在时间和空间成本上寻求一种折衷。加载因子过高虽然减少了空间开销，但同时也增加了查找某个条目的时间（在大多数 Hashtable 操作中，包括 get 和 put 操作，都反映了这一点）。 数据结构和HashMap一样，Hashtable也是一个散列表，它也是通过“拉链法”解决哈希冲突的。 Hashtable里的方法是同步的，故其是线程安全的。 2.3 TreeMapTreeMap 是一个有序的key-value集合，基于红黑树（Red-Black tree）实现。该映射根据其键的自然顺序进行排序，或者根据创建映射时提供的 Comparator 进行排序，具体取决于使用的构造方法。 TreeMap的基本操作 containsKey、get、put 和 remove 的时间复杂度是 log(n) 。 另外，TreeMap是非同步的。 它的iterator 方法返回的迭代器是fail-fastl的。 数据结构TreeMap的本质是R-B Tree(红黑树)，它包含几个重要的成员变量：root、size、comparator。 root 是红黑数的根节点。它是Entry类型，Entry是红黑数的节点，它包含了红黑数的6个基本组成成分：key(键)、value(值)、left(左孩子)、right(右孩子)、parent(父节点)、color(颜色)。Entry节点根据key进行排序，Entry节点包含的内容为value。 红黑数排序时，根据Entry中的key进行排序；Entry中的key比较大小是根据比较器comparator来进行判断的。 size是红黑数中节点的个数。 要彻底理解TreeMap，需要先理解红黑树。参考文章：红黑树(一)之 原理和算法详细介绍 遍历方式 遍历HashMap的键值对 遍历HashMap的键 遍历HashMap的值 2.4 WeakHashMapWeakHashMap 继承于AbstractMap，实现了Map接口。和HashMap一样，WeakHashMap 也是一个散列表，它存储的内容也是键值对(key-value)映射，而且键和值都可以是null。 不过WeakHashMap的键是“弱键”。在 WeakHashMap 中，当某个键不再正常使用时，会被从WeakHashMap中被自动移除。更精确地说，对于一个给定的键，其映射的存在并不阻止垃圾回收器对该键的丢弃，这就使该键成为可终止的，被终止，然后被回收。某个键被终止时，它对应的键值对也就从映射中有效地移除了。 这个“弱键”的原理呢？大致上就是，通过WeakReference和ReferenceQueue实现的。 WeakHashMap的key是“弱键”，即是WeakReference类型的；ReferenceQueue是一个队列，它会保存被GC回收的“弱键”。实现步骤是： (01) 新建WeakHashMap，将“键值对”添加到WeakHashMap中。实际上，WeakHashMap是通过数组table保存Entry(键值对)；每一个Entry实际上是一个单向链表，即Entry是键值对链表。 (02) 当某“弱键”不再被其它对象引用，并被GC回收时。在GC回收该“弱键”时，这个“弱键”也同时会被添加到ReferenceQueue(queue)队列中。 (03) 当下一次我们需要操作WeakHashMap时，会先同步table和queue。table中保存了全部的键值对，而queue中保存被GC回收的键值对；同步它们，就是删除table中被GC回收的键值对。 这就是“弱键”如何被自动从WeakHashMap中删除的步骤了。 数据结构WeakHashMap和HashMap都是通过”拉链法“实现的散列表。它们的源码绝大部分内容都一样，这里就只是对它们不同的部分就是说明。 WeakReference是“弱键”实现的哈希表。它这个“弱键”的目的就是：实现对“键值对”的动态回收。当“弱键”不再被使用到时，GC会回收它，WeakReference也会将“弱键”对应的键值对删除。 “弱键”是一个“弱引用(WeakReference)”，在Java中，WeakReference和ReferenceQueue 是联合使用的。在WeakHashMap中亦是如此：如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。 接着，WeakHashMap会根据“引用队列”，来删除“WeakHashMap中已被GC回收的‘弱键’对应的键值对”。 2.5 总结 HashMap 是基于“拉链法”实现的散列表。一般用于单线程程序中。 Hashtable 也是基于“拉链法”实现的散列表。它一般用于多线程程序中。 WeakHashMap 也是基于“拉链法”实现的散列表，它一般也用于单线程程序中。相比HashMap，WeakHashMap中的键是“弱键”，当“弱键”被GC回收时，它对应的键值对也会被从WeakHashMap中删除；而HashMap中的键是强键。 TreeMap 是有序的散列表，它是通过红黑树实现的。它一般用于单线程中存储有序的映射。 HashMap和Hashtable异同相同点： 都是存储“键值对(key-value)”的散列表，而且都是采用拉链法实现的。存储的思想都是：通过table数组存储，数组的每一个元素都是一个Entry；而一个Entry就是一个单向链表，Entry链表中的每一个节点就保存了key-value键值对数据。 添加key-value键值对：首先，根据key值计算出哈希值，再计算出数组索引(即，该key-value在table中的索引)。然后，根据数组索引找到Entry(即，单向链表)，再遍历单向链表，将key和链表中的每一个节点的key进行对比。若key已经存在Entry链表中，则用该value值取代旧的value值；若key不存在Entry链表中，则新建一个key-value节点，并将该节点插入Entry链表的表头位置。 删除key-value键值对：删除键值对，相比于“添加键值对”来说，简单很多。首先，还是根据key计算出哈希值，再计算出数组索引(即，该key-value在table中的索引)。然后，根据索引找出Entry(即，单向链表)。若节点key-value存在与链表Entry中，则删除链表中的节点即可。 不同点： 继承和实现方式不同。HashMap 继承于AbstractMap，实现了Map、Cloneable、java.io.Serializable接口；Hashtable 继承于Dictionary，实现了Map、Cloneable、java.io.Serializable接口。 线程安全不同。Hashtable的几乎所有函数都是同步的，即它是线程安全的，支持多线程，而HashMap的函数则是非同步的，它不是线程安全的。 对null值的处理不同。HashMap的key、value都可以为null，Hashtable的key、value都不可以为null。 支持的遍历方式不同。HashMap只支持Iterator(迭代器)遍历，而Hashtable支持Iterator(迭代器)和Enumeration(枚举器)两种方式遍历。 通过Iterator迭代器遍历时，遍历的顺序不同。HashMap是“从前向后”的遍历数组；再对数组具体某一项对应的链表，从表头开始进行遍历，Hashtabl是“从后往前”的遍历数组；再对数组具体某一项对应的链表，从表头开始进行遍历。 容量的初始值 和 增加方式都不一样。HashMap默认的容量大小是16；增加容量时，每次将容量变为“原始容量x2”，Hashtable默认的容量大小是11；增加容量时，每次将容量变为“原始容量x2 + 1”。 添加key-value时的hash值算法不同。HashMap添加元素时，是使用自定义的哈希算法，Hashtable没有自定义哈希算法，而直接采用的key的hashCode()。 3. Set3.1 HashSetHashSet 是一个没有重复元素的集合。它是由HashMap实现的，不保证元素的顺序，而且HashSet允许使用 null 元素。HashSet是非同步的。HashSet通过iterator()返回的迭代器是fail-fast的。 数据结构HashSet中含有一个HashMap类型的成员变量map，HashSet的操作函数，实际上都是通过map实现的。 3.2 TreeSetTreeSet 是一个有序的集合，它的作用是提供有序的Set集合。它继承于AbstractSet抽象类，实现了NavigableSet, Cloneable, java.io.Serializable接口。 数据结构TreeSet的本质是一个”有序的，并且没有重复元素”的集合，它是通过TreeMap实现的。TreeSet中含有一个”NavigableMap类型的成员变量”m，而m实际上是”TreeMap的实例”。 4. 参考资料 [1] Java 集合系列01之 总体框架 [2] Java里多个Map的性能比较（TreeMap、HashMap、ConcurrentSkipListMap）]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>collection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle访问IDH2.3中的HBase]]></title>
    <url>%2F2013%2F04%2F17%2Faccess-idh-2.3-hbase-in-kettle%2F</url>
    <content type="text"><![CDATA[摘要Kettle是一款国外开源的ETL工具，纯java编写，可以在Window、Linux、Unix上运行，绿色无需安装，数据抽取高效稳定。big-data-plugin是kettle中用于访问bigdata，包括hadoop、cassandra、mongodb等nosql数据库的一个插件。 截至目前，kettle的版本为4.4.1，big-data-plugin插件支持cloudera CDH3u4、CDH4.1，暂不支持Intel的hadoop发行版本IDH。 本文主要介绍如何让kettle支持IDH的hadoop版本。 方法假设你已经安装好IDH-2.3的集群，并已经拷贝出/usr/lib/下的hadoop、hbase、zookeeper目录。 首先，下载一个kettle版本，如社区版data-integration，然后进入data-integration/plugins/pentaho-big-data-plugin目录，修改plugin.properties文件中的active.hadoop.configuration属性，将其值改为cdh4 active.hadoop.configuration=cdh4修改kettle的log4j日志等级，并启动kettle，检查启动过程中是否报错，如有错误，请修正错误。 进入hadoop-configurations目录，copy and paste cdh3u4并命名为idh2.3。 因为IDH和CDH的hadoop版本不一致，故需要替换hadoop和hbase、zookeeper为IDH的版本，涉及到需要替换、增加的jar有，这些jar文件从IDH安装后的目录中拷贝即可： data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/pmr/hbase-0.94.1-Intel.jar data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/pmr/protobuf-java-2.4.0a.jar data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/pmr/zookeeper-3.4.5-Intel.jar data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/client/hadoop-ant-1.0.3-Intel.jar data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/client/hadoop-core-1.0.3-Intel.jar data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/client/hadoop-examples-1.0.3-Intel.jar data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/client/hadoop-test-1.0.3-Intel.jar data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/client/hadoop-tools-1.0.3-Intel.jar data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/libthrift-0.8.0.jar其他依赖包可以尝试添加，并删除多版本的jar文件。 需要删除CDH的jar有： data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/pmr/hbase-0.90.6-cdh3u4.jar data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/pmr/zookeeper-3.3.5-cdh3u4.jar data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/client/hadoop-client-0.20.2-cdh3u4.jar data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/client/hadoop-core-0.20.2-cdh3u4.jar data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/libfb303-0.5.0-cdh.jar data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/idh2.3/lib/libthrift-0.5.0-cdh.jar修改plugin.properties文件中的active.hadoop.configuration属性，将其值改为idh2.3。重起kettle，观察启动过程中是否报错。 验证 打开hbase output组件，配置zookeeper的host和port 在Create/Edit mappings tab页点击Get table names，发现该组件卡住，kettle控制台提示异常则需要检查客户端jar版本和服务端是否一致： 123456789101112131415161718INFO client.HConnectionManager$HConnectionImplementation: getMaster attempt 0 of 10 failed; retrying after sleep of 1000java.io.IOException: Call to OS-GZP2308-04/192.168.40.84:60000 failed on local exception: java.io.EOFExceptionat org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:1110)at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:1079)at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:150)at $Proxy5.getProtocolVersion(Unknown Source)at org.apache.hadoop.hbase.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:183)at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:335)at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:312)at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:364)at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:710)at org.apache.hadoop.hbase.client.HBaseAdmin.&lt; init&gt;(HBaseAdmin.java:141)at com.intel.hbase.test.createtable.TableBuilder.main(TableBuilder.java:48)Caused by: java.io.EOFExceptionat java.io.DataInputStream.readInt(DataInputStream.java:375)at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:605)at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:538)]]></content>
      <categories>
        <category>pentaho</category>
      </categories>
      <tags>
        <tag>kettle</tag>
        <tag>hadoop</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle中添加一个参数字段到输出]]></title>
    <url>%2F2013%2F04%2F07%2Fadd-a-field-from-paramter-to-output%2F</url>
    <content type="text"><![CDATA[kettle可以将输入流中的字段输出到输出流中，输入输出流可以为数据库、文件或其他，通常情况下输入流中字段为已知确定的，如果我想在输出流中添加一个来自转换的命令行参数的一个字段，该如何操作？ 上述问题可以拆分为两个问题： 从命令行接受一个参数作为一个字段 合并输入流和这个字段 问题1第一个问题可以使用kettle中获取系统信息组件，定义一个变量，该值来自命令行参数，见下图： 问题2第二个问题可以使用kettle中记录关联 (笛卡尔输出)组件将两个组件关联起来，输出一个笛卡尔结果集，关联条件设定恒为true，在运行前设置第一个参数的值，然后运行即可。 下载脚本最后，kettle转换文件下载地址：在这里。]]></content>
      <categories>
        <category>pentaho</category>
      </categories>
      <tags>
        <tag>pentaho</tag>
        <tag>kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用yum源安装CDH集群]]></title>
    <url>%2F2013%2F04%2F06%2Finstall-cloudera-cdh-by-yum%2F</url>
    <content type="text"><![CDATA[本文主要是记录使用yum安装CDH Hadoop集群的过程，包括HDFS、Yarn、Hive和HBase。目前使用的是CDH6.2.0安装集群。 0. 环境说明系统环境： 操作系统：CentOs 7.2 Hadoop版本：CDH6.2.0 JDK版本：java-1.8.0-openjdk 运行用户：root 集群各节点角色规划为： 123192.168.56.121 cdh1 NameNode、ResourceManager、HBase、Hive metastore、Impala Catalog、Impala statestore、Sentry 192.168.56.122 cdh2 DataNode、SecondaryNameNode、NodeManager、HBase、Hive Server2、Impala Server192.168.56.123 cdh3 DataNode、HBase、NodeManager、Hive Server2、Impala Server cdh1作为master节点，其他节点作为slave节点。 1. 准备工作1.1 准备虚拟机参考使用Vagrant创建虚拟机安装Hadoop创建三台虚拟机。 1.2 更新yum源123$ rm -rf /etc/yum.repos.d/*$ yum clean all &gt;/dev/null 2&gt;&amp;1$ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 1.3、禁用防火墙保存存在的iptables规则： 1iptables-save &gt; ~/firewall.rules rhel7系统禁用防火墙： 12$ systemctl disable firewalld$ systemctl stop firewalld 设置SELinux模式： 1$ setenforce 0 &gt;/dev/null 2&gt;&amp;1 &amp;&amp; iptables -F 1.4 禁用IPv6CDH 要求使用 IPv4，IPv6 不支持，禁用IPv6方法： 12345$ cat &gt; /etc/sysctl.conf &lt;&lt;EOFnet.ipv6.conf.all.disable_ipv6=1net.ipv6.conf.default.disable_ipv6=1net.ipv6.conf.lo.disable_ipv6=1EOF 使其生效： 1$ sysctl -p 最后确认是否已禁用： 12$ cat /proc/sys/net/ipv6/conf/all/disable_ipv61 1.5 配置hosts修改每个节点/etc/hosts文： 123456$ cat &gt; /etc/hosts &lt;&lt;EOF127.0.0.1 localhost192.168.56.121 cdh1 cdh1.example.com192.168.56.122 cdh2 cdh2.example.com192.168.56.123 cdh3 cdh3.example.comEOF 在每个节点分别配置网络名称。例如在cdh1节点上： 123$ hostnamecdh1.example.com$ hostnamectl set-hostname $(hostname) 修改网络名称： 123$ cat &gt; /etc/sysconfig/network&lt;&lt;EOFHOSTNAME=$(hostname)EOF 查看hostname是否修改过来： 12$ uname -aLinux cdh1.example.com 3.10.0-327.4.5.el7.x86_64 #1 SMP Mon Jan 25 22:07:14 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux 查看ip是否正确： 123456$ yum install net-tools -y &amp;&amp; ifconfig |grep -B1 broadcastenp0s3: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.0.2.15 netmask 255.255.255.0 broadcast 10.0.2.255--enp0s8: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.56.121 netmask 255.255.255.0 broadcast 192.168.56.255 检查一下： 1$ yum install bind-utils -y &amp;&amp; host -v -t A `hostname` 1.6 设置中文编码和时区123456$ cat &gt; /etc/locale.conf &lt;&lt;EOFLANG="zh_CN.UTF-8"LC_CTYPE=zh_CN.UTF-8LC_ALL=zh_CN.UTF-8EOF$ source /etc/locale.conf 设置时区： 1$ sudo ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 1.7 设置root密码12345# Setup sudo to allow no-password sudo for "admin". Additionally,# make "admin" an exempt group so that the PATH is inherited.$ cp /etc/sudoers /etc/sudoers.orig$ echo "root ALL=(ALL) NOPASSWD: ALL" &gt;&gt; /etc/sudoers$ echo 'redhat'|passwd root --stdin &gt;/dev/null 2&gt;&amp;1 1.8 设置命名服务1234# http://ithelpblog.com/os/linux/redhat/centos-redhat/howto-fix-couldnt-resolve-host-on-centos-redhat-rhel-fedora/# http://stackoverflow.com/a/850731/1486325$ echo "nameserver 8.8.8.8" | tee -a /etc/resolv.conf$ echo "nameserver 8.8.4.4" | tee -a /etc/resolv.conf 1.9 生成ssh在每个节点依次执行： 1$ [ ! -d ~/.ssh ] &amp;&amp; ( mkdir /root/.ssh ) &amp;&amp; ( chmod 600 ~/.ssh ) &amp;&amp; yes|ssh-keygen -f ~/.ssh/id_rsa -t rsa -N "" 1.10 时钟同步1.10.1 搭建时钟同步服务器这里选择 cdh1 节点为时钟同步服务器，其他节点为客户端同步时间到该节点。安装ntp: 1$ yum install ntp -y 修改 cdh1 上的配置文件 /etc/ntp.conf : 123456restrict default ignore //默认不允许修改或者查询ntp,并且不接收特殊封包restrict 127.0.0.1 //给于本机所有权限restrict 192.168.56.0 mask 255.255.255.0 notrap nomodify //给于局域网机的机器有同步时间的权限server 127.127.1.0driftfile /var/lib/ntp/driftfudge 127.127.1.0 stratum 10 启动 ntp： 123#设置开机启动$ chkconfig ntpd on$ service ntpd restart 同步硬件时钟： 1$ hwclock --systohc 查看硬件时钟： 1$ hwclock --show ntpq用来监视ntpd操作，使用标准的NTP模式6控制消息模式，并与NTP服务器通信。 ntpq -p 查询网络中的NTP服务器，同时显示客户端和每个服务器的关系。 1234$ ntpq -p remote refid st t when poll reach delay offset jitter============================================================================== time-a-b.nist.g .INIT. 16 u - 64 0 0.000 0.000 0.000 “* “：响应的NTP服务器和最精确的服务器。 “+”：响应这个查询请求的NTP服务器。 “blank（空格）”：没有响应的NTP服务器。 “remote” ：响应这个请求的NTP服务器的名称。 “refid “：NTP服务器使用的更高一级服务器的名称。 “st”：正在响应请求的NTP服务器的级别。 “when”：上一次成功请求之后到现在的秒数。 “poll”：当前的请求的时钟间隔的秒数。 “offset”：主机通过NTP时钟同步与所同步时间源的时间偏移量，单位为毫秒（ms）。 1.10.2 客户端的配置在cdh2和cdh3节点上安装ntp服务并执行下面操作： 1$ yum install ntp -y 同步系统时钟同服务器： 1$ ntpdate -u cdh1 Ntpd启动的时候通常需要一段时间大概5分钟进行时间同步，所以在ntpd刚刚启动的时候还不能正常提供时钟服务，报错”no server suitable for synchronization found”。启动时候需要等待5分钟。 如果想定时进行时间校准，可以使用crond服务来定时执行。 12# 每天 1:00 Linux 系统就会自动的进行网络时间校准00 1 * * * root /usr/sbin/ntpdate 192.168.56.121 &gt;&gt; /root/ntpdate.log 2&gt;&amp;1 1.11 虚拟内存设置Cloudera 建议将/proc/sys/vm/swappiness设置为 0，当前设置为 60。 临时解决，通过echo 0 &gt; /proc/sys/vm/swappiness即可解决。 永久解决： 12$ sysctl -w vm.swappiness=0$ echo vm.swappiness = 0 &gt;&gt; /etc/sysctl.conf 1.12 安装jdkjdk版本看cdh版本而定，我这里是jdk1.8。每个节点安装jdk： 1$ yum install java-1.8.0-openjdk java-1.8.0-openjdk-devel -y JDK建议安装载 /usr/java/jdk-version 目录，并设置 JAVA_HOME 1.13 配置SSH无密码登陆每个节点先生成ssh公钥，参考上面说明。 配置无密码登陆，在每个节点上执行以下命令： 123$ ssh-copy-id -i /root/.ssh/id_rsa.pub root@cdh1$ ssh-copy-id -i /root/.ssh/id_rsa.pub root@cdh2$ ssh-copy-id -i /root/.ssh/id_rsa.pub root@cdh3 然后，在其他节点上类似操作。 1.14 配置cdh yum源配置cdh6的远程yum源： 12345678$ cat &gt; /etc/yum.repos.d/cdh6.repo &lt;&lt;EOF[cloudera-cdh] name=Cloudera's Distribution for Hadoop, Version 6 baseurl=https://archive.cloudera.com/cdh6/6.2.0/redhat7/yum/gpgkey = https://archive.cloudera.com/cdh6/6.2.0/redhat7/yum/RPM-GPG-KEY-clouderaenabled=1 gpgcheck = 1 EOF 将/etc/yum.repos.d/cdh6.repo拷贝到其他节点： 12$ scp /etc/yum.repos.d/cdh6.repo root@cdh2:/etc/yum.repos.d/cdh6.repo$ scp /etc/yum.repos.d/cdh6.repo root@cdh3:/etc/yum.repos.d/cdh6.repo 也可以将yum远程同步到本地，设置本地yum源： 123456$ yum -y install createrepo yum-utils$ mkdir -p /mnt/yum$ cd /mnt/yum$ reposync -r cloudera-cdh$ createrepo cloudera-cdh$ yum clean all 2. 安装HDFS根据文章开头的节点规划，cdh1 为NameNode节点，cdh2为SecondaryNameNode节点，cdh2 和 cdh3 为DataNode节点 在 cdh1 节点安装 hadoop-hdfs-namenode： 1$ yum install hadoop-hdfs-namenode -y 在 cdh2 节点安装 hadoop-hdfs-secondarynamenode 1$ yum install hadoop-hdfs-secondarynamenode -y 在cdh1、cdh2、cdh3节点安装 hadoop-hdfs-datanode 1$ yum install hadoop hadoop-hdfs-datanode -y NameNode HA 的配置过程请参考CDH中配置HDFS HA，建议暂时不用配置。 2.1 修改hadoop配置文件修改/etc/hadoop/conf/core-site.xml： 123456789&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://cdh1:8020&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/var/hadoop&lt;/value&gt;&lt;/property&gt; 更多的配置信息说明，请参考 Apache Cluster Setup 修改/etc/hadoop/conf/hdfs-site.xml，访问web页面50070端口（默认为0.0.0.0IP，其他机器识别不了）： 123456789101112&lt;configuration&gt; &lt;!-- CLOUDERA-BUILD: CDH-64745. --&gt; &lt;property&gt; &lt;name&gt;cloudera.erasure_coding.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.http.address&lt;/name&gt; &lt;value&gt;cdh1:50070&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 注意：需要将配置文件同步到各个节点。 2.2 指定本地文件目录在hadoop中默认的文件路径以及权限要求如下： 12345目录 所有者 权限 默认路径hadoop.tmp.dir hdfs:hdfs drwx------ /tmp/hadoop-$&#123;user&#125;dfs.namenode.name.dir hdfs:hdfs drwx------ file://$&#123;hadoop.tmp.dir&#125;/dfs/namedfs.datanode.data.dir hdfs:hdfs drwx------ file://$&#123;hadoop.tmp.dir&#125;/dfs/datadfs.namenode.checkpoint.dir hdfs:hdfs drwx------ file://$&#123;hadoop.tmp.dir&#125;/dfs/namesecondary 我在hdfs-site.xm l中配置了hadoop.tmp.dir的路径为/var/hadoop，其他路径使用默认配置，所以，在NameNode上手动创建 dfs.namenode.name.dir 的本地目录： 12$ mkdir -p /var/hadoop/dfs/name$ chown -R hdfs:hdfs /var/hadoop/dfs/name 在DataNode上手动创建 dfs.datanode.data.dir目录： 12$ mkdir -p /var/hadoop/dfs/data$ chown -R hdfs:hdfs /var/hadoop/dfs/data hadoop的进程会自动设置 dfs.data.dir 或 dfs.datanode.data.dir，但是 dfs.name.dir 或 dfs.namenode.name.dir 的权限默认为755，需要手动设置为700： 123$ chmod 700 /var/hadoop/dfs/name# 或者$ chmod go-rx /var/hadoop/dfs/name 注意：DataNode的本地目录可以设置多个，你可以设置 dfs.datanode.failed.volumes.tolerated 参数的值，表示能够容忍不超过该个数的目录失败。 2.3 配置 SecondaryNameNodecdh2配置 SecondaryNameNode，在 /etc/hadoop/conf/hdfs-site.xml 中加入如下配置，将cdh2设置为 SecondaryNameNode： 1234&lt;property&gt; &lt;name&gt;dfs.secondary.http.address&lt;/name&gt; &lt;value&gt;cdh2:50090&lt;/value&gt;&lt;/property&gt; 在SecondaryNameNode中默认的文件路径以及权限要求如下： 12目录 所有者 权限 默认路径dfs.namenode.checkpoint.dir hdfs:hdfs drwx------ file://$&#123;hadoop.tmp.dir&#125;/dfs/namesecondary 设置多个secondarynamenode，请参考multi-host-secondarynamenode-configuration. 2.4 开启回收站功能回收站功能默认是关闭的，建议打开。在 /etc/hadoop/conf/core-site.xml 中添加如下两个参数： fs.trash.interval,该参数值为时间间隔，单位为分钟，默认为0，表示回收站功能关闭。该值表示回收站中文件保存多长时间，如果服务端配置了该参数，则忽略客户端的配置；如果服务端关闭了该参数，则检查客户端是否有配置该参数； fs.trash.checkpoint.interval，该参数值为时间间隔，单位为分钟，默认为0。该值表示检查回收站时间间隔，该值要小于fs.trash.interval，该值在服务端配置。如果该值设置为0，则使用 fs.trash.interval 的值。 2.5 (可选)配置DataNode存储的负载均衡在 /etc/hadoop/conf/hdfs-site.xml 中配置以下三个参数： dfs.datanode.fsdataset. volume.choosing.policy dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction 详细说明，请参考 Optionally configure DataNode storage balancing。 2.6 开启WebHDFS在NameNode节点上安装： 1$ yum install hadoop-httpfs -y 然后修改 /etc/hadoop/conf/core-site.xml配置代理用户： 123456789&lt;property&gt; &lt;name&gt;hadoop.proxyuser.httpfs.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.httpfs.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; 2.7 配置LZO下载repo文件到 /etc/yum.repos.d/: 如果你安装的是 CDH4，请下载Red Hat/CentOS 6 如果你安装的是 CDH5，请下载Red Hat/CentOS 6 然后，安装lzo: 1$ yum install hadoop-lzo* impala-lzo -y 最后，在 /etc/hadoop/conf/core-site.xml 中添加如下配置： 12345678910&lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;&lt;/property&gt; 更多关于LZO信息，请参考：Using LZO Compression 2.8 (可选)配置Snappycdh 的 rpm 源中默认已经包含了 snappy ，直接在每个节点安装Snappy： 1$ yum install snappy snappy-devel -y 然后，在 core-site.xml 中修改io.compression.codecs的值，添加 org.apache.hadoop.io.compress.SnappyCodec 。 使 snappy 对 hadoop 可用： 1$ ln -sf /usr/lib64/libsnappy.so /usr/lib/hadoop/lib/native/ 2.9 启动HDFS将cdh1上的配置文件同步到每一个节点： 12$ scp -r /etc/hadoop/conf root@cdh2:/etc/hadoop/$ scp -r /etc/hadoop/conf root@cdh3:/etc/hadoop/ 在cdh1节点格式化NameNode： 1$ sudo -u hdfs hdfs namenode -format 在cdh1启动namenode，其他节点启动datanode： 12$ /etc/init.d/hadoop-hdfs-namenode start$ /etc/init.d/hadoop-hdfs-datanode start 在cdh1节点上查看java进程： 1234$ jps12721 DataNode12882 Jps12587 NameNode 在 hdfs 运行之后，创建 /tmp 临时目录，并设置权限为 1777： 12$ sudo -u hdfs hadoop fs -mkdir /tmp$ sudo -u hdfs hadoop fs -chmod -R 1777 /tmp 如果安装了HttpFS，则启动 HttpFS 服务： 1$ service hadoop-httpfs start 2.10 测试Hadoop各组件使用的端口可查阅 Ports Used by CDH Components，可以看到namenode的http访问页面端口为 9870 ，datanode的http访问页面端口为 9864 。 所以，通过 http://cdh1:9870/ （需要先在本机配置hosts映射才能访问）可以访问 NameNode 页面。 也可以访问datanode节点，如：http://cdh1:9864/。 如果安装了webhdfs，可以使用 curl 运行下面命令，可以测试 webhdfs 并查看执行结果： 12$ curl "http://cdh1:14000/webhdfs/v1?op=gethomedirectory&amp;user.name=hdfs"&#123;"Path":"\/var\/lib\/hadoop-httpfs"&#125; 更多的 API，请参考 WebHDFS REST API 3. 安装和配置YARN根据文章开头的节点规划，cdh1 为resourcemanager节点，cdh2 和 cdh3 为nodemanager节点，为了简单，historyserver 也装在 cdh1 节点上。 3.1 安装服务在 cdh1 节点安装: 1234$ yum install hadoop-yarn-resourcemanager -y#安装 historyserver$ yum install hadoop-mapreduce-historyserver hadoop-yarn-proxyserver -y 在 cdh2、cdh3 节点安装: 1$ yum install hadoop-yarn-nodemanager -y 3.2 修改配置参数1、要想使用YARN，需要在 /etc/hadoop/conf/mapred-site.xml 中做如下配置: 1234&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 2、修改 /etc/hadoop/conf/yarn-site.xml文件内容： 12345678910111213141516171819202122232425262728&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;description&gt;Classpath for typical applications.&lt;/description&gt; &lt;name&gt;yarn.application.classpath&lt;/name&gt; &lt;value&gt; $HADOOP_CONF_DIR, $HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*, $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*, $HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*, $HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/* &lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 注意： yarn.nodemanager.aux-services 的值在 cdh4 中应该为 mapreduce.shuffle，并配置参数yarn.nodemanager.aux-services.mapreduce.shuffle.class值为 org.apache.hadoop.mapred.ShuffleHandler ，在cdh5之后中为mapreduce_shuffle，这时候请配置yarn.nodemanager.aux-services.mapreduce_shuffle.class参数 这里配置了 yarn.application.classpath ，需要设置一些喜欢环境变量： 123456789101112export HADOOP_HOME=/usr/lib/hadoopexport HIVE_HOME=/usr/lib/hiveexport HBASE_HOME=/usr/lib/hbaseexport HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoopexport HADOOP_COMMON_HOME=$&#123;HADOOP_HOME&#125;export HADOOP_HDFS_HOME=/usr/lib/hadoop-hdfsexport HADOOP_MAPRED_HOME=/usr/lib/hadoop-mapreduceexport HADOOP_YARN_HOME=/usr/lib/hadoop-yarnexport HADOOP_LIBEXEC_DIR=$&#123;HADOOP_HOME&#125;/libexec 在hadoop中默认的文件路径以及权限要求如下： 1234目录 所有者 权限 默认路径yarn.nodemanager.local-dirs yarn:yarn drwxr-xr-x $&#123;hadoop.tmp.dir&#125;/nm-local-diryarn.nodemanager.log-dirs yarn:yarn drwxr-xr-x $&#123;yarn.log.dir&#125;/userlogsyarn.nodemanager.remote-app-log-dir hdfs:/var/log/hadoop-yarn/apps 在nodemanager节点cdh2、cdh3创建 yarn.nodemanager.local-dirs 和 yarn.nodemanager.log-dirs 参数对应的目录： 12$ mkdir -p /var/hadoop/&#123;nm-local-dir,userlogs&#125;$ chown -R yarn:yarn /var/hadoop/&#123;nm-local-dir,userlogs&#125; 在 hdfs 上创建 yarn.nodemanager.remote-app-log-dir 对应的目录： 123$ sudo -u hdfs hadoop fs -mkdir -p /var/log/hadoop-yarn/apps$ sudo -u hdfs hadoop fs -chown yarn:mapred /var/log/hadoop-yarn/apps$ sudo -u hdfs hadoop fs -chmod 1777 /var/log/hadoop-yarn/apps 3、在 /etc/hadoop/conf/mapred-site.xml 中配置 MapReduce History Server： 12345678&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;cdh1:10020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;cdh1:19888&lt;/value&gt;&lt;/property&gt; 4、此外，确保 mapred、yarn 用户能够使用代理，在 /etc/hadoop/conf/core-site.xml 中添加如下参数： 12345678910111213141516&lt;property&gt; &lt;name&gt;hadoop.proxyuser.mapred.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.mapred.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.yarn.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.yarn.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; 5、配置 Staging 目录 /etc/hadoop/conf/mapred-site.xml中添加： 1234&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.staging-dir&lt;/name&gt; &lt;value&gt;/user&lt;/value&gt;&lt;/property&gt; 并在 hdfs 上创建相应的目录： 12$ sudo -u hdfs hadoop fs -mkdir -p /user$ sudo -u hdfs hadoop fs -chmod 777 /user 可选的，你可以在 /etc/hadoop/conf/mapred-site.xml 设置以下两个参数： mapreduce.jobhistory.intermediate-done-dir，该目录权限应该为1777，默认值为 ${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate mapreduce.jobhistory.done-dir，该目录权限应该为750，默认值为 ${yarn.app.mapreduce.am.staging-dir}/history/done 然后，在 hdfs 上创建目录并设置权限： 123$ sudo -u hdfs hadoop fs -mkdir -p /user/history$ sudo -u hdfs hadoop fs -chmod -R 1777 /user/history$ sudo -u hdfs hadoop fs -chown mapred:hadoop /user/history 6、配置resourcemanager服务的端口号 修改/etc/hadoop/conf/yarn-site.xml，添加： 1234567891011121314151617181920&lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;cdh1:8031&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;cdh1:8032&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;cdh1:8030&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;cdh1:8033&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;cdh1:8088&lt;/value&gt;&lt;/property&gt; 3.4 验证 HDFS 结构：1$ sudo -u hdfs hadoop fs -ls -R / 你应该看到如下结构： 123456789drwxrwxrwt - hdfs supergroup 0 2019-06-02 04:28 /tmpdrwxrwxrwx - hdfs supergroup 0 2019-06-02 04:17 /userdrwxrwxrwt - mapred hadoop 0 2019-06-02 04:21 /user/historydrwxrwx--x - mapred hadoop 0 2019-06-02 04:21 /user/history/donedrwxrwxrwt - mapred hadoop 0 2019-06-02 04:21 /user/history/done_intermediatedrwxr-xr-x - hdfs supergroup 0 2019-06-02 04:20 /vardrwxr-xr-x - hdfs supergroup 0 2019-06-02 04:20 /var/logdrwxr-xr-x - hdfs supergroup 0 2019-06-02 04:20 /var/log/hadoop-yarndrwxrwxrwt - yarn mapred 0 2019-06-02 04:20 /var/log/hadoop-yarn/apps 3.5 同步配置文件同步配置文件到整个集群: 12$ scp -r /etc/hadoop/conf root@cdh2:/etc/hadoop/$ scp -r /etc/hadoop/conf root@cdh3:/etc/hadoop/ 3.6 启动服务在 cdh1 节点启动 resourceManager 1$ /etc/init.d/hadoop-yarn-resourcemanager start 在 cdh1 节点启动 mapred-historyserver : 1$ /etc/init.d/hadoop-mapreduce-historyserver start 在 cdh2、cdh3 节点启动nodeManager : 1$ /etc/init.d/hadoop-yarn-nodemanager start 为每个 MapReduce 用户创建主目录，比如说 hive 用户或者当前用户： 12$ sudo -u hdfs hadoop fs -mkdir /user/$USER$ sudo -u hdfs hadoop fs -chown $USER /user/$USER 3.7 测试通过 http://cdh1:8088/ 可以访问 Yarn 的管理页面，通过 http://cdh1:19888/ 可以访问 JobHistory 的管理页面，查看在线的节点：http://cdh1:8088/cluster/nodes。 运行下面的测试程序，看是否报错： 1234567891011# Find how many jars name ending with examples you have inside location /usr/lib/$ find /usr/lib/ -name "*hadoop*examples*.jar"# To list all the class name inside jar$ find /usr/lib/ -name "hadoop*examples.jar" | xargs -0 -I '&#123;&#125;' sh -c 'jar tf &#123;&#125;'# To search for specific class name inside jar$ find /usr/lib/ -name "hadoop*examples.jar" | xargs -0 -I '&#123;&#125;' sh -c 'jar tf &#123;&#125;' | grep -i wordcount.class# 运行 randomwriter 例子$ sudo -u hdfs hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar randomwriter out 查看日志和管理页面。 4. 安装 ZookeeperZookeeper 至少需要3个节点，并且节点数要求是基数，这里在所有节点上都安装 Zookeeper。 4.1 安装在每个节点上安装zookeeper： 1$ yum install zookeeper-server -y 4.2 修改配置文件设置 zookeeper 配置 /etc/zookeeper/conf/zoo.cfg 123456789maxClientCnxns=50tickTime=2000initLimit=10syncLimit=5dataDir=/var/lib/zookeeperclientPort=2181server.1=cdh1:2888:3888server.2=cdh3:2888:3888server.3=cdh3:2888:3888 ##4.3 同步配置文件 将配置文件同步到其他节点： 12$ scp -r /etc/zookeeper/conf root@cdh2:/etc/zookeeper/$ scp -r /etc/zookeeper/conf root@cdh3:/etc/zookeeper/ 4.4 初始化并启动服务在每个节点上初始化并启动 zookeeper，注意 n 的值需要和 zoo.cfg 中的编号一致。 在 cdh1 节点运行： 12$ service zookeeper-server init --myid=1$ service zookeeper-server start 在 cdh2 节点运行： 12$ service zookeeper-server init --myid=2$ service zookeeper-server start 在 cdh3 节点运行： 12$ service zookeeper-server init --myid=3$ service zookeeper-server start 4.5 测试通过下面命令测试是否启动成功： 1$ zookeeper-client -server cdh1:2181 5. 安装 HBaseHBase 依赖 ntp 服务，故需要提前安装好 ntp。 5.1 安装前设置1）修改系统 ulimit 参数，在 /etc/security/limits.conf 中添加下面两行并使其生效： 12hdfs - nofile 32768hbase - nofile 32768 2）修改 dfs.datanode.max.xcievers，在 hdfs-site.xml 中修改该参数值，将该值调整到较大的值： 1234&lt;property&gt; &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt; &lt;value&gt;8192&lt;/value&gt;&lt;/property&gt; 5.2 安装在每个节点上安装 master 和 regionserver，如果需要你可以安装 hbase-rest、hbase-solr-indexer、hbase-thrift 1$ yum install hbase-master hbase-regionserver -y 5.3 修改配置文件修改 hbase-site.xml文件，关键几个参数及含义如下： hbase.distributed：是否为分布式模式 hbase.rootdir：HBase在hdfs上的目录路径 hbase.tmp.dir：本地临时目录 hbase.zookeeper.quorum：zookeeper集群地址，逗号分隔 hbase.hregion.max.filesize：hregion文件最大大小 hbase.hregion.memstore.flush.size：memstore文件最大大小 另外，在CDH5中建议关掉Checksums（见Upgrading HBase）以提高性能，最后的配置如下： 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://cdh1:8020/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/var/hadoop&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 在 hdfs 中创建 /hbase 目录 12$ sudo -u hdfs hadoop fs -mkdir /hbase$ sudo -u hdfs hadoop fs -chown hbase:hbase /hbase 设置 crontab 定时删除日志： 12$ crontab -e* 10 * * * cd /var/log/hbase/; rm -rf `ls /var/log/hbase/|grep -P &apos;hbase\-hbase\-.+\.log\.[0-9]&apos;\`&gt;&gt; /dev/null &amp; 5.4 同步配置文件将配置文件同步到其他节点： 12$ scp -r /etc/hbase/conf root@cdh2:/etc/hbase/$ scp -r /etc/hbase/conf root@cdh3:/etc/hbase/ 5.5 创建本地目录在 hbase-site.xml 配置文件中配置了 hbase.tmp.dir 值为 /data/hbase，现在需要在每个 hbase 节点创建该目录并设置权限： 12$ mkdir /var/hadoop$ chown -R hbase:hbase /var/hadoop 5.6 启动HBase1$ for x in `ls /etc/init.d/|grep hbase` ; do service $x start ; done 5.7 测试通过 http://cdh1:60030/ 可以访问 RegionServer 页面，然后通过该页面可以知道哪个节点为 Master，然后再通过 60010 端口访问 Master 管理界面。 6. 安装hive在一个 NameNode 节点上安装 hive： 1$ yum install hive hive-metastore hive-server2 hive-jdbc hive-hbase -y 在其他 DataNode 上安装： 1$ yum install hive hive-server2 hive-jdbc hive-hbase -y 安装postgresql这里使用 postgresq l数据库来存储元数据，如果你想使用 mysql 数据库，请参考下文。手动安装、配置 postgresql 数据库，请参考 手动安装Cloudera Hive CDH yum 方式安装： 123$ yum install postgresql-server postgresql-jdbc -y$ ln -s /usr/share/java/postgresql-jdbc.jar /usr/lib/hive/lib/postgresql-jdbc.jar 配置开启启动，并初始化数据库： 12$ chkconfig postgresql on$ service postgresql initdb 修改配置文件postgresql.conf，修改完后内容如下： 123$ cat /var/lib/pgsql/data/postgresql.conf | grep -e listen -e standard_conforming_strings listen_addresses = '*' standard_conforming_strings = off 修改 /var/lib/pgsql/data/pg_hba.conf，添加以下一行内容： 1host all all 0.0.0.0/0 trust 创建数据库和用户，设置密码为hive： 123456su -c "cd ; /usr/bin/pg_ctl start -w -m fast -D /var/lib/pgsql/data" postgressu -c "cd ; /usr/bin/psql --command \"create user hive with password 'hive'; \" " postgressu -c "cd ; /usr/bin/psql --command \"drop database hive;\" " postgressu -c "cd ; /usr/bin/psql --command \"CREATE DATABASE sentry owner=hive;\" " postgressu -c "cd ; /usr/bin/psql --command \"GRANT ALL privileges ON DATABASE hive TO hive;\" " postgressu -c "cd ; /usr/bin/pg_ctl restart -w -m fast -D /var/lib/pgsql/data" postgres 这时候的hive-site.xml文件内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:postgresql://localhost/hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;org.postgresql.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;datanucleus.autoCreateSchema&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;cdh1:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.auto.convert.join&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.warehouse.subdir.inherit.perms&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://cdh1:9083&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.client.socket.timeout&lt;/name&gt; &lt;value&gt;36000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.support.concurrency&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt; &lt;value&gt;cdh1,cdh2,cdh3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.min.worker.threads&lt;/name&gt; &lt;value&gt;5&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.max.worker.threads&lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 默认情况下，hive-server和 hive-server2 的 thrift 端口都为10000，如果要修改 hive-server2 thrift 端口，请修改 hive.server2.thrift.port 参数的值。 如果要设置运行 hive 的用户为连接的用户而不是启动用户，则添加： 1234&lt;property&gt; &lt;name&gt;hive.server2.enable.impersonation&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 并在 core-site.xml 中添加： 12345678&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hive.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hive.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; 安装mysqlyum方式安装mysql以及jdbc驱动： 1234$ yum install mysql mysql-devel mysql-server mysql-libs -y$ yum install mysql-connector-java$ ln -s /usr/share/java/mysql-connector-java.jar /usr/lib/hive/lib/mysql-connector-java.jar 创建数据库和用户，并设置密码为hive： 12345678$ mysql -e " CREATE DATABASE hive; USE hive; CREATE USER 'hive'@'localhost' IDENTIFIED BY 'hive'; GRANT ALL PRIVILEGES ON metastore.* TO 'hive'@'localhost'; GRANT ALL PRIVILEGES ON metastore.* TO 'hive'@'cdh1'; FLUSH PRIVILEGES;" 如果是第一次安装，则初始化 hive 的元数据库： 1$ /usr/lib/hive/bin/schematool --dbType mysql --initSchema 如果是更新，则执行： 1$ /usr/lib/hive/bin/schematool --dbType mysql --upgradeSchema 配置开启启动并启动数据库： 12$ chkconfig mysqld on$ service mysqld start 修改 hive-site.xml 文件中以下内容： 1234567891011121314151617&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://cdh1:3306/hive?useUnicode=true&amp;amp;characterEncoding=UTF-8&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; 配置hive修改/etc/hadoop/conf/hadoop-env.sh，添加环境变量 HADOOP_MAPRED_HOME，如果不添加，则当你使用 yarn 运行 mapreduce 时候会出现 UNKOWN RPC TYPE 的异常 1export HADOOP_MAPRED_HOME=/usr/lib/hadoop-mapreduce 在 hdfs 中创建 hive 数据仓库目录: hive 的数据仓库在 hdfs 中默认为 /user/hive/warehouse,建议修改其访问权限为 1777，以便其他所有用户都可以创建、访问表，但不能删除不属于他的表。 每一个查询 hive 的用户都必须有一个 hdfs 的 home 目录( /user 目录下，如 root 用户的为 /user/root) hive 所在节点的 /tmp 必须是 world-writable 权限的。 创建目录并设置权限： 123456$ sudo -u hdfs hadoop fs -mkdir /user/hive$ sudo -u hdfs hadoop fs -chown hive /user/hive$ sudo -u hdfs hadoop fs -mkdir /user/hive/warehouse$ sudo -u hdfs hadoop fs -chmod 1777 /user/hive/warehouse$ sudo -u hdfs hadoop fs -chown hive /user/hive/warehouse 启动hive-server和metastore: 123$ service hive-metastore start$ service hive-server start$ service hive-server2 start 测试123$ hive -e 'create table t(id int);'$ hive -e 'select * from t limit 2;'$ hive -e 'select id from t;' 访问beeline: 12$ beelinebeeline&gt; !connect jdbc:hive2://localhost:10000 hive hive org.apache.hive.jdbc.HiveDriver 与hbase集成先安装 hive-hbase: 1$ yum install hive-hbase -y 如果你是使用的 cdh4，则需要在 hive shell 里执行以下命令添加 jar： 12345$ ADD JAR /usr/lib/hive/lib/zookeeper.jar;$ ADD JAR /usr/lib/hive/lib/hbase.jar;$ ADD JAR /usr/lib/hive/lib/hive-hbase-handler-&lt;hive_version&gt;.jar# guava 包的版本以实际版本为准。$ ADD JAR /usr/lib/hive/lib/guava-11.0.2.jar; 如果你是使用的 cdh5，则需要在 hive shell 里执行以下命令添加 jar： 123456789ADD JAR /usr/lib/hive/lib/zookeeper.jar;ADD JAR /usr/lib/hive/lib/hive-hbase-handler.jar;ADD JAR /usr/lib/hbase/lib/guava-12.0.1.jar;ADD JAR /usr/lib/hbase/hbase-client.jar;ADD JAR /usr/lib/hbase/hbase-common.jar;ADD JAR /usr/lib/hbase/hbase-hadoop-compat.jar;ADD JAR /usr/lib/hbase/hbase-hadoop2-compat.jar;ADD JAR /usr/lib/hbase/hbase-protocol.jar;ADD JAR /usr/lib/hbase/hbase-server.jar; 以上你也可以在 hive-site.xml 中通过 hive.aux.jars.path 参数来配置，或者你也可以在 hive-env.sh 中通过 export HIVE_AUX_JARS_PATH= 来设置。 7. 遇到的问题安装hadoop过程中需要注意以下几点： 每个节点配置hosts 每个节点配置时钟同步 如果没有特殊要求，关闭防火墙 hadoop需要在/tmp目录下存放一些日志和临时文件，要求/tmp目录权限必须为1777 使用intel的hadoop发行版IDH过程遇到问题：1、 IDH集群中需要配置管理节点到集群各节点的无密码登录，公钥文件存放路径为/etc/intelcloud目录下，文件名称为idh-id_rsa。 如果在管理界面发现不能启动/停止hadoop组件的进程，请检查ssh无密码登录是否有问题。 ssh -i /etc/intelcloud/idh-id_rsa nodeX如果存在问题，请重新配置无密码登录： scp -i /etc/intelcloud/idh-id_rsa nodeX2、 IDH使用puppt和shell脚本来管理hadoop集群，shell脚本中有一处调用puppt的地方存在问题，详细说明待整理！！ 使用CDH4.3.0的hadoop（通过rpm安装）过程中发现如下问题： 说明：以下问题不局限于CDH的hadoop版本。 1、 在hive运行过程中会打印如下日志 Starting Job = job_1374551537478_0001, Tracking URL = http://june-fedora:8088/proxy/application_1374551537478_0001/ Kill Command = /usr/lib/hadoop/bin/hadoop job -kill job_1374551537478_0001通过上面的kill command可以killjob，但是运行过程中发现提示错误，错误原因：HADOOP_LIBEXEC_DIR未做设置 解决方法：在hadoop-env.sh中添加如下代码 export HADOOP_LIBEXEC_DIR=$HADOOP_COMMON_HOME/libexec2、 查看java进程中发现，JVM参数中-Xmx重复出现 解决办法：/etc/hadoop/conf/hadoop-env.sh去掉第二行。 export HADOOP_OPTS=&quot;-Djava.net.preferIPv4Stack=true $HADOOP_OPTS&quot;3、 hive中mapreduce运行为本地模式，而不是远程模式 解决办法：/etc/hadoop/conf/hadoop-env.sh设置HADOOP_MAPRED_HOME变量 export HADOOP_MAPRED_HOME=/usr/lib/hadoop-mapreduce4、 如何设置hive的jvm启动参数 hive脚本运行顺序： hive--&gt;hive-config.sh--&gt;hive-env.sh--&gt;hadoop-config.sh--&gt;hadoop-env.sh故如果hadoop-env.sh中设置了HADOOP_HEAPSIZE，则hive-env.sh中设置的无效 5、如何设置JOB_HISTORYSERVER的jvm参数 在/etc/hadoop/conf/hadoop-env.sh添加如下代码： export HADOOP_JOB_HISTORYSERVER_HEAPSIZE=2568. 参考文章 [1] CDH5-Installation-Guide [2] hadoop cdh 安装笔记]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hbase</tag>
        <tag>hive</tag>
        <tag>hdfs</tag>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装Impala过程]]></title>
    <url>%2F2013%2F03%2F29%2Finstall-impala%2F</url>
    <content type="text"><![CDATA[与Hive类似，Impala也可以直接与HDFS和HBase库直接交互。只不过Hive和其它建立在MapReduce上的框架适合需要长时间运行的批处理任务。例如：那些批量提取，转化，加载（ETL）类型的Job，而Impala主要用于实时查询。 Hadoop集群各节点的环境设置及安装过程见 使用yum安装CDH Hadoop集群，参考这篇文章。 1. 环境 CentOS 6.4 x86_64 CDH 5.0.1 jdk1.6.0_31 集群规划为3个节点，每个节点的ip、主机名和部署的组件分配如下： 123192.168.56.121 cdh1 NameNode、Hive、ResourceManager、HBase、impala192.168.56.122 cdh2 DataNode、SSNameNode、NodeManager、HBase、impala192.168.56.123 cdh3 DataNode、HBase、NodeManager、impala 2. 安装目前，CDH 5.0.1中 impala 版本为1.4.0，下载repo文件到 /etc/yum.repos.d/: 如果你安装的是 CDH4，请下载 Red Hat/CentOS 6 如果你安装的是 CDH5，请下载 Red Hat/CentOS 6 然后，可以执行下面的命令安装所有的 impala 组件。 1$ sudo yum install impala impala-server impala-state-store impala-catalog impala-shell -y 但是，通常只是在需要的节点上安装对应的服务： 在 hive metastore 所在节点安装impala-state-store和impala-catalog 在 DataNode 所在节点安装 impala-server 和 impala-shell 3. 配置3.1 修改配置文件查看安装路径： 123456789$ find / -name impala /var/run/impala /var/lib/alternatives/impala /var/log/impala /usr/lib/impala /etc/alternatives/impala /etc/default/impala /etc/impala /etc/default/impala impalad的配置文件路径由环境变量IMPALA_CONF_DIR指定，默认为/usr/lib/impala/conf，impala 的默认配置在/etc/default/impala，修改该文件中的 IMPALA_CATALOG_SERVICE_HOST 和 IMPALA_STATE_STORE_HOST 12345678910111213141516171819202122232425262728IMPALA_CATALOG_SERVICE_HOST=cdh1IMPALA_STATE_STORE_HOST=cdh1IMPALA_STATE_STORE_PORT=24000IMPALA_BACKEND_PORT=22000IMPALA_LOG_DIR=/var/log/impalaIMPALA_CATALOG_ARGS=" -log_dir=$&#123;IMPALA_LOG_DIR&#125; "IMPALA_STATE_STORE_ARGS=" -log_dir=$&#123;IMPALA_LOG_DIR&#125; -state_store_port=$&#123;IMPALA_STATE_STORE_PORT&#125;"IMPALA_SERVER_ARGS=" \ -log_dir=$&#123;IMPALA_LOG_DIR&#125; \ -catalog_service_host=$&#123;IMPALA_CATALOG_SERVICE_HOST&#125; \ -state_store_port=$&#123;IMPALA_STATE_STORE_PORT&#125; \ -use_statestore \ -state_store_host=$&#123;IMPALA_STATE_STORE_HOST&#125; \ -be_port=$&#123;IMPALA_BACKEND_PORT&#125;"ENABLE_CORE_DUMPS=false# LIBHDFS_OPTS=-Djava.library.path=/usr/lib/impala/lib# MYSQL_CONNECTOR_JAR=/usr/share/java/mysql-connector-java.jar# IMPALA_BIN=/usr/lib/impala/sbin# IMPALA_HOME=/usr/lib/impala# HIVE_HOME=/usr/lib/hive# HBASE_HOME=/usr/lib/hbase# IMPALA_CONF_DIR=/etc/impala/conf# HADOOP_CONF_DIR=/etc/impala/conf# HIVE_CONF_DIR=/etc/impala/conf# HBASE_CONF_DIR=/etc/impala/conf 设置 impala 可以使用的最大内存：在上面的 IMPALA_SERVER_ARGS 参数值后面添加 -mem_limit=70% 即可。 如果需要设置 impala 中每一个队列的最大请求数，需要在上面的 IMPALA_SERVER_ARGS 参数值后面添加 -default_pool_max_requests=-1 ，该参数设置每一个队列的最大请求数，如果为-1，则表示不做限制。 在节点cdh1上拷贝hive-site.xml、core-site.xml、hdfs-site.xml至/usr/lib/impala/conf目录并作下面修改在hdfs-site.xml文件中添加如下内容： 1234567891011121314&lt;property&gt; &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.domain.socket.path&lt;/name&gt; &lt;value&gt;/var/run/hadoop-hdfs/dn._PORT&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.hdfs-blocks-metadata.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 同步以上文件到其他节点。 3.2 创建socket path在每个节点上创建/var/run/hadoop-hdfs: 1$ mkdir -p /var/run/hadoop-hdfs 拷贝postgres jdbc jar： 1$ ln -s /usr/share/java/postgresql-jdbc.jar /usr/lib/impala/lib/ 3.3 用户要求impala 安装过程中会创建名为 impala 的用户和组，不要删除该用户和组。 如果想要 impala 和 YARN 和 Llama 合作，需要把 impala 用户加入 hdfs 组。 impala 在执行 DROP TABLE 操作时，需要把文件移到到 hdfs 的回收站，所以你需要创建一个 hdfs 的目录 /user/impala，并将其设置为impala 用户可写。同样的，impala 需要读取 hive 数据仓库下的数据，故需要把 impala 用户加入 hive 组。 impala 不能以 root 用户运行，因为 root 用户不允许直接读。 创建 impala 用户家目录并设置权限： 12sudo -u hdfs hadoop fs -mkdir /user/impalasudo -u hdfs hadoop fs -chown impala /user/impala 查看 impala 用户所属的组： 12$ groups impalaimpala : impala hadoop hdfs hive 由上可知，impala 用户是属于 imapal、hadoop、hdfs、hive 用户组的 4. 启动服务在 cdh1节点启动： 12$ service impala-state-store start$ service impala-catalog start 如果impalad正常启动，可以在/tmp/ impalad.INFO查看。如果出现异常，可以查看/tmp/impalad.ERROR定位错误信息。 5. 使用shell使用impala-shell启动Impala Shell，连接 cdh1，并刷新元数据 12345&gt;impala-shell[Not connected] &gt;connect cdh1[cdh1:21000] &gt;invalidate metadata[cdh2:21000] &gt;connect cdh2[cdh2:21000] &gt;select * from t 当在 Hive 中创建表之后，第一次启动 impala-shell 时，请先执行 INVALIDATE METADATA 语句以便 Impala 识别出新创建的表(在 Impala 1.2 及以上版本，你只需要在一个节点上运行 INVALIDATE METADATA ，而不是在所有的 Impala 节点上运行)。 你也可以添加一些其他参数，查看有哪些参数： 12345678910111213141516171819202122232425262728293031323334# impala-shell -hUsage: impala_shell.py [options]Options: -h, --help show this help message and exit -i IMPALAD, --impalad=IMPALAD &lt;host:port&gt; of impalad to connect to -q QUERY, --query=QUERY Execute a query without the shell -f QUERY_FILE, --query_file=QUERY_FILE Execute the queries in the query file, delimited by ; -k, --kerberos Connect to a kerberized impalad -o OUTPUT_FILE, --output_file=OUTPUT_FILE If set, query results will written to the given file. Results from multiple semicolon-terminated queries will be appended to the same file -B, --delimited Output rows in delimited mode --print_header Print column names in delimited mode, true by default when pretty-printed. --output_delimiter=OUTPUT_DELIMITER Field delimiter to use for output in delimited mode -s KERBEROS_SERVICE_NAME, --kerberos_service_name=KERBEROS_SERVICE_NAME Service name of a kerberized impalad, default is 'impala' -V, --verbose Enable verbose output -p, --show_profiles Always display query profiles after execution --quiet Disable verbose output -v, --version Print version information -c, --ignore_query_failure Continue on query failure -r, --refresh_after_connect Refresh Impala catalog after connecting -d DEFAULT_DB, --database=DEFAULT_DB Issue a use database command on startup. 例如，你可以在连接时候字段刷新： 123456789101112131415$ impala-shell -rStarting Impala Shell in unsecure modeConnected to 192.168.56.121:21000Server version: impalad version 1.1.1 RELEASE (build 83d5868f005966883a918a819a449f636a5b3d5f)Invalidating MetadataWelcome to the Impala shell. Press TAB twice to see a list of available commands.Copyright (c) 2012 Cloudera, Inc. All rights reserved.(Shell build version: Impala Shell v1.1.1 (83d5868) built on Fri Aug 23 17:28:05 PDT 2013)Query: invalidate metadataQuery finished, fetching results ...Returned 0 row(s) in 5.13s[192.168.56.121:21000] &gt; 使用 impala 导出数据： 1$ impala-shell -i '192.168.56.121:21000' -r -q "select * from test" -B --output_delimiter="\t" -o result.txt 6. 参考文章 Impala安装文档完整版 Impala入门笔记 Installing and Using Cloudera Impala]]></content>
      <categories>
        <category>impala</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>cdh</tag>
        <tag>impala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手动安装Hadoop集群]]></title>
    <url>%2F2013%2F03%2F24%2Fmanual-install-Cloudera-Hadoop-CDH%2F</url>
    <content type="text"><![CDATA[安装版本centos版本为6。 hadoop各个组件和jdk版本如下： 1234hadoop-2.0.0-cdh4.6.0hbase-0.94.15-cdh4.6.0hive-0.10.0-cdh4.6.0jdk1.6.0_38 hadoop各组件可以在这里下载。 安装前说明 确定安装目录为/opt 检查hosts文件是否设置集群各节点的hostname和ip映射 关闭每个节点的防火墙 设置每个节点时钟同步 规划集群规划为3个节点，每个节点的ip、主机名和部署的组件分配如下： 123192.168.0.1 desktop1 NameNode、Hive、ResourceManager、impala192.168.0.2 desktop2 SSNameNode192.168.0.3 desktop3 DataNode、HBase、NodeManager、impala 部署过程系统和网络配置 修改每个节点的主机名称 例如在desktop1节点上做如下修改： 123[root@desktop1 ~]# cat /etc/sysconfig/networkNETWORKING=yesHOSTNAME=desktop1 在每个节点上修改/etc/hosts增加以下内容: 1234[root@desktop1 ~]# cat /etc/hosts127.0.0.1 localhost192.168.0.1 desktop1192.168.0.2 desktop2 修改一台机器之后，可以使用scp同步到其他机器。 配置ssh无密码登陆 以下是设置desktop1上可以无密码登陆到其他机器上。 123[root@desktop1 ~]# ssh-keygen[root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop2[root@desktop1 ~]# ssh-copy-id -i .ssh/id_rsa.pub desktop3 每台机器上关闭防火墙： 123[root@desktop1 ~]# service iptables stop[root@desktop1 ~]# ssh desktop2 &apos;service iptables stop&apos;[root@desktop1 ~]# ssh desktop3 &apos;service iptables stop&apos; 安装Hadoop配置Hadoop将jdk1.6.0_38.zip上传到/opt，并解压缩。 将hadoop-2.0.0-cdh4.2.0.zip上传到/opt，并解压缩。 在NameNode上需要修改以下文件： core-site.xml fs.defaultFS指定NameNode文件系统，开启回收站功能。 hdfs-site.xml dfs.namenode.name.dir指定NameNode存储meta和editlog的目录， dfs.datanode.data.dir指定DataNode存储blocks的目录， dfs.namenode.secondary.http-address指定Secondary NameNode地址。 开启WebHDFS。 slaves 添加DataNode节点主机 注意：在desktop1节点上修改如下几个文件的内容： core-site.xml 在该文件中修改fs.defaultFS指向desktop1节点，即配置desktop1为NameNode节点。 修改后的core-site.xml(/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/core-site.xml)目录如下： 123456789101112131415161718&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt;&lt;!--fs.default.name for MRV1 ,fs.defaultFS for MRV2(yarn) --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;!--这个地方的值要和hdfs-site.xml文件中的dfs.federation.nameservices一致--&gt; &lt;value&gt;hdfs://desktop1&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;fs.trash.interval&lt;/name&gt;&lt;value&gt;10080&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt;&lt;value&gt;10080&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 该文件主要设置数据副本保存份数，以及namenode、datanode数据保存路径(/opt/data/hadoop-${user.name})以及http-address。 修改后的hdfs-site.xml(/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/hdfs-site.xml)文件内容如下： 123456789101112131415161718192021222324&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/data/hadoop-$&#123;user.name&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.http-address&lt;/name&gt;&lt;value&gt;desktop1:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;&lt;value&gt;desktop2:50090&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; masters 设置namenode和secondary namenode节点。 修改后的masters(/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/masters)文件内容如下： 12desktop1desktop2 第一行为namenode，第二行为secondary namenode。 slaves 设置哪些机器上安装datanode节点。修改后的slaves(/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/slaves)文件内容如下： 12345desktop3desktop4desktop5desktop6desktop7 接下来将上面几个文件同步到其他各个节点： 12[root@desktop1 ~]# scp /opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/ desktop2:/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/[root@desktop1 ~]# scp /opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/ desktop3:/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/ 配置MapReduce接下来还是在desktop1节点上修改以下几个文件： mapred-site.xml 配置使用yarn计算框架，以及jobhistory的地址。 修改后的mapred-site.xml(/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/mapred-site.xml)文件内容如下： 12345678910111213141516&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;desktop1:10020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;desktop1:19888&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; yarn-site.xml 主要配置resourcemanager地址以及yarn.application.classpath（这个路径很重要，要不然集成hive时候会提示找不到class） 修改后的yarn-site.xml(/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/yarn-site.xml)文件内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;?xml version=&quot;1.0&quot;?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;desktop1:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;desktop1:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;desktop1:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;desktop1:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;desktop1:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;description&gt;Classpath for typical applications.&lt;/description&gt; &lt;name&gt;yarn.application.classpath&lt;/name&gt; &lt;value&gt;$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/share/hadoop/common/*, $HADOOP_COMMON_HOME/share/hadoop/common/lib/*, $HADOOP_HDFS_HOME/share/hadoop/hdfs/*,$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*, $YARN_HOME/share/hadoop/yarn/*,$YARN_HOME/share/hadoop/yarn/lib/*, $YARN_HOME/share/hadoop/mapreduce/*,$YARN_HOME/share/hadoop/mapreduce/lib/*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce.shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt; &lt;value&gt;/opt/data/yarn/local&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt; &lt;value&gt;/opt/data/yarn/logs&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;description&gt;Where to aggregate logs&lt;/description&gt; &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt; &lt;value&gt;/opt/data/yarn/logs&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.staging-dir&lt;/name&gt; &lt;value&gt;/user&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 同样地，将上面2个文件同步到其他各个节点： 12[root@desktop1 ~]# scp /opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/ desktop2:/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/[root@desktop1 ~]# scp /opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/ desktop3:/opt/hadoop-2.0.0-cdh4.6.0/etc/hadoop/ 修改环境变量修改/root/.bashrc环境变量，并将其同步到其他几台机器 1234567891011121314151617181920212223242526[root@desktop1 ~] # cat .bashrc # .bashrcalias rm=&apos;rm -i&apos;alias cp=&apos;cp -i&apos;alias mv=&apos;mv -i&apos;# Source global definitionsif [ -f /etc/bashrc ]; then . /etc/bashrcfi# User specific environment and startup programsexport LANG=zh_CN.utf8export JAVA_HOME=/opt/jdk1.6.0_38export JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=./:$JAVA_HOME/lib:$JRE_HOME/lib:$JRE_HOME/lib/tools.jarexport HADOOP_HOME=/opt/hadoop-2.0.0-cdh4.2.0export HIVE_HOME=/opt/hive-0.10.0-cdh4.2.0export HBASE_HOME=/opt/hbase-0.94.2-cdh4.2.0export HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;export HADOOP_COMMON_HOME=$&#123;HADOOP_HOME&#125;export HADOOP_HDFS_HOME=$&#123;HADOOP_HOME&#125;export YARN_HOME=$&#123;HADOOP_HOME&#125;export HADOOP_YARN_HOME=$&#123;HADOOP_HOME&#125;export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoopexport HDFS_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoopexport YARN_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoopexport PATH=$PATH:$HOME/bin:$JAVA_HOME/bin:$HADOOP_HOME/sbin:$HBASE_HOME/bin:$HIVE_HOME/bin 修改配置文件之后，使其生效。 1[root@desktop1 ~]# source .bashrc 将该文件同步到其他各个节点： 12[root@desktop1 ~]# scp /root/.bashrc desktop2:/root[root@desktop1 ~]# scp /root/.bashrc desktop3:/root 并且使各个节点的环境变量生效： 12[root@desktop1 ~]# ssh desktop2 &apos;source .bashrc&apos;[root@desktop1 ~]# ssh desktop3 &apos;source .bashrc&apos; 启动脚本第一次启动hadoop需要先格式化NameNode，该操作只做一次。当修改了配置文件时，需要重新格式化 在desktop1上格式化： 1[root@desktop1 hadoop]hadoop namenode -format 在desktop1上启动hdfs： 1[root@desktop1 hadoop]#start-dfs.sh 在desktop1上启动mapreduce： 1[root@desktop1 hadoop]#start-yarn.sh 在desktop1上启动historyserver： 1[root@desktop1 hadoop]#mr-jobhistory-daemon.sh start historyserver 查看MapReduce： 1http://desktop1:8088/cluster 查看节点： 12http://desktop2:8042/http://desktop2:8042/node 检查集群进程123456789101112[root@desktop1 ~]# jps5389 NameNode5980 Jps5710 ResourceManager7032 JobHistoryServer[root@desktop2 ~]# jps3187 Jps3124 SecondaryNameNode[root@desktop3 ~]# jps3187 Jps3124 DataNode5711 NodeManager 相关文章 手动安装Hadoop集群 手动安装HBase集群 手动安装Hive群]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>cdh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手动安装Cloudera HBase CDH]]></title>
    <url>%2F2013%2F03%2F24%2Fmanual-install-Cloudera-hbase-CDH%2F</url>
    <content type="text"><![CDATA[本文主要记录手动安装Cloudera HBase集群过程，环境设置及Hadoop安装过程见手动安装Cloudera Hadoop CDH,参考这篇文章，hadoop各个组件和jdk版本如下： 1234hadoop-2.0.0-cdh4.6.0hbase-0.94.15-cdh4.6.0hive-0.10.0-cdh4.6.0jdk1.6.0_38 hadoop各组件可以在这里下载。 集群规划为7个节点，每个节点的ip、主机名和部署的组件分配如下： 123192.168.0.1 desktop1 NameNode、Hive、ResourceManager、impala192.168.0.2 desktop2 SSNameNode192.168.0.3 desktop3 DataNode、HBase、NodeManager、impala 安装HBaseHBase安装在desktop3、desktop1、desktop2节点上。 上传hbase压缩包(hbase-0.94.15-cdh4.6.0.tar.gz)到desktop3上的/opt目录，先在desktop3上修改好配置文件，在同步到其他机器上。 hbase-site.xml内容修改如下： 123456789101112131415161718&lt;configuration&gt;&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://desktop1/hbase-$&#123;user.name&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/data/hbase-$&#123;user.name&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;desktop3,desktop4,desktop6,desktop7,desktop8&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; regionservers内容修改如下： 123desktop1desktop2desktop3 接下来将上面几个文件同步到其他各个节点： 12[root@desktop3 ~]# scp /opt/hbase-0.94.15-cdh4.6.0/conf/ desktop1:/opt/hbase-0.94.15-cdh4.6.0/conf/[root@desktop3 ~]# scp /opt/hbase-0.94.15-cdh4.6.0/conf/ desktop2:/opt/hbase-0.94.15-cdh4.6.0/conf/ 环境变量参考手动安装Cloudera Hadoop CDH中环境变量的设置。 启动脚本在desktop1、desktop2、desktop3节点上分别启动hbase： 1start-hbase.sh 相关文章 手动安装Hadoop集群 手动安装HBase集群 手动安装Hive群]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>cdh</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手动安装Cloudera Hive CDH]]></title>
    <url>%2F2013%2F03%2F24%2Fmanual-install-Cloudera-hive-CDH%2F</url>
    <content type="text"><![CDATA[本文主要记录手动安装Cloudera Hive集群过程，环境设置及Hadoop安装过程见手动安装Cloudera Hadoop CDH,参考这篇文章，hadoop各个组件和jdk版本如下： 1234hadoop-2.0.0-cdh4.6.0hbase-0.94.15-cdh4.6.0hive-0.10.0-cdh4.6.0jdk1.6.0_38 hadoop各组件可以在这里下载。 集群规划为7个节点，每个节点的ip、主机名和部署的组件分配如下： 123192.168.0.1 desktop1 NameNode、Hive、ResourceManager、impala192.168.0.2 desktop2 SSNameNode、DataNode、HBase、NodeManager、impala192.168.0.3 desktop3 DataNode、HBase、NodeManager、impala 安装hivehive安装在desktop1上，注意：hive默认是使用derby数据库保存元数据，这里替换为postgresql，下面会提到postgresql的安装说明，并且需要拷贝postgres的jdbc jar文件导hive的lib目录下。 上传hive-0.10.0-cdh4.6.0.tar到desktop1的/opt，并解压缩。 安装postgres创建数据库这里创建数据库metastore并创建hiveuser用户，其密码为redhat。 123456bash# sudo -u postgres psqlbash$ psqlpostgres=# CREATE USER hiveuser WITH PASSWORD &apos;redhat&apos;;postgres=# CREATE DATABASE metastore owner=hiveuser;postgres=# GRANT ALL privileges ON DATABASE metastore TO hiveuser;postgres=# \q; 初始化数据库12psql -U hiveuser -d metastore \i /opt/hive-0.10.0-cdh4.6.0/scripts/metastore/upgrade/postgres/hive-schema-0.10.0.postgres.sql 编辑postgresql配置文件(/opt/PostgreSQL/9.1/data/pg_hba.conf)，修改访问权限 1host all all 0.0.0.0/0 md5 修改postgresql.conf 1standard_conforming_strings = of 重起postgres拷贝postgres的jdbc驱动到/opt/hive-0.10.0-cdh4.6.0/lib目录。 1su -c &apos;/opt/PostgreSQL/9.1/bin/pg_ctl -D /opt/PostgreSQL/9.1/data restart&apos; postgres 修改配置文件hive-site.xml注意修改下面配置文件中postgres数据库的密码，注意配置hive.aux.jars.path，在hive集成hbase时候需要从该路径家在hbase的一些jar文件。 hive-site.xml文件内容如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;configuration&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:postgresql://127.0.0.1/metastore&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;org.postgresql.Driver&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hiveuser&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;redhat&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;desktop1:8031&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.aux.jars.path&lt;/name&gt; &lt;value&gt;file:///opt/hive-0.10.0-cdh4.6.0/lib/zookeeper-3.4.5-cdh4.6.0.jar, file:///opt/hive-0.10.0-cdh4.6.0/lib/hive-hbase-handler-0.10.0-cdh4.6.0.jar, file:///opt/hive-0.10.0-cdh4.6.0/lib/hbase-0.94.15-cdh4.6.0.jar, file:///opt/hive-0.10.0-cdh4.6.0/lib/guava-11.0.2.jar&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/opt/data/warehouse-$&#123;user.name&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/opt/data/hive-$&#123;user.name&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/opt/data/querylog-$&#123;user.name&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.support.concurrency&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt; &lt;value&gt;desktop1,desktop2,desktop3&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.hwi.listen.host&lt;/name&gt; &lt;value&gt;desktop1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.hwi.listen.port&lt;/name&gt; &lt;value&gt;9999&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.hwi.war.file&lt;/name&gt; &lt;value&gt;lib/hive-hwi-0.10.0-cdh4.6.0.war&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 环境变量参考手动安装Cloudera Hadoop CDH中环境变量的设置。 启动hive在启动完之后，执行一些sql语句可能会提示错误，如何解决错误可以参考Hive安装与配置。 hive与hbase集成在hive-site.xml中配置hive.aux.jars.path,在环境变量中配置hadoop、mapreduce的环境变量 异常说明异常1：1FAILED: Error in metadata: MetaException(message:org.apache.hadoop.hbase.ZooKeeperConnectionException: An error is preventing HBase from connecting to ZooKeeper 原因：hadoop配置文件没有zk 异常21FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.hive.metastore.api.MetaException javax.jdo.JDODataStoreException: Error executing JDOQL query &quot;SELECT &quot;THIS&quot;.&quot;TBL_NAME&quot; AS NUCORDER0 FROM &quot;TBLS&quot; &quot;THIS&quot; LEFT OUTER JOIN &quot;DBS&quot; &quot;THIS_DATABASE_NAME&quot; ON &quot;THIS&quot;.&quot;DB_ID&quot; = &quot;THIS_DATABASE_NAME&quot;.&quot;DB_ID&quot; WHERE &quot;THIS_DATABASE_NAME&quot;.&quot;NAME&quot; = ? AND (LOWER(&quot;THIS&quot;.&quot;TBL_NAME&quot;) LIKE ? ESCAPE &apos;\\&apos; ) ORDER BY NUCORDER0 &quot; : ERROR: invalid escape string 建议：Escape string must be empty or one character.. 参考：https://issues.apache.org/jira/browse/HIVE-3994 异常3，以下语句没反应1select count(*) from hive_userinfo 异常41zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(966)) - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (无法定位登录配置) 原因：hive中没有设置zk 异常51hbase 中提示：WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 原因：cloudera hadoop lib中没有hadoop的native jar 异常61Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/hadoop/mapreduce/v2/app/MRAppMaster 原因：classpath没有配置正确，检查环境变量以及yarn的classpath 参考文章 Hive安装与配置 Hive Installation 相关文章 手动安装Hadoop集群 手动安装HBase集群 手动安装Hive群]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2013%2F02%2F20%2Fsummary-of-the-work-in-2012%2F</url>
    <content type="text"><![CDATA[layout: posttitle: “2012年度总结”description: 2012年是在公司工作的第二年，在总结2012年的得与失的时候，有必要和《2011的度年终总结》相比较，在比较中审视自己在2012年是否有改进2011年存在的不足、是否有实现2011年定下的2012年工作计划。以下是2012年相对于2011年的一些变化。category: work tags: [work]2012年是在公司工作的第二年，在总结2012年的得与失的时候，有必要和《2011的度年终总结》相比较，在比较中审视自己在2012年是否有改进2011年存在的不足、是否有实现2011年定下的2012年工作计划。以下是2012年相对于2011年的一些变化。 2011年，在调研云计算产品过程中，深刻的意识到自身在linux方面存在的不足；2012年，熟悉了基本的linux命令，能够读懂并编写简单shell脚本； 2011年，工作环境是win7+fedora；2012年，一直使用fedora操作系统工作、编码； 2011年，较多的时间花在编写代码、完成开发任务上；2012年，更多的时间花在学习架构的设计、系统的运维、项目的管理上，视野不再局限于开发、精力不再局限于编码。 2011年，在工作中没有及时提交项目周报，没有及时的跟踪、检查分配下去的任务完成情况，对新人的指导不够；2012年，没有写过项目周报，做到了及是跟踪、检查分配下去的任务完成情况； 2011年，博客文章篇数较少，平时的总结与分享不够积极；2012年，很少有时间写技术方面的博客； 2011年，在与客户的交流中底气不足、表达能力不够；2012年，还是发现自己与客户交流中胆怯、没有底气； 2011年，希望能够将Pentaho的咨询服务工作更多交给其他人完成；2012年，发现大部分的工作还是落在自己身上一个人去完成，没有发挥其他人员的作用； 2011年，希望2012年能够深入理解Spring、Jboss、Pentaho、缓存、云计算、架构等技术；2012年，了解gemfire、infinispan、jboss cache、cassandra等分布式缓存的实现及原理，但每一个方面都没有时间去深入研究和学习； 2011年，公司在代码复查方面做的不够；2012年，这方面还是做的不够； 2011年，项目开发方面没有形成一套成型的开发框架；2012年，还是没有看到一个成熟、易用、简单的开发框架以及相配套开发文档； 2011年，项目于项目之间在一些同时使用的相关技术上面的沟通于交流做的不够；2012年，团队在项目上还是缺少沟通交流，尤其体现在XXXX项目网站开发上。 2011年，花了一些时间在Pentaho上，并希望2012年能够创建一个Pentaho社区、一个QQ分享群；2012年，Pentaho方面基本上没有投入； 2011年，编写文档时候，没有可参考的模版，导致文档编写不规范；2012年，每次写文档时都要去找文档模版； 2012年参与了XXXXX项目、cassandra项目，。。。。。。此处省略314.15926个字。 2012年，公司也存在一些不足：上级对下级、项目经理对团队人员了解不足，不知道其工作上、生活上的内心想法以及遇到何种困难；多数情况下，团队自我要求低，积极性不高，没有生机与活力；对新人能力审核不够，对新人培养不够重视，对新人的存在感不够关注；在各个项目的人员安排及使用上、任务分配和工作计划上不合理，导致经常被动加班、熬夜等等。 2013年工作计划： 通过CE考试，熟练掌握shell编程； 做好项目管理者的角色，培养新人，提高团队人员编码、处理问题的能力； 深入理解、学习cassandra源码、原理以及cassandra的运维； 学习hadoop的安装、部署、原理、开发及运维，掌握kettle和nosql的集成，希望积累几个hadoop项目经验； 学习分布式缓存理论知识，阅读源代码，完善缓存系统的监控及运维 在经历了2011年和2012年之后，2013年希望自己能够专注细节，深入理解，在技术、管理、交际方面有所成长；希望公司能够重视对团队的培养，能够规范各种规章制度，能够更上一层楼！]]></content>
  </entry>
  <entry>
    <title><![CDATA[使用Octopress将博客从wordpress迁移到GitHub]]></title>
    <url>%2F2012%2F06%2F03%2Fmigrate-blog-form-wordpress-to-github-with-octopress%2F</url>
    <content type="text"><![CDATA[Step1 - 在本机安装Octopress首先，必须先在本机安装配置Git和Ruby,Octopress需要Ruby版本至少为1.9.2。你可以使用RVM或rbenv安装ruby，安装方法见Octopress官方文档：http://octopress.org/docs/setup/ 我使用rvm安装： rvm install 1.9.2 &amp;&amp; rvm use 1.9.2安装完之后可以查看ruby版本： ruby –version结果为： ruby 1.9.2p320 (2012-04-20 revision 35421) [x86_64-linux] 然后需要从github下载Octopress： git clone git://github.com/imathis/octopress.git octopress 因为我fork了Octopress，并在配置文件上做了一些修改，故我从我的仓库地址下载Octopress，命令如下： git clone git@github.com:javachen/octopress.git运行上面的代码后，你会看到： Cloning into ‘octopress’… remote: Counting objects: 6579, done. remote: Compressing objects: 100% (2361/2361), done. remote: Total 6579 (delta 3773), reused 6193 (delta 3610) Receiving objects: 100% (6579/6579), 1.34 MiB | 35 KiB/s, done. Resolving deltas: 100% (3773/3773), done. 接下来进入octopress： cd octopress 接下来安装依赖： gem install bundler rbenv rehash # If you use rbenv, rehash to be able to run the bundle command bundle install 安装Octopress默认的主题： rake install 你也可以安装自定义的主题，blog为主题名称： rake install[‘blog’] 至此，Octopress所需的环境已经搭建成功。 Step2 - 连接GitHub Pages首先，你得有一个GitHub的帐号，并且已经创建了一个新的Repository。如果你准备用自己的域名的话，Repository的名称可以随便取，不过正常人在正常情况下，一般都是以域名取名的。如果你没有自己的域名，GitHub是提供二级域名使用的，但是你得把Repository取名为你的帐号.github.com，并且，部署的时候会占用你的master分支。 Tips：如果用自己的一级域名，记得把source/CNAME文件内的域名改成你的一级域名，还有在dns管理中把域名的A Record指向IP：207.97.227.245；如果用自己的二级域名，记得把source/CNAME文件内的域名改成你的二级域名，还有在dns管理中把域名的CNAME Record指向网址：charlie.github.com； echo ‘your-domain.com’ &gt;&gt; source/CNAME如果用GitHub提供的二级域名，记得把source/CNAME删掉。 完成上述准备工作后，运行： rake setup_github_pages它会提示你输入有读写权限的Repository Url，这个在GitHub上可以找到。Url形如：https://github.com/javachen/javachen.github.com.git，javachen.github.com是我的Repository的名称。 Step3 - 配置你的博客需要配置博客url、名称、作者、rss等信息。 url: http://javachen.github.com title: JavaChen on Java subtitle: Just some random thoughts about technology,Java and life. author: javachen simple_search: http://google.com/search description: date_format: &quot;%Y年%m月%d日&quot; subscribe_rss: /atom.xml subscribe_email: email: # 如果你使用的是一个子目录，如http://site.com/project，则设置为&apos;root: /project&apos; root: / # 文章标题格式 permalink: /:year/:month/:day/:title/ source: source destination: public plugins: plugins code_dir: downloads/code # 分类存放路径 category_dir: categories markdown: rdiscount pygments: false # default python pygments have been replaced by pygments.rbStep4 - 部署先把整个项目静态化，然后再部署到GitHub： rake generate rake deploy当你看到“Github Pages deploy complete”后，就表示你大功已成。Enjoy! Tips：Octopress提供的所有rake方法，可以运行rake -T查看。如果在执行上述命令中ruby报错，则需要一一修复错误，这一步是没有接触过ruby的人比较苦恼的。 Step5 - 从Wordpress迁移到Octopress备份备份评论内容Octopress由于是纯静态，所以没有办法存储用户评论了，我们可以使用DISQUS提供的“云评论”服务。首先安装DISQUS的WordPress插件，在插件设置中我们可以将现有的评论内容导入到DISQUS中。DISQUS处理导入数据的时间比较长，往往需要24小时甚至以上的时间。 备份文章内容在WordPress后台我们可以将整站数据备份成一个.xml文件下载下来。同时，我原先文章中的图片都是直接在Wordpress后台上传的，所以要把服务器上wp-content/uploads下的所有文件备份下来。 迁移迁移文章jekyll本身提供了一个从WordPress迁移文章的工具，不过对中文实在是不太友好。这里我使用了YORKXIN的修改版本。将上面备份的wordpress.xml放到Octopress根目录，把脚本放到新建的utils目录中，然后运行： ruby -r “./utils/wordpressdotcom.rb” -e “Jekyll::WordpressDotCom.process”于是转换好的文章都放进source目录了。 迁移URL迁移URL，便是要保证以前的文章链接能够自动重定向到新的链接上。这样既能保证搜索引擎的索引不受影响，也是一项对读者负责任的行为是吧。不过这是一项挺麻烦的事情。 幸好我当初建立WordPress的时候就留下了后路。原先网站的链接是这样的： http://XXXXXXXXX.com/[year]/[month]/[the-long-long-title].html http://XXXXXXXXX.com/page/xx/ http://XXXXXXXXX.com/category/[category-name]/这样的格式是比较容易迁移的。如果原先的文章URL是带有数字ID的话，只能说声抱歉了。到_config.yml里面设置一下新站点的文章链接格式，跟原先的格式保持一致： permalink: /:year/:month/:title/ category_dir: category pagination_dir: # 留空 迁移评论既然做好了301，那么迁移评论就显得非常简单了。登录DISQUS后台，进入站点管理后台的“Migrate Threads”栏目，那里有一个“Redirect Crawler”的功能，便是自动跟随301重定向，将评论指向新的网址。点一下那个按钮就大功告成。 迁移图片可以参考使用独立图床子域名 Step6 - 再次部署rake generate rake deploy参考文章 Octopress Setup： http://octopress.org/docs/setup/ Octopress Deploying：http://octopress.org/docs/deploying/ Blog = GitHub + Octopress：http://mrzhang.me/blog/blog-equals-github-plus-octopress.html 从Wordpress迁移到Octopress：http://blog.dayanjia.com/2012/04/migration-to-octopress-from-wordpress/ 使用独立图床子域名：http://log4d.com/2012/05/image-host/ http://log4d.com/2012/05/image-host/]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle dependency management]]></title>
    <url>%2F2012%2F04%2F13%2Fkettle-dependency-management%2F</url>
    <content type="text"><![CDATA[pentaho的项目使用了ant和ivy解决项目依赖,所以必须编译源码需要ivy工具.直接使用ivy编译pentaho的bi server项目,一直没有编译成功. 使用ivy编译kettle的源代码却是非常容易的事情. 该篇文章翻译并参考了Will Gorman在pentaho的wiki上添加的Kettle dependency management,文章标题没作修改. 编写此文,是为了记录编译kettle源码的方法和过程. 以下是对原文的一个简单翻译. 将kettle作为一个产品发行是一个很有趣的事情.有很多来自于pentaho其他项目(其中有一些有依赖于kettle)的jar包被导入到kettle.这些jar包必须在发行的时候构建并且加入到kettle中.如果一个核心的库被更新了,我们必须将其导入到kettle中(如果有必要).bi服务器,pentaho报表以及pentaho元数据编辑器都将kettle作为一个服务/引擎资源而被构建的.自从我们已经将这些jar导入到我们的源码仓库,这些项目必须使用ivy明确列出kettle以及他的依赖.当kettle的依赖变化的时候,我们必须审查libext文件是否需要更新. pentaho创建了一系列的脚本来自动化的安装ivy,解决jar(或者是artifacts),构建并发行artifacts.kettle已经升级使用subfloor(简单的意味着build.xml继承自subfloor的构建脚本).subfloor使用ivy从pentaho仓库()或者ibiblio maven2仓库来获取跟新jar.ibiblio仓库用于大多数第三方的jar文件(如apache-commons).pentaho仓库用于在线的pentaho项目或者一些比在ibiblio的三方库.为了解决kettle的依赖,我们不得不在ivy.xml里创建一个清单.这个文件明确地列出每一个没有传递依赖的jar文件.这意味着libext文件的映射在ivy.xml中是一对一的. 关于Ivy Apache Ivy™是一个流行的致力于灵活性和简单性的依赖管理工具.更多的参考:enterprise features, what people say about it, 以及 how it can improve your build system 在kettle中使用ivyIDE 首先,从svn上下载kettle的源代码: svn://source.pentaho.org/svnkettleroot/Kettle/trunk 如果你想在Eclipse上使用ivyde plugin. 请参考相关文章安装该插件. 如果你不想使用ivyde,你可以简单快速并且容易的开始并编译代码. 1.执行ant resolve,这个命令将会创建一个叫做resolved-libs的文件夹. 2.使用下面命令更新classpath a.手动的添加这些jar文件到你的ide的classpath b.执行ant create-dot-classpath,将会修改你的.classpath文件(注意刷新项目以使改变生效) 注意:kettle项目中的构建脚本会自动安装ivy插件. 构建Kettle 你可以下载kettle源代码然后立即执行ant distrib命令 或者你可以在ide中导入下载的kettle工程,然后按照你的操作系统(默认的是Windows 32-bit)版本修改依赖的swt.jar文件. ivy中未完成的 pentaho-database-这是一个依赖kettle-db的常用项目,但又被kettle-ui使用.这样会导致循环依赖,将来可能会将其引入到kettle项目或是从该项目中去掉对kettle的依赖. swt-swt文件目前没有包括在ivy.xml文件中 library configurations-每一个kettle库(kettle-db,kettle-core等等)应该在ivy.xml中有他自己的依赖.这些库应该继承一些特定的依赖,而取代继承整个kettle依赖. checked-in plugins-当前引入的插件如;DummyJob, DummyPlugin, S3CsvInput, ShapeFileReader3,versioncheck应该都移到ivy的plugin配置中. 参考文章 Kettle dependency management]]></content>
      <categories>
        <category>pentaho</category>
      </categories>
      <tags>
        <tag>pentaho</tag>
        <tag>kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[哈希表]]></title>
    <url>%2F2012%2F03%2F26%2Fhash-and-hash-functions%2F</url>
    <content type="text"><![CDATA[定义一般的线性表、树，数据在结构中的相对位置是随机的，即和记录的关键字之间不存在确定的关系，因此，在结构中查找记录时需进行一系列和关键字的比较。这一类查找方法建立在“比较“的基础上，查找的效率依赖于查找过程中所进行的比较次数。 若想能直接找到需要的记录，必须在记录的存储位置和它的关键字之间建立一个确定的对应关系f，使每个关键字和结构中一个唯一的存储位置相对应，这就是哈希表。 哈希表又称散列表。哈希表存储的基本思想是：以数据表中的每个记录的关键字 k为自变量，通过一种函数H(k)计算出函数值。把这个值解释为一块连续存储空间（即数组空间）的单元地址（即下标），将该记录存储到这个单元中。在此称该函数H为哈希函数或散列函数。按这种方法建立的表称为哈希表或散列表。 哈希表是一种数据结构，它可以提供快速的插入操作和查找操作。 哈希表是基于数组结构实现的，所以它也存在一些缺点： 数组创建后难于扩展，某些哈希表被基本填满时，性能下降得非常严重。 这个问题是哈希表不可避免的，即冲突现象：对不同的关键字可能得到同一哈希地址。 所以在以下情况下可以优先考虑使用哈希表： 不需要有序遍历数据，并且可以提前预测数据量的大小。 冲突理想情况下，哈希函数在关键字和地址之间建立了一个一一对应关系，从而使得查找只需一次计算即可完成。由于关键字值的某种随机性，使得这种一一对应关系难以发现或构造。因而可能会出现不同的关键字对应一个存储地址。即k1≠k2，但H(k1)=H(k2)，这种现象称为冲突。把这种具有不同关键字值而具有相同哈希地址的对象称同义词。 在大多数情况下，冲突是不能完全避免的。这是因为所有可能的关键字的集合可能比较大，而对应的地址数则可能比较少。 对于哈希技术，主要研究两个问题： （1）如何设计哈希函数以使冲突尽可能少地发生。 （2）发生冲突后如何解决。 哈希函数的构造方法构造好的哈希函数的方法，应能使冲突尽可能地少，因而应具有较好的随机性。这样可使一组关键字的散列地址均匀地分布在整个地址空间。根据关键字的结构和分布的不同，可构造出许多不同的哈希函数。 1）．直接定址法直接定址法是以关键字k本身或关键字加上某个数值常量c作为哈希地址的方法。 该哈希函数H(k)为： 1H(k)=k+c (c≥0) 这种哈希函数计算简单，并且不可能有冲突发生。当关键字的分布基本连续时，可使用直接定址法的哈希函数。否则，若关键字分布不连续将造成内存单元的大量浪费 2）．除留余数法取关键字k除以哈希表长度m所得余数作为哈希函数地址的方法。即： 1H(k)=k％m 这是一种较简单、也是较常见的构造方法。 这种方法的关键是选择好哈希表的长度m。使得数据集合中的每一个关键字通过该函数转化后映射到哈希表的任意地址上的概率相等。理论研究表明，在m取值为素数（质数）时，冲突可能性相对较少。 3）．平方取中法取关键字平方后的中间几位作为哈希函数地址（若超出范围时，可再取模）。 设有一组关键字ABC，BCD,CDE，DEF，……其对应的机内码如表所示。假定地址空间的大小为1000，编号为0-999。现按平方取中法构造哈希函数，则可取关键字机内码平方后的中间三位作为存储位置。 4）．折叠法这种方法适合在关键字的位数较多，而地址区间较小的情况。 将关键字分隔成位数相同的几部分。然后将这几部分的叠加和作为哈希地址（若超出范围，可再取模）。 例如，假设关键字为某人身份证号码430104681015355，则可以用4位为一组进行叠加。即有5355+8101+1046+430=14932，舍去高位。 则有H(430104681015355)=4932 为该身份证关键字的哈希函数地址。 5）．数值分析法若事先知道所有可能的关键字的取值时，可通过对这些关键字进行分析，发现其变化规律，构造出相应的哈希函数。 例：对如下一组关键字通过分析可知： 每个关键字从左到右的第l，2，3位和第6位取值较集中，不宜作哈希地址。 剩余的第4，5，7和8位取值较分散，可根据实际需要取其中的若干位作为哈希地址。 6）. 随机数法选择一个随机函数，取关键字的随机函数值为它的哈希地址，即H(key)＝random(key)，其中random为随机函数。 7）. 斐波那契（Fibonacci）散列法平方散列法的缺点是显而易见的，所以我们能不能找出一个理想的乘数，而不是拿value本身当作乘数呢？答案是肯定的。 1，对于16位整数而言，这个乘数是40503 2，对于32位整数而言，这个乘数是2654435769 3，对于64位整数而言，这个乘数是11400714819323198485 这几个“理想乘数”是如何得出来的呢？这跟一个法则有关，叫黄金分割法则，而描述黄金分割法则的最经典表达式无疑就是著名的斐波那契数列，如果你还有兴趣，就到网上查找一下“斐波那契数列”等关键字，我数学水平有限，不知道怎么描述清楚为什么，另外斐波那契数列的值居然和太阳系八大行星的轨道半径的比例出奇吻合，很神奇，对么？ 对我们常见的32位整数而言，公式： 1index = (value * 2654435769) &gt;&gt; 28 如果用这种斐波那契散列法的话，那我上面的图就变成这样了： 冲突的解决方法假设哈希表的地址范围为0～m-l，当对给定的关键字k，由哈希函数H(k)算出的哈希地址为i（0≤i≤m-1）的位置上已存有记录，这种情况就是冲突现象。 处理冲突就是为该关键字的记录找到另一个“空”的哈希地址。即通过一个新的哈希函数得到一个新的哈希地址。如果仍然发生冲突，则再求下一个，依次类推。直至新的哈希地址不再发生冲突为止。 常用的处理冲突的方法有开放地址法、链地址法两大类 1）．开放定址法用开放定址法处理冲突就是当冲突发生时，形成一个地址序列。沿着这个序列逐个探测，直到找出一个“空”的开放地址。将发生冲突的关键字值存放到该地址中去。如 Hi=(H(k)+d（i）) % m, i=1，2，…k (k 其中H(k)为哈希函数，m为哈希表长，d为增量函数，d(i)=dl，d2…dn-l。 增量序列的取法不同，可得到不同的开放地址处理冲突探测方法。 a）线性探测法线性探测法是从发生冲突的地址（设为d）开始，依次探查d+l，d+2，…m-1（当达到表尾m-1时，又从0开始探查）等地址，直到找到一个空闲位置来存放冲突处的关键字。 若整个地址都找遍仍无空地址，则产生溢出。 线性探查法的数学递推描述公式为： 12d0=H(k)di=(di-1+1)% m (1≤i≤m-1) 【例】已知哈希表地址区间为0～10，给定关键字序列（20，30，70，15，8，12，18，63，19）。哈希函数为H(k)=k％ll，采用线性探测法处理冲突，则将以上关键字依次存储到哈希表中。试构造出该哈希表，并求出等概率情况下的平均查找长度。 假设数组为A, 本题中各元素的存放过程如下： 12345678910H(20)=9，可直接存放到A[9]中去。H(30)=8，可直接存放到A[8]中去。H(70)=4，可直接存放到A[4]中去。H(15)=4，冲突；d0=4d1=(4+1)%11=5，将15放入到A[5]中。H(8)=8，冲突；d0=8d1=(8+1)%11=9，仍冲突；d2=(8+2)%11=10，将8放入到A[10]中。 在等概率情况下成功的平均查找长度为： 1（1*5+2+3+4+6）/9 =20/9 利用线性探查法处理冲突容易造成关键字的堆积问题。这是因为当连续n个单元被占用后，再散列到这些单元上的关键字和直接散列到后面一个空闲单元上的关键字都要占用这个空闲单元，致使该空闲单元很容易被占用，从而发生非同义冲突。造成平均查找长度的增加。为了克服堆积现象的发生，可以用下面的方法替代线性探查法。 b）平方探查法设发生冲突的地址为d，则平方探查法的探查序列为：d+12，d+22，…直到找到一个空闲位置为止。 平方探查法的数学描述公式为： 12d0=H(k)di=(d0+i2) % m (1≤i≤m-1) 在等概率情况下成功的平均查找长度为： 1（1*4+2*2+3+4+6）/9 =21/9 平方探查法是一种较好的处理冲突的方法，可以避免出现堆积问题。它的缺点是不能探查到哈希表上的所有单元，但至少能探查到一半单元。 例如，若表长m=13，假设在第3个位置发生冲突，则后面探查的位置依次为4、7、12、6、2、0，即可以探查到一半单元。 若解决冲突时，探查到一半单元仍找不到一个空闲单元。则表明此哈希表太满，需重新建立哈希表。 2）．链地址法用链地址法解决冲突的方法是：把所有关键字为同义词的记录存储在一个线性链表中，这个链表称为同义词链表。并将这些链表的表头指针放在数组中（下标从0到m-1）。这类似于图中的邻接表和树中孩子链表的结构。 由于在各链表中的第一个元素的查找长度为l，第二个元素的查找长度为2，依此类推。因此，在等概率情况下成功的平均查找长度为： 1(1*5+2*2+3*l+4*1)／9=16／9 虽然链地址法要多费一些存储空间，但是彻底解决了“堆积”问题，大大提高了查找效率。 3）. 再哈希法：Hi=R Hi(key)，R和Hi均是不同的哈希函数，即在同义词产生地址冲突时计算另一个哈希函数地址，直到冲突不再发生。这种方法不易产生聚集，但增加了计算的时间。 4）.建立一个公共溢出区这也是处理冲突的一种方法。 假设哈希函数的值域为[0，m-1]，则设向量HashTable[0…m-1]为基本表，每个分量存放一个记录，另设立向量OverTable[0．．v]为溢出表。所有关键字和基本表中关键字为同义词的记录，不管它们由哈希函数得到的哈希地址是什么，一旦发生冲突，都填入溢出表。 哈希表的查找及性能分析哈希法是利用关键字进行计算后直接求出存储地址的。当哈希函数能得到均匀的地址分布时，不需要进行任何比较就可以直接找到所要查的记录。但实际上不可能完全避免冲突，因此查找时还需要进行探测比较。 在哈希表中，虽然冲突很难避免，但发生冲突的可能性却有大有小。这主要与三个因素有关。 第一:与装填因子有关 所谓装填因子是指哈希表中己存入的元素个数n与哈希表的大小m的比值，即f=n/m。当f越小时，发生冲突的可能性越小，越大（最大为1）时，发生冲突的可能性就越大。 第二:与所构造的哈希函数有关 若哈希函数选择得当，就可使哈希地址尽可能均匀地分布在哈希地址空间上，从而减少冲突的发生。否则，若哈希函数选择不当，就可能使哈希地址集中于某些区域，从而加大冲突的发生。 第三:与解决冲突的哈希冲突函数有关 哈希冲突函数选择的好坏也将减少或增加发生冲突的可能性。 java 哈希表实现java中哈希表的实现有多个，比如hashtable，hashmap，currenthashmap，也有其他公司实现的，如apache的FashHashmap,google的mapmarker,high-lib的NonBlockingHashMap,其中差别是： hastable:线程同步，比较慢 hashmap：线程不同步，不同步时候读写最快（但是不能保证读到最新数据），加同步修饰的时候， 读写比较慢 currenthashmap:线程同步，默认分成16块，写入的时候只锁要写入的快，读取一般不锁块，只有读到空的时候，才锁块，性能比较高，处于hashmap同步和不同步之间。 fashhashmap:apache collection 将HashMap封装，读取的时候copy一个新的，写入比较慢（尤其是存入比较多对象每写一次都要复制一个对象，超级慢），读取快 NoBlockingHashMap： high_scale_lib实现写入慢，读取较快MiltigetHashMap，MapMaker google collection，和CurrentHashMap性能相当，功能比较全，可以设置超时，重复的可以保存成list 参考文章 哈希表 哈希表（Hash Table）及散列法（Hashing） Hash碰撞的拒绝式服务攻击 Berkeley DB Hash、Btree、Queue、Recno选择 Java Hashtable Java Hashtable分析]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在Kettle4.2上面实现cassandra的输入与输出]]></title>
    <url>%2F2012%2F03%2F23%2Fhow-to-implement-cassandra-input-and-output-in-kettle4-2%2F</url>
    <content type="text"><![CDATA[这是在QQ群里有人问到的一个问题。 如何在pdi-ce-4.2.X-stable上面实现cassandra的输入与输出,或是实现hadoop,hbase,mapreduce,mongondb的输入输出? 在kettle中实现cassandra的输入与输出有以下两种方式: 第一种方式:自己编写cassandra输入输出组件 第二种方式:使用别人编写好的插件,将其集成进来 当然还有第三种方法,直接使用4.3版本的pdi. 第一种方法需要对cassandra很熟悉编写插件才可以做到,第二种方法可以通过拷贝pdi-ce-big-data-4.3.0-preview中的文件来完成. 在pdi-ce-big-data-4.3.0-preview(下载页面)版本中可以看到kettle开始支持cassandra的输入和输出. 故我们可以将4.3版本中的cassandra相关文件拷贝到4.2.1中.我使用的是pdi-ce-4.2.1-stable. 在pdi-ce-big-data-4.3.0-preview/plugins目录下有以下目录或文件: . |-- databases |-- hour-partitioner.jar |-- jobentries |-- kettle-gpload-plugin |-- kettle-hl7-plugin |-- kettle-palo-plugin |-- pentaho-big-data-plugin |-- repositories |-- spoon |-- steps `-- versioncheckpentaho-big-data-plugin目录是kettle对大数据的集成与支持,我们只需要将该目录拷贝到pdi-ce-4.2.1-stable/plugins目录下即可.最后的结构如下 . |-- databases |-- hour-partitioner.jar |-- jobentries | `-- DummyJob | |-- DPL.png | |-- dummyjob.jar | `-- plugin.xml |-- pentaho-big-data-plugin | |-- lib | | |-- apache-cassandra-1.0.0.jar | | |-- apache-cassandra-thrift-1.0.0.jar | | |-- aws-java-sdk-1.0.008.jar | | |-- commons-cli-1.2.jar | | |-- guava-r08.jar | | |-- hbase-comparators-TRUNK-SNAPSHOT.jar | | |-- jline-0.9.94.jar | | |-- libthrift-0.6.jar | | |-- mongo-java-driver-2.7.2.jar | | |-- pig-0.8.1.jar | | |-- xpp3_min-1.1.4c.jar | | `-- xstream-1.3.1.jar | `-- pentaho-big-data-plugin-TRUNK-SNAPSHOT.jar |-- repositories |-- spoon |-- steps | |-- DummyPlugin | | |-- DPL.png | | |-- dummy.jar | | `-- plugin.xml | |-- S3CsvInput | | |-- jets3t-0.7.0.jar | | |-- plugin.xml | | |-- S3CIN.png | | `-- s3csvinput.jar | `-- ShapeFileReader3 | |-- plugin.xml | |-- SFR.png | `-- shapefilereader3.jar `-- versioncheck |-- kettle-version-checker-0.2.0.jar `-- lib `-- pentaho-versionchecker.jar 13 directories, 29 files启动pdi-ce-4.2.1-stable之后,打开一个转换,在核心对象窗口就可以看到Big Data步骤目录了. 获取pentaho-big-data-plugin源码如果想在eclipse中查看或修改pentaho-big-data-plugin源码,该怎么做呢?你可以从这里下载到源码,然后将src下的文件拷贝到你的pdi-ce-4.2.1-stable源码工程中. 然后,需要在kettle-steps.xml中注册步骤节点例如,下面是MongoDbInput步骤的注册方法,请针对不同插件的不同类路径加以修改. &lt;step id=&quot;MongoDbInput&quot;&gt; &lt;description&gt;i18n:org.pentaho.di.trans.step:BaseStep.TypeLongDesc.MongoDbInput &lt;classname&gt;org.pentaho.di.trans.steps.mongodbinput.MongoDbInputMeta &lt;category&gt;i18n:org.pentaho.di.trans.step:BaseStep.Category.Input &lt;tooltip&gt;i18n:org.pentaho.di.trans.step:BaseStep.TypeTooltipDesc.MongoDbInput &lt;iconfile&gt;ui/images/mongodb-input.png &lt;/iconfile&gt;&lt;/tooltip&gt; &lt;/category&gt; &lt;/classname&gt; &lt;/description&gt; &lt;/step&gt; 注意: 由于pdi-ce-4.2.1-stable中存在hive组件,故添加pentaho-big-data-plugin插件之后有可能会出现找不到类的情况,这是由于jar重复版本不一致导致的,按照异常信息,找到重复的jar并按情况删除一个jar包即可. 扩展阅读: Pentaho Big Data Plugin http://wiki.pentaho.com/display/BAD/Getting+Started+for+Java+Developers pentaho-big-data-plugin cihttp://- - ci.pentaho.com/job/pentaho-big-data-plugin/lastSuccessfulBuild/artifact/pentaho-big-data-plugin/dist/ Pentaho Community Edition (CE) downloads http://wiki.pentaho.com/display/BAD/Downloads]]></content>
      <categories>
        <category>pentaho</category>
      </categories>
      <tags>
        <tag>pentaho</tag>
        <tag>kettle</tag>
        <tag>cassandra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2011年度年终总结]]></title>
    <url>%2F2012%2F02%2F26%2Fsummary-of-the-work-in-2011%2F</url>
    <content type="text"><![CDATA[2011年工作总结，只能算是七个月的工作总结，在这七个月里学到了许多、收获了许多、感悟了许多。以下是对这六个月的一个回顾与总结。 来公司最初的两个月主要负责云计算相关产品的调研工作，相关云计算产品有：Eucalyptus、OpenNebula、OpenStack。在这两个月里，对云计算的原理、服务、架构以及安装和部署有了初步的了解于实践，并积累了一些文档。在一次又一次的安装部署过程中，体验到了失败的痛苦、无助的迷茫、成功的喜悦，深刻的意识到自身在linux方面存在的不足；强烈的感觉到有必要学习一些语言，如Shell、Python、Groovy、Ruby等等。2012年，打算将工作环境切换到ubuntu，并向叶凌学习ubuntu相关方面的知识；打算学习rhl6相关课程，遇到不懂的问题向同事提问、学习。 随后在公司里参与了报表项目的开发，在这次开发中较快的熟悉了Spring+JPA框架、掌握了dhtmlx的使用、接触到了postgresql数据库，顺利的完成了各项开发工作。通过这个项目，熟悉了公司的项目管理方式、版本控制、代码开发规范等等。总体来说，公司在代码复查方面做的不够，项目开发方面没有形成一套成型的开发框架，并且，项目于项目之间在一些同时使用的相关技术上面的沟通于交流做的不够。希望，公司以后能够制定一些编码规范、引入代码审查、形成一套成型的可重复利用的开发框架或是基础开发包。 剩下的时间接触了Pentaho BI，在整体上了解了Pentaho相关组件之后，开始了阅读其代码、特别是阅读了其分析报表和交互式报表的实现方式以及一些底层基础代码。在阅读代码过程中，深切的体会到有必要深入的学习LDAP以及数据挖掘相关的理论知识。当然，更多的时间花在了kettle的学习和使用以及修改、扩展代码上。2012年，LDAP和kettle将会是一个工作重点。2011年，做了三次Pentaho的服务，通过这三次服务意识到有必要加强自己的沟通交流能力、储备足够的专业知识、灵活应对客户的要求和需求。 2011年，工作上存在一些不足。在调研云计算过程中没有整理、形成足够详细的文档，自己所做的工作没有及时分享；在工作中没有及时提交项目周报；没有及时的跟踪、检查分配下去的任务完成情况，对新人的指导不够；编写文档时候，没有可参考的模版，导致文档编写不规范；在与客户的交流中底气不足、表达能力不够；下班之后惰性较强，缺少自主学习性；博客文章篇数较少，平时的总结与分享不够积极等等。 2012年，将会从以下方面指导自己的工作：以CE为目标提高linux水平，多参加云计算相关活动、多学习云计算开源产品；更加深入的掌握、理解Kettle源码及使用，掌握数据挖掘相关理论知识，争取创建一个Pentaho社区、一个QQ分享群，培养新人，希望能够将Pentaho的咨询服务工作更多交给其他人完成；在项目管理上有所进步；及时总结自己存在的不足，发现问题，不断进步，通过博客、微博及时记录、分享一些技术心得。2012年，是职业生涯的第三年，在这一年希望能够深入理解Spring、Jboss、Pentaho、缓存、云计算、架构等技术，希望自己在技术方面不断成长的同时，在项目管理方面能有所进步；希望自己在工作上、生活里能够有更多的自主性、创造性。 2012年，我们有所希望、有所期待。希望公司不断完善公司规则制度、注重团队培养、关心每一个同事的成长；希望公司能够多分享资源、多交流心得；希望每一个同事都能忙碌着、进步着并快乐着，大家一起努力，一起进步；期待公司在开源方面能够走的更远，公司各方面能够更上一层楼！ 2012年，期望明天会更好！]]></content>
      <categories>
        <category>work</category>
      </categories>
      <tags>
        <tag>work</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle运行作业之前的初始化过程]]></title>
    <url>%2F2012%2F02%2F22%2Fthe-init-process-before-job-execution%2F</url>
    <content type="text"><![CDATA[本文主要描述Kettle是如何通过GUI调用代码启动线程执行作业的。 之前用英文写了一篇文章《The execution process of kettle’s job》 ，这篇文章只是用于英语写技术博客的一个尝试。由于很久没有使用英语写作了，故那篇文章只是简单的通过UML的序列图描述kettle运行job的一个java类调用过程。将上篇文章的序列图和这篇文章联系起来，会更加容易理解本文。 在Spoon界面点击运行按钮，Spoon GUI会调用Spoon.runFile()方法，这可以从xul文件（ui/menubar.xul）中的描述看出来。关于kettle中的xul的使用，不是本文重点故不在此说明。 public void runFile() { executeFile(true, false, false, false, false, null, false); } public void executeFile(boolean local, boolean remote, boolean cluster, boolean preview, boolean debug, Date replayDate, boolean safe) { TransMeta transMeta = getActiveTransformation(); if (transMeta != null) executeTransformation(transMeta, local, remote, cluster, preview, debug, replayDate, safe); JobMeta jobMeta = getActiveJob(); if (jobMeta != null) executeJob(jobMeta, local, remote, replayDate, safe, null, 0); } public void executeJob(JobMeta jobMeta, boolean local, boolean remote, Date replayDate, boolean safe, String startCopyName, int startCopyNr) { try { delegates.jobs.executeJob(jobMeta, local, remote, replayDate, safe, startCopyName, startCopyNr); } catch (Exception e) { new ErrorDialog(shell, "Execute job", "There was an error during job execution", e); } } runFile()方法内部调用executeFile()方法，executeFile方法有以下几个参数： local：是否本地运行 remote：是否远程运行 cluster：是否集群环境运行 preview：是否预览 debug：是否调试 replayDate：回放时间 safe：是否安全模式 executeFile方法会先获取当前激活的转换，如果获取结果不为空，则执行该转换；否则获取当前激活的作业，执行该作业。 本文主要讨论作业的执行过程，关于转换的执行过程，之后单独一篇文章进行讨论。 executeJob委托SpoonJobDelegate执行其内部的executeJob方法，注意，其将JobMeta传递给了executeJob方法。SpoonJobDelegate还保存着对Spoon的引用。 SpoonJobDelegate的executeJob方法主要完成以下操作： 1.设置Spoon的执行配置JobExecutionConfiguration类，该类设置变量、仓库、是否执行安全模式、日志等级等等。 2.获得当前Job对应的图形类JobGraph。 3.将执行配置类JobExecutionConfiguration的变量、参数、命令行参数设置给jobMeta。 4.如果本地执行，则调用jobGraph.startJob(executionConfiguration)，如果远程执行，则委托给SpoonSlaveDelegate执行。 JobExecutionConfiguration类是保存job执行过程中的一些配置，该类会在Spoon、JobGraph类之间传递。 本文只讨论本地执行的情况，故往下查看jobGraph.startJob(executionConfiguration)方法。该方法被synchronized关键字修饰。 JobGraph类包含当前Spoon类的引用、以及对Job的引用。初始情况，Job的引用应该为null。该类会做以下操作： 1.如果job为空或者没有运行或者没有激活，则先保存，然后往下执行作业。 2.在仓库不为空的时候，通过仓库加载Job获得一个运行时的JobMeta对象，名称为runJobMeta；否则，通过文件名称直接new一个JobMeta对象，名称也为runJobMeta。 3.通过仓库和runJobMeta对象构建一个Job对象，并将jobMeta对象（此对象通过JobGraph构造方法传入）的变量、参数共享给Job对象。 4.Job对象添加JobEntry监听器、Job监听器。 5.调用Job的start方法，启动线程开始执行一个job。 Job继承自Thread类，该类的run方法内部会递归执行该作业内部的作业项，限于篇幅，本文不做深究。]]></content>
      <categories>
        <category>pentaho</category>
      </categories>
      <tags>
        <tag>pentaho</tag>
        <tag>kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The execution process of kettle’s job]]></title>
    <url>%2F2012%2F02%2F21%2Fthe-execution-process-of-kettles-job%2F</url>
    <content type="text"><![CDATA[How to execute a kettle job in Spoon GUI or command line after we create a job in Spoon GUI? In Spoon GUI,the main class is "org.pentaho.di.ui.spoon.Spoon.java".This class handles the main window of the Spoon graphical transformation editor.Many operations about a job or transformation such as run,debug,preview,zoomIn,etc,are all in this class.This post just writes about the code execution process. When we start a job or transformation,Spoon invokes the method runFile(),and then is distributed to executeTransformation() or executeJob().At now,we mainly study about executeJob() method. This is a simple sequence diagram below.It contains several classes for Starting to execute a job using execute(int nr, Result result) in Job.java.We can see the relation of these classes from it. What is the detail process of job execution? You should look into the Job.run() method for detail information.]]></content>
      <categories>
        <category>pentaho</category>
      </categories>
      <tags>
        <tag>pentaho</tag>
        <tag>kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle中定义错误处理]]></title>
    <url>%2F2012%2F02%2F17%2Fstep-error-handling-in-kettle%2F</url>
    <content type="text"><![CDATA[在kettle执行的过程中，如果遇到错误，kettle会停止运行。在某些时候，并不希望kettle停止运行，这时候可以使用错误处理（Step Error Handling）。错误处理允许你配置一个步骤来取代出现错误时停止运行一个转换，出现错误的记录行将会传递给另一个步骤。在Step error handling settings对话框里，需要设置启用错误处理。 下面例子中读取postgres数据库中的a0表数据，然后输出到a1表： a1表结构如下： CREATE TABLE a1 ( a double precision, id integer NOT NULL, CONSTRAINT id_pk PRIMARY KEY (id ), CONSTRAINT id_unin UNIQUE (id ) ) 从表结构可以看出，a1表中id为主键、唯一。 a0表数据预览： 现在a1表数据为空，执行上面的转换，执行成功之后，a1表数据和a0表数据一致。再次执行，上面的转换会报错，程序停止运行，会报主键重复的异常。 现在，我想报错之后，程序继续往下执行，并记录错误的记录的相关信息，这时候可以使用“定义错误处理”的功能。在“表输出”的步骤上右键选择“定义错误处理”，弹出如下对话框。 相关字段说明： 目标步骤：指定处理错误的步骤 启用错误处理？：设置是否启用错误处理 错误数列名：出错的记录个数 错误描述列名：描述错误信息的列名称 错误列的列名：出错列的名称 错误编码列名：描述错误的代码的列名 允许的最大错误数：允许的最大错误数，超过此数，不在处理错误 允许的最大错误百分比： 在计算百分百前最少要读入的行数： 添加错误处理后的转换如下： 记录错误信息的字段列表如下，可以看出，errorNum、errorDesc、errorName、errorCode都是在定义错误处理时候填入的列名称，a、id来自于输入的记录的列。 记录的错误信息如下： 分析可以看到,错误日志里只是记录了出错的行里面的信息，并没有记录当前行所在的表名称以及执行时间等等，如果能够对此进行扩展，则该错误日志表才能更有实际意义。 说明1.错误日志的错误码含义（如：TOP001）含义见参考文章2. 参考文章 Step Error Handling Step error handling codes]]></content>
      <categories>
        <category>pentaho</category>
      </categories>
      <tags>
        <tag>pentaho</tag>
        <tag>kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Kettle数据迁移添加主键和索引]]></title>
    <url>%2F2012%2F01%2F05%2Fadd-primary-keys-and-indexes-when-migrating-datas-whith-kettle%2F</url>
    <content type="text"><![CDATA[Kettle是一款国外开源的etl工具，纯java编写，绿色无需安装，主要用于数据抽取、转换、装载。kettle兼容了市面上几十种数据库，故用kettle来做数据库的迁移视乎是个不错的选择。 kettle的数据抽取主要在于抽取数据，而没有考虑数据库的函数、存储过程、视图、表结构以及索引、约束等等，而这些东西恰恰都是数据迁移需要考虑的事情。当然，如果在不考虑数据库中的函数、存储过程、视图的情况下，使用kettle进行数据的迁移还算是一个可行的方案。 这篇文章主要是讲述在使用kettle进行数据库的迁移的时候如何迁移主键和索引，为什么要迁移主键和索引？异构数据库之间的迁移很难无缝的实现自定义函数、存储过程、视图、表结构、索引、约束以及数据的迁移，所以多数情况下只需要异构数据库之间类型兼容、数据一致就可以了。但是在有些情况下需要对输出表进行查询以及数据比对的时候，需要有主键和索引方便对比和加快查询速度。 先来看看kettle中的一些组件。 下图是kettle中的一个表输出组件。 在该组件里可以指定表名、字段等信息，并且还可以建表的sql语句。打开建表的sql语句，你可以看到该语句里只指定了字段名称和类型，没有指定主外键、约束、和索引。显然，该组件只是完成了数据的输出并没有将表的主键迁移过去。 下图是kettle中纬度更新/查询的组件。 该组件可以指定输出表名、映射字段、纬度字段、并且指定主键（图中翻译为关键字段），该组件比表输出组件多了一个功能，即指定主键。从上面两个组件中可以看出，kettle实际上预留了设置主键的接口，具体的接口说明需要查看api或者源代码，只是kettle没有智能的查处输入表的主键字段，而是需要用户在kettle ui界面指定一个主键名称。 如果现在想使用kettle实现异构数据库的数据以及主键和索引的迁移，有没有一个完整方便的解决方案呢？我能想到的解决方案如下：1.使用kettle向导中的多表复制菜单进行数据库的迁移，这只能实现数据的迁移还需要额外的方法添加主键和索引，你可以手动执行一些脚步添加约束。2.针对源数据库中的每一张表创建一个转换，转换中使用纬度更新/查询组件，在该主键中指定主键。创建完所有的转换之后，创建一个作业将这些转换串联起来即可。3.扩展kettle向导中的多表复制菜单里的功能，在该功能创建的作业中添加一些节点用于添加输出表的主键和索引。这些节点可以是执行sql语句的主键，故只需要通过jdbc代码获取添加主键和索引的sql语句。 方案1需要单独执行脚步实现添加主键和索引，创建或生成这些脚步需要些时间；方案2需要针对每个表认为的指定主键，工作量大，而且无法实现添加索引；方案3最容易实现和扩展。 下面是方案3的具体的实现。 首先需要在每一个表的建表语句节点和复制数据节点之后添加一个执行sql语句的节点，该节点用于添加主键和索引。多表复制向导的核心代码在src-db/org.pentaho.di.ui.spoon.delegates.SpoonJobDelegate.java的public void ripDBWizard()方法中。该方法如下： public void ripDBWizard(final int no) { final List databases = spoon.getActiveDatabases(); if (databases.size() == 0) return;&lt;/pre&gt; final RipDatabaseWizardPage1 page1 = new RipDatabaseWizardPage1(&quot;1&quot;, databases); final RipDatabaseWizardPage2 page2 = new RipDatabaseWizardPage2(&quot;2&quot;); final RipDatabaseWizardPage3 page3 = new RipDatabaseWizardPage3(&quot;3&quot;, spoon.getRepository()); Wizard wizard = new Wizard() { public boolean performFinish() { try { JobMeta jobMeta = ripDBByNo(no, databases, page3.getJobname(), page3.getRepositoryDirectory(), page3.getDirectory(), page1.getSourceDatabase(), page1.getTargetDatabase(), page2.getSelection()); if (jobMeta == null) return false; if (page3.getRepositoryDirectory() != null) { spoon.saveToRepository(jobMeta, false); } else { spoon.saveToFile(jobMeta); } addJobGraph(jobMeta); return true; } catch (Exception e) { new ErrorDialog(spoon.getShell(), &quot;Error&quot;, &quot;An unexpected error occurred!&quot;, e); return false; } } public boolean canFinish() { return page3.canFinish(); } }; wizard.addPage(page1); wizard.addPage(page2); wizard.addPage(page3); WizardDialog wd = new WizardDialog(spoon.getShell(), wizard); WizardDialog.setDefaultImage(GUIResource.getInstance().getImageWizard()); wd.setMinimumPageSize(700, 400); wd.updateSize(); wd.open(); }该方法主要是创建一个向导，该向导中包括三个向导页，第一个向导页用于选择数据库连接：源数据库和目标数据库连接；第二个向导页用于选表；第三个向导页用于指定作业保存路径。在向导完成的时候，即performFinish()方法里，会根据选择的数据源和表生成一个作业，即JobMeta对象。创建Jobmeta的方法为： public JobMeta ripDB(final List databases,final String jobname, final RepositoryDirectoryInterface repdir,final String directory, final DatabaseMeta sourceDbInfo,final DatabaseMeta targetDbInfo, final String[] tables){ //此处省略若干代码 }该方法主要逻辑在下面代码内： IRunnableWithProgress op = new IRunnableWithProgress() { public void run(IProgressMonitor monitor) throws InvocationTargetException, InterruptedException { //此处省略若干代码 } }上面代码中有以下代码用于遍历所选择的表生成作业中的一些节点： for (int i = 0; i &amp;lt; tables.length &amp;amp;&amp;amp; !monitor.isCanceled(); i++) { //此处省略若干代码 }针对每一张表先会创建一个JobEntrySQL节点，然后创建一个转换JobEntryTrans，可以在创建转换之后再创建一个JobEntrySQL节点，该节点用于添加主键和索引。这部分的代码如下： String pksql = JdbcDataMetaUtil.exportPkAndIndex( sourceDbInfo, sourceCon, tables[i], targetDbInfo, targetCon, tables[i]); if (!Const.isEmpty(pksql)) { location.x += 300; JobEntrySQL jesql = new JobEntrySQL( BaseMessages.getString(PKG,&quot;Spoon.RipDB.AddPkAndIndex&quot;) + tables[i] + &quot;]&quot;); jesql.setDatabase(targetDbInfo); jesql.setSQL(pksql); jesql.setDescription(BaseMessages.getString(PKG, &quot;Spoon.RipDB.AddPkAndIndex&quot;) + tables[i] + &quot;]&quot;); JobEntryCopy jecsql = new JobEntryCopy(); jecsql.setEntry(jesql); jecsql.setLocation(new Point(location.x, location.y)); jecsql.setDrawn(); jobMeta.addJobEntry(jecsql); // Add the hop too... JobHopMeta jhi = new JobHopMeta(previous, jecsql); jobMeta.addJobHop(jhi); previous = jecsql; }获取添加主键和索引的sql语句，主要是采用jdbc的方式读取两个数据库，判断源数据库的表中是否存在主键和索引，如果有则返回添加主键或索引的sql语句。这部分代码封装在JdbcDataMetaUtil类中。该代码见：https://gist.github.com/1564353.js 最后的效果图如下： 说明： 1.以上代码使用的是jdbc的方法获取主键或索引，不同的数据库的jdbc驱动实现可能不同而且不同数据库的语法可能不同，故上面代码可能有待完善。 2.如果一个数据库中存在多库并且这多个库中有相同的表，使用上面的代码针对一个表名会查出多个主键或索引。这一点也是可以改善的]]></content>
      <categories>
        <category>pentaho</category>
      </categories>
      <tags>
        <tag>pentaho</tag>
        <tag>kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kettle进行数据迁移遇到的问题]]></title>
    <url>%2F2012%2F01%2F04%2Fsome-problems-about-migrating-database-datas-with-kettle%2F</url>
    <content type="text"><![CDATA[使用kettle进行oracle或db2数据导入到mysql或postgres数据库过程中遇到以下问题，以下只是一个简单描述，详细的说明以及所做的代码修改没有提及。下面所提到的最新的pdi程序是我修改kettle源码并编译之后的版本。 同时运行两个pdi程序，例如：一个为oracle到mysql，另一个为oracle到postgres，其中一个停止运行 原因：从oracle迁移到mysql创建的作业和转换文件和oracle到postgres的作业和转换保存到一个路径，导致同名称的转换相互之间被覆盖，故在运行时候会出现混乱。 解决办法：将新建的作业和转换分别保存在两个不同的路径，最好是新建两个不同路径的仓库，关于如何新建仓库，请参考《kettle使用说明》文档。 关键字的问题。Oracle初始化到mysql，关键字前面会加上前缀“MY_”。如果在建表的时候出现错误，则需要检查表的字段中是否有关键字。 解决办法：出差的表单独进行处理，新建一个转换，实现关键字段该名称然后初始化出错的表。具体操作参见文档。 oracle中的字段名从中可以有#号，但是到mysql会报错 解决办法：字段改名称，去掉#号 Db2初始化到mysql或是postgres出错 原因：1）db2数据库连接用户没有权限访问出错的表；2）出错的表名存在小写字母 解决办法：使用更新后的pdi程序，更新后的程序会将db2的表名使用双引号括起来。 Oracle到mysql和pg时日期类型数据值有偏差 原因：从oracle中读取日期类型的数据时候，读取结果与oracle数据库中的数据已经存在偏差。少数记录使用oracle10g的驱动读取数据少一个小时，用oracle11g的驱动会多一个小时，该问题尚待oracle工程师给出解决方案。 主键从ORACLE导入不到MYSQL和POSTGRES 原因：pdi程序中没有对主键进行处理 解决办法：使用更新的pdi程序，执行Tools####Wizzard####Copy Tables Extension…功能添加主键；执行Tools####Wizzard####Copy Tables Data Only…功能可以只复制数据 Oracle中存在ascii字符导入到postgres时候报错：ERROR: invalid byte sequence for encoding “UTF8”: 0x00 原因：PostgreSQL内部采用C语言风格的字符串（以0x00）表示结尾，因而不允许字符串中包括0x00，建议在转换时先对字符串类型的数据进行清洗，也就是增加一个节点用于删除字符串数据中的特殊字符0x00。 解决办法:使用新的pdi程序。在kettle的DataBase类中修改PreparedStatement.setString(int index,String value)方法传入的参数，将value的值trim之后在setString 异构数据库之间的类型兼容问题。日期类型和时间类型的数据初始化到mysql或postgres中都为时间类型的数据，导致数据对比时候数据不一致。 原因：Pdi程序中的类型转换采用的是向上兼容的方式，故日期和时间类型都转换为时间类型数据。 解决办法：针对与db2数据初始化到mysql和postgres，该问题在最新的pdi程序中已经处理。因为oracle中的日期类型字段既可以存日期又可以存时间，故没针对oracle数据做出处理。 Db2中没有主键的数据初始化到mysql和postgres需要添加索引 解决办法：使用最新的pdi程序，最新的pdi程序会添加主键和索引。 Db2中decimal（n,m）类型的数据初始化到postgres数据库被四舍五入。 原因：Db2中decimal（n,m）类型的数据初始化到postgres中的类型不对。 解决办法：使用最新的pdi程序。 导数据中途时没有报错，直接软件退出 原因：1）jvm内存溢出，需要修改jvm参数；2）pdi程序报swt错误 解决办法：修改jvm参数 初次使用kettle做db2的初始化会报错 原因：kettle中的db2的jdbc驱动与使用的db2版本不对应。 解决办法：从db2的安装目录下拷贝jdbc驱动到kettle目录（libext/JDBC）下]]></content>
      <categories>
        <category>pentaho</category>
      </categories>
      <tags>
        <tag>pentaho</tag>
        <tag>kettle</tag>
      </tags>
  </entry>
</search>
