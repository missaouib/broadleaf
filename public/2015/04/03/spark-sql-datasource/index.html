<!DOCTYPE html>




<html class="theme-next mist" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
    
  
  <link href="//cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Arial:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






  

<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="spark,avro,parquet,spark-sql,">





  <link rel="alternate" href="/atom.xml" title="JavaChen Blog" type="application/atom+xml">






<meta name="description" content="Spark 支持通过 DataFrame 来操作大量的数据源，包括外部文件（如 json、avro、parquet、sequencefile 等等）、hive、关系数据库、cassandra 等等。">
<meta name="keywords" content="spark,avro,parquet,spark-sql">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark SQL中的数据源">
<meta property="og:url" content="http://blog.javachen.com/2015/04/03/spark-sql-datasource/index.html">
<meta property="og:site_name" content="JavaChen Blog">
<meta property="og:description" content="Spark 支持通过 DataFrame 来操作大量的数据源，包括外部文件（如 json、avro、parquet、sequencefile 等等）、hive、关系数据库、cassandra 等等。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-03-26T08:41:24.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark SQL中的数据源">
<meta name="twitter:description" content="Spark 支持通过 DataFrame 来操作大量的数据源，包括外部文件（如 json、avro、parquet、sequencefile 等等）、hive、关系数据库、cassandra 等等。">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"remove","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://blog.javachen.com/2015/04/03/spark-sql-datasource/">





  <title>Spark SQL中的数据源 | JavaChen Blog</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?50bc6f5d9b045b5895ff44f8bbdbc611";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">JavaChen Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">Ramblings of a coder</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.javachen.com/2015/04/03/spark-sql-datasource/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JavaChen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JavaChen Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Spark SQL中的数据源</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2015-04-03T00:00:00+08:00">
                2015-04-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          <!--/删除
          
              <div class="post-description">
                  Spark 支持通过 DataFrame 来操作大量的数据源，包括外部文件（如 json、avro、parquet、sequencefile 等等）、hive、关系数据库、cassandra 等等。
              </div>
          
          -->
        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Spark 支持通过 DataFrame 来操作大量的数据源，包括外部文件（如 json、avro、parquet、sequencefile 等等）、hive、关系数据库、cassandra 等等。</p>
<p>本文测试环境为 Spark 1.3。</p>
<h1 id="加载和保存文件"><a href="#加载和保存文件" class="headerlink" title="加载和保存文件"></a>加载和保存文件</h1><p>最简单的方式是调用 load 方法加载文件，默认的格式为 parquet，你可以修改 <code>spark.sql.sources.default</code> 指定默认的格式：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = sqlContext.load(<span class="string">"people.parquet"</span>)</span><br><span class="line">scala&gt; df.select(<span class="string">"name"</span>, <span class="string">"age"</span>).save(<span class="string">"namesAndAges.parquet"</span>)</span><br></pre></td></tr></table></figure>

<p>你也可以收到指定数据源，使用全路径名称，如：<code>org.apache.spark.sql.parquet</code>，对于内置的数据源，你也可以使用简称，如：<code>json</code>、<code>parquet</code>、<code>jdbc</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = sqlContext.load(<span class="string">"people.json"</span>, <span class="string">"json"</span>)</span><br><span class="line">scala&gt; df.select(<span class="string">"name"</span>, <span class="string">"age"</span>).save(<span class="string">"namesAndAges.parquet"</span>, <span class="string">"parquet"</span>)</span><br></pre></td></tr></table></figure>

<p>保存操作还可以指定保存模式，用于处理文件已经存在的情况下如何操作。</p>
<table>
<thead>
<tr>
<th align="left">Scala/Java</th>
<th align="left">Python</th>
<th align="left">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="left">SaveMode.ErrorIfExists (default)</td>
<td align="left">“error” (default)</td>
<td align="left">如果存在，则报错</td>
</tr>
<tr>
<td align="left">SaveMode.Append</td>
<td align="left">“append”</td>
<td align="left">追加模式</td>
</tr>
<tr>
<td align="left">SaveMode.Overwrite</td>
<td align="left">“overwrite”</td>
<td align="left">覆盖模式</td>
</tr>
<tr>
<td align="left">SaveMode.Ignore</td>
<td align="left">“ignore”</td>
<td align="left">忽略，类似 SQL 中的 <code>CREATE TABLE IF NOT EXISTS</code></td>
</tr>
</tbody></table>
<h1 id="Parquet-数据源"><a href="#Parquet-数据源" class="headerlink" title="Parquet 数据源"></a>Parquet 数据源</h1><h2 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h2><p>Spark SQL 支持读写 Parquet文件。</p>
<p>Scala:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sqlContext from the previous example is used in this example.</span></span><br><span class="line"><span class="comment">// This is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> people: <span class="type">RDD</span>[<span class="type">Person</span>] = ... <span class="comment">// An RDD of case class objects, from the previous example.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// The RDD is implicitly converted to a DataFrame by implicits, allowing it to be stored using Parquet.</span></span><br><span class="line">people.saveAsParquetFile(<span class="string">"people.parquet"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read in the parquet file created above.  Parquet files are self-describing so the schema is preserved.</span></span><br><span class="line"><span class="comment">// The result of loading a Parquet file is also a DataFrame.</span></span><br><span class="line"><span class="keyword">val</span> parquetFile = sqlContext.parquetFile(<span class="string">"people.parquet"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//Parquet files can also be registered as tables and then used in SQL statements.</span></span><br><span class="line">parquetFile.registerTempTable(<span class="string">"parquetFile"</span>)</span><br><span class="line"><span class="keyword">val</span> teenagers = sqlContext.sql(<span class="string">"SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span><br><span class="line">teenagers.map(t =&gt; <span class="string">"Name: "</span> + t(<span class="number">0</span>)).collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p>Java:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sqlContext from the previous example is used in this example.</span></span><br><span class="line"></span><br><span class="line">DataFrame schemaPeople = ... <span class="comment">// The DataFrame from the previous example.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrames can be saved as Parquet files, maintaining the schema information.</span></span><br><span class="line">schemaPeople.saveAsParquetFile(<span class="string">"people.parquet"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read in the Parquet file created above.  Parquet files are self-describing so the schema is preserved.</span></span><br><span class="line"><span class="comment">// The result of loading a parquet file is also a DataFrame.</span></span><br><span class="line">DataFrame parquetFile = sqlContext.parquetFile(<span class="string">"people.parquet"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//Parquet files can also be registered as tables and then used in SQL statements.</span></span><br><span class="line">parquetFile.registerTempTable(<span class="string">"parquetFile"</span>);</span><br><span class="line">DataFrame teenagers = sqlContext.sql(<span class="string">"SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 19"</span>);</span><br><span class="line">List&lt;String&gt; teenagerNames = teenagers.map(<span class="keyword">new</span> Function&lt;Row, String&gt;() &#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">call</span><span class="params">(Row row)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"Name: "</span> + row.getString(<span class="number">0</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;).collect();</span><br></pre></td></tr></table></figure>

<p>Python:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sqlContext from the previous example is used in this example.</span></span><br><span class="line"></span><br><span class="line">schemaPeople <span class="comment"># The DataFrame from the previous example.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrames can be saved as Parquet files, maintaining the schema information.</span></span><br><span class="line">schemaPeople.saveAsParquetFile(<span class="string">"people.parquet"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read in the Parquet file created above.  Parquet files are self-describing so the schema is preserved.</span></span><br><span class="line"><span class="comment"># The result of loading a parquet file is also a DataFrame.</span></span><br><span class="line">parquetFile = sqlContext.parquetFile(<span class="string">"people.parquet"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parquet files can also be registered as tables and then used in SQL statements.</span></span><br><span class="line">parquetFile.registerTempTable(<span class="string">"parquetFile"</span>);</span><br><span class="line">teenagers = sqlContext.sql(<span class="string">"SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span><br><span class="line">teenNames = teenagers.map(<span class="keyword">lambda</span> p: <span class="string">"Name: "</span> + p.name)</span><br><span class="line"><span class="keyword">for</span> teenName <span class="keyword">in</span> teenNames.collect():</span><br><span class="line">  <span class="keyword">print</span> teenName</span><br></pre></td></tr></table></figure>

<p>SQL:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TEMPORARY</span> <span class="keyword">TABLE</span> parquetTable</span><br><span class="line"><span class="keyword">USING</span> org.apache.spark.sql.parquet</span><br><span class="line">OPTIONS (</span><br><span class="line">  <span class="keyword">path</span> <span class="string">"examples/src/main/resources/people.parquet"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> parquetTable</span><br></pre></td></tr></table></figure>

<h2 id="自动发现分区"><a href="#自动发现分区" class="headerlink" title="自动发现分区"></a>自动发现分区</h2><p>Parquet 数据源可以自动识别分区目录以及分区列的类型，目前支持数据类型和字符串类型。</p>
<p>例如，对于这样一个目录结构，有两个分区字段：gender、country。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">path</span><br><span class="line">└── to</span><br><span class="line">    └── table</span><br><span class="line">        ├── gender=male</span><br><span class="line">        │   ├── ...</span><br><span class="line">        │   │</span><br><span class="line">        │   ├── country=US</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   ├── country=CN</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   └── ...</span><br><span class="line">        └── gender=female</span><br><span class="line">            ├── ...</span><br><span class="line">            │</span><br><span class="line">            ├── country=US</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            ├── country=CN</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            └── ...</span><br></pre></td></tr></table></figure>

<p>将 path/to/table 路径传递给 SQLContext.parquetFile 或 SQLContext.load 时，Spark SQL 将会字段获取分区信息，并返回 DataFrame 的 schema 如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">|-- name: string (nullable = true)</span><br><span class="line">|-- age: long (nullable = true)</span><br><span class="line">|-- gender: string (nullable = true)</span><br><span class="line">|-- country: string (nullable = true)</span><br></pre></td></tr></table></figure>

<h2 id="schema-自动扩展"><a href="#schema-自动扩展" class="headerlink" title="schema 自动扩展"></a>schema 自动扩展</h2><p>Parquet 还支持 schema 自动扩展。</p>
<p>Scala:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sqlContext from the previous example is used in this example.</span></span><br><span class="line"><span class="comment">// This is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a simple DataFrame, stored into a partition directory</span></span><br><span class="line"><span class="keyword">val</span> df1 = sparkContext.makeRDD(<span class="number">1</span> to <span class="number">5</span>).map(i =&gt; (i, i * <span class="number">2</span>)).toDF(<span class="string">"single"</span>, <span class="string">"double"</span>)</span><br><span class="line">df1.saveAsParquetFile(<span class="string">"data/test_table/key=1"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create another DataFrame in a new partition directory,</span></span><br><span class="line"><span class="comment">// adding a new column and dropping an existing column</span></span><br><span class="line"><span class="keyword">val</span> df2 = sparkContext.makeRDD(<span class="number">6</span> to <span class="number">10</span>).map(i =&gt; (i, i * <span class="number">3</span>)).toDF(<span class="string">"single"</span>, <span class="string">"triple"</span>)</span><br><span class="line">df2.saveAsParquetFile(<span class="string">"data/test_table/key=2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read the partitioned table</span></span><br><span class="line"><span class="keyword">val</span> df3 = sqlContext.parquetFile(<span class="string">"data/test_table"</span>)</span><br><span class="line">df3.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment">// The final schema consists of all 3 columns in the Parquet files together</span></span><br><span class="line"><span class="comment">// with the partiioning column appeared in the partition directory paths.</span></span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">// |-- single: int (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- double: int (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- triple: int (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- key : int (nullable = true)</span></span><br></pre></td></tr></table></figure>

<p>Python:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sqlContext from the previous example is used in this example.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a simple DataFrame, stored into a partition directory</span></span><br><span class="line">df1 = sqlContext.createDataFrame(sc.parallelize(range(<span class="number">1</span>, <span class="number">6</span>))\</span><br><span class="line">                                   .map(<span class="keyword">lambda</span> i: Row(single=i, double=i * <span class="number">2</span>)))</span><br><span class="line">df1.save(<span class="string">"data/test_table/key=1"</span>, <span class="string">"parquet"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create another DataFrame in a new partition directory,</span></span><br><span class="line"><span class="comment"># adding a new column and dropping an existing column</span></span><br><span class="line">df2 = sqlContext.createDataFrame(sc.parallelize(range(<span class="number">6</span>, <span class="number">11</span>))</span><br><span class="line">                                   .map(<span class="keyword">lambda</span> i: Row(single=i, triple=i * <span class="number">3</span>)))</span><br><span class="line">df2.save(<span class="string">"data/test_table/key=2"</span>, <span class="string">"parquet"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the partitioned table</span></span><br><span class="line">df3 = sqlContext.parquetFile(<span class="string">"data/test_table"</span>)</span><br><span class="line">df3.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment"># The final schema consists of all 3 columns in the Parquet files together</span></span><br><span class="line"><span class="comment"># with the partiioning column appeared in the partition directory paths.</span></span><br><span class="line"><span class="comment"># root</span></span><br><span class="line"><span class="comment"># |-- single: int (nullable = true)</span></span><br><span class="line"><span class="comment"># |-- double: int (nullable = true)</span></span><br><span class="line"><span class="comment"># |-- triple: int (nullable = true)</span></span><br><span class="line"><span class="comment"># |-- key : int (nullable = true)</span></span><br></pre></td></tr></table></figure>

<h2 id="配置参数"><a href="#配置参数" class="headerlink" title="配置参数"></a>配置参数</h2><ul>
<li><code>spark.sql.parquet.binaryAsString</code>：默认为 false，是否将 binary 当做字符串处理</li>
<li><code>spark.sql.parquet.int96AsTimestamp</code>：默认为 true</li>
<li><code>spark.sql.parquet.cacheMetadata</code> ：默认为 true，是否缓存元数据</li>
<li><code>spark.sql.parquet.compression.codec</code>：默认为 gzip，支持的值：uncompressed, snappy, gzip, lzo</li>
<li><code>spark.sql.parquet.filterPushdown</code>：默认为 false</li>
<li><code>spark.sql.hive.convertMetastoreParquet</code>：默认为 false </li>
</ul>
<h1 id="JSON-数据源"><a href="#JSON-数据源" class="headerlink" title="JSON 数据源"></a>JSON 数据源</h1><p>Spark SQL 能够自动识别 JSON 数据的 schema ，SQLContext 中有两个方法处理 JSON：</p>
<ul>
<li><code>jsonFile</code>：从一个 JSON 目录中加载数据，JSON 文件中每一行为一个 JSON 对象。</li>
<li><code>jsonRDD</code>：从一个 RDD 中加载数据，RDD 的每一个元素为一个 JSON 对象的字符串。</li>
</ul>
<p>一个 Scala 的例子如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sc is an existing SparkContext.</span></span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> org.apache.spark.sql.<span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// A JSON dataset is pointed to by path.</span></span><br><span class="line"><span class="comment">// The path can be either a single text file or a directory storing text files.</span></span><br><span class="line"><span class="keyword">val</span> path = <span class="string">"people.json"</span></span><br><span class="line"><span class="comment">// Create a DataFrame from the file(s) pointed to by path</span></span><br><span class="line"><span class="keyword">val</span> people = sqlContext.jsonFile(path)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The inferred schema can be visualized using the printSchema() method.</span></span><br><span class="line">people.printSchema()</span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">//  |-- age: integer (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Register this DataFrame as a table.</span></span><br><span class="line">people.registerTempTable(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by sqlContext.</span></span><br><span class="line"><span class="keyword">val</span> teenagers = sqlContext.sql(<span class="string">"SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span></span><br><span class="line"><span class="comment">// an RDD[String] storing one JSON object per string.</span></span><br><span class="line"><span class="keyword">val</span> anotherPeopleRDD = sc.parallelize(</span><br><span class="line">  <span class="string">""</span><span class="string">"&#123;"</span><span class="string">name":"</span><span class="type">Yin</span><span class="string">","</span><span class="string">address":&#123;"</span><span class="string">city":"</span><span class="type">Columbus</span><span class="string">","</span><span class="string">state":"</span><span class="type">Ohio</span><span class="string">"&#125;&#125;"</span><span class="string">""</span> :: <span class="type">Nil</span>)</span><br><span class="line"><span class="keyword">val</span> anotherPeople = sqlContext.jsonRDD(anotherPeopleRDD)</span><br></pre></td></tr></table></figure>

<h1 id="Hive-数据源"><a href="#Hive-数据源" class="headerlink" title="Hive 数据源"></a>Hive 数据源</h1><p>Spark SQL 支持读和写 Hive 中的数据。Spark  源码本身不包括 Hive，故编译时候需要添加  <code>-Phive</code> 和 <code>-Phive-thriftserver</code> 开启对 Hive 的支持。另外，Hive assembly jar 需要存在于每一个 worker 节点上，因为他们需要 SerDes 去访问存在于 Hive 中的数据。</p>
<p>Scala:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sc is an existing SparkContext.</span></span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> org.apache.spark.sql.hive.<span class="type">HiveContext</span>(sc)</span><br><span class="line"></span><br><span class="line">sqlContext.sql(<span class="string">"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)"</span>)</span><br><span class="line">sqlContext.sql(<span class="string">"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Queries are expressed in HiveQL</span></span><br><span class="line">sqlContext.sql(<span class="string">"FROM src SELECT key, value"</span>).collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p>Java:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sc is an existing JavaSparkContext.</span></span><br><span class="line">HiveContext sqlContext = <span class="keyword">new</span> org.apache.spark.sql.hive.HiveContext(sc);</span><br><span class="line"></span><br><span class="line">sqlContext.sql(<span class="string">"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)"</span>);</span><br><span class="line">sqlContext.sql(<span class="string">"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Queries are expressed in HiveQL.</span></span><br><span class="line">Row[] results = sqlContext.sql(<span class="string">"FROM src SELECT key, value"</span>).collect();</span><br></pre></td></tr></table></figure>

<p>Python:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sc is an existing SparkContext.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> HiveContext</span><br><span class="line">sqlContext = HiveContext(sc)</span><br><span class="line"></span><br><span class="line">sqlContext.sql(<span class="string">"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)"</span>)</span><br><span class="line">sqlContext.sql(<span class="string">"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Queries can be expressed in HiveQL.</span></span><br><span class="line">results = sqlContext.sql(<span class="string">"FROM src SELECT key, value"</span>).collect()</span><br></pre></td></tr></table></figure>

<h1 id="JDBC-数据源"><a href="#JDBC-数据源" class="headerlink" title="JDBC 数据源"></a>JDBC 数据源</h1><p>Spark SQL 支持通过 JDBC 访问关系数据库，这需要用到 <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.JdbcRDD" target="_blank" rel="noopener">JdbcRDD</a>。为了访问某一个关系数据库，需要将其驱动添加到 classpath，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_CLASSPATH=postgresql-9.3-1102-jdbc41.jar bin/spark-shell</span><br></pre></td></tr></table></figure>

<p>访问 jdbc 数据源需要提供以下参数：</p>
<ul>
<li>url</li>
<li>dbtable</li>
<li>driver</li>
<li>partitionColumn, lowerBound, upperBound, numPartitions</li>
</ul>
<p>Scala 示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> jdbcDF = sqlContext.load(<span class="string">"jdbc"</span>, <span class="type">Map</span>(</span><br><span class="line">  <span class="string">"url"</span> -&gt; <span class="string">"jdbc:postgresql:dbserver"</span>,</span><br><span class="line">  <span class="string">"dbtable"</span> -&gt; <span class="string">"schema.tablename"</span>))</span><br></pre></td></tr></table></figure>

<p>Java:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Map&lt;String, String&gt; options = <span class="keyword">new</span> HashMap&lt;String, String&gt;();</span><br><span class="line">options.put(<span class="string">"url"</span>, <span class="string">"jdbc:postgresql:dbserver"</span>);</span><br><span class="line">options.put(<span class="string">"dbtable"</span>, <span class="string">"schema.tablename"</span>);</span><br><span class="line"></span><br><span class="line">DataFrame jdbcDF = sqlContext.load(<span class="string">"jdbc"</span>, options)</span><br></pre></td></tr></table></figure>

<p>Python:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = sqlContext.load(<span class="string">"jdbc"</span>, url=<span class="string">"jdbc:postgresql:dbserver"</span>, dbtable=<span class="string">"schema.tablename"</span>)</span><br></pre></td></tr></table></figure>

<p>SQL:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TEMPORARY</span> <span class="keyword">TABLE</span> jdbcTable</span><br><span class="line"><span class="keyword">USING</span> org.apache.spark.sql.jdbc</span><br><span class="line">OPTIONS (</span><br><span class="line">  <span class="keyword">url</span> <span class="string">"jdbc:postgresql:dbserver"</span>,</span><br><span class="line">  dbtable <span class="string">"schema.tablename"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h1 id="访问-Avro"><a href="#访问-Avro" class="headerlink" title="访问 Avro"></a>访问 Avro</h1><p>这不是 Spark 内置的数据源，要想访问 Avro 数据源 ，需要做些处理。这部分内容可以参考 <a href="http://blog.javachen.com/2015/03/24/how-to-load-some-avro-data-into-spark.html">如何将Avro数据加载到Spark</a> 和 <a href="http://www.infoobjects.com/spark-with-avro.html" target="_blank" rel="noopener">Spark with Avro</a>。</p>
<h1 id="访问-Cassandra"><a href="#访问-Cassandra" class="headerlink" title="访问 Cassandra"></a>访问 Cassandra</h1><p>TODO</p>
<h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><h2 id="Spark-和-Parquet"><a href="#Spark-和-Parquet" class="headerlink" title="Spark 和 Parquet"></a>Spark 和 Parquet</h2><p>参考上面的例子，将 people.txt 文件加载到 Spark：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">People</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">val</span> <span class="title">people</span> </span>= sc.textFile(<span class="string">"people.txt"</span>).map(_.split(<span class="string">","</span>)).map(p =&gt; <span class="type">People</span>(p(<span class="number">0</span>), p(<span class="number">1</span>).trim.toInt)).toDF()</span><br><span class="line">scala&gt; people.registerTempTable(<span class="string">"people"</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> teenagers = sqlContext.sql(<span class="string">"SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span><br><span class="line">scala&gt; teenagers.map(t =&gt; <span class="string">"Name: "</span> + t(<span class="number">0</span>)).collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p>然后，将 people 这个 DataFrame 转换为 parquet 格式：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; people.saveAsParquetFile(<span class="string">"people.parquet"</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> parquetFile = sqlContext.parquetFile(<span class="string">"people.parquet"</span>)</span><br></pre></td></tr></table></figure>

<p>另外，也可以从 hive 中加载 parquet 格式的文件。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table people_parquet like people stored as parquet;</span><br><span class="line">hive&gt; insert overwrite table people_parquet select * from people;</span><br></pre></td></tr></table></figure>

<p>使用 HiveContext 来从 hive 中加载 parquet 文件，这里不再需要定义一个 case class ，因为 parquet 中已经包含了文件的 schema。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> hc = <span class="keyword">new</span> org.apache.spark.sql.hive.<span class="type">HiveContext</span>(sc)</span><br><span class="line">scala&gt; <span class="keyword">import</span> hc.implicits._</span><br><span class="line">scala&gt;<span class="keyword">val</span> peopleRDD = hc.parquetFile(<span class="string">"people.parquet"</span>)</span><br><span class="line">scala&gt; peopleRDD.registerAsTempTable(<span class="string">"pp"</span>)</span><br><span class="line">scala&gt;<span class="keyword">val</span> teenagers = hc.sql(<span class="string">"SELECT name FROM pp WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span><br><span class="line">scala&gt;teenagers.collect.foreach(println)</span><br></pre></td></tr></table></figure>

<p>注意到 impala 中处理 parquet 文件时，会将字符串保存为 Binary，为了修正这个问题，可以添加下面一行代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sqlContext.setConf(<span class="string">"spark.sql.parquet.binaryAsString"</span>,<span class="string">"true"</span>)</span><br></pre></td></tr></table></figure>

<h2 id="SparkSql-Join"><a href="#SparkSql-Join" class="headerlink" title="SparkSql Join"></a>SparkSql Join</h2><p>下面是两个表左外连接的例子：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;<span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line">scala&gt;<span class="keyword">import</span> org.apache.spark.sql.catalyst.plans._</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Dept</span>(<span class="params">dept_id:<span class="type">String</span>,dept_name:<span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">val</span> <span class="title">dept</span> </span>= sc.parallelize(<span class="type">List</span>( (<span class="string">"DEPT01"</span>,<span class="string">"Information Technology"</span>), (<span class="string">"DEPT02"</span>,<span class="string">"WHITE HOUSE"</span>),(<span class="string">"DEPT03"</span>,<span class="string">"EX-PRESIDENTS OFFICE"</span>),(<span class="string">"DEPT04"</span>,<span class="string">"SALES"</span>))).map( d =&gt; <span class="type">Dept</span>(d._1,d._2)).toDF.as( <span class="string">"dept"</span> )</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Emp</span>(<span class="params">first_name:<span class="type">String</span>,last_name:<span class="type">String</span>,dept_id:<span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">val</span> <span class="title">emp</span> </span>= sc.parallelize(<span class="type">List</span>( (<span class="string">"Rishi"</span>,<span class="string">"Yadav"</span>,<span class="string">"DEPT01"</span>),(<span class="string">"Barack"</span>,<span class="string">"Obama"</span>,<span class="string">"DEPT02"</span>),(<span class="string">"Bill"</span>,<span class="string">"Clinton"</span>,<span class="string">"DEPT04"</span>))).map( e =&gt; <span class="type">Emp</span>(e._1,e._2,e._3)).toDF.as(<span class="string">"emp"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> alldepts = dept.join(emp,dept(<span class="string">"dept_id"</span>) === emp(<span class="string">"dept_id"</span>), <span class="string">"left_outer"</span>).select(<span class="string">"dept.dept_id"</span>,<span class="string">"dept_name"</span>,<span class="string">"first_name"</span>,<span class="string">"last_name"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; alldepts.foreach(println)</span><br><span class="line">[<span class="type">DEPT01</span>,<span class="type">Information</span> <span class="type">Technology</span>,<span class="type">Rishi</span>,<span class="type">Yadav</span>]</span><br><span class="line">[<span class="type">DEPT02</span>,<span class="type">WHITE</span> <span class="type">HOUSE</span>,<span class="type">Barack</span>,<span class="type">Obama</span>]</span><br><span class="line">[<span class="type">DEPT04</span>,<span class="type">SALES</span>,<span class="type">Bill</span>,<span class="type">Clinton</span>]</span><br><span class="line">[<span class="type">DEPT03</span>,<span class="type">EX</span>-<span class="type">PRESIDENTS</span> <span class="type">OFFICE</span>,<span class="literal">null</span>,<span class="literal">null</span>]</span><br></pre></td></tr></table></figure>

<p>支持的连接类型有：<code>inner</code>、<code>outer</code>、<code>left_outer</code>、<code>right_outer</code>、<code>semijoin</code>。</p>
<h1 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h1><ul>
<li><a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes" target="_blank" rel="noopener">Spark SQL and DataFrame Guide</a></li>
<li><a href="http://endymecy.gitbooks.io/spark-programming-guide-zh-cn/content/spark-sql/README.html" target="_blank" rel="noopener">Spark 编程指南简体中文版-Spark SQL</a></li>
<li><a href="http://www.infoobjects.com/spark-cookbook/" target="_blank" rel="noopener">spark-cookbook</a></li>
</ul>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="JavaChen 微信支付">
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="JavaChen 支付宝">
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    JavaChen
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://blog.javachen.com/2015/04/03/spark-sql-datasource/" title="Spark SQL中的数据源">http://blog.javachen.com/2015/04/03/spark-sql-datasource/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/spark/" rel="tag"># spark</a>
          
            <a href="/tags/avro/" rel="tag"># avro</a>
          
            <a href="/tags/parquet/" rel="tag"># parquet</a>
          
            <a href="/tags/spark-sql/" rel="tag"># spark-sql</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2015/03/30/spark-test-in-local-mode/" rel="next" title="Spark本地模式运行">
                <i class="fa fa-chevron-left"></i> Spark本地模式运行
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2015/04/17/spark-mllib-collaborative-filtering/" rel="prev" title="Spark MLlib中的协同过滤">
                Spark MLlib中的协同过滤 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        
  <div class="bdsharebuttonbox">
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
    <a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a>
    <a href="#" class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
    <a href="#" class="bds_tieba" data-cmd="tieba" title="分享到百度贴吧"></a>
    <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
    <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a class="bds_count" data-cmd="count"></a>
  </div>
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "2",
        "bdMiniList": false,
        "bdPic": ""
      },
      "share": {
        "bdSize": "16",
        "bdStyle": "0"
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2009 &mdash; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-post-description"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JavaChen</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="//cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js"></script>
  

  
  
    <script type="text/javascript" src="//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js"></script>
  

  
  
    <script type="text/javascript" src="//cdn.jsdelivr.net/jquery.lazyload/1.9.3/jquery.lazyload.min.js"></script>
  

  
  
    <script type="text/javascript" src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.min.js"></script>
  

  
  
    <script type="text/javascript" src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.ui.min.js"></script>
  

  
  
    <script type="text/javascript" src="//cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.pack.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
